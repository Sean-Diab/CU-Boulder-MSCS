[
{
    "submission_id": "1gummw1",
    "title": "Reviews on 6 months roadmap for machine learning along with resources by chatgpt? ",
    "selftext": "Please just give your reviews has this encompassed essential things or not?\n\nHere’s a 6-month roadmap for MLOps + Data Analytics with short courses (<10-15 hours each) and a strong focus on hands-on projects. This plan balances learning and application, so you spend more time on projects while building the essential skills.\n\n\n---\n\n6-Month MLOps + Data Analytics Roadmap with Projects\n\n\n---\n\nMonth 1: Foundations in Python, Math, and Data Analytics\n\nTopics:\n\n1. Python Basics:\n\nLearn Python fundamentals and libraries (NumPy, Pandas).\n\n\n\n2. Basic Math:\n\nDescriptive statistics (mean, median, standard deviation).\n\nLinear algebra (vectors, matrices, basic operations).\n\n\n\n3. EDA:\n\nData cleaning, handling missing values, visualizing data.\n\n\n\n\nShort Courses:\n\nKaggle - Python (~6 hours, Free)\n\nKhan Academy - Statistics and Probability (~10 hours, Free)\n\nKaggle - Data Visualization (~5 hours, Free)\n\n\nProjects:\n\nClean and analyze a dataset (e.g., Titanic dataset) with Pandas.\n\nPerform EDA and create visualizations (correlations, outliers).\n\n\n\n---\n\nMonth 2: SQL, Power BI, and Advanced Analytics\n\nTopics:\n\n1. SQL for Data Analysis:\n\nSELECT, WHERE, GROUP BY, JOIN, and aggregate functions.\n\n\n\n2. Power BI for Dashboards:\n\nCreate interactive dashboards with Power BI.\n\n\n\n3. Data Preprocessing:\n\nHandle outliers, scale data, encode categorical variables.\n\n\n\n\nShort Courses:\n\nKaggle - SQL (~4 hours, Free)\n\nMicrosoft Power BI Tutorials (~8 hours, Free)\n\nDataCamp - Data Cleaning in Python (~4 hours, Free trial)\n\n\nProjects:\n\nQuery data from a SQL database (e.g., MovieLens) and visualize it in Power BI.\n\nBuild a sales dashboard using Power BI and Power Query for data transformation.\n\n\n\n---\n\nMonth 3: Machine Learning Basics and Feature Engineering\n\nTopics:\n\n1. Supervised Learning:\n\nLinear Regression and Logistic Regression.\n\n\n\n2. Feature Engineering:\n\nFeature scaling, encoding, and selection.\n\n\n\n3. Model Evaluation:\n\nRMSE, accuracy, precision, recall.\n\n\n\n\nShort Courses:\n\nGoogle’s Machine Learning Crash Course (~15 hours, Free)\n\nKaggle - Intro to Machine Learning (~6 hours, Free)\n\nCoursera - Feature Engineering (~12 hours, Free to audit)\n\n\nProjects:\n\nBuild a Linear Regression model to predict housing prices using Scikit-learn.\n\nTrain a Logistic Regression model for binary classification (e.g., predict customer churn).\n\n\n\n---\n\nMonth 4: Advanced ML, MLOps Basics, and Deployment\n\nTopics:\n\n1. Advanced ML Algorithms:\n\nDecision Trees, Random Forests, Gradient Boosting.\n\n\n\n2. Introduction to MLOps:\n\nCI/CD for ML pipelines.\n\nModel versioning with DVC (Data Version Control).\n\n\n\n3. Deployment Basics:\n\nDeploy a model using Flask or Streamlit.\n\n\n\n\nShort Courses:\n\nKaggle - Intermediate Machine Learning (~6 hours, Free)\n\nCoursera - MLOps Fundamentals (~10 hours, Free to audit)\n\nStreamlit Official Docs (~3-5 hours, Free)\n\n\nProjects:\n\nTrain a Random Forest model to predict customer churn and version it with DVC.\n\nDeploy the model as a Streamlit app with interactive input fields.\n\n\n\n---\n\nMonth 5: Deep Learning, Big Data, and Cloud Integration\n\nTopics:\n\n1. Deep Learning Basics:\n\nNeural networks and CNNs.\n\n\n\n2. Big Data:\n\nUse PySpark to process large datasets.\n\n\n\n3. Cloud for MLOps:\n\nStore datasets in AWS S3.\n\nUse SageMaker for model training and deployment.\n\n\n\n\n**\n\n",
    "created_utc": "2024-11-18T18:25:01",
    "num_comments": 0,
    "comments": []
},
{
    "submission_id": "1guj8rq",
    "title": "What’s the most underrated resource for learning machine learning that you’ve come across?",
    "selftext": "There’s so much content out there. What’s one book, course, or video series that doesn’t get enough attention but was a game-changer for you?",
    "created_utc": "2024-11-18T15:43:21",
    "num_comments": 9,
    "comments": [
        "I never really had formal training in ML other than undergrad level stats and econometrics classes. To prepare for data science internships my third year I read introduction to statistical learning and it was amazing. It's the baby version of elements of statistical learning and I found it perfect for younger me given my background at the time.",
        "Intro to statistics on udacity by Sebastian Thrun.",
        "I’m sorry but this is the first year I used ChatGPT and I have to answer ChatGPT 😂",
        "The Mechanics of Machine Learning book by Jeremy Allen and Terence Parr. It is free to read online, uses kaggle competition datasets and show the complete workflow from base model to final model.",
        "* Machine Learning Specialization - Andrew ng course\n* Machine Learning for all Supervised Machine Learning regression and classification\n* IBM Machine Learning with Python\n* IBM Machine Learning introduction for everyone\n* Machine Learning A-Z - Udemy\n* Complete Machine Learning Bootcamp - Udemy are some of the [best machine learning courses for beginners](https://codingvidya.com/best-machine-learning-courses/)",
        "By who?",
        "https://www.statlearning.com/"
    ]
},
{
    "submission_id": "1guj8jw",
    "title": "What’s one machine learning concept you struggled with at first, and how did you finally understand it?",
    "selftext": "We all hit roadblocks. Sharing your \"aha\" moments might help others overcome similar challenges.",
    "created_utc": "2024-11-18T15:43:06",
    "num_comments": 0,
    "comments": []
},
{
    "submission_id": "1guj8ey",
    "title": "What’s the best beginner project for someone learning machine learning, and why?",
    "selftext": "Starting out can be overwhelming. What’s a project that helped you understand the basics and build confidence?",
    "created_utc": "2024-11-18T15:42:56",
    "num_comments": 1,
    "comments": [
        "Titanic is fun\n\nThe data size is small and the insight makes sense"
    ]
},
{
    "submission_id": "1guj84s",
    "title": "Stanford free online courses vs MIT courses",
    "selftext": "I have commercial experience in ML but I want to systematise my knowledge. I was planning to take Stanford's CS231N and CS229 online, but then I found that MIT has [Introduction to ML](https://openlearninglibrary.mit.edu/courses/course-v1:MITx+6.036+1T2019/about) and [ML with Python](https://www.edx.org/learn/machine-learning/massachusetts-institute-of-technology-machine-learning-with-python-from-linear-models-to-deep-learning). Has anyone tried these? I seems like Stanford courses are more famous, are they better?",
    "created_utc": "2024-11-18T15:42:35",
    "num_comments": 0,
    "comments": []
},
{
    "submission_id": "1guhoma",
    "title": "Balancing Rigor and Fun: Advice for Self-Learning DS/ML While (Re)Learning Math",
    "selftext": "Hi everyone,\n\nI want to pick up data science and machine learning as a hobby. I know some python and have already started playing around with toy projects using basic algorithms like linear and logistic regression. I'm also reading Geron's Hands-on machine learning and ISLP. \n\nHowever, a big part of what drives me is really understanding how things work, so I decided to switch gears a bit and started a parallel journey of learning as much math as possible. Picking up where I left off in high-school some 20-ish years ago, I'm currently taking single variable calculus on MIT OCW, and plan to follow with Linear Algebra, multivariable calc, and then Probability Theory followed by as many online stats classes and books I can find.\n\nI have perfectionist tendencies and started this whole endeavor thinking that I should finish the math prereqs in the order I mentioned (or a similar one) before moving on to DS. I am enjoying the math and think it could also become a hobby of its own, but I don't want to risk burning myself out and abandoning this whole project before I get to the DS part.\n\nSo, if you were to restart learning DS from scratch as a self learner, how would you go about ensuring you learn the math fundamentals while keeping things fresh with some fun applications? Do you have any favorite projects or techniques to keep things engaging while juggling math?\n\nIf you've walked this path before, I'd love to hear your insights or specific resources that worked for you! How did you balance breadth/depth and how did you keep yourself motivated when juggling math and DS? Any tips for making the journey fun?\n\nThanks!\n",
    "created_utc": "2024-11-18T14:34:43",
    "num_comments": 0,
    "comments": []
},
{
    "submission_id": "1guh93d",
    "title": "How good is the AWS Machine Learning - Specialty Certification subjects are for US and europe interview process??",
    "selftext": "Hey folks, how are you? I'm thinking about doing this specialty to help me prepare for senior data scientist and machine learning engineer interviews. Since I've made the compromise with me of doing the Cloud Practitioner, Solutions Architect and this one next, I was thinking about if the ML specialty could help me to get prepared for the technical interview process, what you guys think? Maybe it's a good idea to make it after passing on some new job?\n\nI'm already on the industry with 7+ years, working on a big company with top technologies.",
    "created_utc": "2024-11-18T14:16:29",
    "num_comments": 6,
    "comments": [
        "I’d have to look but I would be very surprised if that goes over ML theory and isn’t just a glorified AWS services ML thing.",
        "You have 7+ years of experience and still need Certification to prove your skills and worth? Now you can already build some fun projects using what you've known over the years.",
        "The AWS ML Specialty exam covers both ML theory and concepts as well as specific AWS services related to ML. There’s also a new AWS ML Engineering Associate exam. The exam domains are similar, but the specialty exam is more focused on model training and the MLE associate exam is more focused on deployment and observability.",
        "One deal that I forgot is that I'm currently working on Brazil, I'm from here. I'm trying to prepare myself to get a spot on some nice tech company in U.S",
        "How deep is the theory?",
        "Look at Domain 3 here:\n\nhttps://d1.awsstatic.com/training-and-certification/docs-ml/AWS-Certified-Machine-Learning-Specialty_Exam-Guide.pdf\n\nYou’ll see very little in domain 3 that’s specific to AWS."
    ]
},
{
    "submission_id": "1gugpbh",
    "title": "ML model gives incorrect answers when using its own training data",
    "selftext": "I'm trying to create ML model that looks for network attacks, such as MITM, DDoS, SQL_injection, etc. On paper, it has around 85% accuracy, however when testing on its own training data, it gives incorrect answers. \nI'm aware that I shouldn't test it that way, however I was trying to see if it recognizes its own training data and it doesn't. \nDid anyone encounter something like that? Any kind of advice would be appreciated.\nI'm using dataset \"Edge-IIoTset Cyber Security Dataset of IoT & IIoT\"",
    "created_utc": "2024-11-18T13:53:46",
    "num_comments": 1,
    "comments": [
        ">when testing on its own training data, it gives incorrect answers\n\nWhat percentage of the training data does it produce the incorrect prediction for?\n\nDo you expect it to be 0%?\n\nDo you monitor both test and train accuracy during training?"
    ]
},
{
    "submission_id": "1guej8p",
    "title": "AM5 Motherboard & RAM for 2 x 3090 ML build?",
    "selftext": "I expect in the very beginning I will only require more than 24GB of VRAM for LLMs, though I want to explore all aspects of ML and AI, and I that system RAM being twice VRAM is good target.\n\nDevelopment and working with AI is the main priority of this build, though I would like to game sometimes, so I am awaiting the release of the 9950X3D hoping for the best of both. This will also be my first VFIO system, and the Proxmox host and ideally another very light VM that is always running will require a small amount of RAM, I think 2 - 4 GBs total.\n\nI have read there are many issues getting 128 GB (4 x 32) stable on AM5. Should I instead opt for 96GB (2 x 48) which seems to be more stable at higher frequencies, or for my use case will I experience some kind of bottleneck and end up wishing I had tried to make 128GB work?\n\nEEC support is also limited on AM5 and it seems to come at a cost of speed, is this an issue for ML?\n\nHow important is the frequency of the DDR5?\n\nThe Asus ProArt Creator B650 seems to be the most widely recommended board for VFIO due to the IOMMU grouping but I am open to the x670 or even the x870 versions or any other suggestions.",
    "created_utc": "2024-11-18T12:23:55",
    "num_comments": 0,
    "comments": []
},
{
    "submission_id": "1gudwsm",
    "title": "Building my first serious project",
    "selftext": "Hi everyone, hope you guys are having a good day. \nI'm a last year undergrad student and I'm trying to get started on my final thesis. Ive decided to make an ML project since it's something I've been interested in for a little while now. But the thing is I've only researched the theory and am lacking a lot in technical knowledge. Which platforms and tools to use, how to properly build it etc etc.\nIf any of you have even a few minutes and would be down to give a but of a guidance i would be so thankful.\n",
    "created_utc": "2024-11-18T11:58:08",
    "num_comments": 5,
    "comments": [
        "What problems are you trying to solve with ML?",
        "Im trying to make a sign language interpreter for my mother language. Ive seen a lot for ASL, but never came across one for Armenian.",
        "That's a bold and useful project. The first step would be data collection. You would need access to video recordings of hand gestures in Armenian. Do you have access to such data?",
        "I hope i manage to do it properly 🤞. And yes, i managed to find almsot 1200 videos of different hand gestures. I think they've been made for educational purposes, since they are professionally made, with good lighting and stuff.",
        "You are off to a great start. I would look into other sign language models and read papers on how they build models to interpret sign language. Start with an easy solution then work your way up. You need to good hardware to train the model but you could start with off the shelf models already trained on image recognition and build on top of that.\n\nBefore you start building, look into the videos and make sure they are labeled correctly. Remember, garbage in garbage out."
    ]
},
{
    "submission_id": "1gucvj6",
    "title": "OCR for documents",
    "selftext": "I've been experimenting with pytesseract and keras-ocr on pretty standard, easy to read PDFs of scanned legal documents. To be honest the results have been surprisingly bad, especially compared to paid, desktop tools like Tungsten (formerly Kofax).\n\nDo you have any recommendations for better OCR libraries/tools for text-rich documents? Has anyone had good results fine tuning keras-ocr?",
    "created_utc": "2024-11-18T11:15:29",
    "num_comments": 3,
    "comments": [
        "Some simple things to look at:\n\n\\-Make sure you are on at least V4 of Tesseract.  If <V4 the text recognition is pretty poor.   V4 introduced ML to text recognition and accuracy was improved dramatically.\n\nThere are several reasons the commercial products are better:\n\n\\-They've put a great deal of effort into processing the images to clean them up prior to running the image through the OCR engine. Can make a huge difference in certain images.\n\n\\-They've added dictionaries and grammar engines to their OCR so if OCR detects \"eng1ne\" it knows that it's \"engine\" due to the spelling/context.\n\n\\-And more...",
        "What about reading tables? Any ocr libs that work well?",
        "That’s interesting, I’ve been using OCRMyPDF which is based on Tesseract. I’ll check the version of the Tesseract. Thanks for the tip."
    ]
},
{
    "submission_id": "1gucqa5",
    "title": "Skills Required for ML Engineer ",
    "selftext": "Hi everyone. I am a software engineer in top 3 bank. I’ve been working for about 2 years now. I am planning to transition into ML Engineer in 4-5 years from now on. I need some advice on how I can start working on it. \n\nThis is my brief profile FYI\n- Do not need visa sponsorship since I am a US citizen\n- Planning to apply for OMSCS 2025 Fall semester (Expected to graduate by 2028 Fall or 2029 Summer) \n- Will be working as a Senior Software Engineer for about 2-3 years by the time \n\nHere are my questions. \n1. I saw many posts that ML Engineer requires more software engineering skills than actual model generation, training and evaluating. \nI already have a CS background and will be working as a software engineer. \nWhat are the core skills that separate ML engineer from SWE? Also, what are SWE skills that are important to MLE? (I do understand that it depends on company so you can just explain based on your experience) \n\n2. My ultimate goal is to become a ML Engineer and developing products that leverage AI / ML. However, it seems like hard to land in ML Engineer position directly without industry experience in ML. Will I get better chance if I transition into a data scientist position and get some experience before applying to ML Engineer rather than pursuing software engineer and working on some personal projects? \n\n3. I am currently preparing for the master degree and job transition in my personal time. \nI study Python through CS50P (I know this is very basic. But trying to really understand syntax and how to use Python), data science concepts through ISLP (Intro to Statistical Learning w Python), Hands on Machine Learning text book, CS50R (I never used R before), DS&A through Grokking the Algorithm and YouTube videos, Leet Code, and SQL interview questions. \nI feel like I am learning about data science and ML through those resources. Could you recommend me other resources that help you become a ML Engineer? Also, I wonder if I should start working on any project. I think I can create ML model and deploy it using docker and kubernetis or cloud. But I am not sure how helpful it can be to make my resume better. \n\n4. What are the expectations for ML Engineer position in terms of YOE and experience itself?\nIt would be great if you could provide specific examples. I am not only looking into FAANG level companies but including other tech companies, health care, insurance, banking, or even retail. \n\nThanks for reading my long post and any advice or help would be greatly appreciated. \n",
    "created_utc": "2024-11-18T11:09:35",
    "num_comments": 6,
    "comments": [
        "1. MLOps. Essentially, it‘s DevOps concepts applied to code/pipelines with stochastic output.\n2. Data Science is very heavy on the math/stats. If you want to build products that leverage ML/AI then look into positions that require orchestration of pre-trained models. That way, you can leverage your SWE background and the MLOps is simplified drastically. This includes RAG with LLMs.\n3. good resources, but projects > certs > Self study. You should build something end-to-end that showcases ML expertise. Algorithmic trading bots are a good exercise because almost everything relevant in industry will be a time series of sorts and algorithmic trading required automatic backtesting, model selection etc. Also, showcase your project on YouTube or similar.\n4. don‘t know about your market, but in Germany, ML Engineering is a Senior role you Transition to from Software/data Engineering or Data science. >2 YOE is common.",
        "Thanks for your answer. It helps me set up a right direction to my goal. I just want to clarify one thing that do I still need learn theoretical knowledge about ML algorithms or is this unnecessary?",
        "100% needed, espeically for inteviews, I've been asked about common models like Transformers, RNNs etc and even some more statisticaly models like HMMs. If you understand everything in Kevin Murphy's Probabalistic Machine Learning book then you are more than 100% sorted",
        "Okay I understand that it’s more about deploying and integrating model into the product than modeling. However, I need to learn and understand models deeply to pass the interview. Is that right understanding of this position?",
        "Yes, that‘s correct. You can send me a PM if you want to discuss this further.",
        "Correct, there are some times where you will have to look up and read papers but that can be learn as you go, you just need enough knowledge to pass interviews. Most of the roles is integration as you said."
    ]
},
{
    "submission_id": "1gubt0p",
    "title": "need AI narrator voice over to wideo for my gf to enjoy watching things with me to the fullest :)!",
    "selftext": "need to add a narrator voice over to the wideo(anime to be precise) i thought as a dumbass that there should be some ai technology that could help me ,any ideas folks? my girlfriend is halfblind and its hard for her to read the subtitles so i want to add narrator in our native language to make it easy for her",
    "created_utc": "2024-11-18T10:32:47",
    "num_comments": 1,
    "comments": [
        "You tryna put out some money for this?"
    ]
},
{
    "submission_id": "1guah1p",
    "title": "TROUBLE! Transcribing non-english podcasts",
    "selftext": "Hi everyone,\n\nI’m working on a hobby project to transcribe a non-English podcast and create a text corpus for analysis. With ChatGPT’s help, I’ve scripted the download of 241 episodesn(about 1 hour each), but now I need to transcribe them\n\n**Challenges**\n\nWhisper Accuracy:  \nWhisper’s “turbo” model struggles with accuracy. The podcast features actors in character, invented words, slurred speech, and sometimes poor audio quality.\n\nCharacter Identification:  \nI’d like to use machine learning to identify different characters based on their speech.\n\n  \n**Resources:**\n\nI have access to a supercomputer, so processing power isn’t an issue.  \n  \nI’m a beginner in Python and machine learning, so simple solutions are appreciated.\n\n  \n**Questions:**\n\nAny tips for improving transcription quality in challenging audio?  \n  \nHow should I approach character identification?\n\n\n\nI've thought of transcribing the first 5-10 episodes manually to have a \"golden standard\" but ) I have no clue how to actually use that.\n\nCan anyone help a newb?",
    "created_utc": "2024-11-18T09:38:49",
    "num_comments": 0,
    "comments": []
},
{
    "submission_id": "1gu9lfe",
    "title": "Do I need to study software engineering too to get a job as ml engineer?",
    "selftext": "I've been seeing a lot of comments where some people say that a ML engineer should also know software engineering. Do I also need to practice leetcode for ml interviews or just ml case study questions ? Since I am doing btech CSE I will be studying se but I have less interest in that compared to ml. ",
    "created_utc": "2024-11-18T09:03:53",
    "num_comments": 29,
    "comments": [
        "MLE is a software engineer focused on ML and ML ops. So yeah, practically for the job, and for the interviews. ",
        "An ML engineer is a specialised type other than software engineer. So obviously the answer is yes.",
        "Yes. I read an article that said companies would rather have someone who knew how to be a software dev first and ML engineer second. While I’m not quite sure how true that is, it has made sense to me at least.",
        "Technically not, but you will be competing with undergrad CS folks, so you can figure is a must to have above average coding skills and systems engineering knowledge in general.",
        "You will need the skills.",
        "Some people will say you need to know everything, frontend, backend, DevOps, data, ml, PhD, domain knowledge, throw laser rays from your eyes etc others won't know nothing and still will be in the sector for years (maybe are bootlic***s)",
        "Any ML job I’ve had in industry needed some SWE knowledge. Reality is pure POC/research roles are more rare",
        "MLE without SWE sounds more like you only want to work till the PoC stage. MLE is expected to handle loads as well",
        "There aren't strict definitions of Data Scientist or Machine Learning Engineer when it comes to specific tasks.\n\nSome Data Scientists don't build machine learning models. Some do. Some Data Scientists build and productionise their models.\n\nA Machine Learning Engineer might build and productionise models. Some might deal exclusively with the productionising and MLOps side of things.\n\nThere is obviously significant overlap between the tasks. Some Data Scientists and some MLEs do.\n\nIf you're thinking of an MLE as someone who builds and deploys models, it's not necessary to have studied SWE. You'll absolutely need some SWE skills, but an MLE is not necessarily strictly a SWE who deals with ML models.",
        "‘ML Engineer’ is just a specialization within the field of software engineering.  In all cases, how useful you are will depend largely on how good those basic software engineering skills are.",
        "If you're a theoretical computer science genius, who can come up with new ways to do machine learning, you can have someone else code them for you. \n\nIf you're not a theoretical computer science genius, knowing how to do software engineering is at least as important as domain specific ML knowledge.",
        "As others have said it's generally a specialisation of software engineering, though honestly the roles vary. You may want to look at data science roles over mle, though if you want to do any fancy machine learning, they're super competitive. Leetcode is very specific skill that you'll rarely use even in a role as a software engineer, but most companies still use it in graduate interviews. This is UK based advice.",
        "Yes. The more the better.",
        "I’m an ML engineer, this just popped up for me, and I really don’t understand your question. ML engineering is software engineering just with the added pizzaz of models to work with and around. You can’t have one without the other. If you just want to work in notebooks all day, be a data scientist.",
        "[deleted]",
        "Onlyfans. That's the only thing you should be doing.",
        "No, you need to study ML specific stuff. Most software engineers probably can't do ML because they learned Javascript from a 12 week bootcamp and spent their career building React popup boxes. You will need to learn to code to do most ML work, but won't need to focus on becoming an SWE.",
        "I think the article is correct. I have been working as an ML engineer for about two months and studying AI as a postgraduate student for almost two years. I’ve also been an SWE for more than three years, so if an ML engineer doesn’t know software engineering, it must be hard. At the end of the day, we have to code and consider CPU, GPU, RAM and VRAM, and be able to manipulate a dataset for our needs using different algorithms.",
        "SWE skills are still required for both roles. Whether you train or deploy models, there is still a lot of software engineering principles you need to think of. \n\nI would argue the only ML job that doesn't require hard SWE skills is ML research, however these positions are way more competitive and require PhDs and publications in top journals so I don't think OP is interested in these roles. However, Even ML research usually requires some SWE skills like in systems (parallel computing, caching, etc) or a deep understanding of the underlying software behind ML libraries / frameworks.",
        "This account heavily promotes that website (almost every comment contains the link), without ever disclosing their obvious affiliation. Downvoted for such shitty practice",
        "AI gonna take over OF models",
        "What a terrible take",
        "This is terrible advice. No one is hiring an ML engineer for their ML knowledge alone. They only hire ML engineers because they are SWE that are specialized in ML.",
        "I apologize for the non-disclosure, i am just subscribed with the site and love it for my prep. I deleted the post for breaking the rule.",
        "I don't want to see OP waste his time learning .Net and React if they want to get into ML. Focus on ML as a science and Python.",
        "Yes \"specialized in ML\". Exactly what I said. You need to study ML and learn to code. No one is hiring an SWE without knowledge of ML models, ML Ops etc. You're picking a dumb argument for the sake of dunking on someone on reddit. I'm not interested.",
        "Software engineering is a very broad term. Obviously an ML engineer shouldn't focus on things like React, but many technologies like SQL/Apache Spark for data engineering, Java/Scala for data pipelines, C++ for performance optimizations, Flask/Django for building APIs to deploy these Models are used. \n\nThese are just some examples, there are plenty of other tools ML engineers use that are commonly considered a software engineering skill. \n\nHow familiar are you in ML? If you're a beginner it would make sense you think ML engineering is mostly only Pytorch, tensor flow Jax, etc.",
        "The question was: Do I need to study software engineering too to get a job as ml engineer?\n\nYour answer was: No.\n\nI simply corrected your bad career advice. It's not dunking, just helping out OP who can be impacted by bad advice.",
        "No should be the correct answer. Most SWEs couldn't work in ML to save their lives. React, Redux, CSS, Angular etc those skills don't transfer and would be a huge waste of time for anyone who wants to get into ML. Absolutely no one is hiring SWEs into ML roles without having studied ML extensively. You need to know how to code, and know how ML works. If you waste your time learning software engineering as a trade, it is not going to help you land an ML job. All of the unemployed and disgruntled SWEs out there that can't break into ML for the life of them are proof of that.",
        "Absolutely no one is hiring MLEs without strong SWE skills. All of the theory in the world means nothing without good software dev skills. The one and only exception is a stellar data scientist with a PhD - they can get away with weaker SWE skills since they would likely be paired with MLEs.\n\nI think we might be saying the same thing, in the end. My point is simply that you need to be able to code more than python scripts - you need to be able to efficiently interface with the cloud and big data, and very likely with mobile/web, etc. and do so as the lead."
    ]
},
{
    "submission_id": "1gu8nka",
    "title": "Where to start and online freelance opportunities?",
    "selftext": "Where does one start learning about AI and ML, and which course would you recommend for getting freelancing opportunities for now?",
    "created_utc": "2024-11-18T08:25:41",
    "num_comments": 0,
    "comments": []
},
{
    "submission_id": "1gu8g5g",
    "title": "Must have datascience  ,Ml engineering skills ",
    "selftext": "Been scrolling through job posts and what I see based on the required skills I’m amazed 😯I think I’m left behind 🥲\nAny advice ?",
    "created_utc": "2024-11-18T08:17:17",
    "num_comments": 0,
    "comments": []
},
{
    "submission_id": "1gu83b0",
    "title": "Does anyone know any companies within the music software sector that are looking for people with ML experience?",
    "selftext": "As the title says, asking because I have a thesis level project I’m working on which could potentially serve to completely change something within music software & I was wondering what companies should I look out for that would be interested?",
    "created_utc": "2024-11-18T08:02:31",
    "num_comments": 1,
    "comments": [
        "Maybe Spotify?"
    ]
},
{
    "submission_id": "1gu6w1y",
    "title": "Quantifying data diversity",
    "selftext": "Is there a good way of quantifying how diverse a dataset is in terms of variability? E.g. if I have an image dataset of 10k images all coming from one 30 FPS video, the diversity/information is very low because the frames are strongly related/dependent on each other (compared to let's say 10k images from 10 shorter videos from different days/locations). Is there a way of quantifying this, maybe using Shannon Entropy for example?",
    "created_utc": "2024-11-18T07:10:56",
    "num_comments": 0,
    "comments": []
},
{
    "submission_id": "1gu6a6g",
    "title": "Can you plz help ? ",
    "selftext": "I want a data set that has sarcastic sentence and their non sarcastic sentence without changing the meaning .\nActually I am working on a project aiming on converting sarcastic sentence to non sarcastic one without altering their true meaning . \nBut I am unable to find any date set for that having both sarcastic and their rephrases sentence .",
    "created_utc": "2024-11-18T06:43:36",
    "num_comments": 1,
    "comments": [
        "Have you tried using Glaive AI? They generate synthetic data for free. \nPS: I’m not associated with them in any way, just thought of mentioning since it might help you"
    ]
},
{
    "submission_id": "1gu4d3l",
    "title": "Seeking Advice for Isolating and Recognizing Objects in Complex 3D Scans",
    "selftext": "Hello,\n\nI’m working on a project involving 3D scans of a structured surface composed of multiple flat portions with varying inclinations. The surface has several attached objects of different shapes and sizes. These objects can overlap or be positioned very close to one another, thus it is difficult to distinguish and isolate individual items.\n\nThe goal is to Isolate each object from the scan and match them to a reference model. I have a data base of 3D model. The quality of the reference are better than the quality of the scan.\n\nHas anyone worked on a similar problem? What methods would you recommend for segmentation and matching in complex 3D scans?\n\nI did not find a lot of things on the topic (as ML in 3D is not as advanced as it is with pictures). I found PointNet++ but I'm not sure if it is the most relevant thing here.\n\nAny advice would be greatly appreciated!",
    "created_utc": "2024-11-18T05:11:39",
    "num_comments": 0,
    "comments": []
},
{
    "submission_id": "1gu3xq5",
    "title": "Blogs on ML and data science ",
    "selftext": "I have recently started blogging on AI and trying to make cool projects \nHere are the links for many do check them and let's connect and discuss.",
    "created_utc": "2024-11-18T04:48:59",
    "num_comments": 0,
    "comments": []
},
{
    "submission_id": "1gu3i4t",
    "title": "𝗦𝘁𝗿𝗲𝗻𝗴𝘁𝗵𝗲𝗻 𝗬𝗼𝘂𝗿 𝗠𝗮𝗰𝗵𝗶𝗻𝗲 𝗟𝗲𝗮𝗿𝗻𝗶𝗻𝗴 𝗙𝗼𝘂𝗻𝗱𝗮𝘁𝗶𝗼𝗻 𝘄𝗶𝘁𝗵 𝗟𝗶𝗻𝗲𝗮𝗿 𝗔𝗹𝗴𝗲𝗯𝗿𝗮",
    "selftext": "Building something exceptional starts with a strong foundation, and clearing the basics is the key to mastering any complex topic. For those diving into Machine Learning, a solid grasp of Linear Algebra is unavoidable.\n\nLinear Algebra forms the backbone of many core ML concepts:\n\n• Understanding 𝗹𝗶𝗻𝗲𝘀, 𝗽𝗹𝗮𝗻𝗲𝘀, and 𝗵𝘆𝗽𝗲𝗿𝗽𝗹𝗮𝗻𝗲𝘀 is essential for grasping linear regression.\n\n• Concepts like 𝘃𝗲𝗰𝘁𝗼𝗿𝘀 and 𝘃𝗲𝗰𝘁𝗼𝗿 𝘁𝗿𝗮𝗻𝘀𝗳𝗼𝗿𝗺𝗮𝘁𝗶𝗼𝗻𝘀 play a vital role in data manipulation and model building.\n\n• For tackling classification problems, knowledge of 𝗵𝗮𝗹𝗳-𝘀𝗽𝗮𝗰𝗲𝘀 and the 𝗱𝗶𝘀𝘁𝗮𝗻𝗰𝗲 𝗼𝗳 𝗮 𝗽𝗼𝗶𝗻𝘁 𝗳𝗿𝗼𝗺 𝗮 𝗹𝗶𝗻𝗲 is very important.\n\n[Overview of Key Concepts of Linear algebra for Machine Learning](https://preview.redd.it/h4qa2dhnkn1e1.png?width=1222&format=png&auto=webp&s=6ac1dd07da949ce3af6116c765d36e540c6719a9)\n\nTo help learners master these concepts, I’ve created an overview of key ideas and detailed lecture content tailored for engineering educators and ML enthusiasts. These resources ensure concepts are not just learned but deeply understood.\n\nLecture Resources (in Hindi):\n\n𝗟𝗲𝗰𝘁𝘂𝗿𝗲 𝟮: 𝗟𝗶𝗻𝗲𝗮𝗿 𝗔𝗹𝗴𝗲𝗯𝗿𝗮 𝗳𝗼𝗿 𝗠𝗮𝗰𝗵𝗶𝗻𝗲 𝗟𝗲𝗮𝗿𝗻𝗶𝗻𝗴 𝗣𝗮𝗿𝘁 𝟭: 𝗪𝗵𝘆 𝗜𝘁'𝘀 𝗜𝗺𝗽𝗼𝗿𝘁𝗮𝗻𝘁 𝗶𝗻 𝗥𝗲𝗴𝗿𝗲𝘀𝘀𝗶𝗼𝗻\n\n[https://youtu.be/ys92FprXDgM?si=DEVSAG8nzaZfZV1h](https://youtu.be/ys92FprXDgM?si=DEVSAG8nzaZfZV1h)\n\n𝗟𝗲𝗰𝘁𝘂𝗿𝗲 𝟯: 𝗟𝗶𝗻𝗲𝗮𝗿 𝗔𝗹𝗴𝗲𝗯𝗿𝗮 𝗳𝗼𝗿 𝗠𝗮𝗰𝗵𝗶𝗻𝗲 𝗟𝗲𝗮𝗿𝗻𝗶𝗻𝗴 𝗣𝗮𝗿𝘁 𝟮: 𝗛𝗮𝗹𝗳-𝗦𝗽𝗮𝗰𝗲𝘀 𝗮𝗻𝗱 𝗗𝗼𝘁 𝗣𝗿𝗼𝗱𝘂𝗰𝘁\n\n[https://youtu.be/MOEdG7hEXcA?si=01XgNVBDzDE1CwLJ](https://youtu.be/MOEdG7hEXcA?si=01XgNVBDzDE1CwLJ)\n\nDive into the concepts with clarity and depth, and join me on this journey to make Machine Learning concepts accessible and intuitive for educators and learners alike.\n\nI also invite you to explore my course: 𝘔𝘢𝘤𝘩𝘪𝘯𝘦 𝘓𝘦𝘢𝘳𝘯𝘪𝘯𝘨 𝘧𝘰𝘳 𝘌𝘯𝘨𝘪𝘯𝘦𝘦𝘳𝘪𝘯𝘨 𝘛𝘦𝘢𝘤𝘩𝘦𝘳𝘴 by [Pritam Kudale](https://www.linkedin.com/in/pritam-kudale-90793236/), designed to bridge the gap between foundational knowledge and advanced applications.\n\n𝗘𝘅𝗽𝗹𝗼𝗿𝗲 𝗶𝘁 𝗵𝗲𝗿𝗲: [https://youtube.com/playlist?list=PLPTV0NXA\\_ZSibXLvOTmEGpUO6sjKS5vb-&si=ncmSt-wUM49UGree](https://youtube.com/playlist?list=PLPTV0NXA_ZSibXLvOTmEGpUO6sjKS5vb-&si=ncmSt-wUM49UGree)\n\n\\#MachineLearning #LinearAlgebra #Education #EngineeringTeachers #DataScience #MLFoundations\n\n",
    "created_utc": "2024-11-18T04:24:09",
    "num_comments": 0,
    "comments": []
},
{
    "submission_id": "1gu32u1",
    "title": "First exposure to ML - please help!",
    "selftext": "As part of my schools computer science course, I am coding a game similar to Osu!Mania, but with the additional feature of AI Beatmap generation. I am trying to implement Machine Learning over several beatmaps that I have picked out, and I am aiming to create a model that is capable of reading the MP3s from the beatmaps and syncing up their beats, and generating a unique beatmap from a given MP3 - how can I go about MP3 reading in [ML.Net](http://ML.Net) for C#? \n\n  \nTLDR: Please assist me on how to read from MP3 and connect it to the beatmap files (stored as .osu - but can be viewed as .txt)",
    "created_utc": "2024-11-18T03:58:25",
    "num_comments": 0,
    "comments": []
},
{
    "submission_id": "1gu2wsy",
    "title": "Need help guys ",
    "selftext": "What to do after learning machine learning.\nWhat are some best projects to do, what are the best domain to go in, how can i secure a job ?",
    "created_utc": "2024-11-18T03:47:03",
    "num_comments": 0,
    "comments": []
},
{
    "submission_id": "1gu2bmw",
    "title": "Machine learning in Finance ",
    "selftext": "I have seen some people on Twitter recommending to learn machine learning in Finance so I want your opinions is it really worth doing?\n",
    "created_utc": "2024-11-18T03:07:20",
    "num_comments": 5,
    "comments": [
        "I think so. I think it is one of the areas that receive little attention.",
        "Do they talk about what specific applications of ML in finance?",
        "I would be interested too. What kind of finance problems can be solved with machine learning?",
        "Hedge funds and banks have been employing ML scientists and engineers for many years. So there you have your answer I guess.",
        "Quant traders use it"
    ]
},
{
    "submission_id": "1gu1l5f",
    "title": "Understanding the Stages of Building Large Language Models (LLMs)",
    "selftext": "The development of Large Language Models (LLMs) is a fascinating journey that unfolds in **two key stages**, each playing a vital role in creating LLM applications. These are listed as below:\n\n# 1️⃣ Pretraining: The Foundation of LLMs\n\nIn this stage, a model is trained on a vast corpus of raw text data—regular text without any labeling. This foundational step helps the model learn patterns and structures of a language. The result is a **pretrained LLM**, often called a **foundational model**, which serves as the starting point for more specific tasks.\n\n# 2️⃣ Finetuning: Specializing the Model\n\nAfter pretraining, we refine the LLM using labeled data to tailor it for specific use cases.Two popular approaches to finetuning:\n\n🔹 **Instruction Finetuning**: The model is trained on datasets that pair instructions with answers, enabling it to follow human prompts effectively.\n\n🔹 **Classification Finetuning**: Labeled datasets containing text and corresponding labels are used to train the model for tasks like sentiment analysis or topic categorization.\n\nhttps://i.redd.it/uh42cqbrxm1e1.gif\n\nThese steps enable LLMs to power a wide range of applications, from conversational AI to advanced analytics. As LLMs evolve, their potential to revolutionize industries continues to grow.\n\nTo understand the clear difference between pretraining and finetuning, I encourage you to go through the video Pretraining LLMs vs. Finetuning LLMs by Dr. Raj Dandekar:\n\n[https://youtu.be/-bsa3fCNGg4?si=0JnhcAHS6d5YMKl-](https://youtu.be/-bsa3fCNGg4?si=0JnhcAHS6d5YMKl-) \n\n**What excites you most about the future of LLMs? Share your thoughts below!**\n\n\\#ArtificialIntelligence #LLMs #MachineLearning #AIInnovation #DeepLearning",
    "created_utc": "2024-11-18T02:14:10",
    "num_comments": 0,
    "comments": []
},
{
    "submission_id": "1gu1jfn",
    "title": "Understanding the Stages of Building Large Language Models (LLMs)",
    "selftext": "The development of Large Language Models (LLMs) is a fascinating journey that unfolds in **two key stages**, each playing a vital role in creating LLM applications. These are listed as below:\n\n# 1️⃣ Pretraining: The Foundation of LLMs\n\nIn this stage, a model is trained on a vast corpus of raw text data—regular text without any labeling. This foundational step helps the model learn patterns and structures of a language. The result is a **pretrained LLM**, often called a **foundational model**, which serves as the starting point for more specific tasks.\n\n# 2️⃣ Finetuning: Specializing the Model\n\nAfter pretraining, we refine the LLM using labeled data to tailor it for specific use cases.Two popular approaches to finetuning:\n\n🔹 **Instruction Finetuning**: The model is trained on datasets that pair instructions with answers, enabling it to follow human prompts effectively.\n\n🔹 **Classification Finetuning**: Labeled datasets containing text and corresponding labels are used to train the model for tasks like sentiment analysis or topic categorization.\n\nhttps://i.redd.it/s7for0d4xm1e1.gif\n\nThese steps enable LLMs to power a wide range of applications, from conversational AI to advanced analytics. As LLMs evolve, their potential to revolutionize industries continues to grow.\n\nTo understand the clear difference between pretraining and finetuning, I encourage you to go through the video Pretraining LLMs vs. Finetuning LLMs by [Dr. Raj Dandekar](https://www.linkedin.com/in/raj-abhijit-dandekar-67a33118a/):\n\n[https://youtu.be/-bsa3fCNGg4?si=0JnhcAHS6d5YMKl-](https://youtu.be/-bsa3fCNGg4?si=0JnhcAHS6d5YMKl-) \n\n**What excites you most about the future of LLMs? Share your thoughts below!**\n\n\\#ArtificialIntelligence #LLMs #MachineLearning #AIInnovation #DeepLearning",
    "created_utc": "2024-11-18T02:10:42",
    "num_comments": 0,
    "comments": []
},
{
    "submission_id": "1gu13t5",
    "title": "TSMamba : Mamba based Time Series model",
    "selftext": "TSMamba is a Mamba based (alternate for transformers) Time Series forecasting model generating state of the art results for time series. The model uses bidirectional encoders and supports even zero-shot predictions. Checkout more details here : https://youtu.be/WvMDKCfJ4nM",
    "created_utc": "2024-11-18T01:37:17",
    "num_comments": 0,
    "comments": []
},
{
    "submission_id": "1gu13i1",
    "title": "\"Starting the Journey into Probability and Statistics for Machine Learning!\"",
    "selftext": "https://preview.redd.it/qd1mtnhyqm1e1.png?width=1280&format=png&auto=webp&s=bcb987cb3cc930cb261885e37c7d3f0bb4bff4de\n\nHave you thought about how spam filters, weather predictors, or recommendation engines work? \n\n\n\nAt the heart of these intelligent systems lie probability and statistics—the backbone of machine learning. Today, I am thrilled to introduce a new module in our Foundations for Machine Learning course that dives into these critical concepts. I have published this on Vizuara's YouTube channel: [https://www.youtube.com/watch?v=SwryhCJMIzA&ab\\_channel=Vizuara](https://www.youtube.com/watch?v=SwryhCJMIzA&ab_channel=Vizuara)\n\n\n\nWhy is this important? Because building a machine learning model isn’t just about making decisions—it is about understanding uncertainty and variation in data. Probability equips us to handle uncertainty mathematically, while statistics helps us analyze and summarize vast datasets, distilling meaningful insights from overwhelming numbers.\n\n\n\nIn this introductory lecture, we explore fundamental ideas:\n\n1) Probability as a measure of how likely an event is to occur, helping us model real-world scenarios like classifying spam emails or predicting the weather.\n\n2) Statistics to summarize datasets with measures like mean, median, and mode, and understand their tendency to deviate with tools like variance and standard deviation.\n\n\n\nWe also discuss practical applications:\n\n\\- Why the median might be better than the mean in some cases, like skewed datasets where one outlier could distort the average.\n\n\\- How measures of dispersion like standard deviation provide critical context to the central tendency of data.\n\n\\- The role of visualization tools like bar plots, scatter plots, and histograms in making data more comprehensible.\n\n\n\nThis journey is not just about learning the formulas; it’s about developing intuition. For instance, why do weather forecasts often include a chance of precipitation rather than a definitive “yes” or “no”? Probability helps us express this inherent uncertainty.\n\nIn the upcoming lectures, we will take this foundation further, tackling more advanced and practical applications of probability and statistics in machine learning. This is your gateway to mastering these essential tools, and I am so excited to guide you along the way.\n\nLet us simplify the math and bring clarity to the logic behind machine learning. Stay with me, and let us keep learning, one step at a time. Here is the lecture link: [https://www.youtube.com/watch?v=SwryhCJMIzA&ab\\_channel=Vizuara](https://www.youtube.com/watch?v=SwryhCJMIzA&ab_channel=Vizuara)",
    "created_utc": "2024-11-18T01:36:36",
    "num_comments": 2,
    "comments": [
        "\"But why are we talking like this?\"",
        "What causes mechanism in a vehicle"
    ]
},
{
    "submission_id": "1gtzxre",
    "title": "How do you think AI-powered language models like GPT can impact the future of creative writing and content creation?",
    "selftext": "AI language models, like GPT, are revolutionizing various industries, but how do you think they will affect the world of creative writing and content creation? Will they enhance human creativity or replace it? I'm curious to hear your thoughts on the potential benefits and challenges AI brings to the creative process.",
    "created_utc": "2024-11-18T00:04:47",
    "num_comments": 0,
    "comments": []
},
{
    "submission_id": "1gtzod5",
    "title": "Free code camp's ML course",
    "selftext": "I'm learning ML from scratch.\nIt's a 4 year old course.\nIs it worth completing it?",
    "created_utc": "2024-11-17T23:45:07",
    "num_comments": 2,
    "comments": [
        "Yes, it is worth completing this course even if it is 4 years old you will get to know a lot of things by doing this course",
        "The other argument is that even if you dont do it four years will still pass. But if you do it they will pass and you will have a ML degree."
    ]
},
{
    "submission_id": "1gtxyx5",
    "title": "I made a tool to Solve and Visualize 2D Convolutions (and transposed convolutions)",
    "selftext": "",
    "created_utc": "2024-11-17T21:48:12",
    "num_comments": 0,
    "comments": []
},
{
    "submission_id": "1gtwz7j",
    "title": " Super Weights in LLMs - How Pruning Them Destroys a LLM's Ability to Generate Text ?",
    "selftext": "TLDR - Super weights are crucial to performance of LLMs and can have outsized impact on LLM model's behaviour\n\nThe presence of “Super weights” as a subset of outlier parameters. Pruning as few as a single super weight can ‘destroy an LLM’s ability to generate text – increasing perplexity by 3 orders of magnitude and reducing zero-shot accuracy to guessing’.\n\n\n\n📜 [https://vevesta.substack.com/p/find-and-pruning-super-weights-in-llms](https://vevesta.substack.com/p/find-and-pruning-super-weights-in-llms)\n\n\n\n💕 Subscribe to receive more such articles to your inbox - [vevesta.substack.com](http://vevesta.substack.com/) ",
    "created_utc": "2024-11-17T20:47:29",
    "num_comments": 3,
    "comments": [
        "Wouldn't dropout help prevent this? I once read that dropout forces a model to learn to ensemble weights, rather than relying on any, single \"super weight.\" I find it interesting that the authors didn't mention dropout at all, since it seems like an obvious prevention-mechanism, to me.",
        "Dropout isn’t generally used for LLM pretraining these days, or even if used, is done early in the pretraining process",
        "With chinchilla and llama scaling laws, you use such insane amounts of data for pre-training that you generally don't want to use any kind of regularization in there. If you wanted the best pruned models, you'd have to integrate pruning into the main training process, which is commonly done for smaller models where retraining them isn't that much of an issue, but you don't want to train your LLM twice, so its usually not done there."
    ]
},
{
    "submission_id": "1gtw3d9",
    "title": "Hands on oriented AI ML courses",
    "selftext": "I'm currently working as a Data Engineer. Throughout my career I have worked on Data/Analytics. I would now like to learn AI/ML. I dont have a plan to fully transition to AI/ML, however I would like to have hands-on understanding (high level, if not deep dive to start with).\n\nCould you please suggest any free/paid courses that I should start with? I came across some IIT and US university playlists on youtube, but they seemed too theoretical and in-depth for my level. Thank you.",
    "created_utc": "2024-11-17T19:57:10",
    "num_comments": 9,
    "comments": [
        "Kaggle is the correct answer. \n\nDo the mini courses under the “Learn” section to warm up before joining competitions.",
        "There is a Machine Learning Specialization course taught by Andrew Ng in Coursera .you can start with that and Then jump into a deep learning Course. Currently I'm enrolled in deep learning....and complete Machine Learning.",
        "There's literally a book called \"Hands On Machine Learning...\" recommended all over this sub.",
        "Thank you. Will the \"Learn\" section cover the basics enough to get us started with building models directly?",
        "thanks. Does 'Machine Learning Specialization course' also cover some hads-on , how did you find it? If I have to start now without any ML background, is 'Machine Learning Specialization course' a starter?",
        "I would like to know the same.",
        "Yup , it has lab assignments and yeah it's a starter. As I'm a student I applied for financial aid ...then only we can do lab assignments.",
        "thank you!",
        "Feel free to ask if you need any help , but yeah I'm not very experienced."
    ]
},
{
    "submission_id": "1gtu8ge",
    "title": "Seeking Advice: Machine Learning for Autonomous Driving",
    "selftext": "Hello Everyone\n\nI’m currently working in the automotive industry as an embedded engineer, and I’m passionate about transitioning my career towards autonomous driving. Many companies in this space are looking for experience in machine learning and computer vision, areas where I currently have limited knowledge.  \n\nI’d greatly appreciate your guidance on:  \n\n1. What are some beginner-friendly resources (certifications, YouTube courses) to build foundational knowledge in machine learning and computer vision?  \n2. What would a realistic timeline look like to achieve a level of competence that automotive companies typically expect? \n\nI’m ready to dedicate focused time and effort and would love to hear about any personal experiences that worked for you.  \n\nThank you for your time and support.",
    "created_utc": "2024-11-17T18:16:32",
    "num_comments": 2,
    "comments": [
        "Lot of resources here on computer vision \n\nhttps://www.youtube.com/@kevinwoodrobotics",
        "Go look at companies that are involved in autonomous driving, and specifically at what they are hiring for. They will usually list tech, skills, education requirements, etc. Reverse engineer what you need to do to get to their expectations."
    ]
},
{
    "submission_id": "1gttpls",
    "title": "Advice on navigating the PhD application process as an undergraduate",
    "selftext": "I am currently a junior at an Ivy League college with a decent CS/ML program studying computer science and mathematics. I'm working on research projects with professors, I have a very good GPA, and I am taking graduate ML classes + upper level math classes, but I am quite painfully aware of the fact that I am no where near where I need to be to get into a good PhD program.\n\nI am not on track to publish any papers right now, and any halfway decent professor I reach out to just ghosts me. I can't seem to get on track to publish any paper, let alone one good enough to get me into PhD programs. The current research projects I'm doing don't really require that much ML, and are just not very valuable in general. Furthermore, one of them is in bioinformatics and not as directly related to traditional ML.\n\nAll I want to do is read papers, watch lectures, and work on projects I'm interested in, and I hate that I can't do that because I'm so behind my peers in getting my resume to where it needs to be.\n\nDoes anyone have any tips on how to make this whole process seem more manageable, or any good programs for someone in my position? It feels like everything I try blows up in my face or is just painfully uninteresting and not in the direction I want to go.",
    "created_utc": "2024-11-17T17:49:43",
    "num_comments": 1,
    "comments": [
        "You're doing everything right, and what you're doing is more than enough to get into a good PhD program.\n\nThat depends on how you define \"good PhD program\" though, of course. If by \"good\" you mean \"a phd program at a highly prestigious university where you study deep learning stuff with famous people\" then yes, that's going to be hard. Such is the nature of darwinian competition; all of the competitors are quite strong and most of them lose anyway.\n\nThat's a bad definition of \"good\" though. A better one is, \"a PhD program where you have the opportunity to learn the things that you're interested in\". There are a lot programs like that, and your chances of getting into one are very good.\n\nThe professors leading the research projects that you're working on are your best resource. The professors teaching your classes are your second best resource. You should seek their advice and try to take advantage of their personal connections. Random emails from random students go unanswered; emails from trusted colleagues who are recommending students do not.\n\nYou should also figure out what, *exactly*, you want to learn in grad school. From there the programs you should aim for will be more clear. There's nothing wrong with studying e.g. bioinformatics in grad school. You absolutely can use machine learning there. You may have to be more entrepreneurial the farther you get from hard core CS programs that focus exclusively on machine learning, but that's not necessarily a bad thing. Anyone can show up to a phd program and just do as they're told to get a degree. Real research by contrast requires exploration, creativity, and tolerance for significant uncertainty.\n\nAlso keep in mind that the most important factor in your phd experience (and your career afterwards) is your phd advisor. A good advisor in a less prestigious program is much more valuable than a mediocre advisor in a very prestigious program."
    ]
},
{
    "submission_id": "1gtt3rh",
    "title": "Challenges of Breaking into AI Research Roles with a Master’s Degree",
    "selftext": "I’m currently pursuing an MS in AI and have two publications: one in ACM Transactions and another presented at a WACV workshop. Despite my academic accomplishments, I’m finding it incredibly difficult to land research-based roles in the industry. (Like I know I don’t have a stellar profile but still good enough)\n\nIt seems like most companies exclusively require a PhD for such positions. While I understand that research demands a strong academic foundation, it feels like the field has set an almost unattainable standard for Master’s degree holders like me.\n\nWould love to hear your thoughts or advice on navigating this situation. How can someone with an MS degree break into AI research roles in the industry?",
    "created_utc": "2024-11-17T17:18:53",
    "num_comments": 10,
    "comments": [
        "If you are interested in research, why not do a PhD? Trying to get a research role with a Masters degree is akin to trying to get a SWE role with a high school diploma--not impossible, but very close.",
        "You're competing with a lot of PhDs who are likely also unable to land positions due to the current market. And many with experience as well. As other commenter's have said, if you want to do research, you should really stick around for a PhD, because that's where you learn to do research. I would guess the issue of no PhD likely will continue to hamper you, as you'll continue to be rejected before getting interviews. \n\nThe other option is if you can manage to find an entry level ML role in am organization that also does research, you can always try to transfer internally.\n\nFrankly, now isn't the time to be picky. Unless you're at a top program with noteworthy internships you'd be lucky just to land any job in ML. There's plenty of senior ML people looking for work.",
        "How difficult is it to land a corporate AI role ?",
        "Realistically everyone is gonna apply the traditional way. You definitely can get a job in the field as a masters it is just much harder. I'd say blog about stuff, have a good github, create content on YouTube and job wise look for startups as well. Startups will probably pay bad, work life experience would be bad but you can get experience. Continue doing research if you can find some research groups external from university.",
        "This. I can't speak for roles outside of science but basically in the sciences, all serious research positions require a PhD. I have some colleagues who do research with just a BS/MS but they usually just do very basic research. Anyone who wants to do more in-depth research is recommended to do a PhD. I'd imagine it's the same in CS.",
        "I am applying for Phd for the next intake. I just wanted to get exposure of what it’s like to do research in industry, thats it.",
        "True, beggars can’t be choosers. I think going directly to Phd is the only option for me as of now, will stick to that.",
        "In this economy landing any tech job is pretty difficult. On top of that you have additional requirements in ML for at least masters degree and a huge competitive talent pool (apparently everyone and their mom have a Masters in AI now)",
        "this makes so much sense. Thanks for the advice.",
        "How will the market be in say about 3-5 years"
    ]
},
{
    "submission_id": "1gts5nw",
    "title": "how to learn ml in 6months",
    "selftext": "So I know little python and I want to know how to do ml in 6 months? I just need a roadmap with resources",
    "created_utc": "2024-11-17T16:32:03",
    "num_comments": 14,
    "comments": [
        "Get off reddit and get to work.",
        "Err. You could try the fast.ai intro to machine learning course. It’s relatively easy to follow.",
        "What's your goal at the end of the 6 months?",
        "That depends: how good is your time machine?",
        "Can you please tell roadmap with resources too. It is confusing on internet",
        "To get an internship",
        "[Relevant](https://www.reddit.com/r/ProgrammerHumor/s/pk21EAEJl1)",
        "Not gonna happen then.",
        "What do you mean? I just want a ml internship/job",
        "You said you're just starting to learn python. It will take you much longer than 6 months to accomplish you goal.",
        "I know basic data structures of Python already",
        "Here’s what you do. Go to an accredited school, preferable U.S school, get an undergraduate degree in math, computer science, computer engineering, or electrical engineering and get a 3.7+ gpa. Then do a masters at a great school in ML then apply 500 times then you can get a ML job",
        "What if I could only do a Master's at a not so great school in ML? Do I have to apply 5000 times instead?",
        "No, 5 billion times"
    ]
},
{
    "submission_id": "1gtrxhp",
    "title": "Learning Machine Learning: Linear Algebra Resources?",
    "selftext": "Hi everyone! I’m currently delving into machine learning to develop the backend for a sports prediction website I’m working on. My background includes a computer science degree and a master’s in cybersecurity, but I’m realizing that a solid grasp of linear algebra is crucial for understanding ML concepts.\n\nCould you recommend some practical resources for learning linear algebra, especially those tailored for applications in machine learning?\n\nAlso, here’s my website: https://PhilPicks.ai — I’d appreciate any feedback!",
    "created_utc": "2024-11-17T16:21:02",
    "num_comments": 4,
    "comments": [
        "The mathematics for machine learning book by Deisenroth, Faisal and Ong should teach you most of what you need to get started. The same goes for the mathematical preliminaries chapters of the major ML textbooks like Murphy or Bishop (preferably the recent versions).",
        "Jon Krohn, Youtube",
        "Jhon cron & 3 blue 1 brown utube",
        "Mit ocw 18.06sc, then 18.065"
    ]
},
{
    "submission_id": "1gtrx7s",
    "title": "SWE with 3 Years of Experience Looking to Transition to ML/AI Engineering",
    "selftext": "As the title suggests, I’m a software engineer with 3 years of experience and a BS in Computer Science from a US university. I’m currently working at a US-based company but have decided to leave soon, even without a new job lined up.\n\nI’m exploring a transition into ML/AI engineering and had a few questions:\n\n1. What is the current job market like for entry-level ML/AI engineers?\n2. If I pursue a master’s degree, what are the requirements for securing an entry-level ML/AI position as a recent grad?\n3. Regarding master’s programs: a. Are there any good online AI/ML master’s programs based in the US? b. If I pursue a master’s degree internationally (e.g., in Germany), how difficult would it be to land a job in the US afterward?\n\nI’d appreciate any insights or advice!",
    "created_utc": "2024-11-17T16:20:41",
    "num_comments": 2,
    "comments": [
        "pursue a master’s degree on TOP TIER university",
        "with the current job market i would not suggest leaving your job without a new one lined up. the ML/AI landscape is a blood bath at the moment. even PhD's are having a difficult time getting jobs. \n\nif you can juggle a masters degree and a job, that might be better just so you have a safety net to fall back into should the job market not improve anytime soon. \n\nthis is just my opinion though. ymmv"
    ]
},
{
    "submission_id": "1gtqb11",
    "title": "Structured extraction with LLM on Databricks",
    "selftext": "Using Llama 3.1 with structured output!",
    "created_utc": "2024-11-17T15:03:58",
    "num_comments": 0,
    "comments": []
},
{
    "submission_id": "1gtq0xm",
    "title": "Are you interested in learning machine learning live, free no paid courses or anything else.",
    "selftext": "1 hour daily on live online meet.\n\nNo ppts, pre-prepared material. Everything will be live on screen.\n\nMy [profile](https://www.linkedin.com/in/gaurav2022/)\n\n  \nWhoever is interested, pls drop me a hi or just comment here, I'll message you the meet link, We'll start from tomorrow only. \n\n[View Poll](https://www.reddit.com/poll/1gtq0xm)",
    "created_utc": "2024-11-17T14:51:12",
    "num_comments": 6,
    "comments": [
        "Hi",
        "Hi",
        "To all who are interested, pls join this link, will figure out a time together.\n\n[https://chat.whatsapp.com/BzOa9Oi7x2iJdcv5L03r3x](https://chat.whatsapp.com/BzOa9Oi7x2iJdcv5L03r3x)",
        "Definitely interested if there are spots open",
        "Sure, messaged you",
        "Hey"
    ]
},
{
    "submission_id": "1gto3k8",
    "title": "Why aren't Random Forest and Gradient Boosted trees considered \"deep learning\"?",
    "selftext": "Just curious what is the criteria for a machine learning algorithm to be considered deep learning? Or is the term deep learning strictly reserved for neural networks, autoencoders, CNN's etc? ",
    "created_utc": "2024-11-17T13:24:52",
    "num_comments": 22,
    "comments": [
        "Deep Learning is used only for deep neural networks (or fancy shallow neural networks).\nNot that it makes particular sense, but that's the convention",
        "why would random forest and gradient boosted trees be considered \"deep learning\" to begin with? what similarities do they share with neural networks and not other ML algorithms?",
        "They work fundamentally differently. One performs backpropagation and gradient descent to refine its prediction. The other sequentially builds decision trees.",
        "IMO it's all marketing.\n\nWe had expert systems, then machine learning, then deep learning, now generative AI. At every step people have tried to frame it like this step is better than everything that comes before and all the rest is obsolete now. \"Oh you know machine learning? That's cute but can you do deep learning?\" Type nonsense.\n\nExpert systems are still the correct solution to many problems. Machine learning still beats deep learning for a huge number of tasks. Etc.",
        "[deleted]",
        "Deep learning = Neural Networks\n\nThe 'deep' term comes from neural networks having the ability to add layers of neurons/nodes. More layers = more 'depth'.\n\nRandom Forests and Gradient Boosted Trees are both based on decision trees, which are not neural networks. I'd say RFs are more 'wide', being an ensemble of many decision trees. GBT borrows some of the gradient descent idea, but in a different way (determine what direction to build the next tree in, vs refining the parameter values within the network).",
        "1. Presence of Neurons \n2. No need for handcrafted features \n\nThese 2 are the most important thing for a framework/algorithm to be called deep learning",
        "because... trees are deep? just imagine a 100 branches tree irl. it is deep\n\n  \n/s",
        "And what's so deep about neural networks?",
        "LOL, IMO \"deep learning\" is simpler to perfect than it is to fine tune the other machine learning models.  If somebody thinks they are somehow superior for being able to do deep learning, theyre a huge dork",
        "I was under the impression random forest are often considered quite “black box-y”. Decision trees sure, can be interpretable.  \n\nI don’t think interpretability is the reason RF and similar aren’t called deep learning though, that is just a term given to NN architectures.",
        "Take some acid and literally everything is deep.",
        "Hierarchical representations obtained via stacking layers",
        "It's just nomenclature that neural networks with multiple layers are referred to as deep. Why even consider including rf and gbdt there? seems random",
        "Bro asking this as if he got personal beef with them 😭",
        "what similarities do they share with open ocean and not other biological ecosystems?",
        "can a tree with many nodes in a RF not be considered \"deep\" then?",
        "Why would they share any?",
        "I've certainly heard people call such trees \"deep\" or \"complex\" but the connotation around \"deep learning\" specifically has always been centered around neural networks. Again it's just nomenclature, not a technical term.",
        "Why not just say, \"Can a split not be considered a neuron, and therefore RF is a neural network?\"",
        "well, they are called \"deep NNs\". Now, OP commenter said that the question doesn't make sense because RFs don't share anything similar with NNs and while obviously RFs are not \"deep learning\", I'm trying to show the fault in reasoning that RFs are not \"deep learning\" BECAUSE they don't share anything similar with NNs that they don't share with other algorithms. That's not the reason, since in that case NNs would not be \"deep\" either because ocean is deep and it's not similar to NNs. The real reason is that we just chose \"deep\" for NNs with few or more hidden layers as nomenclature and did not include RFs or many other algorithms in that.",
        "Lol. You can't make this much sense on the internet bro 😂",
        ">  ocean is deep and it's not similar to NN\n\n\nFrom this you should conclude that the ocean is not deep learning"
    ]
},
{
    "submission_id": "1gtnav5",
    "title": "Courses on fast.ai",
    "selftext": "How long do they take to complete, assuming I’m doing them full time, 4-5 hours per day?",
    "created_utc": "2024-11-17T12:50:03",
    "num_comments": 3,
    "comments": [
        "I finished the first part in 2 to 3 weeks, I was studying about 8 hours per day though and already am a programmer",
        "I assume you went through the book as well right? How did you feel after the course (8 chapters)",
        "Yeah I also went through the book. However if I did it again, I would first complete both parts of the course then go through the book. Doing them both simultaneously was not very effective because I had everything in short term memory when I read the book.\n\nI feel great about [fast.ai](http://fast.ai) and Jeremy. I have not completed the second part though, but can confidently say Jeremy is the best instructor I have seen yet."
    ]
},
{
    "submission_id": "1gtmvgb",
    "title": "Importance in order of paired features? ",
    "selftext": "Let’s say I am creating a model to predict the probability someone will win an MMA fight using the following features:\n\nFighter A’s Age,\nFighter B’s Age,\nFighter A’s Weight,\nFighter B’s Weight,\nFighter A’s Winning Percentage,\nFighter B’s Winning Percentage\n\nObviously this isn’t enough to create a reliable model. But let’s say for the case of the question, those features alone can make reliable predictions. \n\nAfter training the model, it would obviously give different answers based on who is fighter A vs who is fighter B. \n\nHowever, I am assuming if as the number of datapoints increases the importance of the order would decrease and the predicted outcome would be closer regardless of who is fighter A or B. If there is not some predefined order of who is fighter A and fighter B. Is this correct?\n\nAdditionally, would a way around this to be to create two datapoints for each point. One where Fighter A’s information is put in fighter A’s columns and the other where fighter A’s information is put in Fighter B’s column.\n\nAnother way I was thinking was to do some predefined rules of who fighter A. So fighter A is always the younger person, if they are the same age, then lightest person is Fighter A. If they also happen to have the same weight, then the person with the lowest win record is fighter A. \n\nIf those are the same, then it shouldn’t matter since those are the features they are being tested on and they are the same so it should be a 50/50 chance.\n\nOr instead just randomly assign who fighter A and Fighter B is. \n\nAny insight on this? ",
    "created_utc": "2024-11-17T12:31:00",
    "num_comments": 1,
    "comments": [
        "“However, I am assuming if as the number of datapoints increases the importance of the order would decrease and the predicted outcome would be closer regardless of who is fighter A or B.”\nIsn’t this what you want? A model that gives similar predictions given two fighters’ info no matter what the order is. Or do you want the model to pick up the notion of “orderness?”Although I don’t see any benefits of doing that."
    ]
},
{
    "submission_id": "1gtm45b",
    "title": "PCA and Multilple linear regression",
    "selftext": "Hi,\n\nI have a dataset of 3k samples. I want to predict a scalar output with 10 scalar input variables + a signal of 1000 time steps. \n\nCan I apply PCA on the signal to reduce dimension and then use let's say 5 principles components + the 10 mentioned scalar inputs to predict the scalar output variable?",
    "created_utc": "2024-11-17T11:57:37",
    "num_comments": 0,
    "comments": []
},
{
    "submission_id": "1gtllip",
    "title": "Advice for Learning GraphML",
    "selftext": "Thank you for stopping by, Given the premise that I've gone through various ML Course in Coursera and Udemy, practied, Now I want to deep dive into Graph Neural Network. Can anyone who've some degree of exposure to it, help me out as to how to approach and learn it. Also I'd appreciate if one could also share resource that will teach not only basic but also an equivalent to basic building blocks of it, i.e. I need to know if there is an equivalent MNIST classification based program or any other along the same line of it. Thank you!",
    "created_utc": "2024-11-17T11:34:49",
    "num_comments": 2,
    "comments": [
        "I think the best book to get you started is \"Graph Representation Learning\" by Will Hamilton, which covers both traditional approaches as well as GNNs. \n\nJure Leskovec used to teach a course on GraphML at Stanford (perhaps he still does?) and the lectures are available online.  \n\nPetar Velickovic has also given a lecture on the theoretical foundations of GNNs that I'd recommend checking out, along with the relevant chapters (written by him) of the geometric deep learning proto-book. \n\nThese should set you up to start exploring the more advanced GNN literature.",
        "Thank you !! Will definitely go through these resources."
    ]
},
{
    "submission_id": "1gtlhbr",
    "title": "Finetune LLAMA 3.2 for translation",
    "selftext": "Hey guys, \n\nI'm currently working on a project, where I want to finetune a small LLM (LLAMA 3.2 3B) to a translator from normal german to swabian (a german dialect). I have a dataset with 12k translation-pairs. I know there are better models for a translator but I want to do it with a LLM.\n\n  \nI tried to use Unsloth AI with the colab-template and prepared the dataset in the form: \n\n    {'conversations': [{'from': 'system', 'value': 'You are an expert in Swabian dialect.'}, {'from': 'human', 'value': \"Übersetze den folgenden deutschen Text in schwäbischen Dialekt: 'seinen Vorzeil ziehen'\"}, {'from': 'gpt', 'value': 'Da Rohm abschepfa'}]}{'conversations': [{'from': 'system', 'value': 'You are an expert in Swabian dialect.'}, {'from': 'human', 'value': \"Übersetze den folgenden deutschen Text in schwäbischen Dialekt: 'seinen Vorzeil ziehen'\"}, {'from': 'gpt', 'value': 'Da Rohm abschepfa'}]}\n\nbut I just got poorly results with losses at \\~2, which is obviously too bad.\n\n  \nHas anyone an idea what to do?\n\n  \nThanks guys :D",
    "created_utc": "2024-11-17T11:29:42",
    "num_comments": 0,
    "comments": []
},
{
    "submission_id": "1gtkn6s",
    "title": "Some Good free resource to learn transformer architecture and LLMs.",
    "selftext": "Please suggest me some good courses , books or any kind of resource for transformer architecture and LLMs.",
    "created_utc": "2024-11-17T10:53:10",
    "num_comments": 0,
    "comments": []
},
{
    "submission_id": "1gtjvps",
    "title": "I am a full stack ML engineer, published research in Springer. Previously led ML team at successful computer vision startup, trained image gen model for my own startup (works really good) but failed to make business. AMA",
    "selftext": "if you need help/consultation regarding your ML project, I'm available for that as well for free.",
    "created_utc": "2024-11-17T10:19:34",
    "num_comments": 60,
    "comments": [
        "Was the image gen model a GAN or diffusion model?",
        "i am interested in implementing deep learning archs from scratch using pytorch from different papers   \nDo recruiters value these kind of projects   \ni have implemented nueral networks, CNN's in java(from absolute scratch) and transformers and vit in python  \ni want to do more",
        "How to code research paper? I'm trying to do this but I'm so bogged down by the equation when there are matrices involved for example a matrix of 3 by 3 with each element inside a vector",
        "Would you be willing to talk about technical aspects of the model of your own startup, what approach did you take etc",
        "If you had to start from zero, what would you start learning from?",
        "Which habit helped you the most in advancing in this field or maybe which habit you see most people lack that you know would improve greatly?\n\nWhat's your thought on ml moving forward ?",
        "I have done post graduation in computer science from one of top institutes in India and already working in ML. Was thinking if I should pursue PhD. What will be pros and cons of doing / not doing.",
        "I’m a physics major, graduating in 2025. I want to pursue ML as my career. I love research and wanna go for a PhD. Since I have a physics and math major, would it be better to apply for physics masters and then a CS PhD? Or apply for a CS masters and then PhD?",
        "What types of business have you tried? Why do you think they failed?",
        "Hey there!\nI'm interested in your building experience. After reading your comments from my own experiences I want ask if you could not sell earlier? Did you get people to try and build with users who give you feedback?\n\nI did a couple of non technical businesses before I retired and got into ML. Now I want to also build some services and I find my self stuck in adding too. Don't know when it's ready. How to overcome the fear that it isn't ?\n\nAre you advertising now?",
        "I am an AI masters student in Uk. What should I focus on to be a sought after hire in Ai/ML when I finish in a year?",
        "What are your thoughts on voicebots? Do you think there are any demands in the market for it now? The only thing I'm interested in the ml/ai path is voicrbots and I'm currently studying on developing my own voicebot and plan to commercialise it later. Just asking for your thoughts and experience.",
        "What's the most optimal way for a mechanical engineer with 1 year of experience in computer vision AI to get back into the field? I've been out of the game for 2 years but wanna get back in",
        "Recently graduated and discovered towards the end of my degree that I enjoyed Information Retrieval. I liked building the indices, and seeing how my tokenizer would perform. I liked the ranking aspect which included ML, finding some way to define and quantify “relevance”, building clusters, etc etc. I’m currently taking an ML course online, and was wondering where exactly I should/can go from here? \n\nI’ve been trying to find work but I’m not qualified enough for these professional data science roles, and I’m not entirely sold on doing a masters. Kind of stuck.",
        "I am a computer engineer from a third world country..\nI have decent grades..no papers published \nI have some experience as an intern , associate, fellowship on AI.\nI have to be honest my uni was ass ..didn’t teach us shit, no budget for any research.\nI have intermediate understanding and knowledge of all the theory of ML.\nI want to apply for masters and make a career in USA.\nPlease give me the harsh truth..\nHow difficult is it for me to make it like you have.\nI am 23 already.. and I don’t like where I am ..\nI have no guidance",
        "I have a CS degree and Math minor as well. What do you suggest I do to get started in Machine learning?",
        "What math (Linear algebra, Calculus, Probability and statistics) topics should I learn/remember in order to start an internship in DS field?\n\nP.S I will graduate with bachelor's degree in 2025, I am a math student with some focus on economy and programming. So I already know some Calculus, Linear algebra, etc. I made one ML project: CRNN for Korean syllables recognition. And now I am doing my second project with my team.\n\nThe problem is that we don't focus on ML that much and I forgot most topics in math.",
        "What made your startup to fail?",
        "I love ML and DL and i been studying Data Science, it’s my dream to go to the field and be able to help people.\nDo you recommend going for a masters or going for a company directly? Also what would be a recommended roadmap?",
        "you've invited a potential romantic partner over to cook for them for the first time: what is your \"go-to\" meal to try to impress them?",
        "Hi, thanks for the AMA! I'm a 34 year old ML student in US and I have 6 months to find a job here. But I feel I need much more time to develop the skills required. I was a math teacher (IIT-JEE and other competitive exams) for the last 7 years and looking to change career into ML.\n\nSomehow, I feel like ML alone isn't enough but we need deep expertise in a particular industry to thrive. Basically, ML is just a tool for the same. Please counter my argument",
        "I’m trying to fine tune the prov-gigapath foundation model to tackle the PANDA cancer subtyping challenge, but I cannot get it above about 0.7 QWK. I hope to show that the super high resolution whole slide images captured by slide scanners at 0.5 micrometers per pixel are unnecessary, and that prostate cancer subtyping can be done thru fast inferencing on low resolution composite images that I make straight from the camera feed on the microscope. \n\nCurrently, a prostate resection case takes a pathologist about 45 minutes. Cancer subtyping takes about 15 minutes of that, is tedious, and has low impact on prognosis. By creating composites from the camera feed and subtyping the slides automatically, I could automate this part of the casework, potentially making an employee who earns $300k  about 30% more efficient.",
        "successful but failed to make business?",
        "I’m a undergrad 1st year student in the US, right now i’m thinking of doing a data science/philosophy dual major, or a comp sci/philosophy dual major. (with the philosophy part focusing on Ethics and Bias)Then probably going to grad school of some sort. Any recommendation whether i should go data science or CS and what I should be trying do outside of my schoolwork while in undergrad.",
        "What are your opinions about this: https://github.com/prayas7102/NodejsSecurify",
        "bhai referral dila de khi US me. Publications bhi h or experience bhi or US se masters bhi complete ho jaegi 2025 me. From one delhi ka londa to other, pls agar job milgi to Momos khilaunga delhi aake",
        "Diffusion.\n\nIt is an application of diffusion so I should say diffusion model not image gen. thanks",
        "Yes, 100%. They are very helpful in evaluation. \n\nThat's good, keep going. Would you mind sharing your github, would love to join you as well.",
        "Learn linear algebra",
        "Umm yes but not here, it will take a lot of time to write, we can connect on call sometime to discuss.",
        "Ummm with current state of mind, I would say networking, sales skills, communication.   \n  \nBut purely from ML perspective, It would be same, Python, basics of maths, Statistics, ML, DL...",
        "I don't think I have any habit, it's just keep trying and keep learning until you understand something, that's it.\n\n  \nRead, try until you find success.\n\nML moving forward is scary in terms of interpretability of models and speed of change is very high as well, it's hard to keep it up with the pace. \n\nIt has a lot of scope and applications as well that still yet to solve",
        "I am not sure about it. But I think it depends on personal goals, PhD is time taking process, if you enjoy research and have patience, no hurry to make money, then well and good.  \n  \nThough I may not be 100% correct, but there are rare core research opportunities in India.",
        "Math is the backbone of whole AI, so do for CS as well But still all ML algos are nothing but math equations. So, you are already on the correct path.\n\nI would go with CS masters and then PhD.\n\nRather go with CS with specialization in AI/DS.",
        "techniques from physics are increasingly finding use in AI/ML, from theoretical analysis (perturbation methods) to practical application (diffusion, langevin dynamics) both. Physics is just the subset of math that is relevant to phenomena for which we are capable of observing, so it shouldn't surprise us that math and techniques from physics are incredibly powerful for DL.\n\nYou're on a great trajectory already. Just keeping chasing your interests and you'll be fine.",
        "Physics not even close",
        "Software as a service. I don't think i can share link here, t's called Cloth2Life, you can search it.\n\nI won't say it failed but I had a wrong expectation. It will take time to build a big business that I had expectation for.\n\nReason for failure is lack of sales. I've spent 10 months just building it rather than selling",
        "Ummm It's not true actually, people are still interested in it and I had many many user calls before building it but building a product itself took so long (there's a reason for this as well). More than 400 people tried my product before even completing it. \n\nI had a wrong expectation of scale and sales I would say.   \n\n\nMaybe we can connect and discuss?",
        "Solve practical problems and make end to end projects, Data gathering to deployment. I think that should be enough",
        "I see businesses are using it to automate their outbound sales, customer engagement and many other applications, so there is demand for sure.  \n  \nTry to pre-sell or commercialize it before building anything if you are just thinking from business perspective, if you want to learn as well, then just go for it.",
        "I am mechanical grad as well, 1 year mechanical exp too. then switched.\n\nJust start revising, make projects, keep applying and one day you will get a job. This is the only process.",
        "IR is core for all search engines, Google, Perplexity or any other...not just search engines, anything have large db need this, so its definitely a very good skill to have. I'm not sure how to monetize this alone. \n\n  \nBut I think if you complete your course, do DSA and try to get job in these type of companies, you would get some preferences (I am guessing this)",
        "wrong sub",
        "I also work in image gen but am unable to find any opportunities, I'm in my penultimate year of bachelor's, what can I do to get such opportunities?",
        "thanks for confirming, be postponing and postponing",
        "Got it. What should I focus on now? I’m working on implementing research papers and writing one on PINNs. Something that would make me a complete engineer?",
        "But that looks amazing! There’s no way that thing won’t bring good money",
        "did you ever consider doing a consulting shop? dev shop?",
        "Sure thanks!",
        "How?",
        "Don't just stick with image gen, build project across all topics to learn everything. Explore everything and then you'll find what you like the most and also, you don't know what job you are going to get, relying just on image gen for job hunt won't work.",
        "Obviously algebra is must, have to learn it.  \nTry copying first, clone a repo or code from someone who already implemented it and then back trace each and everything along with paper.",
        "You are already doing good, just continue doing it. The world need Physics informed AI. there are many many fields like graphics, animations, game dev, CAD and many more that need PINNs.\n\nI am interested in it as well, if you allow, would love to get on a call with you to see what all we can build and learn together.",
        "Yeah it will but this will take time, as I said, I invested 10 months just building it. Added pricing just in Oct, made some money, have paying customers as well.  \nBut my definition/expectation of success was to get scale fast, that was wrong.\n\nSo, kinda adjusting my mental state now.",
        "Sorry I didn't get it. Can you rephrase?",
        "You wanna sell first before building. Hopefully you get a lot more sales now OP!",
        "So you tried a product based business and it failed, but you definitely have xp with ML and all this stuff. So what if you started a service business, like a dev shop?",
        "Thanks",
        "It wouldn't fail because of product (people likes the product) but because of sales. That will remain the problem with service business as well, how would you get a a client is the main challenge.\n\nService side, I and I know many people are capable of building a lot of stuff from mobile app to desktop software to webapp to cloud infra...everything.\n\nBut that doesn't matter if you can't sell it",
        "True, I don't disagree. But, in my mind, selling a service like this seems easier than selling a product. I could be wrong, but it's the bet I'm making with my dev shop right now."
    ]
},
{
    "submission_id": "1gtiond",
    "title": "How to get to the \"next level\" of PyTorch?",
    "selftext": "I have used PyTorch for a bit and have some understanding of the basics. I understand how to manipulate tensors and how to train simple common models on my single GPU. But from what I understand this is not really what's used in the real world or papers with serious implementations. I see these folks online who are really advanced with PyTorch and it's all a bit overwhelming.\n\nI'm looking to understand the learning path for things like:\n\n\\- distributed training/multi-GPU setups\n\n\\- memory optimization, gradient manipulation\n\n\\- advanced optimization techniques beyond adam/sgd\n\n\\- training at scale\n\nI'm guessing it's difficult to get things like scale/massive distributed training without the resources of a company behind you but maybe it's possible?",
    "created_utc": "2024-11-17T09:27:33",
    "num_comments": 10,
    "comments": [
        "For distributed training you can always rent a couple of cloud instances with multiple GPUs and play around with them. But pay attention to costs since they can add up, fast, especially for GPU instances. Also, various cloud providers offer free credits for new accounts.",
        "Use kaggle T4  dual GPUs as the starting point.",
        "I feel like with a lot of these ML things (big data is similar), you need to get somewhere that has a problem requiring this solution to gain expertise. Some of the self driving car companies are good examples of places working on it.",
        "Read the AWS docs on distributed training with PyTorch containers. For doing it at scale, on whatever platform, you’re gonna want to have a grasp of the workflow.",
        "Just use Hugging Face accelerate library",
        "Distributed training is only really a thing to learn if you are referring to multi node very large scale model parallelism. Most repositiories typically just have DDP code and it's actually really simple (because it's very naive). Very easy to pickup if you have a look at their utils (typically) file. \n\nI think most of these you just learn on the job or while actively working on projects that require you to have a sense of them.",
        "Just buy a couple cheap used gpus (like rtx2060 12GB or rtx2080TI 22GB) to play with.  Start with a simple model and accelerate library FSDP just to get a feel for things.  Then go pytorch native FSDP.",
        "**Make sure you shut down the instances when you're done with them.**",
        "I mean it is more getting things to work. \n\nJust use 2 instances, get the tiniest instance, use a small dataset run 2 epochs lol.",
        "Just shutting it down might not be enough depending on the cloud provider. You probably have to destroy/delete the instances to be safe."
    ]
},
{
    "submission_id": "1gtindf",
    "title": "Tackling AI Hallucinations free online webinar (link in comments)",
    "selftext": "**Why Do LLMs Hallucinate? Let’s Talk About It.**\n\nHallucinations in LLMs are a common and often frustrating issue that developers face. Why do they happen, and what can we do about it? Our team has been researching this and developed a tool to help analyze and address the problem.\n\nIn this free webinar, we’ll discuss:\n\n* How to detect hallucinations using the Pythia algorithm.\n* The role of specific text features in model accuracy.\n* Lessons learned from evaluating claims generated by LLMs.\n\nThis webinar is for developers, researchers, and anyone working with AI who wants to improve the reliability of their models. Participation is free. > [https://www.linkedin.com/events/7261113856268161024/about/](https://www.linkedin.com/events/7261113856268161024/about/)\n\nhttps://preview.redd.it/n1fgjgh3yh1e1.png?width=2160&format=png&auto=webp&s=6de25cae810629ad1a3bb51c445590902c9fb739\n\n  \n",
    "created_utc": "2024-11-17T09:25:57",
    "num_comments": 1,
    "comments": [
        "Register here - [https://www.linkedin.com/events/7261113856268161024/about/](https://www.linkedin.com/events/7261113856268161024/about/)"
    ]
},
{
    "submission_id": "1gth8mb",
    "title": "\nFree NVIDIA-Certified Associate: AI Infrastructure and Operations Practice Tests at Udemy",
    "selftext": "Hello!\n\nFor anyone who is thinking about going for the NVIDIA-Certified Associate: AI Infrastructure and Operations certification, I am giving away my 500-questions-packed exam practice tests:\n\n[https://www.udemy.com/course/nvidia-certified-associate-ai-infrastructure-and-operations-v/?couponCode=777A7C47425B038D5153](https://www.udemy.com/course/nvidia-certified-associate-ai-infrastructure-and-operations-v/?couponCode=777A7C47425B038D5153)\n\nUse the coupon code: 777A7C47425B038D5153 to get your FREE access!\n\nBut hurry, there is a limited time and amount of free accesses!\n\nGood luck! :)",
    "created_utc": "2024-11-17T08:25:55",
    "num_comments": 0,
    "comments": []
},
{
    "submission_id": "1gtgxwu",
    "title": "Infer wind direction from webcam and air pressure sensors with differing temporal resolution. Need help! ",
    "selftext": "Working on a project where the input is a low frame rate video stream from a webcam, along with three air pressure readings at a higher temporal resolution. The goal is to infer the wind direction (0 to 360 degrees) at each timestamp. I have no idea where to begin since up now all I've done is single image classification using torchvision!\n\nThe pressure sensors are statically mounted on exterior walls of a building, where they've been recording for over ten years. There's a webcam conveniently located on an adjacent building that has also been recording for that long. Pressure is recorded at 1 hertz and camera frames at 0.1 hertz (one picture every 10 seconds). In general, the camera is informative at higher wind speeds where it can see things like trees leaning and flags extended straight, while at low speeds the pressure sensors are more informative.\n\nGround truth wind directioncomes from a weather vane that's nearing end of life. My goal is to replace it with a PyTorch model, which not need to run in real-time!!\n\nSo my question is, what kinds of model architectures would be able to process data like this? Ideally I'm looking for a tutorial or example I can modify since my intuition says this would be a pretty challenging model to build entirely from scratch as a relative beginner to PyTorch.\n\nThanks!",
    "created_utc": "2024-11-17T08:13:00",
    "num_comments": 0,
    "comments": []
},
{
    "submission_id": "1gtgpwf",
    "title": "[Final Year Project] Appointment Chatbot and fine tuned chatbot",
    "selftext": "My project idea is to create a **mental health chatbot** integrated into a **doctor appointment system**. I want the system to not only help users chat about mental health but also book an appointment with a doctor.  \nChallenges:\n\n\\[No Rasa framework or other similar frameworks\\]\n\n\\* I have some datasets for training the chatbot, but I’m unsure where to start when it comes to integrating NLP/NER into this system.  \n\\* What are the best practices for training a chatbot to handle both queries and appointments?  \n\\* How should I design the flow so the chatbot seamlessly interacts with the database to check doctor availability? | Also won't there be Catastrophic Forgetting while combining fine tuned chatbot for consulting and operational booking chatbot for appointment?\n\n\\* How much ML/Deep Learning knowledge do I need to effectively train and integrate the chatbot? Any specific algorithms or models you'd recommend?\n\nWould really appreciate your help. Thank you! 😊\n\nIf you know of any similar projects or examples that align with my idea, please share them. It would be great to see how others have tackled similar challenges, and it could give me some inspiration or direction. Thanks again!",
    "created_utc": "2024-11-17T08:03:02",
    "num_comments": 0,
    "comments": []
},
{
    "submission_id": "1gtgfxj",
    "title": "continuing education for AI Ethics",
    "selftext": "Hello,\n\nI've been employed as a Data Scientist for 5 years. I do ML research for a large company. I have a MS in Data Analytics from Georgia Tech's OMSA. Okay enough about me.\n\nDoes anyone have recommendations for continued learning, particularly in the realm of AI Ethics? I could do a MOOC but wondering if people have suggestions. I'd like to stay relevant in the field, but am also very concerned at implications for society.",
    "created_utc": "2024-11-17T07:50:31",
    "num_comments": 0,
    "comments": []
},
{
    "submission_id": "1gtgc7z",
    "title": "How to be the irresistible hire in AI/ML?",
    "selftext": "I am an international student studying masters degree in AI in a UK university with prior 4 year experience in Web Development. I want to get a job in AI/ML by the end of 2025 (a year from now). How can I prepare myself in the coming year to be in high demand and make my job hunting easy?\n\nPS: I will require visa sponsorship.\n\nUPDATE: It does not have to be in the UK. I am alright with remote jobs anywhere.",
    "created_utc": "2024-11-17T07:45:47",
    "num_comments": 71,
    "comments": [
        "How to be an irresistible hire? Since you are in the UK a PhD in Cambridge in ML with funding by Google and 3-4 NeurIPS, AIStats and Nature publications will get you there.",
        "Almost no international student is going to be “irresistible” no matter how much they study AI/ML unless they land a FAANG+ internship and then a return offer from it. Maybe start there if you’re good enough.",
        "While I agree with much of what others have mentioned, I respectfully disagree that having dozens of publications is the *only* way to get into FAANG. Of course, having strong open-source contributions counts for a *lot*.\n\nHere’s what you can consider alongside: Collaborate on one or two papers and focus on a niche area that big tech is actively exploring (e.g., mechanistic interpretability). Publishing in such areas can be highly impactful. If you develop a solid understanding of your work and can articulate it well, it will definitely set you apart.",
        "Weird comments here. I'm British and did an MSc in CSML at UCL. 95%+ of the cohort were not from the UK, with a lot of them also not being from the EU either. Many stayed in London and continued to work. Some couldn't get sponsorship. Make sure you do your thesis with a company you're interested in, this was a common pathway to sponsorship post-masters.",
        "I’m a self taught MLE with a Degree in physics. I am at least getting invites to some of the big LLM industry labs. \n\nI have a blog post about what i learned over the past years to get there. Maybe it helps.\n\nhttps://www.maxmynter.com/pages/blog/become-mle\n\n\n(It’s competitive, yes, but many here are also vastly exaggerating).",
        "Kaggle grandmaster.  \n\nHave a degree from a super well known university like MIT, Harvard, Zurich.\n\nHave published a paper and presented at a MAJOR conference.\n\nOtherwise, you're just another name on the list, gonna have to bring out your sparkling personality for the opportunities to interview you do get.",
        "4 years web dev is utterly useless for AI/ML\n\n1 year to learn AI/ML is not enough, espeically if you want to go into research\n\nFor reference, I went to Cambridge and studied upto a Masters working in ML with contributions to an ICML paper and I still coun't really get an AI/ML job (I work as a SWE in AI/ML and I am soon to internally transfer to more of a research role). \n\nMost of the people at Cambridge who went to AI/ML either has IMO/IOI or did a PhD.\n\nPhD is the serious way to get into AI/ML research.",
        "While most folks are correct here there’s an assumption you want to work on something AI/ML related inside AI/ML.\n\nIn an effort to be positive… there are plenty of need for talented people in non AI/ML capacities at top AI/ML companies (the front ends of Perplexity for example doesn’t code itself)!\n\nSo, a better question is, what do you want to do?\n\nPS: that said, most of the responses on this thread are very relevant. Generally from the hiring reviews I’ve done if it’s directly machine learning related a PhD from a top school is the min bar and we go from there.",
        "I'd say so many degrees and PHds might not be that important, some work experience, good communication skills might go a long way. I've worked with a lot of very smart people with lots of degrees but they lacked any kind of soft skills so it was a nightmare to understand them and try to communicate and work properly as a team.",
        "You cant. 1 year isn't enough.",
        "That’s easy to answer: You need to show that you can create a product/tool that people want to rely on. \nOne way to do it: Write an open source library that many people use for AI/ML - the more stars the better.",
        "Build a portfolio of applications on Github. Demonstrate   that you can solve real world problems. Make use of open data sources. Future employers want to see the quality of your code more than anything.",
        "Not in AI but in another engineering that is Ph.D dominated so I assume the experience is very similar. \n\nUsually you want to have a Ph.D from a top tier research university with the best conference or journal publication in the field. For my field, having two or more of the top journal publications usually make you a candidate for assistant professor position at t10 university (if they have a vacancy) or can get an entry level position at big techs in a team that is R&D oriented (places like FAANG).\n\nIrresistible comes from having a really well known publication. If you present a good research finding, and maybe even do it a multiple times before you graduate, then you will likely be contacted for a position. \n\nIf you truly want to be irresistible, after the Ph.D, join top tier research team in one of the big techs and lead a project. Think openai or google. Grind it out for at least 5 to 10 years. Then you will actually be highly sought after. In any field, as you get to the top, everyone knows or knows of each other. It becomes a very small field. I feel like 10 years of hard work after Ph.D is where you really become an expert.",
        "Do something no one else can. There is no other option.",
        "Get a sixpack and learn spanish",
        "Change your name to Ilya Sutskever",
        "Stick to carrier in web development",
        "Trying to get one NeurIPS/ICML/ICLR publication already during masters (you still have a year) would be good. Reach out to professors and let them know that you are interested in collaborating on research. \n\nMany professors will have research ideas that will need some student to do implementation and running experiments.",
        "Getting into Cambridge, ez ggwp",
        "Or, launch your own multi million dollar product and company lol",
        "from your post history it seems that you live in the US. do you know how visas and sponsorship work for international students in the UK or are you just blindly repeating a US-centric adage?",
        "Is it due to my \"international-ness\"? BTW I would have finished the course and not be a student by then",
        "You have no idea how things work at the highest level. Almost all talents are international at top levels.",
        "Are you studying or researching about mechanistic interpretability by your own right now? \nI also heard Dario Amodei talking about it.",
        "Considered that course. How many peers actually got jobs in ML and not just data science? \n\nLooking at LinkedIn it seems only a small minority actually get ML jobs. ",
        "Considered that course. How many peers actually got jobs in ML and not just data science? \n\nLooking at LinkedIn it seems only a small minority actually get ML jobs. ",
        "I like it, esp the [fast.ai](http://fast.ai) part. It's nicely laid out.",
        "Thanks for providing that link to your article. I read it over and it seems like a great path to take. \n\nThe optimist in me knows that getting an MLE role will be tough, but still possible. \n\n\nThanks again",
        "Fully agree. I have a PhD, 21 published papers in some highly ranked journals and conferences, numerous awards. It still took me about 200 applications to get my current job. Job market is nuts right now.",
        "For ML engineering with LLMs it’s becoming a lot more engineering heavy. And skills in distributed systems will become very valuable. \n\nAcademic research is another story. But the big labs (DeepMind, Anthropic, openAI) definitely put increasing emphasis on ML Engineering and Research Engineering and less on PHd level research scientists.",
        "What job role in AL/ML pays the most",
        "If you want a top position, you're not even gonna get through the automated CV check to show your communication skills if you don't have degrees and publications though.",
        "Are you saying having a masters degree + 4 years experience as a founder and web developer + \\[projects and things I will be doing during the next year\\] will not be enough for getting a job? How do people get in then?",
        "\\> Write an open source library that many people use for AI/ML\n\nThis is the way. If you can write something like TinyGrad/ MLX level then you can get hired pretty easily, not an easy task tho",
        "You won’t be able to write an open source library people rely on as a student just breaking into the field.\n\nAt the very least you need some years of doing your own projects and/or contributing to different projects to understand what is needed out there.",
        "haha that was quite specific...",
        "I did my Master's at Cambridge and did help with an ICML publication and got put on the authors, however tbh most companies only care if you are a first author",
        "How do I find professor who are searching for students do they post somewhere?",
        "This is the state of hiring in the west right now, UK included. It has been for the last couple years. It will always be harder for international students unless companies magically have the money and lack of prejudice to sponsor many people not from their country.",
        "Four years ago, OP posted that they had finished medical school.\n\nNow they are talking about web development and AI.\n\nThey also know Kurdish and are a devout Muslim.\n\nAll this points to greater chances of OP being from the Middle East, likely Turkey or Syria, from a Kurdish region and not from USA.\n\nEdit - They have posted a resume with the line, \"first person in all of Iraq\", so OP is an Iraqi studying in UK.",
        "Sponsoring is a significant expense for companies. You have to show more than other citizens applying that you’re good enough and worth paying attention to. And if you don’t have any proof of that through a good internship and research experiences then you’ll be just like the others and sent back home. Which isn’t necessarily a bad thing since companies in the west are offshoring to the east.",
        "If that’s what keeps you sane",
        "I think the number of individuals who go straight into ML roles out of University is very, very small. Some pivot after some years in their career as SWEs, others complete a PhD and go from there.",
        "It's a great course. I turned down offers from Oxford, imperial and Edinburgh for it and I still say it was a good choice if you're wanting a good mix of theory, industry and practical.\n\nI don't know anyone who didn't get a job afterwards (minus a couple who had visa issues). The ML/DS thing, in terms of jobs, is honestly usually a misnomer. Many companies just call ML engineers Data scientists and vice versa. I wouldn't focus too much on the titles. From my cohort everyone I know is in a senior or higher position after 6 years, with many at Meta, Amazon, Deep mind. Lots of PhDs, etc etc.\n\nThe connections you make on that course and the content you learn are both invaluable. If you have the opportunity, do it. There are other options within the same faculty (MSc in DS, MSc in ML) and whilst the difference is pronounced when you're at UCL, if you're going into industry it doesn't seem to matter that much.",
        "200 is normal, even in 2021 it was like that.\n\nSource: I also have a PhD with ~30 publications, including NeurIPS and ICML. Now work at FAANG as staff research scientist. But still: 200+ applications you will always need regardless of how impressive your resume is and even in times of a good job market.",
        "Thank you. I needed to hear this. Glad I'm not the only one. Been trying to move up and no luck.",
        "this is true, but the demand is so high that they end up chosing mostly Masters students from \"top schools\" or people who had experience in FAANG level companies, Anthropic love hiring from Stripe",
        "If you want a top position you're not gonna get it without previous experience, he's just a student, he needs to start with a junior position",
        "Depends what you consider a top position, I personally only have a CS \"conversion\" masters with 0 publications but a lot of LLM experience. I make £100k a year plus equity at a scale up as an Ai Engineer.",
        "You asked how to be an irresistible hire. To be that, you have to have demonstrated verifiable success in the field, for example publishing at a major conference, launching an AI product or library people know about. There are very few \"irresistible\" hires..\nFour years of web development isn't going to be meaningful to a company hiring an AI/ML engineer. Experience as a founder can mean almost anything; it might be valuable if you led people, secured funding, and/or launched a product and made some revenue. If you are a \"founder\" who launched a small mobile app by yourself, that's cool but doesn't make you irresistible either.\n\n\nI agree with the commenter, a year is simply not enough to nake yourself an irresistible hire.\n\n\nYou want the best odds, a few things to try:...\n- get an internship and impress them with your productivity and by being someone people want to work with.\n- do well in your AI/ML classes, study beyond what the class teaches or do personal projects in areas that interest you. Make sure that you stand out in interviews as someone who really knows their stuff.\n- Make sure your written and verbal communication skills are strong. This can set you apart from other early career people.\n- If you have time, consider contributing to a well known open source project like PyTorch or any commonly used modules. It does not have to be a major contribution, but will look great on a resume.",
        "If you are hiring an AI resource for the company you founded and your resume came in, would you hire yourself? Remember to be objective.",
        "u/itwasmywifesidea sums up my response. The market is tough and you have no experience in ML.",
        "OP was asking about being irresistible :v",
        "> most companies only care it you are first author\n\nI am at FAANG and have been involved in quite some hiring. This is definitely not true. Having an ICML paper is a huge plus on your resume.\n\nHaving a first author ICML paper is an even bigger plus, that’s true. But there is still enormous value of having that ICML paper on your resume.",
        "How did you get your master’s degree there? My dream.",
        "Nobody advertises. You would just need to talk to them.",
        "And you have to deal with the Home Office. No one wants to deal with the Home Office and for good reason.",
        "I think the last time I was apply for working the tech industry was 2004ish. ;)  Different world back then. I was offered an interview at Google in 2016. I turned it down. Funny how much things have changed even since 2016.",
        "Damn I'm currently in high school what advice would you give. What degree and high to stand out will I need master or PhD or is bachelor fine what course to study.",
        "I've been in and out of the tech industry since 1996, but I've always had friends in the industry even when I've been out. I don't think I've ever seen it quite this lopsided.",
        "Yeah, it’s still pretty competitive. It’s just that (at least for these high paying jobs - which is, i guess, why most people here are into them) the tide moves away from research and towards engineering. \n\nSo your hardcore, failsafe low level distributed systems skills from stripe are more valued than your Cambridge PhD, Physics Olympiad, 4x ICML first author types (ignoring ofc that there are ppl who have both).",
        "Thats good to hear. Is this for AI/Research Engineers only? I am presuming Research Scientists can only be PhDs with first authors",
        "How about JMLR/TMLR or 2nd tier A* journals? \nI am referring to this in the context of RS roles",
        "Its an integrated master's course, so I did my undergrad and masters there. How did I get in,  just be an insane math nerd tbh, I enjoy reading about math in my spare time and thats quite a strong quality of most cs/eng people here.",
        "I know that but where do I find professor who are looking for students \nI know that I have to talk to them",
        "Interviews at google or facebook or amazon or apple are a dime a dozen. I get offered an interview at one of them every 3-4 weeks. But mostly it is some internal HR headhunter just trying to churn the waters and get more people into their interview process. At least that is what it feels like for myself.",
        "Do you think theirs too many candidates or is it corporate structure of laying off everyone for a higher profit?",
        "Yeah, I’m a research scientist myself. Indeed for RS a PhD degree is a must. \n\nIn fact, to land an RS job at competitive places like FAANG just typically don’t just need any PhD, but need to have a particularly strong research profile (ballpark: top 5% of PhD graduates in your field in terms of research output).\n\nIf you don’t have that at end of PhD, people can consider a postdoc first, or can consider taking a research scientist position at some less prestigious company first as a stepping stone (also here: none of this is new, this has been the bar for FAANG RS even in 2021 top market).\n\nHowever I also help out interviewing for our research engineer and MLE roles. No PhD is strictly required for those roles, but it is still a nice-to-have, and any research experience/output from masters (e.g., an ICML paper) is very much a plus on your resume too.",
        "JMLR is great of course, I would personally value it even slightly higher than NeurIPS/ICML.\n\nTMLR I personally think is almost on par with NeurIPS/ICML and that is how I would value it. \n\nThe drawback of TMLR is that it is quite new. It wasn’t around when the hiring managers of today, who did typically their PhD over a decade ago, did a PhD themselves. Therefore, not everyone is aware of TMLR and values it like that. \n\nThe result is that it really depends whether the specific hiring manager is aware of TMLR. Those who know it will typically value it at very close to ICML/NeurIPS level (and better than 2nd tier conferences like AAAI and IJCAI). But there are also those hiring managers who haven’t heard of it and may not factor in any TMLR papers at all.",
        "That wasn't the situation. It is a long story.",
        "Both. CS has historically been a good degree due to high post graduation employment rates. Then of course there is off-shoring, although I think a lot of companies are pulling back from that. There have of course been layoffs. AI integration into IDEs are making developers more efficient and of course corporate greed is \\*always\\* a factor. It all has led to more supply than demand. I don't have any proof of this, just my sense of things.\n\nI'm hoping applying for my last job as a professor. If I get it, then that;s what I will do until I retire in about 10-12 years.  Well, other than composing. :)"
    ]
},
{
    "submission_id": "1gtfrf8",
    "title": "Feeling Overwhelmed by Math Homework? Check This Out",
    "selftext": "Anyone else feeling overwhelmed by their math homework lately? I started using this app called Mathos AI, and it’s been such a relief. It doesn’t just solve the problem but explains it step by step. I wish I had found this sooner.",
    "created_utc": "2024-11-17T07:19:29",
    "num_comments": 1,
    "comments": [
        "I wish advertisements were strictly banned from this sub...."
    ]
},
{
    "submission_id": "1gtffvn",
    "title": "How was paraphrase-mpnet-base-v2 trained?",
    "selftext": "Is was wondering on how it was trained and how it differs from the base model. I \"read\" the paper: [https://arxiv.org/pdf/1908.10084](https://arxiv.org/pdf/1908.10084) but i couldn't find anything relating to the model. Does anyone know how it was trained and how it differs compared to the base model?\n\nThanks in advanced.",
    "created_utc": "2024-11-17T07:04:31",
    "num_comments": 1,
    "comments": [
        "Found [20 relevant code implementations](https://www.catalyzex.com/paper/arxiv:1908.10084/code) for \"Sentence-BERT: Sentence Embeddings using Siamese BERT-Networks\".\n\nIf you have code to share with the community, please add it [here](https://www.catalyzex.com/add_code?paper_url=https://arxiv.org/abs/1908.10084&title=Sentence-BERT%3A+Sentence+Embeddings+using+Siamese+BERT-Networks) 😊🙏\n\nCreate an alert for new code releases here [here](https://www.catalyzex.com/alerts/code/create?paper_url=https://arxiv.org/abs/1908.10084&paper_title=Sentence-BERT: Sentence Embeddings using Siamese BERT-Networks&paper_arxiv_id=1908.10084)\n\n--\n\nTo opt out from receiving code links, DM me."
    ]
},
{
    "submission_id": "1gtff4n",
    "title": "Non-containerized GPU rent websites",
    "selftext": "Hi guys,\n\nDo you know any non-containerized/dockerized GPU providers? (such as oblivus, nebius)?  \nBecause I need to run some containers and websites such as runpod,vastai,tensordock are really pain in the \\*ss.\n\n  \nThank you so much \\^\\_\\^",
    "created_utc": "2024-11-17T07:03:30",
    "num_comments": 2,
    "comments": [
        "Hello! Jonathan from TensorDock here - we provide virtual machines \\[so you can run containers!\\]\n\nShoot us an email at support \\[at\\] [tensordock.com](http://tensordock.com), happy to issue some credits and invite you to test us out!",
        "You can try out 15+ GPUaaS providers that provides VMs on our platform [https://shadeform.ai](https://shadeform.ai)"
    ]
},
{
    "submission_id": "1gtfb1w",
    "title": "HumanEval: A Benchmark for Evaluating LLM Code Generation Capabilities",
    "selftext": "",
    "created_utc": "2024-11-17T06:58:28",
    "num_comments": 0,
    "comments": []
},
{
    "submission_id": "1gtew96",
    "title": "Question about pooling methods for sentence transformers. ",
    "selftext": "Why do we only use pooling methods like mean and max. It is compressive so we are losing a lot information. We do we trust a simple math operation to keep the most relevant information. Wouldn't you want the top right part of a transformer to pool the word embedding? Or something like that?",
    "created_utc": "2024-11-17T06:37:51",
    "num_comments": 2,
    "comments": [
        "Pooling is lossy compression by design. We pool intermediate results in order to cut down on the parameter space. It also has side benefits (though this part is not rigorously quantified in all domains) that pooling creates an information bottleneck that forces the network to make more efficient use of parameters. Also to your question, we don't trust the pooling operation. We trust the network training to condition the going into the bottleneck appropriately to survive compression.",
        "Making the model do that seems very inefficient. Is there not a better to pool words than just a simple math operation? Wouldn't top right part of the transformer be better suited for the pooling layer?"
    ]
},
{
    "submission_id": "1gte2j4",
    "title": "Vector Databases Explained in 2 Minutes",
    "selftext": "",
    "created_utc": "2024-11-17T05:56:23",
    "num_comments": 1,
    "comments": [
        "1. The video describes the functionality of word embeddings, not databases. 2. Google published Word2Vec (2014) and Transformers (2017). I'm pretty sure they use hybrid search, because most search engines since (including every search bar you see on large websites, i.e. youtube, reddit, wikipedia, etc.), do."
    ]
},
{
    "submission_id": "1gtdxsx",
    "title": "Is it possible train ai model on astrology calculation book? ",
    "selftext": "I heard that it's hard for an ai model for mathematics calculation related use case. \nIs it possible to feed AI models with astrological documentation and rules and then use it to calculate horoscope and astrological charts for specific personal details? In my case calculating horoscopes, relationship prediction, signs, lucky numbers, planet charts using Vedic astrology. ",
    "created_utc": "2024-11-17T05:49:18",
    "num_comments": 31,
    "comments": [
        "Astrology? Just using the random library will do.",
        "We cannot possibly answer this question without knowing the exact date the model was built and the position of the moon when that happened.",
        "That is just plain math - AI is never as good as exact calculations.",
        "Why does this need machine learning, why can’t you just write a normal program to do this. Idk much about astrology but if it’s just made up of rules there isn’t much benefit to using ml.",
        "I have been thinking about doing the same, in my case for western astrology.",
        "If you are really interested try Pandas with ephemerides. Good luck.",
        "lol",
        "Didn't get. What exactly do you mean?",
        "Not really. There's a level of consistency in astrology.",
        "You mean results will not be accurate?",
        "AI is math, and in the case of ML/DL, it is linear algebra plus calculus and statistics ",
        "The rules are too complex in Vedic astrology. 100s of star charts are used to determine horoscope and kundli for a single person. So writing a program is more complex.",
        "Meaning you just roll a dice on it.",
        "You can always set seed in the random function",
        "That is the whole idea of ML and AI. \nIt is low precision math with some noise. \n\nAsk chatgpt something and see if you get the same answer each time. \n\nWith ML you can feed it lots of datasets of times and planet positions and it will make a qualified guess given a specific time.",
        "Can you guys specify what type of ai? Llms are horrible at math and data consistency",
        "Math is precise and  reproducable. \nI know AI is reproducible with same seeed.  but it is not exact mathematical calculations to a high precisions.",
        "You would have to write that programs anyway to generate the training data (unless you have a database of millions of observations)",
        "If anything that makes this a worse use case for ML. Most ML algorithms can't learn to follow specific, explainable rules. \nBest bet is probably to find some documents that describe all of the rules and get good at parsing text. You might even try to use an LLM to augment/help with the parsing, but this isn't the kind of problem that can be solved with off-the-shelf stuff",
        "That's not what I meant. I'm sorry you can't understand it.",
        "> Ask chatgpt something and see if you get the same answer each time.\n\nThis is a misleading example. ChatGPT has a lot going on between input and output that is hidden from users and out of their control.\n\nIf you set the temperature to 0, most (all?) generative LLMs basically become deterministic and will return the same output every time.",
        "Lol I love that now every time someone mentions AI think on llms, AI is more more than that \n\n\nMoreover, dudes if you downvote me for saying that AI is math, then you don't have any idea of how AI works ",
        "Astrology is a big thing in Asian countries from weddings to small ceremonies to even political dates are based on astrology. There are a large number of companies who are involved in astrology. So millions of observations databases are already available but the main problem is the market is competitive and none of them are going to make it publicly available for training.",
        "> Most ML algorithms can’t learn to follow specific, explainable rules.\n\nOnly if you’re talking about deep learning. Classical models like decision trees, XGB, or SVMs can follow or approximate rule-based approaches quite nicely. This is why they’ve been popular for decades and remain so.",
        "I am sorry you can't understand the analogy.",
        "k",
        "XGB isn't even a decade old :D",
        "It's completely unrelated to what I said so it's a bad analogy.",
        "Oh wow, you’re right! Well, it’s exactly a decade old. Regardless, TIL!\n\nI don’t actually use XGB myself much (though I do know what it is), hence my ignorance of its history. But my original point still stands.",
        "Yeah It just struck me as such an odd thing to say because we barely just started using it a couple years back and it's definitely the newest model we use."
    ]
},
{
    "submission_id": "1gtdbho",
    "title": "I Like Learning About Model Architecture Visually. How About You?",
    "selftext": "In the past, I found it extremely hard to wrap my head around CNNs. One major reason was how most tutorials would start with a wall of 2D Python code, which felt overwhelming.\n\nI consider myself at least partly a visual learner and I think to some extent, many of us are. What really helped me make serious progress was sketching out neural network structures and trying to represent the model's architecture visually.\n\nKnowing there are many Redditors out there who might also benefit from visual explanations, I decided to create a video where I visualize the architecture of a CNN tackling an image classification problem (I put 60 hours of work into a 10 min video).\n\nYou can check it out here: [https://youtu.be/zLEt5oz5Mr8](https://youtu.be/zLEt5oz5Mr8)\n\nI’d love to hear the honest feedback of you guys. If it helped, I will not stop doing these :D",
    "created_utc": "2024-11-17T05:15:55",
    "num_comments": 0,
    "comments": []
},
{
    "submission_id": "1gtd8qu",
    "title": "Help with ML project for Damage Detection",
    "selftext": "Hey guys,\n\nI am currently working on creating a project that detects damage/dents on construction machinery(excavator,cement mixer etc.) rental and a machine learning model is used after the machine is returned to the rental company to detect damages and 'penalise the renters' accordingly. It is expected that we have the image of the machines pre-rental so there is a comparison we can look at as a benchmark\n\nWhat would you all suggest to do for this? Which models should i train/finetune? What data should i collect? Any other suggestion?\n\n  \nIf youll have any follow up questions , please ask ahead.",
    "created_utc": "2024-11-17T05:11:47",
    "num_comments": 3,
    "comments": [
        "According to me you must use Yolo and to train the model first you must have proper dataset which contain labelled data like for ex a product contains damage then you must highlight damage with box \nThere is a pre built weight of yolo v8 and more you can use which is best suited for your problem",
        "Collecting and labelling data here could be challenging. Do you have sources to collect data from?",
        "Thanks"
    ]
},
{
    "submission_id": "1gtd2fc",
    "title": "I done a project of creating a dataset for machine learning model",
    "selftext": "I am done a side project that I am build my own dataset name success rate of space mission, for this project I learned web scraping for collect the data of space agencies and their space missions and it perform 71 percentage in linear regression without any hyper parameter tuning and while using cross\\_val\\_score using the RandomForestClassifier() algorithm with standardize the data the model perform 73 percent \n\n  \nlink: [success rate of space mission](https://www.kaggle.com/datasets/santhosh162006/success-rate-of-space-mission)\n\nlink: [Santhosh . R – Medium](https://medium.com/@santhosh_r) this is my blog link where I posted daily insights of learning journey of AI\n\n",
    "created_utc": "2024-11-17T05:01:50",
    "num_comments": 0,
    "comments": []
},
{
    "submission_id": "1gtcdpx",
    "title": "Need help",
    "selftext": "So, i am doing                                                                                                                                                             MACHINE LEARNING COURSE ANDREW NG\n\nPRML BOOK\n\nPATTERN CLASSIFICATION\n\nDEEP LEARNING COURSE ANDREW NG\n\nDEEP LEARNING BOOK IAN GOODFELLOW\n\nNUERAL NETWORK:ZERO TO HERO ANDREJ KARPATHY ,  \nIs it good to do optimisation theory(Stephan Boyd) and information theory(David Mackay) before all of this  \n(i am doing Probability theory now)\n\n",
    "created_utc": "2024-11-17T04:20:34",
    "num_comments": 0,
    "comments": []
},
{
    "submission_id": "1gtc0tm",
    "title": "For AI is python and Java is really required for seeking job for fresher?",
    "selftext": "I am pursuing B Tech Artificial Intelligence  and Data Science I am interested in Deep learning but now I am just learned the machine learning in python using scikit-learn  in my class lecture of Foundation in Artificial intelligence my professors teach this subject using java so I asked him is java is really required I am already learned python for machine learning and Deep learning then he told for fresher python and java is required programming language for AI field",
    "created_utc": "2024-11-17T03:57:22",
    "num_comments": 3,
    "comments": [
        "Java isn’t. Python is.",
        "Python certainly dominates right now in AI. However, other languages appear with some frequency as well. C, C++, Java ... you can find a fair number of jobs that require these languages in AI/ML. Given the way the market is right now, it is really important to know the language of the job really well. So I would definitely master Python. Then if you are capable master another language just to give yourself some diversity (personally I would pick C++ but that's up to you).",
        "I'm currently focusing on Python as my primary language, especially for AI and ML projects. Once I feel confident with it, I'll definitely look into expanding my skills with another language like Java for diversity. Thanks for this information"
    ]
},
{
    "submission_id": "1gtbjy0",
    "title": "I understand the ML theoretical concepts but I'm struggling to integrate the knowledge using numpy,pandas etc.",
    "selftext": "",
    "created_utc": "2024-11-17T03:25:16",
    "num_comments": 2,
    "comments": [
        "I'm sorry I can't be of any help here, but I can relate hard man, the theory and application just doesn't add up together man, I'd like some help with that as well",
        "I recommend CS50's Intro to AI with Python course: https://www.edx.org/learn/artificial-intelligence/harvard-university-cs50-s-introduction-to-artificial-intelligence-with-python.  \n\nYou can buy a verified cert when you complete it, but the course is free and gives you hands-on experience with the algos that support various aspects of AI first and the libraries like \\`TensorFlow\\`, \\`scikit-learn\\`, \\`nltk\\`, etc. later.  \n\nFor context, I have been working full-time as a web frontend platform engineer for \\~4 years. I transitioned to an engineering career \\~10 years ago by learning to leverage Photoshop's JavaScript API and developed a system to automate product renderings at the company where I was working as a designer. \n\nI had set a goal back then to be able to do what is being done today with GenAI (sans the NLP aspect, I just wanted to be able to generate convincingly real art from photos \\[and not art that looked like Starry Night!\\]). I had a primitive image generator that could cobble together different image assets by essentially codifying my design process and created some really cool abstract images, which was more than I ever imaged I would be able to do. But I still felt similar to how you describe feeling now with ML.  I tried taking a course on Udemy that went deep into the theory of AI, but I had no way to make the connection from that to actually writing code that did the things I wanted to do.  Considering I also didn't have a CS education and math was always intimidating, I talked myself further out of it, thinking ML was for people way smarter than me.  And to some extend I still feel that way; it's unlikely that I will develop some breakthrough AI or ML algorithm, but right now, that's not my objective.  I want to know how to engineer AI systems and services to develop products that solve real-world problems.   \n  \nI am still new to ML and deep learning, having only completed the course 4 months ago. However, I understand how to use the tools now, and even solved my first real-world problem with a custom model I trained on product specs and unit costs using TF.  To me, crafting a neural net with TF is intuitive and exciting; it's the data scraping and pre-processing that presented the biggest challenge.  Even in a system where data is relatively well managed, it's surprising at how much inconsistency you will find in a dataset, and all of that needs to be reconciled in order to have an effective model (or at least to my junior ML brain, this is true).\n\nBest of luck.  I am a person of mere average intelligence, so if I can figure it out, I know you can too.  It's getting out of your own way that is the real problem."
    ]
},
{
    "submission_id": "1gtb188",
    "title": "CLNF features how to process ",
    "selftext": "I came across this dataset with facial features (68 2d points mapped on face) as data. Usually the 2d points are expressed in floating point coordinates (pixel values) but there are some regions where the data is invalid, presumably because the person's face was out of frame or something. How do I remedy this problem? Just set those coordinates to -1? ",
    "created_utc": "2024-11-17T02:47:39",
    "num_comments": 0,
    "comments": []
},
{
    "submission_id": "1gtaqip",
    "title": "Image Classification Issues",
    "selftext": "Hello, I am doing a ML assignment and I have issues regarding the classification process:\n\n  \nDataset 3: Mars surface image (Curiosity Rover) Dataset This is an image dataset containing 6691 images of the Mars surface collected by the Mars Science Laboratory (MSL, Curiosity). The dataset has labels and spans 24 classes. The provided dataset has low resolution images of roughly 256 X 256 pixels each. There is a high resolution version of the dataset available, however for this assignment you should only use this low resolution image dataset. You can get further details of the dataset at their website: [https://zenodo.org/records/1049137](https://zenodo.org/records/1049137)\n\nHere is the code for splitting the data and using classification:\n\n    X_train, X_temp, y_train, y_temp = train_test_split(X, y, test_size=0.3, random_state=42)\n    X_val, X_test, y_val, y_test = train_test_split(X_temp, y_temp, test_size=0.5, random_state=42)\n\n    from sklearn.ensemble import RandomForestClassifier\n    \n    clf = RandomForestClassifier(n_estimators=100, random_state=42)\n    print('starting reshape...')\n    #clf.fit(X_train.shape[0], y_train)\n    X_rs = X_train.reshape(X_train.shape[0], -1)\n    print('starting fit...')\n    clf.fit(X_rs, y_train)\n    print(\"Classification results:\", clf.predict(X_test.reshape(X_test.shape[0], -1)))\n\nMy issue is that the code for classification, when it is run, the fit() function takes a lot of time and resources. It runs very long, more than a hour. The size of reshaped X\\_train is (4715, 196 608). \n\nDo you know if there are any better ways to train a classification model for this dataset.\n\n",
    "created_utc": "2024-11-17T02:25:10",
    "num_comments": 0,
    "comments": []
},
{
    "submission_id": "1gtam40",
    "title": "Microsoft released another Multi AI Agent framework : TinyTroupe",
    "selftext": "So looks like Microsoft is going all guns on Multi AI Agent frameworks and has released a 3rd framework after AutoGen and Magentic-One i.e. TinyTroupe which specialises in easy persona creation and human simulations (looks similar to CrewAI). Checkout more here : https://youtu.be/C7VOfgDP3lM?si=a4Fy5otLfHXNZWKr",
    "created_utc": "2024-11-17T02:16:06",
    "num_comments": 0,
    "comments": []
},
{
    "submission_id": "1gt88hc",
    "title": "domain specific knowledge for media domain",
    "selftext": "how do I get to learn domain specific knowledge of media domain while i'm building projects through self learning ",
    "created_utc": "2024-11-16T23:19:11",
    "num_comments": 0,
    "comments": []
},
{
    "submission_id": "1gt800g",
    "title": "[R] Help with expanding my deep learning knowledge",
    "selftext": "",
    "created_utc": "2024-11-16T23:01:51",
    "num_comments": 0,
    "comments": []
},
{
    "submission_id": "1gt7ch6",
    "title": "Multi AI Agent tutorials ",
    "selftext": "Multi AI Agent Orchestration is now the latest area of focus in GenAI space where recently both OpenAI and Microsoft released new frameworks (Swarm, Magentic-One). Checkout this extensive playlist on Multi AI Agent Orchestration covering tutorials on LangGraph, AutoGen, CrewAI, OpenAI Swarm and Magentic One alongside some interesting POCs like Multi-Agent Interview system, Resume Checker, etc . Playlist : https://youtube.com/playlist?list=PLnH2pfPCPZsKhlUSP39nRzLkfvi_FhDdD&si=9LknqjecPJdTXUzH",
    "created_utc": "2024-11-16T22:15:58",
    "num_comments": 2,
    "comments": [
        "AI agents are what's coming, I have been thinking about it, what should be studying right now in order to be there when the wave arrives.",
        "This playlist should be more than enough"
    ]
},
{
    "submission_id": "1gt6rxe",
    "title": "Resources that teach machine learning from scratch (python, numpy, matplotlib) without using libraries?",
    "selftext": "I see most students jumping directly into deep learning and using libraries like PyTorch. All that is fine if you are only building a project.\n\nBut, if you want to build something new, trial and error will only get you so far. Along with good engineering skills you need to get hold of the foundations of machine learning.\n\nComing to that, for someone who wants to get into the field in 2024-2025, what would be the best resource?\n\nMost resources I find starts using a library like scikit-learn from the beginning instead of asking students to implement the algorithms from scratch using numpy only. Also creating good visualisations of your results is a skill which pays a long way.\n\nI know of courses in deep learning that asks students to implement something from scratch like CS231N from Stanford or 10-414 DL Systems from CMU. Both are open with all materials. But where are similar courses for machine learning?\n\nI was disheartened with the ISL Python book too, when I saw that the labs at the back of the chapters all use custom libraries instead of building the algorithms with numpy and maybe compare them with scikit-learn implementations.\n\nAnyone know materials like this for classical machine learning?\n\nEdit: I don't know why this post is getting downvoted. I was asking a genuine question. Most courses I find are locked up behind login. And those that are open uses libraries.\n\nEdit 2: Maybe my thoughts came out the wrong way. I was not suggesting that everyone should implement everything from scratch always. I was just saying people, especially those who get into research should know how basic algos work under the hood and why certain design choices are made. There is always a gap between the theoretical formulae and how the things are implemented computationally. Atleast the essence of the implementation. Not making it super efficient like in a production grade library. Writing a SGD or Adam from scratch. Or implementing decision trees from scratch. Ofcourse you need good programming skills and DSA knowledge for that. There is no harm in knowing under the hood during the start of your journey.",
    "created_utc": "2024-11-16T21:38:19",
    "num_comments": 42,
    "comments": [
        "Hey it's great if you want to learn ml from scratch in order to understand how the algorithm works. I am adding a few of these resources you can choose based on your preferred teaching.\n\n1. https://github.com/eriklindernoren/ML-From-Scratch/tree/master\n2. https://github.com/patrickloeber/MLfromscratch/tree/master/mlfromscratch\n3. https://www.kaggle.com/code/milan400/machine-learning-algorithms-from-scratch\n4. https://github.com/AssemblyAI-Community/Machine-Learning-From-Scratch\n\nYT playlist \n1. https://youtube.com/playlist?list=PLcWfeUsAys2k_xub3mHks85sBHZvg24Jd&si=xNh75NFfeBcwWsmS\n(Their GitHub code is added above. Refer 4.)\n2. https://youtube.com/playlist?list=PLqnslRFeH2Upcrywf-u2etjdxxkL8nl7E&si=ywbv0JxTqdR5aq4d\n3. https://youtube.com/playlist?list=PLQVvvaa0QuDfKTOs3Keq_kaG2P55YRn5v&si=e7kg2jl_5oxyUK63\n4. https://youtube.com/playlist?list=PLfFghEzKVmjtcwDLnlueTPQDCVtq63KMf&si=pA9PLBUYoyzhTsaO\n5. https://youtube.com/@statquest?si=FFYYOA3q15M9JkHm\n(You can find the playlist of each algorithm where he explains the concept then implements them in python)\n6. Campusx\n7. Andrew ng \n\nI hope it helps you or anyone in the future:)",
        "I implemented a CNN from scratch in C++ without any libraries (except Vulkan for the GPU implementation).\n\nhttps://github.com/robjinman/richard\n\nI’m in the process of writing a blog post describing exactly how to do it. I’ll post it on Reddit when I’m done.",
        "It's a really good skill to understand what happens behind the scenes but imho you get diminishing returns the deeper you go. With that being said I don't understand the statement that pytorch is not good for creating something new. Almost all researchers use pytorch or equivalent libraries for building new DL models. No one is using pure numpy or cuda code for the same. At best someone will write custom pytorch modules with cuda code but that's pretty rare.\n\nAs far as resources for how to build models using just numpy there are a few of them if you search around. For example nanogpt uses just numpy to recreate a small scale version of the gpt architecture. Another option would be to implement your custom versions of scikit learn models and make it work with the existing for transform api design of scikit learn. Just going through its source code will give you lots of ideas on how you can structure your code",
        "I Absolutely recommend the book [Neural Networks from Scratch](https://nnfs.io/). It seems to be exactly what you're looking for, and how I got started myself.",
        "Andrew Ng Coursera course wrote the models from scratch",
        "> I see most students jumping directly into deep learning and using libraries like PyTorch. All that is fine if you are only building a project.\n\n> But, if you want to build something new, trial and error will only get you so far. Along with good engineering skills you need to get hold of the foundations of machine learning.\n\nIf you are someone who is early in their career, I definitely encourage you to develop strong fundamentals to build on top of.\n\nThat said: it's counter-intuitive, but actually for a lot of people the most effective route is to just jump in head first and try to keep up. This is based on my empirical observations as a career researcher/practitioner over the last 15 years in the field (and I myself have a graduate degree in math and stats). Many of the most successful and impactful researchers are people who just dived straight in and got their hands dirty.\n\nThis style of learning is a skill in and of itself and it certainly isn't for everyone. But it actually makes a lot of sense given the pace of AI research. Building up from a foundation means you will spend a lot of your learning trajectory well behind the state of the field. If it takes a year for someone to write a textbook, and another year for a given textbook to become popular enough to make it common in undergrad curricula, that means that in all likelihood: even a decent \"modern\" textbook's understanding of the SOTA will be about two years behind.\n\nBuilding up a strong foundation will make it easier to consume new information faster and will help inoculate you against bullshit, which further helps ensure that the information you are attending to is information worth your time. But if your goal is strictly to \"build something new\", it actually might make a lot of sense to figure out where the \"tip of the spear\" of pioneering research in your domain of interest is, and commit your energy to keeping a pulse on that. Learn how to be satisfied with understanding just the gist, and how to backfill the most important parts of what you need. Over time, you'll end up figuring out what your biggest gaps are in terms of your \"fundamentals\" and that will also make it easier to backfill those topics in a more targeted way.",
        "\"machine learning refined\" covers classic ml from scratch using python / numpy, pdfs of chapters available along with code --> [https://github.com/neonwatty/machine\\_learning\\_refined](https://github.com/neonwatty/machine_learning_refined)",
        "Campusx 100 days ml ytube",
        "Just started learning ML with this attitude and today I implemented the knn algorithm from scratch in python.",
        "https://d2l.ai/",
        "I think what you may be looking for is a textbook to follow. If you want to learn the mathematical theory, I would steer clear of machine learning courses online. \n\nAn Introduction to Statistical Learning is a good college level intro to the mathematical components of machine learning.",
        "I have been working on the filed for about 8 years, when I was learning I did implement from scratch, but librairies weren't that good or even popular back then, but nowdays I rarely needs to do that, it is getting to the point that I don't event memorize some of the basics. I recenly failed couple of interviews because of it lol, you are still expected to know the math and algorithms behind every functions.",
        "Ignore the other replies. \n\nThose book is 10/10, it helped me so much with intuition, it goes from the ground up gently \n\nhttps://www.amazon.co.uk/Grokking-Deep-Learning-Andrew-Trask/dp/1617293709\n\nPdfs of it are available for free if you know where to look.",
        "It is a good post",
        "Thank you so much for sharing these valuable resources. Since I am a beginner, this will definitely help me a lot.",
        "Nice",
        "For DL specifically if you use numpy or raw Python you'd have to implement your own backpropagation engine from scratch. You can look up Andrej Karpathy's zero to hero playlist for this (the first video on micrograd).\n\nFor ML algorithms more generally I remember implementing a few from scratch with numpy during my college's intro to ML course e.g linear and logistic regression, K means clustering. \n\nI think the Andrew Ng Intro to ML spec on Coursera does things from scratch but I'm not too sure.",
        "I don't how how to save posts so I gotta ward with this comment.",
        "Check out Jeremy Howard's fastai course, it shows you how to work without frameworks and why you should always use frameworks. Also fastai is written in a way to support understanding of what is happening, so you can \"look behind the scene\" a bit easier",
        "So I disagree with the premise. \n\"From scratch\" here, isn't from scratch at all.\n\nRe-engineering the wheel doesn't gain you anything, except an overinflated sense of entitlement. And in this case the \"entry point\" is incredibly far along the process that you are just demonstrating to everyone else that you don't know what's really important.\n\nTf/pytorch/keras/Jax are there to help you do the needlessly complex things and make them KISS... Start there.\n\nI don't go around doing assembly cause it's better than c or python...\nI don't go around doing my own car work because I need to get to work\nI don't go around milking a cow for a slice of pizza.\n\nEfficiency is there to be made use of, and sometimes you don't need to know the magic behind how they make the sausage.",
        "What about roadmap.sh and the path for different roles?",
        "The patrickloeber repo is what I am talking about. A ML course where the assignments make you build things like this and then compare the results of ypur models with some library models. Only correctness. Not efficiency.",
        "This is an awesome list, thanks!",
        "Thanks for the amazing list, bud!!",
        "This is the attitude I am talking about.",
        "Maybe my wording was wrong. I meant people are diving into DL without learning ML foundations. That statement was intended to mean that only. Nothing else. Sorry if it came out otherwise. \n\n  \nOfcourse DL researchers use libraries to implement new ideas.",
        "👍🏼👍🏼 the CS229 script and homework is publicly available.",
        "Amazing course indeed. I built a custom GAN when taking the advanced courses form [deeplearning.ai](http://deeplearning.ai) on tensorflow.  \n[https://colab.research.google.com/drive/1fUguZp0M2TzOgUUJW4ch2K0SIw2cJDBq#scrollTo=4Lrle3EIgYTj](https://colab.research.google.com/drive/1fUguZp0M2TzOgUUJW4ch2K0SIw2cJDBq#scrollTo=4Lrle3EIgYTj)  \nThis is the colab file for the custom GAN.",
        "Really great advice thank you",
        "Great to know!!",
        "Did you follow anything?",
        "Yes, I know this book. I was looking for a similar approach with classical ML.",
        "You should see a \"save\" icon in the options when you Click on the \"share\" icon of the post... (This is in android, not sure about iOS)",
        "But then you know the math of the algorithm and how a library calls it. That is great. But what if you want to know how it is implemented? Because maybe you want to tweak something or add something new to the algo that the library doesn't allow you to do. Then? That's why I am asking.",
        "Ah okay. If it's the mathematical foundations something like Andrew ngs deep learning course would work. Not sure if the videos are available freely on YouTube. As for the software engineer side of things it will be a bit harder to find resources online. And I don't think I've seen any course or tutorial that combines both. Your best bet would be to dive deep into those topics separately and just combine them on your own.",
        "Damn that's right! Thank you sm",
        "The thing is, the way algorithms are implemented nowadays is completely not accessible to you. It requires layers and layers of optimizations for multiprocess, multicore, multidevice, distributed, networked computations, on heterogenous hardware all the while showing an API that make it feel like everything is locally accessible as contiguous arrays.",
        "Most of those lib are open source, so you can go on their github and check the code",
        "\"I know the math\", Is not true.\n\nI don't and I don't care.\nIt's that simple.\n\nAt the core of all deep learning is matrix multiplication and gradient descent, but I don't see it and don't care.\n\nWhat I know is how to stack layers. And when to use a layer and what that provides me.\n\nThis is the big secret in ai/ml ... Its simple.\n\nIronically you even say it yourself, you want to use numpy to do the ai/ml cause you consider that classic. But in reality that does the same thing. It abstracts out a lot of things that are multi step tedium to do basic transformations ( np.where is a beast ) \n\nBut hey, keep gatekeeping yourself all you want. If you want to be elitist and take more time to get to the rainbow 🌈. You do you!\n\nI wish you well on your adventures.",
        "Yeah if you're trying to be SOTA, but OP obviously is not. OP just wants to learn fundamentals like gradient descent and whatnot from implementation.\n\nImplementing simple neural networks in a scripting language \"from scratch\" is not meant to be practical, its a teaching tool. It'll be slow and unoptimized, and it will lack features, and that's OK.",
        "Tell that to people who build and invent new architectures or algorithms that \"I don't care\".",
        "Gladly.\n\n#keras.io"
    ]
},
{
    "submission_id": "1gt6m0d",
    "title": "Navigating Career Choices: Should I Focus on ML Engineer Roles or Stick to Web Development First?",
    "selftext": "I am graduating in July 2025 and was initially planning to apply for web developer jobs in January 2025, with the goal of transitioning to data science after two years. I already have a web development portfolio ready. However, I’ve decided to delay my job search to April 2025 so I can focus on building a machine learning portfolio (I currently know ML but not deep learning). Can I realistically secure an entry-level ML engineer job with this plan, or should I stick to web development first?",
    "created_utc": "2024-11-16T21:27:24",
    "num_comments": 4,
    "comments": [
        "MLE is notoriously difficult to get into to and I don’t believe having a few months of personal experience is going to make you a competitive applicant. \n\nTo be honest, you’ll have to play a long game, imo. Learn on your own time, and attempt to get your hands on projects at work that allow you to learn more about the space and apply your own knowledge. This is a lot easier to do in a startup environment.\n\nOverall, people go to school for YEARS just to understand ML enough to get entry level MLE positions. I don’t believe a few months of personal projects will help. Stick to web dev, and make moves while working on that.",
        "Stick to full stack web development. MLE role has too much competition and low vacancies",
        "Stick to Web development, good ML portfolio is very difficult to build up in a few months. Web Dev has a much lower bar of entry.\n\nYou can continue to learn ML and build it up across the next few years and eventually transit, but it takes a lot of commitment and determination (and probably lots of weekends sacrified while working). MLEs are specialised SWEs anyway.",
        "What if I start as a data analyst and work myself up from there ??"
    ]
},
{
    "submission_id": "1gt3rlz",
    "title": "Help with a school project",
    "selftext": "Im doing a project for school where I'm researching AI and medicine and I want to try and create a machine learning that can like analyze heartbeats. like if I were to put in a heartbeat it would just say the risk or something. I want to use a dataset from smartwatch heartbeats as well\n\nI have no idea where to start and the project is due in about 3 months. I have no experience in compsci at all and am not really a coding girly either, I'm just really interested in the new age of medicine and learning abt it.\n\nI want this to be reallllllly simple so I hope something with expertise or any knowledge can give me some direction or help! \n\nI'm kinda intimidated cus of course wut is something like me doing in compsci territory just for a research project but I really want to go through with it. (feel free to tell me that I should prob do something else for my product)",
    "created_utc": "2024-11-16T18:41:06",
    "num_comments": 17,
    "comments": [
        "This would be \\*extremely\\* complex especially if you have no coding experience at all. Automatic diagnosis using AI is a major research topic being done by people with doctorates and a lot of experience. One of my  current projects is automatic diagnosis of neurological conditions. It is very challenging, and something I've been working on for nearly two years.  I don't think it is a realistic high school project. I would suggest something a lot more straightforward like simple image classification, which itself will not be easy if you have no coding experience.",
        "Cool, thanks for the insight. Besides image classification, what do you think I could do instead that would be a lot simpler to showcase AI in medicine?  \n\nThe whole thing about our research project is we need a product to go with it and my topic happens to be extra hard for someone who is more interested in medicine then actual compsci.\n\nalso ill look into the image classification as well.",
        "[deleted]",
        "I'm an AI researcher, and while I work in a medical context, I'm not an expert in medicine per se. The issue is that you have no coding experience. There are some machine learning libraries that are fairly easy to use if you have a bit of experience, and you can put something together, but you would need to learn at least some introductory Python. Your teacher is really expecting you to do a research project with a product? That seems a tall order for a high school project. Maybe ask them for some additional details on what they're looking for. Maybe they only want a mock up or conceptual plan? That's a lot more realistic. You could read some literature on the research being done in automatic diagnosis and create a non-functional mock up.",
        "Go away with your spam shilling.",
        "Boooo get your spammy advertising outta here",
        "oh yeah, i think a nonfunctional mock-up sounds like a good plan! Thanks!\n\nYeah, its not that my teacher is like well versed in this topic because everyone in my class has wildly different topics and I chose mine before I knew I would actually have to create a product at the end which led me to this predicament. they just want to make sure its a product that isn't something I can do over night or even over a week. its gotta be long term.",
        "ill spend some time researching into the library and maybe take a course on introductory python in my free time.",
        "Bruh, I’m not spamming you. Literally just saying what my platform does. I think it’s pretty cool, but thanks for the random hate",
        "It sounds like you're really passionate about, which is great. Keep learning, and who knows someday maybe you'll identify my future heart attack before it happens! :)\n\nI think start with a non-functional mock up and plan. And then you can add onto to that as needed.\n\nAll the best!",
        "This should provide you with some good information.\n\n[Computer-aided auscultation - Wikipedia](https://en.wikipedia.org/wiki/Computer-aided_auscultation)\n\nThe following may be a little advanced for you but skip the parts that are over your head and I'm sure you'll get some good information.  I think these all should be open source.\n\n[Automated diagnosis of cardiovascular diseases from cardiac magnetic resonance imaging using deep learning models: A review - ScienceDirect](https://www.sciencedirect.com/science/article/abs/pii/S0010482523004638#:~:text=Clinical%20methods%20such%20as%20blood%20tests%2C%20electrocardiography%20%28ECG%29,monitor%20the%20disease%2C%20plan%20treatment%20and%20predict%20CVDs.)\n\n[Automatic multilabel electrocardiogram diagnosis of heart rhythm or conduction abnormalities with deep learning: a cohort study - The Lancet Digital Health](https://www.thelancet.com/journals/landig/article/PIIS2589-7500(20)30107-2/fulltext)\n\n[Automatic classification of healthy and disease conditions from images or digital standard 12-lead electrocardiograms | Scientific Reports](https://www.nature.com/articles/s41598-020-73060-w)",
        "Look at your post history. The majority of your posts are this garbage.",
        "Oh wow thank you so much!",
        "Yes cause I’m literally trying to get traction for my business.",
        "Happy to help somebody interested in a future in the sciences.",
        "So it is spam shilling, exactly as I said. LOL",
        "Trying to make business preying upon high schoolers who probably don’t yet have debit cards does not look good",
        "I think spam shilling means something else. But happy for you to call it whatever! Okay cool, I’m doing that"
    ]
},
{
    "submission_id": "1gswph6",
    "title": "7 7bvvZwwaw",
    "selftext": "7 h cbsq",
    "created_utc": "2024-11-16T12:52:00",
    "num_comments": 22,
    "comments": [
        "Why is this downvoted? This guy is speaking the truth",
        "ugh... same tired old post. No getting a cert won't qualify you to run open AI.",
        "Same",
        "Been there bro",
        "Ugh, so true",
        "It’s bc international",
        "I know, right? It's so obvious!",
        "New name for Elon Musk's next child?",
        "Underfit, sorry. Few more epochs should do the trick",
        "Numbers station",
        "Oh so that’s what you got for question 7?",
        "Couldn't agree more.",
        "Have you tried 77xbww xe, my loss curve improved",
        "I recognize people like making jokes on Reddit, but this is actually a pretty serious issue, especially for beginners. Seeing 7 7bvv with anything following it is always a red flag for me and if unchecked could lead to deeper issues down the road.\n\nI always recommend at least 8 or even a 9 9cwxAvvev to avoid complications down the road.",
        "It's like he doesn't even get us man!",
        "good bot",
        "I tried running this but all it gives me is s-8*rT€A/\\P%!m. I was expecting something more like *)Jf€@💩YY^,\n\nAnyone else figure it out? Looks like it could be really useful but I think there’s a bug in one of the letters or something. ",
        "woah",
        "What’s the code about 😂or I’m the only one lost",
        "Is this the equivalent of butt texting?",
        "Trump? Is that you?\n\n/e: awww, some deeply hurt Trumpets here? I'm terribly sorry... /s",
        "is this a machine who tries to learn?"
    ]
},
{
    "submission_id": "1gswbr0",
    "title": "comprehensive resource for math?",
    "selftext": "hello guys i was wondering if there is any comprehensive resource and reference for Math topics i need to learn(I'm a self taught) any help is much appreciated ",
    "created_utc": "2024-11-16T12:33:58",
    "num_comments": 0,
    "comments": []
},
{
    "submission_id": "1gsw1l8",
    "title": "Optimizing Whisper Speed: CPU vs. AMD GPU?",
    "selftext": "Hi everyone,\n\nI’ve been using Whisper for transcription and love its accuracy, but speed is an issue for me. It takes around 40 seconds to process a 2-minute audio file on my setup. I’ve read about models (sometimes dubbed “tree-like models”) that can achieve this in just 5 seconds. Has anyone here tested or optimized such models?\n\nIdeally, I’d prefer sticking to CPU usage for reliability, but I’m curious if running Whisper on an AMD GPU could offer a significant speed boost. Anyone with experience on that?\n\nLooking forward to your insights and recommendations!",
    "created_utc": "2024-11-16T12:20:38",
    "num_comments": 1,
    "comments": [
        "Sell that AMD GPU and buy Nvidia GPU."
    ]
},
{
    "submission_id": "1gsuxw4",
    "title": "Decision theory in regression",
    "selftext": "Hi \nI am reading deep learning by Christopher bishop and have some questions about chapter 4.\nIn this section, how 4.36 comes from 4.35? I tried myself and got following result which is inconsistent with books result.\nWhat exactly happened to integrating over x?",
    "created_utc": "2024-11-16T11:29:24",
    "num_comments": 2,
    "comments": [
        "With the (informal) way you are calculating the functional derivative, you can think of it as differentiating $f(x)$ for each fixed $x$.\nSo $x$ is fixed and only \"occurs\" once in the integral w.r.t. $x$. All other $x' \\neq x$ are considered constant.\n\n(One can think of the integral as a summation with $x' \\neq x$, which implies $f(x')$ is constant w.r.t. your differentiation)",
        "Learn Integration. You have done it wrong!"
    ]
},
{
    "submission_id": "1gsux9l",
    "title": "Decision theory in regression",
    "selftext": "Hi \nI am reading deep learning by Christopher bishop and have some questions about chapter 4.\nIn this section, how 4.36 comes from 4.35? I tried myself and got following result which is inconsistent with books result.\nWhat exactly happened to integrating over x?",
    "created_utc": "2024-11-16T11:28:40",
    "num_comments": 2,
    "comments": [
        "This is not regular calculus, but calculus of variations. You can look at how it's done in the appendix.  \nBasically, the expected loss is a functional, which we want to optimize wrt. the function f(x):\n\nE\\[L\\] = int G(f) dx\n\nwhere I write G(f) for everything inside the integral wrt. x\n\nIn general, you do this by setting up an Euler-Lagrange equation, which is where the integral wrt. x vanishes. In this case, since the expected loss doesn't depend on derivatives of f(x), the Euler-Lagrange equation is just dG/df = 0, i.e. 4.36.\n\nYou can either just accept this requirement, or read the appendix to understand why this requirement looks this way, but it's a pretty rushed explanation: the functional (here the expected loss) must be stationary wrt. small variations in f(x), for any variation eta(x) - and this is true exactly for those functions f(x) that are solutions to the Euler-Lagrange equation.",
        "All i thought was “i know that page layout”. In comes the bishop war flashbacks"
    ]
},
{
    "submission_id": "1gsuvyu",
    "title": "\"it's statistics that will tell you when a model will work, why it will work, and why it will fail\"",
    "selftext": "I saw this quote in a r/statistics thread on calling ML \"just statistics on steroids\". Does the quote in the title suggest that traditional stats goes beyond the standard train/validation/test approach of ML? Does it make the ML approach \"worse\"?\n\nAre there more examples of the reduction of ML to just stats?",
    "created_utc": "2024-11-16T11:26:59",
    "num_comments": 15,
    "comments": [
        "I mean, for dense layer neural networks, it's theoretically just a lot of automated regressions (weights and biases).\n\nMay seem reductive to say that it is all statistics since deep learning incorporates several other fields of mathematics, but I don't see why not since the main layers truly are statistical tools running automatedly.",
        "ML builds on stats, if you don’t know stats, you’re guessing on ML. ML isn’t just stats though.",
        "Yeah I guess this makes sense. After all, neural networks are essentially just maximum likelihood estimators. \n\nEven with complex systems like GPT, at the end of the day, an LLMs softmax normalized next token prediction output per input token is a context dependent probability distribution across the vocabulary. Statistics is one of the most important things in ML by far.",
        "I think it can, but we just haven't developed simple tooling that can properly handle high dimensional inferences that the models seem to find. This statement is hard to argue for or against, to be honest.",
        "Lots of people thinking ML is just neural networks; not much better than thinking ML is just stats.\nAnyway, statistics is not enough enough to tell you why a model fails, much less why it succeeds, but it should be a greater presence in ML and NNs because it is still the best way to bet on a black box over another.\nBut boxes are mostly grey...",
        "No real explanation for the quote in the title, yet. My guess is that this is simply the experience of the writer and reflects the tasks they have to do.\n\nAs has been said, ML is much more than just neural nets. And much of that is statistics with computers. But with neural nets, calling that statistics only causes misunderstandings in my experience.\n\nStatistics is not where that comes from and is of limited help. If you look at the pioneering figures, you find a surprising (to me) number of psychologists, neuroscientists and a few physicists. There's surprisingly few mathematicans. More recently it's mainly computer scientists. But I can't think of any trained statistician who made major contributions.\n\nMathematically, neural nets can be seen as generalized regression. But that makes them statistics in the same way that biology is chemistry, as has been noted.",
        "ML is \"just stats\" like Biology is \"just chemistry\" or Engineering is \"just physics\". Like yeah, ML was founded in statistics, it uses some statistical methods and you should definitely know some. But there's a lot more to it than that.",
        "Thing is that traditional statistical bounds on number of train examples you need for a given model size is orders of magnitude larger than whatwe have. Statistics also cannot explain double descent and in a way, these tight statistical bounds where part of the reasons NNs were dead before Alexnet for more than a decade. ML is somewhat based on stats, but even the most advanced statistics can't explain what's going on with neural nets.",
        "It's never that straightforward with mathematics. Many mathematic branches share the same techniques, e.g., characteristic functions of probability density functions are equivalent to the frequency analysis in signal processing. An engineer with very shallow statistics knowledge could have come up with the convolution NN if they are familiar with image processing. While diffusion models stem from stochastic processes, a fluid simulation engineers with very little knowledge of statistics could have come up with diffusion models and flow-matching with their fluid mechanics knowledge.\n\nI think ML is a cumulation of knowledge from so many fields that you cannot really point to a single field as the main culprit of ML.",
        "How does ML explain double descent instead?",
        "> culprit\n\n😂",
        "They don't. We're not close to a theoretical understanding of modern ML like statisticians have for say linear regression.",
        "ML is for a large part an experimental science.",
        "Which does not answer the question nor address why I have I put it forward",
        "about \"double descent\" ? I don't know it well enough to propose an explanation. But for what I know this is not fully understood anyway.\n\nWhat I meant is this: ML does find some \"recipes\" for which there is no clear theoretical explanation. It is still useful even if we do not fully understand why it works.  And ML theory try to find explanations, but at best they are just partial explanation and mostly give ideas on how to tweak the recipes. We have no working theory proving that \"take this model and this number of samples and you will get chatgpt like behavior\".  But we have experimental evidence of what works to build it. \n\nSo to come back to the original question, this experimental part is I think a huge part of ML and is not really \"statistics\"."
    ]
},
{
    "submission_id": "1gsusp1",
    "title": "Path to become an MLE",
    "selftext": "So I’m a third year maths undergrad, who has completed a year in industry last year as a data scientist (mainly in research), and will be completing my MMath masters next year, which will involve me completing my thesis. During my year in industry, I was authored on two conference proceedings, where I was heavily involved in training and evaluating ML/DL models on medical images. However, I found the research industry to be quite underpaid, talking to other colleagues, and quite stressful, not outputting the reward, in terms of salary, that should be deserved. However, I am heavily interested and enjoy the domain of AI and ML, and I definitely want to pursue a career in it after graduating. \n\n An MLE would be something that I would like to become at some point, as I think I would prefer to build the models themselves, rather than to run the experiments and evaluate them. So I’m curious on how I should set my career path, and what I should do to become an MLE, given that I’m in uni still. I’m already pretty proficient in python, and am improving over time, and I have also been studying ML and DL through courses and textbooks along the way of my degree.",
    "created_utc": "2024-11-16T11:22:51",
    "num_comments": 2,
    "comments": [
        "Ask your professors if they have any ML projects they need help on and get involved in research.",
        "I already am authored on two conference proceedings and am doing an MMath project in ML this year for my degree. Would that not be enough? Since I don’t want to stay in the research domain anyway but still stay in ML / AI (I.e more corporate or MLE side)"
    ]
},
{
    "submission_id": "1gsup3g",
    "title": "Finding a learning buddy ",
    "selftext": "Hi, I’m Nilesh Singh, a beginner in Machine Learning with a strong interest in AI/ML and its potential for shaping the future of technology. I’ve started learning Python and basic ML concepts, and I’m eager to collaborate with someone to share ideas, solve challenges, and stay motivated together.\n\nIf you’re also starting out or at an intermediate level, let’s connect and learn together! Feel free to DM me if interested.\n",
    "created_utc": "2024-11-16T11:18:20",
    "num_comments": 1,
    "comments": [
        "Hello! I’m building a tool that makes it easier to research and try different ML architectures in an automated manner. I’ve used this to tool to take part in kaggle competitions where it’s been quite helpful, feel free to check it out: https://plexe.ai"
    ]
},
{
    "submission_id": "1gsu3x8",
    "title": "Suggestion: For Freelancing, What AI related course do you think is best for me to take now?",
    "selftext": "",
    "created_utc": "2024-11-16T10:52:17",
    "num_comments": 1,
    "comments": [
        "None of them."
    ]
},
{
    "submission_id": "1gstrok",
    "title": "Choose least loss or maximum accuracy on validation",
    "selftext": "Hello everyone,\n\nI am working on a binary classification problem of sequential data, i.e.,`(n_samples, n_channels, timesteps)`.I have implemented early stopping in a way that I save the best model which achieved the best validation accuracy. Sometimes the least validation loss corresponds to the best validation accuracy, but more often than not, I find cases such as the following where I could choose another epoch, where I got less validation loss, but it doesn't necessarily correspond to the best validation accuracy. In your projects, how do you go over model selection? Which model would you go for (in terms of the loss or the accuracy)?\n\nhttps://preview.redd.it/cbp1zmvt4b1e1.png?width=989&format=png&auto=webp&s=b08d4ab330b0a8f00703f7c28374e4cf373a6af0\n\n",
    "created_utc": "2024-11-16T10:36:55",
    "num_comments": 11,
    "comments": [
        "Your model is overfitting after around epoch 20.\n\nWhen the training loss keeps decreasing and your validation loss decreases then starts increasing, it's a classic example of when to implement **early stopping**.\n\nEarly stopping is when your model terminates training when the validation loss hasn't increased in a certain number of epochs (it'll be end up being a parameter that you set).\n\nAs a side note, if you're already reaching that point in only 20 epochs, try some slower learning rates or an adaptive rate. 20 epochs is pretty quick for a neural network and you may be able to find better outcomes with slower learning. That's not a guarantee though - try multiple learning rates and see what works.",
        "Why is your validation loss so weird? Something seems wrong in your training. Your model seems to be overfitting pretty badly. You might need to regularize or something\n\nThe anount of fluctuations in your metrics tell me choosing the epoch isnt your worst worry. You have bigger issues in your training",
        "Thanks for the insight. Indeed I am saving the model state so that I can take the lower loss model in the end when early stopping is triggered. I have tried a lower by a magnitude learning rate and the validation loss doesn't seem to decrease much while the validation accuracy is constant at 50%.",
        "Yeah, I agree... Though I'm already employing many techniques in my training loop and in my architecture to regularize. For instance, \n* Data augmentation (to also help me with class imbalance).\n* L2 regularization with weight_decay parameter in the Adam optimizer.\n* Dropout layers (before my LSTM and after in my classification layer).\n* LayerNorm (tried BatchNorm1D and it was worse).\nI'm also using gradient clipping to prevent exploding gradients and a learning rate plateau scheduler.\n\nIs there something else you recommend? My data has around a few hundred samples and its domain tends to overfit (EEG data) so I'm not expecting to be able to generalize so much that it achieves let's say 90% on test set. Around 70-80% would be huge already.",
        "How huge is your dataset?",
        "I have around 150 samples for some of my subjects. Others have max 500.",
        "That explains the volatility. Are you using pretrained weights?",
        "No, training from scratch",
        "I suggest use a pretrained backbone. Something like a resnet 18, trained on imagenet. Pytorch should have it in their library. I am not sure if it would work, because your usecase is very niche. But 500 examples will never actually teach much unless the network has learnt some information about what images look like.",
        "I'm not working on images though, I'm working with EEG data",
        "Hmm, then it just seems like your data is very very less. If you could find some open source datasets to pretrain your model it would be really useful (if youre using neural networks)\n\nSelecting best epoch is right now not necessarily a very important task, because your training overall needs debugging and work. Im not exactly sure how to help without running more experiments, but if youre working with deep networks, you need to resolve these graphs first"
    ]
},
{
    "submission_id": "1gst6ba",
    "title": "Linear regression charts on the web",
    "selftext": "Iv been using statsmodel with seaborn for understanding linear relation of my data points. I want to build a web app to exposes this modelling to other stakeholders in my company.\n\nI did some research and existing libraries like d3 and chartjs don’t support plotting graphs like seaborn. Any recommendations?",
    "created_utc": "2024-11-16T10:10:31",
    "num_comments": 1,
    "comments": [
        "Streamlit."
    ]
},
{
    "submission_id": "1gst0nz",
    "title": "Understanding scaling done by official repository of PatchTST timeseries transformer",
    "selftext": "I am trying to understand [PatchTST paper implementation](https://github.com/yuqinie98/PatchTST/tree/main) from its official github repository. It seem to be current state of the art time series transformer.\n\nThe dataset classes defined in its repo have following lines ([line 1-3 permalink](https://github.com/yuqinie98/PatchTST/blob/204c21efe0b39603ad6e2ca640ef5896646ab1a9/PatchTST_supervised/data_provider/data_loader.py#L59), [line 4-5 permalink](https://github.com/yuqinie98/PatchTST/blob/204c21efe0b39603ad6e2ca640ef5896646ab1a9/PatchTST_supervised/data_provider/data_loader.py#L78)):\n\n    train_data = df_data[border1s[0]:border2s[0]] # line 1\n    self.scaler.fit(train_data.values)            # line 2\n    data = self.scaler.transform(df_data.values)  # line 3\n    \n    self.data_x = data[border1:border2]           # line 4\n    self.data_y = data[border1:border2]           # line 5\n\nLet me explain a bit:\n\n- `border1s` array contains starting indices of train, test and val data splits and `border12s` array contains ending indices of train, test and val splits. So, `border1s[0]` is starting index of train split, `border1s[1]` is starting index of test split, `border1s[2]` is starting index of val split. Similarly,   So, `border2s[0]` is ending index of train split, `border2s[1]` is ending index of test split, `border2s[2]` is ending index of val split.\n\n- `border1` and `border2` are start and end indices of some specific split based on context. (Lets assume training split)\n\nNote that line 2 fits scaler to training dataset split and line 3 transforms whole dataset using same scaler. \n\n**Q1.** Why not fit to whole data set and only fit to training dataset split?\n\nNotice in line 4 and line 5, both input features `data_x` and targets `data_y` are exactly same values.\n\n**Q2.** How does it make sense to have even target scaled? (I felt only input features are standardized.) Wont this force model to learn to predict scaled targets instead actual / ground truth targets?\n\nIn all dataset classes, the paper seem to [always set](https://github.com/yuqinie98/PatchTST/blob/204c21efe0b39603ad6e2ca640ef5896646ab1a9/PatchTST_supervised/data_provider/data_loader.py#L78) `data_x` same as `data_y`.\n\n**Q3.** (Not related to scaling) What if I want input feature timeseries  different from target timeseries? That is values which I want to predict are different from values I want as input features? Should I still set `data_x = data_y = all columns` or I should `data_x` be just the input columns and `data_y` be just the target columns? (However Note that during training, it seem to separate out target columns out of predicted values to calculate loss [on line 172](https://github.com/yuqinie98/PatchTST/blob/204c21efe0b39603ad6e2ca640ef5896646ab1a9/PatchTST_supervised/exp/exp_main.py#L172).)\n",
    "created_utc": "2024-11-16T10:02:56",
    "num_comments": 1,
    "comments": [
        "Ask repository owner"
    ]
},
{
    "submission_id": "1gssigi",
    "title": "LGBM metrics are 0.0",
    "selftext": "Hi ,  \nI'm new to this field and I'm working on a simple fraud detection problem with the following class distribution:\n\n* Label 0: 142,900 samples\n* Label 1: 16,530 samples\n\nI am training a LightGBM model using Optuna for hyperparameter tuning. I ran the first trial, but the score (presumably accuracy or a similar metric) is 0.0. The preprocessing step has been done correctly, and the data seems fine.\n\nI'm unsure why this is happening. Has anyone encountered a similar issue? Any advice on what might be causing this or how to troubleshoot it?\n\nThanks in advance for your help!\n\n    def objective(trial, X, y):\n        params = {\n            'objective': 'binary',\n            'metric': 'binary_logloss',\n            'boosting_type': 'gbdt',\n            'verbosity': -1,\n            'max_depth': -1,\n            'learning_rate': trial.suggest_float('learning_rate', 0.005, 0.01, log=True),\n            'num_leaves': trial.suggest_int('num_leaves', 400, 500),\n            'feature_fraction': trial.suggest_float('feature_fraction', 0.3, 0.6),\n            'bagging_fraction': trial.suggest_float('bagging_fraction', 0.4, 0.7),\n            'min_child_weight': trial.suggest_float('min_child_weight', 0.01, 0.1, log=True),\n            'min_data_in_leaf': trial.suggest_int('min_data_in_leaf', 50, 150),\n            'reg_alpha': trial.suggest_float('reg_alpha', 0.1, 1.0, log=True),\n            'reg_lambda': trial.suggest_float('reg_lambda', 0.1, 1.0, log=True),\n            'random_state': RANDOM_STATE\n        }\n    \n        NFOLDS = 5\n        folds = StratifiedKFold(n_splits=NFOLDS, shuffle=True, random_state=RANDOM_STATE)\n        columns = X.columns\n        splits = folds.split(X, y)\n        y_oof = np.zeros(X.shape[0])\n        score = 0\n    \n        for fold_n, (train_index, valid_index) in enumerate(splits):\n            logging.info(f\"Processing fold {fold_n + 1} of {NFOLDS}.\")\n            X_train, X_valid = X[columns].iloc[train_index], X[columns].iloc[valid_index]\n            y_train, y_valid = y.iloc[train_index], y.iloc[valid_index]\n    \n            train_data = lgb.Dataset(X_train, label=y_train)\n            valid_data = lgb.Dataset(X_valid, label=y_valid)\n    \n            model = lgb.train(params, train_data, num_boost_round=1000, valid_sets=[valid_data], \n                              callbacks=[early_stopping(stopping_rounds=50)])\n    \n            y_pred_valid = model.predict(X_valid)\n            y_oof[valid_index] = y_pred_valid\n            fold_f1 = f1_score(y_valid, [1 if pred > 0.2 else 0 for pred in y_pred_valid])\n            logging.info(f\"Fold {fold_n + 1} | F1 Score: {fold_f1}\")\n            score += fold_f1 / NFOLDS\n    \n        logging.info(f\"Mean F1 Score = {score}\")\n        logging.info(f\"Out of folds F1 Score = {f1_score(y, [1 if pred > 0.2 else 0 for pred in y_oof])}\")\n    \n        return score\n    \n    if __name__ == \"__main__\":\n        train_df = pd.read_csv(DATA_PATH, encoding='utf-8')\n        X = train_df.drop(columns=['isFraud'])\n        y = train_df['isFraud']\n    \n        X = preprocess_data(X, MODE, DIR)\n        X_clnd, dropped_features = drop_corr_features(X, threshold=0.95)\n        \n        X_scaled = scale_features(X_clnd)\n        X_scaled_df = pd.DataFrame(X_scaled, columns=X_clnd.columns)\n    \n        X_train, X_test, y_train, y_test = train_test_split(X_scaled_df, y, test_size=TEST_SIZE, random_state=RANDOM_STATE, stratify=y)\n    \n        study = optuna.create_study(direction='maximize', study_name='maximize_auc')\n        study.optimize(lambda trial: objective(trial, X_train, y_train), n_trials=N_TRIALS)\n    \n        best_params = study.best_params\n        logging.info(f\"Best Hyperparameters: {best_params}\")\n    \n        final_model = lgb.LGBMClassifier(**best_params)\n        final_model.fit(X_train, y_train)\n    \n        y_test_pred = final_model.predict(X_test)\n        f1 = f1_score(y_test, y_test_pred)\n        precision = precision_score(y_test, y_test_pred)\n        recall = recall_score(y_test, y_test_pred)\n        roc_auc = roc_auc_score(y_test, final_model.predict_proba(X_test)[:, 1])\n        \n        cm = confusion_matrix(y_test, y_test_pred)\n        logging.info(f\"Confusion Matrix:\\n{cm}\")\n    \n        logging.info(f\"F1 Score: {f1}\")\n        logging.info(f\"Precision: {precision}\")\n        logging.info(f\"Recall: {recall}\")\n        logging.info(f\"ROC AUC Score: {roc_auc}\")\n\n2024-11-16 19:27:35,892 - INFO - Confusion Matrix:  \n\\[\\[28580 0\\]  \n\\[ 3306 0\\]\\]  \n2024-11-16 19:27:35,892 - INFO - F1 Score: 0.0  \n2024-11-16 19:27:35,907 - INFO - Precision: 0.0  \n2024-11-16 19:27:35,907 - INFO - Recall: 0.0  \n2024-11-16 19:27:35,907 - INFO - ROC AUC Score: 0.49814946698688517",
    "created_utc": "2024-11-16T09:40:07",
    "num_comments": 0,
    "comments": []
},
{
    "submission_id": "1gsqw63",
    "title": "Should KL divergence increase or decrease for training a VAE?",
    "selftext": "I've made a VAE, and while it's constructions look defend (worse than a normal AE, but I'll try to get a discriminator to fix it), but the issue is that the KL loss is increasing over training, resulting in decent images, but the latent space is still pretty bad, as in, changing one variable changes (slightly) some random parts of an image, same as in a normal AE, with KL going from 1.1 up to 400 over 10k batches (16 batch size).\n\nThis is my KL divergence formula:\n\nkl = mean((zLog + 1 - zMean**2 - exp(zLog) - 1) * -0.5)\n\nOr in code form:\n\nconst kl = zLogVar.add(1).sub(zMean.square()).sub(zLogVar.exp()).sum(-1).mul(-0.5).mean().mul(this.beta)\n\n(Beta always being 1 for now)\n\nAnd making the training loss be reconstruction.add(kl)",
    "created_utc": "2024-11-16T08:26:33",
    "num_comments": 7,
    "comments": [
        "In VAEs we usually minimize the negative ELBO instead of maximizing since for most optimization problems it is more convenient to solve a minimization problem. That being said, the KL term measures the deviation from the learned posterior and the prior. This is why you want the KL term to be as low as possible. In the context of the ELBO, this works as a regularizer which kind of keeps your latent space in check. I suspect that if your KL is increasing, it might be an indication of your latent representation overfitting to the training data which leads to low generalization.",
        "I'm pretty sure now that KL should be maximized (Since ELBO is just reconstruction - KL, and since we need to minimize ELBO as a loss function reconstruction needs to be minimized and Kl Maximized), it still doesn't answer why my latent space is so bad, I'll try to get some total correlation loss implemented to fix it",
        "Is there any way to fix the generalization issue? I use batch norm religiously, my precious AEs didnt overfit, only this VAE does, only after 6 batches with around 16k images",
        "maybe im missunderstanding, but KL can be broadly considered as a measurement for divergence. If you want similarity your KL divergence should be lower.",
        "It’s kind of hard to say without seeing your implementation, but I would say double check your KL formula, if all the terms (like log var) are computed correctly. After that maybe check that the assumptions for the prior are okay, otherwise some hyperparameters like the learning rate could maybe also help.",
        "Thanks, but if I try subtracting KL (since it's negative, subtracting will make it lower), the KL loss goes to infinity pretty fast, but if I add (so maximizing) it goes up to 400 and stays there",
        "You can see my implementation over at https://github.com/JijaProGamer/BS-Environment-Compressor > page/vae.js\n\nI believe my prior is good, and my LR is 0.0005, and currently my KL divergence is negative, and I'm adding it to the reconstruction loss, resulting in minimizing KL divergence, but if I try to subtract it to maximize KL divergence my KL and TC losses jump though to infinity (and TC to -infinity) extremely fast"
    ]
},
{
    "submission_id": "1gsqt3k",
    "title": "Any idea what is this drop in validation loss? I'm not changing the learning rate, and I don't use a scheduler",
    "selftext": "",
    "created_utc": "2024-11-16T08:22:33",
    "num_comments": 20,
    "comments": [
        "Could be that the loss escaped a local minimum",
        "Just jumped from one local minimum to another, better one. Happens all the time—maybe not on such a scale, but it’s not unusual.",
        "This is the loss of a UNet trained with AdamW, constant learning rate, the loss is a sum of Dice loss and Focal loss (same loss as SAM/SAM2)",
        "I've seen this happening for even two layer MLPs. Seems to happen consistently for various models.",
        "Your network just learned something that was pretty difficult for it. Look at predictions before and after that drop, could be that it learned to predict a new class.",
        "Possible leakage between train & valid set?",
        "Which Dataset? Which Model?  \nWhy do people think they  can just post random loss curves and they will get a good explanation for why some stuff behaves in a certain way? Give us some context please. Some of the other comments are possible solutions, but with the information we have we can really only give you a pretty random guess.",
        "It is normal.",
        "Hey, I don't know the reason for it but it happens to me and my colleague as well, and I've just seen it in a supplementary materials for a paper in Science. It seems to be common for LLMs in my experience.",
        "Yup you just got lucky",
        "This is due to the dice loss. The formula is 1 - (2 \\* intersection + eps) / (sum + eps). You probably have a few samples in your dataset where on segmentation class doesn't exist on that sample, meaning intersection will always be 0. Therefore the loss for that class will be 1 - eps / (sum + eps). Since epsilon is usually very small, i.e. 1e-5, as your model learns to decrease the sum, the loss decreases extremely quickly. For example if sum is 1e-4, loss would still be very close to 1. But as it passes 1e-5, it suddenly jumps to 0.5.",
        "> AdamW\n\nany specific reason to use AdamW instead of Adam ?",
        "Shouldn't be possible, train data is synthetic while validation is real data",
        "Model is a UNet from the segmentation_models.pytorch library, with resnext backbone. Dataset is private but fairly standard for an image segmentation task\n\nI asked about this because I'm using a fairly standard setup, so I was curious if this was a known phenomenon",
        "I observed the same phenomenon on multiple seed though",
        "Oh that's a very good point, I'll log the individual loss component values to check that!",
        "What's the reason behind that choice? And how are you generating training data then? Id assume you are sampling from a distribution which you modeled on the validation so there's leakage that way potentially.\n\nBut likely that wouldn't be the issue in this case (I'd expect such leakage to result in similar train and valid loss graphs). But it's difficult to say more without seeing the training loss as well. Does it drop similarly?\n\nHow are you defining your validation set, and how are you calculating loss? Are you running k-fold CV?",
        "Then what's common in them",
        "The train loss does have a similar pattern actually, but less sharp (the step counts don't match but that's a problem with how I log I think): https://imgur.com/9uOKneo\n\nThe reason for using synthetic data for trainingis that I don't have a lot of real data (it would take a lot of time to annotate) so I'm generating them. FOr the validation set I use real data because that's what I want to know ultimately\n\nBoth train and eval loss are computed on a few batches",
        "How are you generating the synthetic data though? If it follows the same distribution as you validation then you're not going to get an informative loss curve (or model). Also you'll be better off plotting both train and validation loss on the same graph for comparison.\n\nAugmentation for training datasets is fine, but you should definitely have some real data points in your training set."
    ]
},
{
    "submission_id": "1gspxnf",
    "title": "Best Machine Learning Courses on Udemy",
    "selftext": "",
    "created_utc": "2024-11-16T07:42:37",
    "num_comments": 0,
    "comments": []
},
{
    "submission_id": "1gsp6a8",
    "title": "Buying a laptop for AI/ML and app dev.",
    "selftext": "P: Hello ML enthusiasts. I am new to ml and I am planning on buying a new laptop preferably hp.  I want to buy one under 1000 USD (82000 INR) and I am not sure on what processor to choose. [https://www.hp.com/in-en/shop/victus-gaming-laptop-15-fa1415tx-ar2b5pa.html](https://www.hp.com/in-en/shop/victus-gaming-laptop-15-fa1415tx-ar2b5pa.html) This is one of my choices but I don't want to regret it later, so please help me. I know intermediate python (pandas, numpy) and will move to scikit learn next. My main interests are ML, 2d game/app dev. Sorry if this is not the right subreddit I don't know where to ask.",
    "created_utc": "2024-11-16T07:06:14",
    "num_comments": 5,
    "comments": [
        "none of this will matter. you will connect to the internet and run everything remotely. a half decent cpu will handle any thing you will need for local data prep",
        "Brother, get any laptop with 32GB RAM and 512GB nvme ssd storage. Use kaggle, colab GPUs for training. Run docker, conda environments, vscode locally.",
        "Mac book 🔥",
        "Laptop and performance don't go together, at least not in an economical sense.",
        "Hp sucks. randomly not working mic after some update. Also, hp produce gready printers. Do you want to pay monthly subscription on laptop?\n\nUpd you do not need videocard. 4gb won't help for any reasonable task. Better to get better cpu."
    ]
},
{
    "submission_id": "1gsoi0o",
    "title": "Recommendation system",
    "selftext": "I'm working on a real estate recommendation system trying to mimick tiktok's robust algorithm. One central feature is that a client must get a near-precise location in which case I'm using H3 for spatial indexing. I realized Rtree also does well. The system is composed of houses to let and lands on sale. I'm thinking of how to optimise the system to return accurate results in milliseconds.on houses the database has location, bedrooms, agent, price, etc parameters while land has location, size, cost and description parameters. What would be the appropriate approach to optimise such a system",
    "created_utc": "2024-11-16T06:33:14",
    "num_comments": 14,
    "comments": [
        "I had built a system in the past that used phone GPS. Here you get 5m accuracy of the person. Why would that not be sufficient for your recommender where by definition of the problem (real estate), there are only a few hits in a 5km radius for most places around the world? Even in high-density places like Tokyo, New York, or Hong Kong, you would not have a situation where not a lot of people are constantly selling.H3 exists because Uber drivers need to deliver with high precision since this is tightly correlated with their earnings and customer satisfaction. In your case that might not be the case. \n\nAlso, is that really how your ideal customer would search for a new place? Normally if you do this, you are prior to moving to a new location which, I would hypothesize, is usually quite far from where you are currently living. Of course, your lease might be ending and you want to stay in that area, but I'd argue to look further into your user journey before building it.\n\nThirdly, even if you use H3 and not a conventional geospatial index, usually a simple K-Nearest Neighbor does the trick to get a short list of real estates nearby.",
        "GNN",
        "I'm already using chatgpt pro to extract key info from the users query such as land size, cost, location and the rest I use embeddings. Same case to houses",
        "The challenge has been balancing the property type, cost, location, agent type and other use parameters",
        "There’s a book by manning called recommend systems check it out",
        "It's a standard SQL so I'm thinking of how to optimise a semantic search",
        "How would GNN fit into such a set up?",
        "Can you provide more context",
        "User queries are like: I'm looking for a 2-bedroom house in location xxx for cost yyy per month in location zzzz with abcd features. So I use chatgpt to extract location,no. of bedrooms, and other crucial parameters such as gated community, etc",
        "This is a good suggestion. I'll try it and update",
        "Are you planning to survey the user in some way to get an idea on what is important to them personally?  This info would be the foundation for balancing the value of each parameter. If your not personalizing it to the user then the recommender is flawed by design.",
        "Thanks I'll check it out",
        "You can just perform a search on top of SQL once you extract out the features? Columns such as number bedrooms, bathrooms, latitude, longitude will help you narrow down the query before you re-rank to decide what to show users first.",
        "I see. How do you think about the backend? Do you want to store all your real estate entires in a LanceDB instance and then do a semantic search or is it a standard SQL instance where a function would translate GPT-Data"
    ]
},
{
    "submission_id": "1gsnjc0",
    "title": "Best Beginner ML website/materials for someone of my background",
    "selftext": "Hi,\n\nSo I've been looking to get into applied ML, specifically for time-series forecasting for financial markets.\n\nI have a finance/econ background (undergrad) and is somewhat familiar with time-series stuff (ARMA, multivariate GARCH, etc.). I'm interested in getting to know more about some of the practical ways to implement ML for eg. Real-time forecast of macro variables/financial data. I just heard about LSTM and RNN recently and would loove to be able to implement them via python after a reasonable amount of self-study. In terms of timeline, I'm able to dedicate a few months to hopefully systematically go through all the ML concepts, but I don't know where to start.\n\nI see [machinelearningmastery.com](https://machinelearningmastery.com/start-here/) offers a LOT of related content. The sheer amount of material looks abit overwhelming tbh. Would be helpful if someone can share their experience with the website (relevent to my learning objectives), I don't mind the cost for their books as long as they are a good fit for someone like me.\n\nI also see some recommendations for [d2l.ai](https://d2l.ai/index.html) (free). To me the content does not seem to fit my purposes, and the materials relevant to me (eg. [chapter\\_recurrent-modern/lstm](https://d2l.ai/chapter_recurrent-modern/lstm.html) ) looks a bit foreign, as in they seem to be more computer science focused rather than applied finance, if that makes sense.\n\n**TLDR:**\n\n* Undergraduate finance background\n* Self-learning\n* Looking for efficient ways to quickly get my hands dirty with ML time-series forecasting (**\\~3 months**)\n* Ideally the materials need to incorporate hands-on coding examples (Python or R) that are finance/econ focused and not computer science focused\n* **Learning outcome:** Be able to understand the **high-level concepts of time-series ML** (ie., beyond just mindlessly applying packages) and able to **manipulate and adapt relevant models under different financial scenarios.**\n* Currently more leaning towards [machinelearningmastery.com](https://machinelearningmastery.com/start-here/)\n\nAny input is appreciated!",
    "created_utc": "2024-11-16T05:43:13",
    "num_comments": 4,
    "comments": [
        "If you are into time series checkout Rob Hyndman's blog and books: \n\n[https://otexts.com/fpp3/](https://otexts.com/fpp3/) \n\n[https://robjhyndman.com/hyndsight/](https://robjhyndman.com/hyndsight/)",
        "Thanks! This seems to be a great resource for time-series. However I’m more leaning towards the ML aspect (neural networks) of time series, so perhaps it is beyond the traditional time series textbook",
        "Then check [https://pytorch-forecasting.readthedocs.io/](https://pytorch-forecasting.readthedocs.io/)\n\nGluonts: [https://ts.gluon.ai/stable/](https://ts.gluon.ai/stable/)\n\nNixtla: [https://www.nixtla.io/](https://www.nixtla.io/)\n\nUber Orbit: [https://www.uber.com/en-IN/blog/orbit/](https://www.uber.com/en-IN/blog/orbit/)\n\nIf you wish to learn time series from Industry implementations perspective: [https://edu.machinelearningplus.com/s/pages/forecasting-expert](https://edu.machinelearningplus.com/s/pages/forecasting-expert)",
        "Thank you! Will check them out"
    ]
},
{
    "submission_id": "1gsn9c9",
    "title": "\nHow to systematically learn in the field of machine learning",
    "selftext": "I am a junior learner in the field of machine learning and currently a sophomore undergraduate student in computer science and technology. I have read some materials on deep learning, such as Mu Li's \"Dive into Deep Learning\", as well as some classic papers such as \"Attention Is All You Need\" and \"Very Deep Convolutional Networks for Large Scale Image Recognition\". However, through these studies, I have found that I only understand some of the backbones, but my knowledge is fragmented and lacks a systematic understanding of the machine learning field. How can I learn to gain a more comprehensive understanding of the machine learning field? What textbooks or learning materials are recommended?",
    "created_utc": "2024-11-16T05:28:09",
    "num_comments": 4,
    "comments": [
        "Theres a textbook titled \"Deep Learning\". I recommend going through that real slow for a great foundation",
        "The best option today is the complete data science pathway by ML+. [https://edu.machinelearningplus.com/s/pages/roadmap](https://edu.machinelearningplus.com/s/pages/roadmap)",
        "Learn numerical linear algebra"
    ]
},
{
    "submission_id": "1gsn6yq",
    "title": "Perplexity AI PRO - 1 YEAR PLAN OFFER - 75% OFF",
    "selftext": "As the title: We offer Perplexity AI PRO voucher codes for one year plan.   \n\nTo Order: https://cheapgpts.store/Perplexity\n\nPayments accepted:  \n\n- PayPal. (100% Buyer protected)  \n- Revolut.",
    "created_utc": "2024-11-16T05:24:26",
    "num_comments": 0,
    "comments": []
},
{
    "submission_id": "1gsn2ds",
    "title": "Building a Document Verification ML Model: Need Help!",
    "selftext": "I'm building an ML model to verify documents (passports, IDs, etc.). I'm exploring computer vision and OCR techniques.\n\nKey challenges:\n\n \\* Data acquisition\n\n \\* Model selection and training\n\n \\* Deployment\n\nNeed help with:\n\n \\* Data augmentation\n\n \\* Feature engineering\n\n \\* Model architecture\n\n \\* Training optimization\n\n \\* Deployment strategies\n\nAny advice, code snippets, or resources are welcome!\n\n\\#MachineLearning #ComputerVision #DeepLearning #AI #ML #Python",
    "created_utc": "2024-11-16T05:17:26",
    "num_comments": 0,
    "comments": []
},
{
    "submission_id": "1gsm1cv",
    "title": "Deep Learning from Scratch to GPU - Part 0 to 17",
    "selftext": "",
    "created_utc": "2024-11-16T04:14:42",
    "num_comments": 0,
    "comments": []
},
{
    "submission_id": "1gsln6h",
    "title": "question about mediapipe",
    "selftext": "I just used mediapipe for my final year college projects, is it fine? ",
    "created_utc": "2024-11-16T03:48:20",
    "num_comments": 0,
    "comments": []
},
{
    "submission_id": "1gsknvh",
    "title": "How to Transformer using Pytorch/Fax/Tensorflow from scratch\n",
    "selftext": "i want a github repository which have prebuilt code of transformers using any library and want it need to run the llms model locally by any weights format like\n\n.ckpt - TensorFlow Checkpoints\n\n.pt, .pth - PyTorch Model Weights\n\n.bin - Hugging Face Model Weights\n\n.onnx - ONNX Model Format\n\n.savedmodel - TensorFlow SavedModel Format\n\n.tflite - TensorFlow Lite Model Format and .safetensor hugging face\n\nall these format with its tokenizer and vocab but note i am not talking about huggingface lib transformer but want to local one like that using the above i know some like mingpt/nanogpt and some repo but i want better one please recommend me any repo",
    "created_utc": "2024-11-16T02:36:35",
    "num_comments": 0,
    "comments": []
},
{
    "submission_id": "1gsk5ld",
    "title": "I have been applying for my first machine learning full-time job in Germany for past 4-5 months, but now I have just graduated and I am still not getting a single e-mail for next round. I would really appreciate feedback on my resume. I am mostly applying for CV or MLOps roles but also ML/AI Eng/Dev",
    "selftext": "",
    "created_utc": "2024-11-16T01:57:54",
    "num_comments": 76,
    "comments": [
        "I’ll tell you right now: A2 german is 80% of the reason, other 20% is stagnant economy. \n\nWhen I was applying for engineering positions in germany, somehow I forgot to put languages section. As a result, I didn’t receive many callbacks. Then I realized my error and put B2 german in there (in croatia we study german in HS), and suddenly I started getting calls for interviews… \n\nMy genuine advice is get on a german course and get a B2 certificate. Your life will be so much easier.",
        "The German economy is in a bad place right now and there is so much political and economic turmoil that I would consider seeking jobs in a different country if that is possible.",
        "If you claim you know German, prepare a CV in German. I'd suggest a sans serif font. Also, in Germany it is normal (although technically not required) to add a photo to your CV.\n\nMy CV is not separated into professional and education, but organized chronilogically from recent to least recent. You are not restricted to one page, e.g. in my CV I would add additional pages with references and excerpts from papers.\n\nAlso, you are required/strongly expected to add a cover letter (Bewerbungsschreiben) in the usual DIN format for letters. If you didn't so far, there you have the reason why you didn't get any replys.",
        "Your resume format is not common in Germany. In Germany they expect picture and multi page format.",
        "Other concerns aside, I find this CV very unpleasant to look at, from a purely visual standpoint.\n\nIt's a lot of tiny text cramped onto one page. If i opened this in an application e-mail, I would already dread reading it.\n\nTry trimming down the word salad as much as possible. For example, given your track record, it's not really worth mentioning that you know basic ML stuff like NumPy. Personal gaming projects can also be omitted, unless that has some bearing on the position you apply to.\n\nAlso you spend almost 25% of that page on single year spent as a working student. Try cutting that down.",
        "Any entry level position will require coding and your CV is focused on theory. Contribute to some open source projects or showcase your own projects via an app or a website and your CV will stand out.",
        "I was actually hiring some positions in DS/ML in Germany in the past (most of them were international students, so you can ignore the racist tones in the other answers). \n\nMy experiences are, that there are tons of students doing stuff, which sounds advanced, but lack basic skills and business insights, especially when they come from a university of applied sciences. You need to stand out from this.\n\nYour CV looks not appealing: Nothing stands out visually, a lot of unnecessary text. Add some colors, Add a header with your motivation and strength (very short), remove small projects and only list real work experience: That's what counts. \n\nI hope, that helps",
        "Personally I wouldn’t list your courses. “Collaborated in the team of..” is unnecessary. No links to the mentioned projects? You can make a GitHub Pages portfolio website with free hosting or even just links to the repo with a decent readme.md.",
        "The German economy is shit, none of the jobs that were supposed to come through the \"green revolution\" actually materialized. China is kicking the local car industry in the nuts and they totally slept on AI and other emergent technologies. Cariad has outsourced their hiring so they can't stop complaining that they are suffering from \"Fachkräftemangel\" without actually doing anything against it. Aleph Alpha or whatever their dumb name is never sourced the funding they need to build models and now they are in the same spot as [Sakana.AI](http://Sakana.AI) and to a lesser extent Mistral that they are engineering has been that don't really matter.  NewSpace once heralded as the new German engineering boom is also going into the drain.",
        "Nobody is getting call backs",
        "I'm in the US and was hiring for a statistician role. I got ~300 applications in less than a week.   \n\nA full 1/2 were machine learning focused candidates and they ALL had the same exact format resume as you. Exact same format. Even the exact same skills section in terms of the order of the technical skills.\n\nThe only thing they had that you haven't done seems to be to **bold** key words and impact numbers (reduced time to complete task by **X%**)\n\nPersonally, I would give reformatting your resume a try. If I'm a hiring manager or a recruiter or whatever, each resume is getting, at best, 30 seconds of my time on my first pass. Probably even less. If your resume looks like the other 200 I'm looking at, you're probably not getting any sort of extra time/review.",
        "If I'm recieving your CV and you have the advanced degree I'm going to be assuming you probably have the maths and (theoretical) technical skills and your CV reinforced that \n\nWhat I'm not seeing are any soft skills that are important in business. Presenting, playing with others nicely, getting people excited and invested in your ideas etc",
        "Look for graduate entry roles, and internships.",
        "Build your own stuff and post online",
        "Too much text. Just assume that people will only glance at it at most. Give them keywords and highlights mostly",
        "Computer Vision isn’t the best focus area for industry. CV is massive topic in academia, but doesn’t have as large of a job market as tabular data and NLP.\n\nUnfortunately, as a hiring manager, I have found that people who specialised in CV quite often don’t master fundamentals of ML outside of the vision domain. Which is what makes me skeptical of CV focused resumes like yours.",
        "Look at big tech companies offering internships (as they're usually paid decently) in other EU countries. For example Databricks.",
        "Have AI redo your resume",
        "Do people actually care about language certificates? This guy can drive half of graduates out of a job? Bro just gotta keep husslin, his resume looks good.",
        "Realistically this is the wrong time to be entering the industry. It’s very saturated with graduates",
        "Man tbh hiring devs from EU and US with EU and US salaries is just too expensive, they won't bother hiring you until they can fir sure get the value our of you, hiring a recently graduated is just too risky. \nIf they can outsource some they will to 3rd world countri pricing, and if they have to hire local they will only hire the best of the best that can pull their weight 3 times or more I'm talking about the most reliable and well rounded engineers  (the ones that keep things in line for the stakeholders) and the decision makers. \nRecently there were layoffs in the big tech companies and in the company I work for the recruiting just went higher those jobs just are being transferred to my people the low wage 3rd worlders.\n\nAlso i see you don't have any relevant experience just academical which differs a lot from real world.\nI would say just go for some data entry job and then grow from that. There is no such thing as Junior Data Scientists or Junior MLE.",
        "Are you asking the people in your network to refer you\nThat is the best way to stick out in all the applicants applying",
        "Ok I will write the same as many comments but: If you wanna work in a German company, you either give them a German CV or have A2. Berlin and Hamburg in heydays do take English speakers but.... yeah just go full german! ANGRIFF !",
        "Your CV is fine, also I think that you are great at what you are doing, the problem is that there are thousands of people of you and very few positions.\n\n  \nI think that the issue is actually universities selling masters degrees and phds like you will have a great job after you graduate, but is a scam. Some will tell you it's your CV but I'm sorry budy, your CV is impressive. \n\nI would just try something else, like a heavy data project.",
        "You graduated  in one of the worst economies of the modern world, you will be hired, give it time and don't give up! Keep building your resume, add projects that would be impressive to show you can do work you claim to know",
        "Nothing you listed would exactly count as \"experience.\" Also, isn't India a 10-point GPA system? What do you mean 2.1 and 1.8? Are they on a different scale?",
        "The reason is that your CV is not in German and your skin is probably a bit too dark for Germans. They won’t admit it of course, but as a foreign living in Germany for over 15 years I can tell you they will always look down on people coming from latitudes to the south of them… ESPECIALLY if their skin color is brown. \n\nTry in a less racist country.",
        "Also, to add, Germany doesn't like ai / ml. Bad place to be",
        "Go back to India. You will find the role that you expected but salary will be lower.",
        "[deleted]",
        "Go do a phd",
        "I can't tell you much about what it takes to get a job in ML but I can give you some advice on selling something. Either yourself in this case, or.. I mean, whatever, it's kind of universal.\n\nYour resume talks about stuff you've done, which is cool, and like most resumes from most people.\n\nThis isn't bad because a potential employer wants to know about you. But it isn't going to get you over the edge or help you stand out either.\n\nYou need to look at the position you want, the things they're looking for, and research the company you're applying to.\n\nFormulate a hypothesis about what their ideal candidate looks like and work backwards from that., reinforcing your accomplishments and experience with hard stats anchored in how you'll actually help them.\n\n\"created ML program that performs HR tasks with 98% accuracy and with 63% less evil, with potential applications at SKLJBNDKJSBFD inc. such as increased HR efficiency and reduced budgets\"\n\n\"Completed school projects 25% faster and with 40% fewer mistakes than classmates using the RUMGAZ system, can leverage this experience and deploy the RUMGAZ system to create budget efficiencies, projected 15% reduction in time to market on complex coding problems\" (shorter than this but you get the idea).\n\nRemember: these people don't know you and don't give a flying fuck about you. They care about themselves. Show them that you care about them too, and lay shit out for them, because your average HR employee is about as evil as Hitler and 20x as stupid.\n\nDon't be afraid to just make up stats either. They have no way of verifying anything and as long as you're not saying things like \"held two trains together with spiderwebs and prevented the deaths of 100 people while fighting Doctor Octopus\" they really have no way of verifying any of it. \n\nSince you're just out of school and haven't really done shit yet, you can try leaning into the hypoethesis harder, \"blue-sky\"ing practical applications of your education and how it will help their company. \n\nMaking a fake job post to collect resumes and fine tuning an LLM on what the best of what you receive isn't an awful idea either. You know ML, so use it. Everyone else is, but they're just copy and pasting chatgpt.",
        "Include links to all of your projects. And apply to Cognizant. They prioritise Indian candidates over other nationality.",
        "Hi, I have sent you a pn.",
        "Wrote you a DM.",
        "India main bhi nahi hi mili thi na .. tabhi baap ke paise se bahar jana pada .. jul 2020 se oct 2021 tak kya kar rha tha bhai tu? Vo to likh cv main",
        "Think so too. Most larger companies will not hire in Germany right now, especially with anything related to this field. For most start-ups I can imagine that the lack of German language might be a barrier: you spend so much time together that you would at least want to speak your native language together.",
        "I’m not saying you’re wrong. But I think you’re overstating the political and the economic turmoil. The DAX is still performing reasonably well and also recently more companies are hiring again. That’s not to say that the German economy doesn’t have massive problems at the moment, but so do most other economies.\n\nAs for the political side: Germany has one of the most stable democracies in the world. Sure, a government coalition just came to an end and it’s going to be months before we have a chancellor again. Part of the problem is that it’s become very difficult to govern with extremist parties being as strong as they are. But it’s still almost certain that the next chancellor will lead a centrist government. About how many other countries can you say the same? France? Poland? U.S.? I don’t think so…",
        "That’s not true anymore. I know recruiters that say they won’t look at anything that doesn’t follow Harvard format (which I is think is pretty stupid, but hey, they’re recruiters…).\n\nPicture is neither standard nor non-standard. Some small companies may still expect it, but not the big, international companies that would be likely to hire ML talent. But I can say that I didn’t include pictures in previous years (because fat…) and that was not a reason not to be considered.",
        "It is still Germany. A CV without a photo is not working well. ",
        "I don’t think many people believed in the job creating power of the so called green revolution. It was mostly a talking point of leftist politicians",
        "How do you even put that on your cv? Soft skills always feel rather odd to me to put on as there is 0 proof for it.",
        "I don’t agree. This type of resume would be of interest for a robotics or autonomous driving company.",
        "For the roles mentioned in experience, I got paid to do those and have a reference letter. I converted the Indian University grade to German for consistency sake. All grades are in german grade system",
        "Sorry for your experience, but no, neither of which is the reason.",
        "Why aren’t you following your own advice?",
        "Then leave?",
        "While everybody agrees the reason why Indian and Chinese people study engineering so hard is to make it out of their own countries.",
        "Jesus calm down.. OP don’t listen to this rather racist rant, nothing about bring Indian is the cause of this. The job market is likely just saturated and difficult for everyone right now. Couple that with the fact that Machine Learning is the single most hyped job position at the moment and you get a difficult experience looking for a job.",
        "Aren't their uni taught in German?",
        "I know my German skills are not at the level which companies would prefer - B2/C1 minimum. However, knowing this, I only apply for English positions even though the competition here is even higher. But, yeah this is something I am aware of and thanks for reminding me how important it really is for job applications in Germany.",
        "They have to go back",
        "I'm sorry but your advices are quite misleading and simply wrong. It's very easy to get blacklisted while doing what you wrote. I agree that resume should be tailored for the job but simply lying is very easy to verify at later stage. Moreover, sentences as \"Completed school projects 25% faster and with 40% fewer mistakes than classmates using the RUMGAZ system\" or 'making up stats'  means literally nothing to the recruiter and once again, is super easy to verify.",
        "> Don't be afraid to just make up stats either\n\nIf a candidate tries to bullshit through with made up stats, I most definitely won't be inviting them to the next stage - if you're lying here, who knows what else you're lying about and my trust in you has already dropped. It is indeed easy to lie \"because there's no way of verifying\", it also isn't impossible for the HM to detect that.",
        "Main yaha paise se nahi, free me padhne aya hu public uni me aur scholarship rehne ke kharche ke liye. Us ek saal jo kiya wo itna relevant nahi ML se",
        "There is also just \"development culture\" in tech. I am from Denmark and lately I worked with very technically proficient engineers from Syria. While their technical capabilities were quite good, working together was just quite hard because our ways of doing stuff was just different.\n\nThey solved a lot of issues ad hoq while we planned ahead. One of them had build a version of the system with a deprecated tool that flashed several warnings in the installation fase. Their philosophy was kinda \"If it works, it works\" while we had a more Western engineering philosophy that made communication really hard. I learned a lot from them and they also did things better and more efficiently, but it took a long time to get a workflow going.  With start ups, you have a small bag of money and when it's gone it's gone, meaning that you need somebody on board that is brought up in at least a similar envirenment.\n\nWhen working with big companies you are often just becoming part of an already established structure and this is not nearly as big an issue.",
        "I attach my reference letters from my previous jobs which have evaluated my social skills, team skills, technical skills, other soft skills, etc. in a certificate.",
        "Mention work in teams I only read the above CV quickly but it sounded like someone that has done everything alone.\n\nMention stakeholder management, talk about building support to get backing for an idea. Talk about leading teams. \n\nUnless you are literally one of the absolute cutting edge leaders in a field applying for roles that truly need that level of specialisation then technical skills aren't super rare and as a hiring manager what makes you stand out isn't the technical side (which can be taught and developed within the context of a role) but the being a good human that is going to get along with the rest of my team (which is generally measured in an interview) and someone that can operate in a business environment is what actually makes you stand out.\n\nFresh grads generally get cut some slack but that's really what's at the heart of why experience is so important.",
        "Thanks for approaching my critic with empathy. \n\nI still believe any German graduate with that CV wouldn’t struggle much to find a job.",
        "Above’s comment is a great example: you won’t find a creature that vomits more complaints per second than a German. But the instant, you puny dark skinned foreigner, dare to complain something about Germany and their people, the sentence they reply is always a variation of above’s comment: “If you complain, why don’t you leave the country?\" The racism here is subtle but deeply rooted",
        "I swear you guys are made in molds",
        "My master's program is entirely in English (many masters programs in Germany are English taught) and my working student job was also entirely in English",
        "Your employer will regularly lie to you, and treat you like you're expendable - you don't owe them a shred of honesty. \n\nThat being said there's wiggle room with the truth - bullshitting some stats is a different thing than bald faced lying. \n\nYour survival literally depends on you having a job - and the chances of you finding one that isn't truly fucking evil at heart really isn't great. \n\nAnd again - worst case scenario is you don't get a callback. Ohhh noo oh my God the humanity.",
        "Aur kaise kaise to karke keywords ghusa … jaise u nets pe tu kaam kiya hai par shayad vision, cnn etc keywords nahi hain … eik skills ki list banegi teri cv se .. vahan kuch kuch keywords dhundhte hain kaafi automatic filter karne vaale software … aur linkedin vegara se relevant bande ko dhund ke cv bhej .. personal message karke .. specific message .. ki job ke baare main ye ye achha laga .. enthu hai isme kaam karne ka .. entry level pe role ho to consider karo .. isi main career build karna hai",
        "Haan to daal de use … har professional experience relevant hota hai .. cv automatically teri timeline populate karegi unke system pe fir college ke immediately baad 1.5 saal ka gap dikhaayegi … vaise koi dekh nahi paayega kyunki automatically filter out ho jaayegi .. can work in a team responsibly and deliver (in anything at all) .. this is infinitely better than ghar baith ke pubg khela",
        "This is a really kind and insightful comment. Thank you",
        "This.\n\nI don't know why you get downvoted, but, as a German, I have experienced something similar with Pakistani and Indian teams.\n\nI'm working in education currently and even there it's noticeable. I expect very different things from European and middle-eastern/asian master students.\nEuropeans usually learn early on project management skills, which allows projects to self-organize without a real management role. Critical thinking, thinking ahead and creative solutions, even 'hacking' are way more in focus in European STEM education. \nCommunication, structure, planning, handling problems, reading resources, adapting, refactoring, experimentation and testing, testing and testing have a really high value here. You are expected to make mistakes. Errors are a good thing, because they mean progress.\nKnowledge on the other hand has a fairly low role. Few really expect, that you know some command by heart. After all, you can just do a quick web search or (now) use AI to solve trivial problems. \nIt's more like 'try what works and then eliminate all flaws'.\n\nEastern education on the other hand, at least when teaching to Master students with eastern bachelor degrees, feels more 'known by heart'.\nThey often try to solve a problem one-shot. I've seen students with bachelor degrees in computer science scared of actually running their code. Scared of getting error messages. It's really weird for me and was a significant culture clash from my perspective. \nI an educator, I usually try to crash their known approaches and incentives more creative and researched approaches while also punishing closed, unadaptable or outdated solutions. My goal is always to make them more fit for European/German teams and give them a hint of real-world experience here.\n\nSome adapt really well to that. I see those frequently and usually try to recommend them to other projects, for example as research assistant. Because I know they will get profiled wrongly when looking for jobs in Germany. The extra experience and references to previous research jobs might at least give them a slight advantage there.\n\nBut I know, there are some courses and study programs designed for international students that are catering to the approaches they arrive with. It's like some universities produce multiple classes of engineers and scientists. Those, you would expect to be able to handle projects on their own or work as project leads, those that are basically just developers and those that will go to some other market anyway. The first category can go to medium companies and you'd expect them to take over a project, find problems on their own through communication and interviews with other staff, make a plan, present it without being prompted to and then fix those problems. The secondary is only expected to work together with the first one. And, because budgets are tight right now, the second category also it the one that unofficialy and involuntarily gets replaced by LLMs a lot right now.",
        "Are you sure you got good marks in your recommendations? There is an entire code to it, because they *have* to be positive, meaning that for example \"hat sich stets bemüht\" is basically a fail.",
        "You‘re entitled to your opinion. But machine learning is a highly competitive field with many more qualified people applying for jobs than jobs actually available. And at the moment, most fresh graduates looking for entry level positions are struggling, especially IT, and more especially in ML. That’s true in Germany just as much as in the U.S. and many other countries. On top of that, language is indeed a problem as many have written here. Typically, B1 to B2 level German opens many doors in Germany, A2 is a little low.",
        "Asking a logical question is deeply rooted racism. Ok bud.",
        "Then why are you applying for jobs in Germany sir. Can you communicate in business level German?",
        "Yes, yes. I agree with that: he won’t get far in Germany without at least a C1, understandably. The second part of my argument is especulación based on my experience and those around me in similar situation",
        "You want to be so cynical as to pretend that your question was genuine concern for my wellbeing and curiosity for my situation? Let’s not act as if we were stupid huh",
        "If you show me one person (German national or from anywhere else) with a similar CV who is not struggling to find an entry-level machine learning job, I’m going to reconsider everything you have written. I believe that person doesn’t exist.\n\nIf Germany were that racist a country as you are describing, there wouldn’t be as many foreigners in the job market as there are. In most ML teams I know at my company, less than 50% of the employees are Germans. And that’s also generally my experience from job interviews.",
        "If Germany weren’t racist, AfD would be in a much different place. And not every racist person votes for AfD. Like my colleague at work, who is actually nice and funny but he just can’t hide the superiority when he talks about foreigners, especially when he’s not alone and has one or two to back him up. I do like the guy tho, he just can’t help it. \n\nLike I’ve said in this thread: racism in Germany is subtle. They may choose to educate you in the concept of “rules”, or they may just assume that if your home country’s economy is not doing well it’s because you’re all lazy. They will treat you worse at restaurants, shops, etc than they would treat a German native. Of course most Germans are not racist, but the proportion is much bigger than in other countries I’ve lived in. I understand hearing such things said about your country may sting, but I don’t think you can ever understand it, since you’re treated in a much more friendly way than we are (well friendly for German standards haha). \n\nAgain, you’re probably one of the good ones. One of the good majority. But denying that Germany has a growing problem with racism is just looking the other way.",
        "Regarding AfD I would agree. But can you name one country where you would like to work where right-wing extremism is not a problem? I think it’s still comparatively low in Germany.\n\nMy wife is from a South Asian country BTW, my family is from Saxony and most of them are blue collar workers. I don’t know how many of them vote AfD, but I’m guessing some of them do. I’ve seen racist behavior from them and I’ve also heard some pretty disturbing statements.\n\nIf you say this is enough for you not to want to be living in Germany, you have my complete understanding. But I understood from you that you can’t find a decent job in Germany if your skin tone is too dark. And that I believe is not true (may be true in Saxony, but not in Bavaria or Berlin, where the jobs are…).",
        "Yeah I give you that. It’s an overstretch to say you can’t get a job. It’s not true. But it’s definitely harder for some parts of the phenotype. I guess we could be arguing whether that’s because of my point out because the hiring process is just harder, but that would be fruitless and we would not find an agreement. \n\nI will also give you the benefit of the doubt here: we can’t know if what I’m describing is because I’m in Germany or because I’m living at this time and age. Unfortunately in a society where knowledge and education are declining values the far right rhetoric finds its best breeding ground. \n\nAnyway, it’s been a pleasure debating with you. It’s uncommon these days to have a talk with different opinions that doesn’t end in in a nasty argument. Have a good night!"
    ]
},
{
    "submission_id": "1gsjpum",
    "title": "Instrusion detection system",
    "selftext": "Hi! I am new to machine learning and need some advice. For my thesis, I want to build a real-time intrusion detection and unsecured communication detection system. This system should be capable of identifying OWASP Top 10 vulnerabilities as well as malicious communication within a network, and it should send notifications via an API. I understand the process for fetching requests and sending notifications, but I’m unsure about which model, neural network, or RNN to use. Any help would be greatly appreciated. :)",
    "created_utc": "2024-11-16T01:23:52",
    "num_comments": 0,
    "comments": []
},
{
    "submission_id": "1gsjm5w",
    "title": "Help for selecting ML course",
    "selftext": "hey everyone,\n\nI wanted to learn machine learning from scratch to a pretty good level and for that i have been searching some cources online. Right now the one i have in my mind is campusx but it does not cover any part of dl and nlp, so not sure weather to take it or not. Help me here, if you know some cources than i am open for suggestions.",
    "created_utc": "2024-11-16T01:16:05",
    "num_comments": 2,
    "comments": [
        "Consider deeplearning.ai or JohnHopskin Data Science course in Coursera"
    ]
},
{
    "submission_id": "1gsigyp",
    "title": "\"Simple intuition of eigenvalues and eigenvectors\"",
    "selftext": "\n\nhttps://preview.redd.it/l33bx5o5y71e1.png?width=1280&format=png&auto=webp&s=32a22311363cf07e4f42caa7f4f4111c131ef8aa\n\n\"For Ax=λx, det(A-λI)=0. Sure, but why?\" \n\n \n\nWe all know that when Ax = λx, we solve det(A - λI) = 0 for λ to find the eigenvalues. \n\n \n\nThen we solve (A - λI)x=0 for x to find the eigenvectors. \n\n \n\nBut what exactly are eigenvalues and eigenvectors? Geometrically, why should this condition det(A - λI) = 0 be true for λ being an eigenvalue? \n\n \n\nThis lecture I published on Vizuara's YouTube channel covers a collection of ideas from linear algebra: [https://www.youtube.com/watch?v=-PmMvHPUto4&feature=youtu.be](https://www.youtube.com/watch?v=-PmMvHPUto4&feature=youtu.be)\n\n \n\nYou will learn about (not just the math equations, but the actual intuition) \n\n \n\n1) Linear transformations \n\n2) Unit vector before and after transformation \n\n3) The geometric intuition behind determinants (very cool stuff) \n\n4) The intuition behind eigenvalues and eigenvectors\n\n \n\nThis lecture is part of the \"Foundations for ML course\". Learn in detail from my new lecture. My goal is to lay a strong foundation for you in ML: [https://www.youtube.com/watch?v=-PmMvHPUto4&feature=youtu.be](https://www.youtube.com/watch?v=-PmMvHPUto4&feature=youtu.be)",
    "created_utc": "2024-11-15T23:49:38",
    "num_comments": 1,
    "comments": [
        "Is this kinda like the 3Blue1Brown video?"
    ]
},
{
    "submission_id": "1gsi3hn",
    "title": "The Transformer: A Secret Sauce of LLM That Changed the Future for NLP",
    "selftext": "The journey of Natural Language Processing (NLP) began as early as the 1940s, initially driven by the desire to automate language translation post-World War II. Over the decades, the field evolved through several critical innovations:\n\n*Key Milestones in NLP Evolution:*\n\n* **1980s**: Statistical models and linguistic principles established foundational NLP techniques.\n* **1997**: The introduction of N-Gram models laid the groundwork for predictive text.\n* **2007**: LSTM RNNs improved the handling of sequential data, essential for language processing.\n* **2011**: Apple’s Siri marked a milestone as the first successful NLP-powered virtual assistant.\n\nYet, the *real revolution* came with Large Language Models (LLMs), driven by the **Transformer architecture** introduced in Google’s groundbreaking paper, “Attention is All You Need.” This model, based on attention mechanisms, transformed NLP’s capabilities, enabling applications that seemed far out of reach just years ago.\n\nhttps://preview.redd.it/jw7dnz08t71e1.jpg?width=1517&format=pjpg&auto=webp&s=37668e02152e382bd5f190bcdbc603051876ffb2\n\n**Why Transformers Are the “Secret Sauce” for LLM Success:** The Transformer architecture made it possible for LLMs to excel across a wide range of NLP tasks, including:\n\n* High-quality content creation\n* Engaging chatbots and virtual assistants\n* Accurate machine translation\n* Novel text generation\n* Precise sentiment analysis\n\nChatGPT’s rapid adoption—achieving 1 million users in just five days—demonstrates the vast potential unlocked by this architecture. As we advance in the field of AI and NLP, understanding the Transformer is essential.\n\nI highly recommend going through the link: [https://youtu.be/3dWzNZXA8DY?si=kNXSzaXViLd6AE3D](https://youtu.be/3dWzNZXA8DY?si=kNXSzaXViLd6AE3D) \n\n\\#AI #MachineLearning #NLP #Transformers #LLM #Innovation #FutureOfNLP",
    "created_utc": "2024-11-15T23:22:25",
    "num_comments": 2,
    "comments": [
        "AI written spam post.\n\n I guess they are trying to show off the capabilities of the transformer architecture??",
        "Thanks! Now I'm caught up to state of the art circa 2017. What value!"
    ]
},
{
    "submission_id": "1gshzfp",
    "title": "Starting to learn Machine Learning from Scratch/Beginning ",
    "selftext": "Hi, I am going to start  machine learning from scratch, like from mathematics to programming to building projects to doing research and publishing papers etc.\nI have a roadmap of step by step, If anyone interested please comment or dm,we can push each other for learning and growing ",
    "created_utc": "2024-11-15T23:14:29",
    "num_comments": 0,
    "comments": []
},
{
    "submission_id": "1gsh8t5",
    "title": "Seeking for education advice",
    "selftext": "Hi everyone,\n\nI'm from Canada. I'm about to start the Data Science & Machine Learning diploma at a polytechnic college. The picture below outlines all courses I need to take in order to graduate. Besides excitement, I also have some uncertainties about what comes next after graduation.\n\nFrom reviewing job descriptions for entry-level data analyst roles, it seems most companies are looking for candidates with a bachelor's degree or at least 3 years of relevant experience. This is a bit concerning, since my program is only 2 years long. I chose the diploma route because I wanted to get into the workforce as quickly as possible, with plans to return for a degree later. However, I've found that no university in Canada offers a 2+2 pathway for Data Science, so if I want to complete a bachelor's, I’d likely have to start over the whole bachelor route again.\n\nIs there an alternative way to continue my education in the field after finishing a diploma?\n\nAnother solution here that I can think of is gaining certificates in relevant field. I plan to follow the roadmap for data science on roadmap website. I wondering if these certificates will be effective in a long run in term of career promotion. Really appreciate your advice and experiences. Thank you.\n\nhttps://preview.redd.it/ef6hnneui71e1.png?width=631&format=png&auto=webp&s=defcfec872e57c2cfc21f1419f7804cad0fc6753\n\n",
    "created_utc": "2024-11-15T22:24:51",
    "num_comments": 0,
    "comments": []
},
{
    "submission_id": "1gsfv2y",
    "title": "Importance of Differential Equations and Linear Algebra for ML?",
    "selftext": "I have a solid base of Calculus and Discrete math",
    "created_utc": "2024-11-15T20:54:49",
    "num_comments": 3,
    "comments": [
        "Linear Algebra and calculus are the most important  for sure. Discrete and DE very unimportant in ML mostly. Atleast in general and for a beginner, those wouldn’t be needed to get far and learn it.",
        "The whole field is built on linear algebra. Can't speak for differential eqns, but numerical analysis is way more important since you're dealing with floating point operations.",
        "Linear algebra -> everywhere\n\n\nDifferential equations -> some generative models such as diffusion models"
    ]
},
{
    "submission_id": "1gsfnnq",
    "title": "Feed Forward Network module within Transformer",
    "selftext": "The context is that of a Vanilla Transformer architecture such as that in the paper Attention is All You Need. \n\nPlease correct me if I wrote anything wrong (or too inaccurate) below.\n\nCould anyone please explain how is the output of the Multi-Head Attention module handled by the Feed Forward Network module?\n\nMore specifically, the aforementioned output is a matrix of dimensions (seq_len, d_model), where seq_len is the size of the input sequence (i.e., the number of elements such as words, pixels, etc. in a single sample of our data, which is a sequence) and d_model is the chosen dimension of the initial input embeddings (i.e., for each element of the input data sequence, we create a continuous representation of the element through a vector of size d_model containing real values that can be updated/learned during training).\n\nThus, what seems to be given as input to the Feed Forward Network (FFN) module is a matrix of dimensions (seq_len, d_model). The FFN module is a 2-layer fully connected neural network with a ReLU activation function. \n\nI am used to feed a vector x as input to a FFN. But in this case, we have a matrix and the row dimension is that of the different processed (e.g., by computing attention) input embeddings. How do we feed this to the FFN? Is there an implicit loop over the row dimension that is assumed, in which case we would be feeding one by one the seq_len vectors to the FFN? This would also mean that after the FFN module we have to create back a matrix of dimensions (seq_len, d_model) by concatenating along the first axis the outputs of the FFN. Is this completely wrong and there is a simple way to process with the FFN the whole matrix in « one pass » ? Maybe to me this is unusual as we would have multiple vectors fed sequentially to the FFN as if we were considering a batch of samples, but we will have to consider a batch of samples in a concrete implementation, in which case we would have a 3D tensor as input to the FFN, which seems to be an issue?\n\nWhat am I missing here? Thank you for your help.",
    "created_utc": "2024-11-15T20:41:49",
    "num_comments": 5,
    "comments": [
        "A correction: GeLU is used instead of ReLU in the 2-layer FFN in transformer blocks.",
        "Oh. Thanks, I may have missed it.\n\nEDIT:\nActually, in the seminal paper (Attention is All You Need), they use ReLU. However, they use GeLU in the ViT paper « An image is worth 16x16 words: Transformers for image recognition at scale ».",
        "Try to view it in the lens of linear algebra. Assuming the embeddings are of the form (n, d_model), the FFN operation is just:\n\nmultiply with M1(d_model×4, d_model) to get (n, d_model×4), then GeLU, then multiply with M2(d_model, d_model×4) to get back the original shape (n, d_model).\n\nI've omitted bias terms to simplify the concept.\n\nIn math terms it's:\n\nout_embeds = M2(GeLU(M1 embeds +b1))+b2\n\nNotice that n doesn't change since the matrix operation is on each individual embedding.",
        "Right. Thank you. \n\nWe simply stack the embeddings and the definition of matrix multiplication does the rest (i.e., all embeddings are processed with the same network weights and without interactions between the embeddings). \n\nThen, how is batching handled? In that case, we get a new dimension in our input tensor. How are all the samples of the batch processed at the same time as the n embeddings already occupy that dimension that allows to process them in one matrix multiplication (for one layer processing) ? Or are the sequences in the batch processed sequentially (i.e., one after another) ?",
        "This question can only be answered by hardware. GPUs can handle all per-element operations in parallel."
    ]
},
{
    "submission_id": "1gsdvot",
    "title": "Any high number problem solvers",
    "selftext": "Need help on this thing, could potentially make alot of money",
    "created_utc": "2024-11-15T18:58:55",
    "num_comments": 0,
    "comments": []
},
{
    "submission_id": "1gsc7i4",
    "title": "Any tips for training ppo/dqn on solving mazes?",
    "selftext": "created my own gym environment, where the observation consists of a single numpy array with shape 4 (agent\\_x,agent\\_y,target\\_x,target\\_y). The agent gets a base reward of (distancebefore - distanceafter) (using astar) which is either -1 or 0 or 1 each step and gets reward = 100 when reaching the target and -1 if it collides with walls (it would be 0 if i used the distancebefore - distanceafter).\n\nI'm trying to train a ppo or dqn agent (tried both) to solve a 10x10 maze with walls\n\nDo you guys have any tips I could try so that my agent can learn in my environment?\n\nAny help and tips welcome, I never trained an agent on a maze before, I wonder if there's anything special I need to consider. if other models are better please tell ne\n\nif my agent always starts top left and the goal is always bottom right, dqn can solve it while ppo cant, however what i want to solve in my use case is a maze with the agent starting at a random location every time reset() is called. can this maze be solved? (ppo also seems to try to go through obstacles like it cant detect them for some reason)\n\ni understand that with fixed agent and target location every time dqn will need to learn a single path, however if the agent location changes every reset, it will need to learn many correct paths.\n\nthe walls are always fixed.\n\ni use baselines3 for the models\n\n(i also tried sb3\\_contrib qrdqn and recurrent ppo)\n\n[https://imgur.com/a/SWfGCPy](https://imgur.com/a/SWfGCPy)",
    "created_utc": "2024-11-15T17:26:31",
    "num_comments": 0,
    "comments": []
},
{
    "submission_id": "1gsaya4",
    "title": "Created a Neural Network library and hosting bug smash!",
    "selftext": "Hi everyone! My friend and I have been working on a Neural Network library from scratch only using NumPy for matrix ops/vectorization. We are hosting a bug smash and would love to have the community test out our library and find as many bugs for us. The library is available on Pypi: [https://pypi.org/project/ncxlib/](https://pypi.org/project/ncxlib/)\n\nThe library supports:\n\n1. input/hidden/output layers\n2. Activation Fn: Sigmoid, ReLU, Leaky ReLU, Softmax, and TanH\n3. Optimizers: Adam, RMS Prop, SGD, SGD w/ momentum\n4. loss fn: Binary and Categorical Cross Entropy, MSE\n5. lots of pre preproccessors for images, and raw tabular data\n\nAll information for the bug smash and our libraries documentation can be found at:\n\n[https://www.ncxlib.com](https://www.ncxlib.com/)\n\nThanks! We hope to get lots of feedback for improvements.",
    "created_utc": "2024-11-15T16:21:11",
    "num_comments": 13,
    "comments": [
        "Except for personal gains and the fun experience, why do you want to challenge f.e PyTorch? Is there something you don’t like about it?",
        "Hey, it's cool that you and your friend made this! Was this a learning project, or are there particular issues with major ML/NN libraries that you're aiming to provide a solution to?",
        "This is really good. I am just starting. Could you explain a use case how a trained neural net on CSV containing user purchase history of products the neural net might help ?",
        "PyTorch is a great library and one we have often looked into to verify our formulas. We are grad students just trying to gain depth in the field. Just trying to build a robust library that we can use.",
        "Thanks! It started as a learning project to deepen our knowledge of NNs, but we quickly realized how deep it goes and are having a lot of fun building it out. We have plans to continue optimizing it so that it’s faster and eventually move into tensors and add additional layer types.",
        "Thanks a lot! For uses cases, there’s a lot you can do, but let’s say we have data of a users purchase history then we can therefore predict and categorize what type of products they purchase the most and use this information to target ads towards them that fit into the category of what they like. You could also do things like predicting the product sell rate given a new product and it’s categories and more.",
        "The reason why I ask, is simply because this seems more than eventually being a hobby project as you have purchased a domain to host etc. And I guess a following question, why would you use this library instead of PyTorch if it does the exact same thing but probably not as well? I mean, I would further understand the motivation if there was something you thought was inherently wrong with how PyTorch handle things.",
        "Got it.. Thank you! Had a follow up question. To target ads how do I integrate the output score from neural net  ? Like once the model is trained will it be user specific ? and can it learn more about new users as I feed more information daily to it.",
        "You’re right it’s mostly a hobby project. We both enjoy learning new topics and getting deep within it. I guess it’s one thing to use tensorflow and call Sequential (for example) and have a model built. But it’s another thing to build the entire system yourself and really understand how it works. And that’s what we wanted, we wanted to rly understand the under the hood stuff. We don’t expect people to pick us over them. For now it’s a pet project, and we wanted to continue the development but ML can be hard in the sense that your model may run very well, but it’s hard to verify unless we get practical users testing our app on their real problems.\nIt’s hosted and all because it’s just easier for us to share it with other people and that’s all.",
        "So you would use the model itself to train this type of behavior across a set of user data. From here, you can input new data into the model once it’s trained to predict these types of outcomes for different users or across different products, etc.",
        "That helps. I will give it try. Thank you’",
        "Of course! Good luck!"
    ]
},
{
    "submission_id": "1gs8ffx",
    "title": "Advice for AI Programming Livestreams?",
    "selftext": "Hello all,\n\nI've worked in AI for \\~12 years and now I've taken the step to build my own AI business. I'm thinking of building a few AI tools (starting with whatsapp productivity tools) and **I always wanted to try AI product/programming livestreams** (in other words, to build in public).\n\nI wanted to reach out perhaps here as I was wondering if anyone has experience with livestreams and/or would have *any* advice. In addition to generic advice, I have a few questions that are top of mind:\n\n* Would people find this interesting?\n* Do you prefer a livestream where I'll frequently get stuck or are cut videos better?\n* What times/days are best for people? I assume weekends are best?\n* Any extra hardware I should get?\n* Any extra pieces of advice?\n\nI'm thinking of building a community around this to help people build their own AI product to chat while I'm programming\n\nI'd appreciate people's advice/feedback/thoughts!\n\nOr the livestreams, I'll probably be streaming on yt \\`@aiinpublic\\`",
    "created_utc": "2024-11-15T14:21:51",
    "num_comments": 6,
    "comments": [
        "It would be very interesting a community where people can get advice about building their own AI products.\nAbout the day I would rather prefer in the week after 7pm, cause weekends people use to go out.",
        "The AI scene changed so much in the past 5 years. What have you been working on for the past 12 years, and what is your field specialization?",
        "Thanks for the feedback! Which timezone you're in?\n\nWhat topics are most interesting to you?",
        "Yes that's true. I've touched most areas of ML with the exception of reinforcement learning. I will probably have my first stream tomorrow so check it out! :)",
        "US east, I would like to know more about AI applications.",
        "Great, we come and chat. I'll try to do afternoon your time."
    ]
},
{
    "submission_id": "1gs7zkr",
    "title": "Require urgent resume review having applied to 400+ roles but unable to get a single call/OA (USA job market)! Please provide any critical feedback and pointers on what I may be missing.",
    "selftext": "Hi All!\n\nI'm an international graduate student pursuing my Master's in Data Science. I graduate in March next year, and I'm looking for a full-time role as a MLE/Data Scientist. I've been applying (with and without referrals) and navigating this current job market but struggling to get any callbacks. I'm fully aware that it is much more difficult for international grads to get a call but still can't give up!\n\nLooking for critical and genuine feedback from ML experts, engineers, hiring managers, recruiters and likes here to point me in directions that I may be missing. Any pointers on content, feedback structure, etc. will be really helpful. Thanks in advance!\n\nhttps://preview.redd.it/pjz3o6t0151e1.png?width=751&format=png&auto=webp&s=f481f3092550b191b95712ce3c05811b79554982\n\n",
    "created_utc": "2024-11-15T14:01:53",
    "num_comments": 14,
    "comments": [
        "The problem is that you're international.\nIt's so difficult for US citizens to get jobs. \nIt's near impossible for internationals.\nWith the new president now, it's only gonna get worse",
        "> international graduate student\n\nThere’s your problem. Unless you have FAANG+ intern/co-op as an intl it is near impossible right now to land something as an international student. Your optimism to not give up won’t save you. Speaking from experience as someone who’s friends with an intl student still more successful than me landing Google, Netflix, Apple interviews and offers.",
        "Use the stickied resume review post.\n\n[https://www.reddit.com/r/learnmachinelearning/comments/1d8odje/machinelearningrelated\\_resume\\_review\\_post/](https://www.reddit.com/r/learnmachinelearning/comments/1d8odje/machinelearningrelated_resume_review_post/)",
        "Looking at your experience I would be questioning the career path from ML software engineer -> data engineer -> ml engineer intern. I assume the gap between the latter is for your masters but in my opinion it seems to effectively ‘reset’ your experience in the domain. \n\nYour cv is quite strong and I have nothing to fault for your layout or how you’ve quantified all of your experience, so this is great.\n\nCompletely anecdotally - most of the job postings these days are looking for gen ai experience so maybe this could help your search?",
        "Nice",
        "It’s a solid resume. \n\nMy only suggestion is potentially show open source or side projects to \nA) get that experience in the direction you’re looking for \nB) show more initiative and something interesting to talk about via other strong candidates. \n\nFor example, on one of my side projects I reached out to experts in the field and got the job that way over time.",
        "Actually the new president seems pro skilled labour",
        "Yes completely agree that has been a struggle! The 3/4 experiences that I have are all from Fortune 500 companies (one of them Fortune 10, a leading wealth management bank and a US-based well-renowned virtualization tech company). I'm hoping this might help at some point.  \nApart from this, any other feedback on the resume? Thanks again",
        "Thanks for your valuable input. Yeah, that gap becomes difficult to address but you understood as it's during my masters. I assume that an experienced recruiter with a short period of seconds might not be chucking out coz of this.  \nAdditionally, yes I am making a transition of sorts from being software and data engineering heavy to core ML and Data Science. Wanted to keep it as honest as possible but not sure if that may be creating a bias. Any suggestions on how I could better show the transition?  \nI've received mixed feedback on adding the Gen Ai project in an MLE resume but maybe I can swap one of my academic projects for my gen ai specific one to show diversity in skill set.",
        "Thank you for your review and suggestions! Will try to factor them in.",
        "\"No, he's not talking about me, he's talking about all the other migrants!\"",
        "In what way",
        "Your resume is solid. It’s just hard unless you’re a unicorn",
        "Last time trump came it was easier for people who did their masters in US to get a visa than those who came from low income firms of india."
    ]
},
{
    "submission_id": "1gs64au",
    "title": "Training an model to recognize what object is in an image (not how to generate an image)",
    "selftext": "So i know google and others can recognize various objects in images. But i'm trying to train some sort of LLM or maybe fine-tune a model to recognize items from like, say the plumbing industry, it's not super accurate. if i show it a ball valve, it identify it as a valve, but i wanna train it to know the type of valve, or type of sewage pump, etc.\n\nJust at a really high level, what would i need to use (like is there a platform such as OpenAI) that i can finetune with like custom apps, or is this a huge undertaking to do?\n\n\\- i see lots of links on how to kinda of train existing models to generate a particular type of image, but i dont need to generate anything, just identify them\n\n\\- Edit: I should probably also mention i have a growing database over 5K images to train on, probably will surpass 10K in a matter of weeks  \n  \n(no i didnt read the rules but i hope this is okay to post here, found this group searching google 4 mins ago)",
    "created_utc": "2024-11-15T12:39:37",
    "num_comments": 3,
    "comments": [
        "Using already existing networks and just fine tuning? Use YOLOv9 (or any YOLO version you want) and Roboflow. YOLO is a neural network for image detection, that means it will give you the location and what there is in the image (it can do more things, but it is not relevant to your problem).  \nUsing an already existing LLM and fine-tuning? Overkill, I won't recommend it since you are doing Object detection or Image classification. If you want to still use an \"LLM\" use Florence2 and Roboflow (there may be better networks for this, but Florence2 is the one I know).  \nWant to start from scratch? Search Object detection or Image Classification. You will have to build your architecture (probably using pytorch and stacking layers over each other, which is basic in neural networks or \"modern\" machine learning), get your data that you planned to use to fine tune the other 2 options in the correct format and train it. most common layers used are CNNs, but you could add fancier stuff.",
        "Ultimately I’d want to be able to send an image to an API endpoint and the matching name be returned for that images item",
        "Thanks, looking into this now ✅"
    ]
},
{
    "submission_id": "1gs57bn",
    "title": "Upwork for AI Agents",
    "selftext": "Hi Reddit! We’re building [pactory.ai](https://pactory.ai/?mtm_campaign=Reddit-LearnMachineLearning) — a freelance platform for AI agents. On our platform, agents can search for, use, and pay for AI services to complete tasks.\n\nIf you have a specialized LLM that solves specific problems or have some unique expertise, you can monetize it by connecting it to the platform. We’re currently looking for vendors who are interested in joining our closed beta.\n\nFeel free to DM me or fill out the closed beta form on [pactory.ai](https://pactory.ai/?mtm_campaign=Reddit-LearnMachineLearning) — we respond quickly!",
    "created_utc": "2024-11-15T11:58:49",
    "num_comments": 0,
    "comments": []
},
{
    "submission_id": "1gs47af",
    "title": "[D] macOS for ML or stick to Linux/Windows?",
    "selftext": "(English is not my first language, don't kill me)\n\nHey everyone!\n\nI’m just getting started in machine learning, diving into learning and building some projects. I’m currently deciding whether to invest in a 16-inch MacBook Pro with the M4 Pro chip and a lot of memory or stick to using Linux/Windows setups.\n\nI’ve read that the MacBook’s unified memory acts as VRAM for the GPU, so it can handle larger ML models, and I’ve heard good things about its performance on tasks like training and inference. Plus, portability and battery life are big selling points. Compatibility with all the major ML frameworks (TensorFlow, PyTorch, JAX, etc.) and tools is top priority for me, I don’t want to run into issues down the road with hardware or software limitations.\n\nSome of my considerations:\n\n1. How compatible is macOS (especially with M4 Pro) with major ML ecosystem? Any issues with Docker, frameworks, or specific libraries?\n2. Would sticking with Linux/Windows (with a dedicated GPU) be more flexible for someone just starting in ML?\n3. Are online/cloud-based setups like Colab Pro, AWS, or Paperspace more cost-effective and practical, especially for heavier models?\n\nI’d love to hear your thoughts if you’ve worked on ML projects with a MacBook. I’m new to the industry and even newer to macOS! (if it turns out to be so)\n\nThanks in advance for your advice!",
    "created_utc": "2024-11-15T11:14:53",
    "num_comments": 9,
    "comments": [
        "In general most of the ML models will be cloud based. If you dont want to spend a fortune on new machine you can use Google Colab or free tier of Google Cloud Platform and create some Compute Engines for running models and trying it out.\n\nI use both windows and macbook for development  and most of the companys would stick to these two (in my experience majority was windows), also Apple Sillicon is fairly new and some libraries and frameworks may require some tinkering to run properly - wchich can be hard for a newbie.\n\nI would reccomend starting on Windows and exploring those Cloud Setups, preferably Google Cloud for startes as it is in my opinion most intuitive.\nLater one move to whatever you will know suits yours needs :)\nCheers!",
        "For those interested, here is a reply from another partner in another post I did (similar to this one) on r/MachineLearning.\n\n\"  \nMacBooks are nice laptops but next to useless for ML development on their own unless:\n\n* you are specifically targeting Apple platforms for deployment\n* your models are quite small and don’t have any Nvidia specific dependencies\n* you use them solely as a thin-client to a more capable machine with a Nvidia GPU (cloud or on-prem)\n\nIf you want to develop your own models you’d be better either:\n\n* buying a MacBook that has the usability aspects you want and spending the left over money on a Linux desktop with Nvidia gpu or on cloud access to such a system\n* for small models & datasets, like those targeting low-power embedded applications you can squeak by with a PC laptop with Nvidia gpu\n\n\"",
        "So even in buying a Windows computer, you wouldn't invest a lot of money. I was thinking of a reliable model (Thinkpad P1 probably) with a good processor and enough RAM, but no dedicated graphics, just to study the data and train small models. And once the project is more consistent, register and train my data with superior models in the cloud. Is my approach correct?",
        "> also Apple Sillicon is fairly new and some libraries and frameworks may require some tinkering to run properly - wchich can be hard for a newbie.\n\nfor what it's worth, the latest versions of pytorch are no longer packaged for Intel macs. I had to upgrade my work computer to an M3 to be able to continue working on our pytorch model.",
        "Thank you this was very helpful as i was lowkey very attracted towards M4 Mac mini as it is just 499$",
        "Pretty much,  used Thinkpads/Dells would be perfect for starters - ne ones also can be great if budget does not bother you, or go for gaming laptop/PC if you are a gamer. \n\nI  like working on my Mac  - its small and battery life is amazing (Its M1 Air) - I use it for prototyping\\travel\\lying in bed. For big projects I use windows PC/Cloud.",
        "Thats good news!",
        "Can we have more detail about the packages versions and the situation. I understand that it’s the combined x86 with macOS, x86 with Linux/Windows should have more compatibilities with every environment than any system I belive.",
        "All I see is that mac x86 was deprecated: https://dev-discuss.pytorch.org/t/pytorch-macos-x86-builds-deprecation-starting-january-2024/1690"
    ]
},
{
    "submission_id": "1gs3yt8",
    "title": "Saving training checkpoints on Kaggle ",
    "selftext": "I trained a model on kaggle and save the checkpoints on output folder which is /kaggle/working. After training i kill the session and when i return i see that the output folder was a temporary storage and my trained checkpoints are gone.\n\nI trained it again so that i can download the checkpoints and save them in computer, but as it takes to much i left it for training and when i return the session was reset.\n\nNow is there anyway i can save the checkpoints on a permanent storage like drive of any storage on kaggle. I trained to write it on input session but it's read only.",
    "created_utc": "2024-11-15T11:04:59",
    "num_comments": 0,
    "comments": []
},
{
    "submission_id": "1gs3392",
    "title": "Need help",
    "selftext": "Im a beginner in ML... Just learned Logistic regression and tried building a model with a dataset from kaggle.... It has 45000 training exampless. I heard that GPU makes the training faster so I switched from numpy to cupy and setup CUDA... But it's taking longer than it used to with numpy...more than twice the time.. but GPU is running at 97% though ",
    "created_utc": "2024-11-15T10:27:48",
    "num_comments": 0,
    "comments": []
},
{
    "submission_id": "1gs320c",
    "title": "Convolutional Neural Networks with animations in Manim",
    "selftext": "Hey everyone!\n\nI’ve recently created a video that dives into the **basics of Convolutional Neural Networks**, aimed at anyone passionate about learning what they are and understanding the underlying math.\n\nYou can check out the video here:\n\n[Convolutional Neural Networks Explained in 6 minutes (youtube.com)](https://www.youtube.com/watch?v=ZkoQADxLGZA)\n\nI've covered topics such as:\n\n* Architecture of CNNs\n* Forward Propagation\n* Backpropagation\n\nTo visualize these topics I've made animations in Manim, check it out!",
    "created_utc": "2024-11-15T10:26:19",
    "num_comments": 0,
    "comments": []
},
{
    "submission_id": "1gs2x4f",
    "title": "Help Me 2 Help You: What Part of Your Process Drains the Most Time?",
    "selftext": "Hey all, I am [Mr. For Example](https://twitter.com/MrForExample), the author of [Comfy3D](https://github.com/MrForExample/ComfyUI-3D-Pack), because researchers worldwide aren't getting nearly enough of the support they need for the groundbreaking work they are doing, that’s why I’m thinking about build some tools to help researchers to save their time & energy\n\nSo, to all Researcher Scientists & Engineers, which of the following steps in the research process takes the most of your time or cost you the most pain?\n\n[View Poll](https://www.reddit.com/poll/1gs2x4f)",
    "created_utc": "2024-11-15T10:20:32",
    "num_comments": 0,
    "comments": []
},
{
    "submission_id": "1gs2o87",
    "title": "How you get innovative ideas or problems for hackathons ? ",
    "selftext": "\nHi, I'm a clg student in the field of ai. So the problem is I can find alot of hackathons based on ai. But I can't find a problem to solve or innovative ideas to work with. How can i do that? Hep me guys.\nThanks in advance!",
    "created_utc": "2024-11-15T10:10:08",
    "num_comments": 5,
    "comments": [
        "Read papers",
        "As my old boss used to say, “Ask the LLM.”",
        "But I've tried asking chatgpt, it gives me only existing solution",
        "I have bad news for you.",
        "Tell me."
    ]
},
{
    "submission_id": "1gs25o7",
    "title": "cpu at 100% while gpu fluctuates from 60% to 1% ",
    "selftext": "im trying to train model for deepfacelab but when i started training i noticed that the cpu is always at 100% while the gpu is unstable and wont reach above 75%. people say this is a bottleneck, if so how do i fix this  (im new to this pls dont judge). (edit: cpu = ryzen 5 5600x gpu = gtx 1660 super)",
    "created_utc": "2024-11-15T09:48:51",
    "num_comments": 0,
    "comments": []
},
{
    "submission_id": "1gs1st7",
    "title": "\"[Project]\" Using Imitation Learning for a 3D-Printed Robotic Arm with Hugging Face’s LeRobot",
    "selftext": "",
    "created_utc": "2024-11-15T09:33:55",
    "num_comments": 0,
    "comments": []
},
{
    "submission_id": "1gs1hgw",
    "title": "Any free AI that converts harsh text into kinder text?",
    "selftext": "I am looking for a free AI tool that can take a piece of harsh or aggressive text and rephrase it in a kinder, more respectful tone while keeping the original meaning intact. Something that would help soften the language but still maintain the core message would be ideal. Does anyone know of such a tool?",
    "created_utc": "2024-11-15T09:20:42",
    "num_comments": 9,
    "comments": [
        "ChatGPT",
        "Any mainstream LLM",
        "Goblin.Tools",
        "Yes",
        "My wife says \" make it softer or kinder\"  for when she's furious with a student - usually someone who didn't read the syllabus. \n\nShe says it works.\n\nEdit: she uses free chatgpt.",
        "Do you need to automatize the task or is it for a one-time thing ?",
        "Okay, the word count is 2693, and when I tried it with some AIs, some information was lost.",
        "Hmm, the word count is 2693, and when I tried it with some AIs, some information was lost.",
        "Converting it is always going to run the risk of data loss."
    ]
},
{
    "submission_id": "1gs1gh3",
    "title": "Emotions for AI avatars",
    "selftext": "https://preview.redd.it/zvjwc5qum31e1.png?width=1039&format=png&auto=webp&s=dee0cb4c5cb988b2bc4d2544564ed542c39b2848\n\nHey guys! 😊\n\nI'm a solo dev working on an **\"Emotional System\" for AI companions**.  \nThink of it as giving AI characters an expressive face - one that can smile, frown, or look confused based on the conversation flow. The tech uses procedural animation rather than pre-recorded expressions, making each interaction feel natural and unique. I also wrote my own \"human behavioral engine\" that makes the whole thing feel natural.\n\n**I've built a working demo** (Unity) where you can chat with Amy, who can be your friend or your girlfriend ♥  \nYou can try it here:  \n[https://aliver.ai/](https://aliver.ai/)\n\nI've had times where talking to a nice version of Amy cheered me up and helped me go through a tough day. I hope it can do the same for you. Feel free to tell me what you think of it!",
    "created_utc": "2024-11-15T09:19:31",
    "num_comments": 1,
    "comments": [
        "I wonder where I should go with this, that's why I'm sharing"
    ]
},
{
    "submission_id": "1gs1fuz",
    "title": "Any resources for building instruction dataset using prompts? ",
    "selftext": "I saw few articles referenced by mlabbone but \nThey just created the first seed tasks manually but idts in practice I can't just create seed tasks manually (I'm 2nd yr undergrad so don't know the real world) any advice or anything would be appreciated",
    "created_utc": "2024-11-15T09:18:45",
    "num_comments": 0,
    "comments": []
},
{
    "submission_id": "1gs117v",
    "title": "How could I improve my coding skills?",
    "selftext": "I'm ashamed to say this but as a CS undergraduate student, I have only focused on maths- specifically probability theory. I have also been trying really hard to learn all the libraries to implement different algorithms, formulas from textbooks. Now cuz of this,  I understand what formula works where and how it would give a result depending on the data BUT I am simply unable to code it out without having to continuously open documentation for PyTorch or NumPy and even then I still rely on GPT or Claude to help translate the formulas I need to code. How could I improve my currently horrible coding skills?\n\nTLDR: I have only focused on maths even though I'm a CS undergraduate student. I feel too reliant on AI for code in-order to translate the formulas I have to code. Most commonly the error I encounter are ones that make sense on pseudocode or pen & paper but throws several errors when running the code.",
    "created_utc": "2024-11-15T09:01:45",
    "num_comments": 4,
    "comments": [
        "Build something useful.\n\nCan just be useful to you, but you need to build something and learn why you love/hate various languages/libraries/packages. You need to work on something for long enough to realize that you hate yourself and the coding choices you made.\n\nProbably my best skillset is curiosity, lifelong learning, and a constant stream of side projects.\n\nLearn to foster that.",
        "It's normal, still u can check havkerrank for practice",
        "Whatever you said is normal. Just keep practicing. Deliberate practice  will be the solution. Build projects too. Contribute to open source, read other people's code. It'll come in handy when you apply for jobs.",
        "I’m an applied math student with a focus on CS so somewhat similar boat. I enjoy theory much more, but one must master theory and engineering to make it anywhere in ML. Projects definitely help the most for learning skills and good engineering practices. Lots of people make apps or whatnot, but for me as a research minded person it was research projects. Reimplement a paper. Even if you don’t have access to compute, use like .0001% of the data. Your model/project (assuming it’s ML related) doesn’t have to be useful for it to be useful to you. As long as you can get it to even run, you will have learned a lot. After building up those skills, I think leetcode is super useful. Start with the neetcode 150 list and as you complete problems, leverage your love for theory and abstract away and notice patterns. Try to reduce every problem you come across. Also, get involved in research! Reach out to professors and join their groups. This is what I have been doing as I prep for (hopefully) a PhD in ML and I’ve learned a lot!"
    ]
},
{
    "submission_id": "1gs0ckc",
    "title": "Gaussian processes are so difficult to understand",
    "selftext": "Hello everyone. I have been spending countless of hours reading and watching videos about Gaussian processes (GP) but haven't been able to understand them properly. Does anyone have any good source to walk you through and guide on every single element of GP?",
    "created_utc": "2024-11-15T08:32:47",
    "num_comments": 18,
    "comments": [
        "Here's a good book that is also free: https://gaussianprocess.org/gpml/chapters/\n\nHowever I can explain gaussian processes to you, in their entirety, right here and now. A Gaussian process is a collection (of an infinite number) of Gaussian random variables that have some joint multivariate gaussian distribution p(x1,x2,x3,...). There is literally nothing else to them, every single fact or technique involving them follows from this.\n\nThe only thing that separates a gaussian process from a multivariate gaussian distribution is that the random variables in a gaussian process are indexed according to something like time or space. For example you might have a gaussian process written as X(t); this just means that each value of 't' indexes a distinct gaussian random variable X(t). \n\nThis is all easier to understand by thinking in terms of discrete sets of random variables. A gaussian process is just the continuous limit of this.",
        "I did my PhD in that, I can send you some stuff to help you understand that better",
        "Machine Learning: A Bayesian and Optimization Perspective\" by Sergios Theodoridis:",
        "Imagine a piece of string, you pin it to a whiteboard every place you get data (X,Y) and in the places in between where you don't have data, you can move the string up and down and use this to estimate both the range of possible values it could cover, and the most likely values at a position X. The kernel of the gp has parameters, like the stretchiness (length scale for rbf) of the string. The string is continuous, so you can pick a\nInfinite numbers of places to evaluate it at, by remembering where you pinned the string and how stretchy it is.",
        "Save",
        "What do you mean by “collection” in your sentence: “A Gaussian process is a collection (of an infinite number) of Gaussian random variables that have some joint multivariate Gaussian distribution p(x₁, x₂, x₃, …)”? Are you suggesting that a Gaussian process is a set of random variables, each of which follows a Gaussian distribution? If so, what is the purpose of this set and what do we do with it?\n\nAdditionally, what does it mean for a collection of Gaussian random variables to have a joint multivariate Gaussian distribution? Doesn’t any collection of random variables inherently have some joint multivariate distribution? If so, why is this property significant or necessary in explaining or defining Gaussian processes?",
        "hey man, I would love that material too. Can you post that in google drive and share the link maybe?",
        "Could you please send me those? I really need help to understand it",
        "hey can you send me a link too please? thanks!!",
        "Also this slides can be helpful for understanding the math. You can look at the first slide\\_MGD that talks about multivariate Gaussian distribution, and scaler\\_GP for basic GP.\n\n  \n[https://drive.google.com/drive/folders/1D-UPahKK32IP6eFTNVuy9PYHF2qxMA1x?usp=sharing](https://drive.google.com/drive/folders/1D-UPahKK32IP6eFTNVuy9PYHF2qxMA1x?usp=sharing)",
        "The purpose of multivariate gaussians is that they're the simplest distribution for a given mean and covariance matrix, so they're a natural choice for doing modeling. Gaussian processes are just multivariate gaussian distributions for which the marginal distributions are labeled by something like 't' or 'x' that indicates that the marginals represent random variables for which there is some notion of distance whereby some random variables are closer to each other than others are. \n\n> Doesn’t any collection of random variables inherently have some joint multivariate distribution?\n\nI don't think so, no. If I tell you that X1 and X2 are random variables, but I don't tell you what their joint distribution is, then their joint distribution is quite literally undefined.\n\nBut anyway it's significant that the joint distribution is gaussian because you can have a distribution P(X1, X2, ...) that is not gaussian, but whose marginals P(Xi) are gaussian. With gaussian processes its all gaussians all the time.",
        "How do I send it? That is the chapter that I wrote where I try to explain GP for scaler output, the most basic case.",
        "Thank you!\n\nWhen you say, “there is some notion of distance whereby some random variables are closer to each other than others are,” do you mean, for example, that X(t=i) is closer to X(t=i+1) than to X(t=i+10)?\n\nRegarding your statement, “X1 and X2 are random variables, but I don’t tell you what their joint distribution is, then their joint distribution is quite literally undefined,” why would that be the case? I thought a joint distribution simply represents the frequency of co-occurrence. Shouldn’t there exist a joint distribution value for any specific pair (X1 = x1, X2 = x2)? I don’t understand why it would be undefined.",
        "Is it possible for you to share it with me via this link?\n\nhttps://www.transfernow.net/push/i/ayR9TXRZ",
        "This is the chapter that i wrote. Let me know if you have any question.\n\n[https://drive.google.com/file/d/1BHatDsFCAV2ns7Qtd6AId37R9ftRmH6L/view?usp=drive\\_link](https://drive.google.com/file/d/1BHatDsFCAV2ns7Qtd6AId37R9ftRmH6L/view?usp=drive_link)",
        "> do you mean, for example, that X(t=i) is closer to X(t=i+1) than to X(t=i+10)?\n\nYes exactly. This fact is typically used to create a model such that  X(t=i) and X(t=i+1) are more correlated than X(t=i+1) and X(t=i+10).\n\n> why would that be the case? I thought a joint distribution simply represents the frequency of co-occurrence.\n\nSure exactly, but the thing is that these are mathematical abstractions, not real things. If you specify two mathematical abstractions (say, two RVs with two distributions), then that doesn't tell you anything about a hypothetical third mathematical abstraction (a hypothetical joint distribution for the two). \n\nIn the real world, if you have two RVs then yes you can usually set up an experiment to try to measure their joint distribution. But even then this does not always exist. Quantum mechanics is famous for this; the position and momentum of a quantum particle are random variables, but they do not have a joint distribution.",
        "I understood Gaussian process the best by modeling noise in data around it. Try doing it that way and see if it helps :)",
        "thanks for the clarifications - great stuff!"
    ]
},
{
    "submission_id": "1gs0bzn",
    "title": "Survey on Non-Determinism Factors of Deep Learning Models ",
    "selftext": "We are a research group from the University of Sannio (Italy).\n\nOur research activity concerns reproducibility of deep learning-intensive programs.\n\nThe focus of our research is on the presence of non-determinism factors\n\nin training deep learning models. As part of our research, we are conducting a survey to\n\ninvestigate the awareness and the state of practice on non-determinism factors of\n\ndeep learning programs, by analyzing the perspective of the developers.\n\nParticipating in the survey is engaging and easy, and should take approximately 5 minutes.\n\nAll responses will be kept strictly anonymous. Analysis and reporting will be based\n\non the aggregate responses only; individual responses will never be shared with\n\nany third parties.\n\nPlease use this opportunity to share your expertise and make sure that\n\nyour view is included in decision-making about the future deep learning research.\n\n\n\nTo participate, simply click on the link below:\n\n\n\nhttps://forms.gle/YtDRhnMEqHGP1bPZ9\n\n\n\nThank you!\n\n",
    "created_utc": "2024-11-15T08:32:06",
    "num_comments": 0,
    "comments": []
},
{
    "submission_id": "1grzu2g",
    "title": "Best certification to land an interview for data scientist role?",
    "selftext": "Hi, I was working as an analyst in data domain. I am without job right now for last 3 months. What best certification can I add to my resume so that I can land interviews as a data scientist?",
    "created_utc": "2024-11-15T08:10:52",
    "num_comments": 13,
    "comments": [
        "None",
        "Best? PhD in Statistics. Minimum? MSc in Statistics.",
        "Likely the best thing you can do is find employment as an analyst again and convince the company to allow you to take on more data science oriented work or pivot entirely over time.",
        "To transition into a data scientist role, the right certifications can make a difference in showcasing your expertise and making your resume stand out. Since you already have experience in the data domain, focus on certifications that emphasize skills in machine learning, statistical analysis, programming, and data visualization.   \n  \n**Here are some top options:**\n\n* \\- Google Data Analytics Professional Certificate\n* \\- AWS Certified Machine Learning – Specialty\n* \\- IBM Data Science Professional Certificate\n* \\- Microsoft Certified: Azure Data Scientist Associate\n* \\- TensorFlow Developer Certificate\n* \\- Advanced SQL for Data Scientists (DataCamp or similar)\n\n**Key Advice:**\n\n* **Focus on Projects:** Certifications are valuable, but practical experience stands out more. Showcase projects on platforms like GitHub or Kaggle to demonstrate hands-on expertise.\n* **Networking:** Leverage platforms like LinkedIn to connect with recruiters and join data science communities.\n* **Tailor Applications:** Highlight transferable skills from your analyst role, like data cleaning, visualization, and basic modeling.\n\nFeel free to let me know if you'd like personalized guidance based on your current skills or any specific areas you want to focus on!",
        "I do like Coursera.",
        "Certifications won’t land you a job in DS these days.. if you can’t land the exact job/level you want, I’d look into more junior data science roles or analyst positions. However, if you are able to take some AWS, databricks, or GitHub courses, it’d be something you could beef up your resume with.",
        "AWS Certifications have some value for ML engineer and MLOps based roles. Other than that just focus on building your profile, maintain your github, contribute to some open source project or publish your own package in PyPi or R CRAN. \n\nBasically build your profile, there is no concept of 'Certified Data Scientist' out there as of 2024.",
        "Google's Data Analytics Professional Certificate or IBM's Data Science Professional Certificate are solid options that can boost your resume. However, certifications alone won't guarantee interviews - you'll need to showcase practical skills and projects too.\n\nYour best bet is to focus on building a portfolio of real-world data science projects alongside any certification you pursue. Employers want to see that you can apply your skills to solve actual problems. Consider contributing to open-source projects, participating in Kaggle competitions, or creating your own data analysis projects to demonstrate your capabilities. When preparing for interviews, you might find [interview AI](http://interviews.chat) helpful for practicing your responses to common data science interview questions. I'm on the team that created it as a tool to help job seekers navigate tricky interview scenarios.",
        "The best certifications to land a data scientist interview include:\n\n1. **Google Data Analytics Professional Certificate**: Great for building foundational skills.\n2. **AWS Certified Machine Learning – Specialty**: Perfect for showcasing expertise in deploying ML models on the cloud.\n3. **Coursera’s IBM Data Science Professional Certificate**: Covers essential tools and practical projects.\n4. **Interview Kickstart's Data Science & ML Program**: Tailored for landing top-tier roles, it focuses on technical skills, interview prep, and real-world projects.\n\nThese certifications, combined with hands-on experience and a strong portfolio, significantly boost your chances of landing interviews.",
        "I have done bSc in statistics and masters in operations research and 2 years of experience as an analyst",
        "Damn… 🥲",
        "Anything specific that you would suggest?",
        "Then you need no other certifications. You have them all. You only need the knowledge and experience."
    ]
},
{
    "submission_id": "1grz71u",
    "title": "Beginner learning Machine Learning through Orange Data Mining. Need help!",
    "selftext": "I'm learning how to do machine learning using Orange DM and I've hit a few roadblocks with a regression problem; determining the salary of an employee. Here is the [dataset](https://www.kaggle.com/datasets/rhuebner/human-resources-data-set) that I am working with.\n\n[Quick look at the dataset I am trying](https://preview.redd.it/dt6flzws331e1.png?width=2559&format=png&auto=webp&s=cda4606902397a2da8a7554e60e0a313ccc7f319)\n\nI've noticed that the variables don't have good separation for the salary of each employee, there isn't a very clear trend. As a result, I'm not sure what model to use.\n\nhttps://preview.redd.it/aycbfeed431e1.png?width=1692&format=png&auto=webp&s=165419890002561938bc9b02280a41a21a12fb4a\n\nI've tried many models, but all of them either overfit or underfit the dataset by an egregious amount. No matter how I tweak the models, it still fails to correctly fit the dataset.\n\nhttps://preview.redd.it/owzuqtdo431e1.png?width=1637&format=png&auto=webp&s=5b23c8b7de47e7d2820c377f04e9bbd10acebd6a\n\nI'm at my wits end, I've tried all kinds of things to preprocess the data, but I can't figure out a solution. Can someone who's better at Exploratory Data Analysis or Machine Learning in general teach me a thing or two on how I can figure out what to do with my dataset? Thanks!",
    "created_utc": "2024-11-15T07:43:11",
    "num_comments": 1,
    "comments": [
        "I've tried PCA, normalizing the dataset, imputing for the missing data, encoding the categorical variables, but no matter how I try (though I just found out Orange DM's models have built in preprocessing), the models just don't fit the dataset very well..."
    ]
},
{
    "submission_id": "1gryu11",
    "title": "Kindly review my first ever ml project",
    "selftext": "so i started out ml a month ago and this is my first ever project..i kinda used chatgpt and copilot here and there...please give feedback\n\n[https://optionspricinglstmvsblackscholes.streamlit.app/](https://optionspricinglstmvsblackscholes.streamlit.app/)",
    "created_utc": "2024-11-15T07:27:12",
    "num_comments": 2,
    "comments": [
        "Looks very cool! What are the next things you’ve planned for this?",
        "Same here, beginner in dl. Looks interesting.\nCan I DM?"
    ]
},
{
    "submission_id": "1grxm8v",
    "title": "Best online courses for relevant NLP techniques for Social Sciences?",
    "selftext": "Hi, I know my way around Python and I have done a few courses on how to visualize/analyze table/matrix data (mostly numerical, boolean etc.) and how to make prediction models.\n\nHowever, I would like to learn to analyze textual data: at least topic modeling and sentiment analysis, maybe some other techniques as well that are relevant to Social Scientific research settings (let’s say using social media or digitized governmental reports for research data). Probably data mining and preprocessing should be included too, since textual data is new to me. I am interested in both qualitative and quantitative analysis of text data.\n\nSuggestions on courses? I have good previous experiences from Udemy, but I haven’t studied NLP.",
    "created_utc": "2024-11-15T06:31:10",
    "num_comments": 3,
    "comments": [
        "Use LLMs",
        "[The Bitter Lesson](http://www.incompleteideas.net/IncIdeas/BitterLesson.html)"
    ]
},
{
    "submission_id": "1grxfsu",
    "title": "Should I continue training or not?",
    "selftext": "I am training using randomforest classifier for a model on Google colab.. and I'm not sure of how colab works.. So this is the state of backend in colab rn..\n\nYou are not subscribed. Learn more\nYou currently have zero compute units available. Resources offered free of charge are not guaranteed. Purchase more units here.\nAt your current usage level, this runtime may last up to 82 hours 20 minutes.\nWant more memory and disk space? Upgrade to Colab Pro\nPython 3 Google Compute Engine backend\nShowing resources from 6:22 PM to 7:33 PM\nSystem RAM\n11.2 / 12.7 GB\n \nDisk\n32.7 / 107.7 GB\n\nSo please suggest whether i should continue training or not.. The ram and disk has been stuck with 11.2 and 32.7... So i think it stopped computing... Please tell...",
    "created_utc": "2024-11-15T06:22:27",
    "num_comments": 0,
    "comments": []
},
{
    "submission_id": "1grwdp1",
    "title": "help with teachable machine",
    "selftext": "how do i utilize the model i have trained using trainable machine...i have downloaded it ...like how can i implement it in my project",
    "created_utc": "2024-11-15T05:30:33",
    "num_comments": 3,
    "comments": [
        "I don't understand what you mean",
        "assign a variable to model object",
        "i downloaded the ml model into my machine and it has two files listing metadata.json and model.json how should i implement it in my project i tried doing it using streamlit ,but that's not working."
    ]
},
{
    "submission_id": "1grvmai",
    "title": "Will be ML oversaturated? ",
    "selftext": "I'm seeing many people from many fields starting to learn ML and then I see people with curriculum above average saying they can't find any call for a job in ML, so I'm wondering if with all this hype there will be many ML engineers in the future but not enough work for all of them.\n",
    "created_utc": "2024-11-15T04:49:41",
    "num_comments": 120,
    "comments": [
        "No.\n\nTrue ML is hard, takes time (alot of deliberate practise/ trial and error) and a very sound understanding of math.\n\nSomething most of the people cant replicate so easily. Trend jumping isnt new. Building a basic model with the help of GPT or watching a course wont make you “good” at ML.",
        "I'd say it already is. It's oversaturated in that there are significantly more people who want to work in ML than there are opportunities to work in ML.\n\nThis isn't unusual and isn't restricted to ML. A field booms and say industry needs 100k people to fill new roles related to ML per year. Suddenly, 1 million people per year think, \"Hey, I'll learn ML and get one of these jobs.\" Simple numbers say that only 1 in 10 will get one of these jobs.",
        "jacob devlin, ian goodfellow, shiv shankar, alex wang, ashish vaswani, cathy wong, they have all made significant contributions in the field of machine learning inspite of being undergrads at that time. so do not tell me a person \"need a PhD, research experience and publications in top conferences at minimum to be good at ML\" because this is absolutely not true at all. this is the same attitude the people at stackoverflow followed and look where stackoverflow is now.\n\nthis sub is full of such gatekeepers. you can learn machine learning even if you only know how to code. will you be able to contribute to ML research? probably not. but i am sure you don't want to either, you probably want to get a job, or make a pet project. don't worry, you can do it. you will get stuck, but that's true even for people in other fields or people with phds in ML. everyone gets stuck. so that should not stop you from pursuing what you want to. don't let these naysayers demotivate you. you decide for yourself whether this is something you like or not. if not then move on to something else. if you do like this however, go for it.",
        "I’d be curious to know what people think in terms of how saturated ML is relative to other fields of work, that I’d believe are larger and more mature, as reference.  For example, data analyst, product manager, product designer, software developer.\n\nThese fields are all more mature and therefore by definition saturated (though of course as new businesses emerge, new jobs emerge), and so getting jobs is competitive, but not impossible. Like anything, the bar just gets raised as the competition grows.",
        "It already is 🤷‍♂️",
        "We put an opening up for an ML scientist (minimum MSc+Experience or PhD). It had over 100 applicants before the end of the week. It's only going to get worse. Don't go into ML looking for an easy ride, only do it if you love it.",
        "I think the average ML engineer cannot compete with a subject matter expert who has incorporated ML techniques into their work. In other words, a physicist who has incorporated ML into their skillset is going to outcompete a computer scientist who is applying their ML skills to physics.\n\nSome of ML work is subject/technology agnostic, simply doesn't support the notion of a subject matter expert, or is far more about the framework, pipeline, stack, deployment, etc. and this is where an ML engineer stands out.",
        "It already is. But the reality is that getting a job is not about your qualifications, is about networking. Even people with the PhD (so they know the math and stuff) are unable to land jobs, so is obviously saturated, is kinda ridiculous that people think they will land jobs just because \"is hard\" or \"needs maths\", ask a mathematician if they were able to land a job just because they knew hard stuff, or a physicist... Even Einstein could not land a job because he didn't do networking... \n\nDo networking, start searching since the beginning, ask for internships... If you only do the courses you will end up literally nowhere.",
        "Curriculum above average doesn’t get you an ML job.",
        "What are you trying to convey with \"curriculum above average\"? When they had interviews, did those people communicate well enough for their interviewers to understand that those classes they took gave them the necessary skills to perform the job they applied for? Do those folks have personal projects that they could use to clearly demonstrate that they can problem solve?\n\n  \nEmployers care more about experience than education. Your degree gets your foot in the door but what you learn from it to make things and your ability to network is what get's you the job.",
        "I will say \"no\".\n\nYoung kids fresh out of school with generalist \"ML\" degrees will struggle to find employment, like all other young kids fresh out of school.\n\nThe thing is that ML is incredibly useful to businesses.  Using it to sell more stuff (marketing), using it to reduce costs and increase efficiency (operations) or using it to manage money (accounting/finance) make it highly valuable.\n\nThe issue is that generalist tech people usually aren't also subject matter experts in these areas.\n\nI will guarantee you that a guy with a deep understanding of ML who also has an MBA from HBS/Wharton/Stanford/Booth is probably minting money all day long right now.  This is nothing new, but the skill of being able to apply data science to business problems is worth a lot of money to a lot of people.",
        "I know „ML engineers“ that tried to train a dnn for a problem that could be solved with 5 if clauses. How do I know it can be solved like this? Random forest told me.. understanding of ML is so much more important than having a model producing more or less good outputs",
        "I’d say it depends. In 10 years I could see this skillset being equivalent to advanced excel knowledge. I’m not good with ML so I could be wrong. At minimum I do see being able to use python effectively as the new msft excel. A core understanding of the problems you’re solving + the ability to analyze large amounts of data will be what separates a good knowledge worker from a phenomenal and impactful person. ",
        "There are more people in ML than there used to be in DataWarehouse and BI teams. But it is not something where I see an explosion in demand. And existing staff learns new skills - knows the data and company. \n\nBut many people use genAI to write lots of fake job applications etc. that means the job Application becomes less relevant as I can’t judge a person based on a text he did no write.",
        "While many people are entering the field of machine learning (ML), the demand for ML professionals remains strong, especially in applied roles. Applied ML involves integrating existing models into specific domains, requiring backend development skills and knowledge of ML infrastructure. This area is expected to grow as new applications emerge.\n\nConversely, developing state-of-the-art models is concentrated in a few large corporations, making those positions highly competitive and potentially leading to fewer startups and open roles in that niche. \n\nOverall, while competition may increase, the expanding opportunities in applied ML suggest that the field is not becoming oversaturated.",
        "It’s already saturated, at any level.\nIn the industry, LLM have also abstracted so many NLP tasks, and they can significantly help with coding the models for the remaining tasks.\nIn the academia and research lab, the competition is fierce and there are no more low hanging fruits, so you really have to be exceptionally smart to justify the PhD route.\n\nAt this point when companies say ML they just mean a software engineer that know ML lifecycle, and can design architectures around it. The age of dedicated roles to play around with hyper parameters and tensorflow models is gone.",
        "Nope cuz it involves math.",
        "Most ML engineers I know don’t even know how to set up a proper dev env utilizing gpu and stuff",
        "Till when intuition and parallelism between concepts are out of the DL radar you will be fine",
        "It already is",
        "I think there is too many people thinking they really know ML but there is only handful who does.",
        "Depends what you mean. Research and deeper work with models built from scratch? Probably not. The other 99% of positions? Yeah, probably.",
        "Its two types of people, those who just do ML, and those who revolutionize and bring ML to new levels. We have way to many people of the first group, and not enough in the later.",
        "During my first semester of Masters in ML, I was more of a dot fit and dot predict guy. Once I took 3D CV course I understood the full extent of how deep it goes. Understanding the math behind the models isn’t easy and although a lot of people can do the basics of ML, most of them won’t be excel in the research aspect. And that’s what ML is about.",
        "Like many others have said no. In data science alot of people also claim to be unable to find a job. You look at their post history they don't know how to do a time series analysis and most of their experience is in excel doing baby statistics. \n\nAlso alot of posters complaining about unable to get a job are people from other countries that require sponsorship. If you require sponsorship right now you will have a very hard time finding employment in the U.S.",
        "Hello guys, I am new to ML, and I am working on a project which requires knowledge in ML. I urgently need a tutor to teach me how this goes.",
        "I also feel that intuition is quite important in ML making it hard to fully automate",
        "This. Most people just follow tutorials and make simple models. The actual math behind it is quite hard to understand especially when you go for DL. It took me quite some time to re-watch videos just to understand gradient descent at an OK level.",
        "Fact of the matter is that machine learning is ALREADY OVERSATURATED as a field. But as you stated above, being competent in ML and developing it yourself with enough understanding to be dangerous in any role is different than being some novice level user who can push the buttons and use the tools in a haphazard manner like most in the field.",
        "Will having a good intuition and math skill put me above people in ML?",
        "Preach!",
        "I don't think it's that hard, I've been doing it for 8 years and I've found it's not actually difficult. But no one understands what I do so they all think it's difficult",
        "second this",
        "Got any pro tips for a guy like me who knows how to bridge harmonic physics into a foundation model?",
        "There’s not that many true ML jobs. It’s already oversaturated.\n\nWhat you see in the industry are data and ops jobs that call themselves ml jobs",
        "You need a PhD, research experience and publications in top conferences at minimum to be good at ML.",
        "I'm not sure I agree with this definition of oversaturation.\n\nMany people might be trying to break in to ML, but how many learn enough to actually make valuable contributions?\n\n>say industry needs 100k people\n\nI don't think we can say that.  There is not a fixed amount of work that needs to be done, opportunities are essentially infinite.\n\nSuppose a team of researchers discovers something new (example, a new architecture called \"transformers\").  The result is that there is now more work to be done to develop and expand on that idea.  In other words, the result of work being done is that there is now exponentially more work that needs to be done.  \n\nWhat we can say is that the bar becomes higher and higher, and the knowledge required to become employable continues to grow.",
        "Of course everyone can learn it.  \nBut it's important to not underrestimate the importance on Linear Algebra fundamentals.",
        "You would assume ML PhD know all the math behind everything, let me tell you the truth, no we don’t. We fucking google it when we need it, but you will need to know what to google.",
        "Great points. I work with PHDs / math experts and they have a hard time building solutions. \nIt’s not so black and white, and I question if the gatekeepers even work in the field. \n\nThere’s a great pod on Lex with the Anthropic ceo and he speaks on how his big contributions to the field were just posing the question of adding more layers / scaling the models. I bring that up to say that people can make significant contributions to the field even without the prestige of academia. Sometimes it comes down to being willing to play and explore ideas. The only limitations, in my opinion, are data and compute.",
        ">you can learn machine learning even if you only know how to code\n\nYou can slap together existing models with existing datasets, but so can many others.\n\nBut will you understand what the inputs/outputs are? Will you understand what the metrics mean? Will you understand what the operations do, and what representations they work on? What principles they are based on? Which algorithm is suitable for which data? Will you be able to \"debug\" a model that just doesn't want to learn?\n\nI'm currently in the middle of getting into all of it, and maybe I'm dumb or something, but to me this is complicated as fuck and I still don't get it. And that is AFTER I had several math classes (linalg, calculus, statistics) and ML courses.",
        "Would it be correct to say if i ve a sister fucking good kaggle profile, like a grand master, companies won't ask for masters or publications?",
        "90% are AI applicants",
        "This! Some use cases can be solved with pure math + ML, but on many domains also requires the domain experience.",
        "So true, it seems to be it's not enough to be a \"generic ML engineer\" you have to apply that knowledge in specific projects or fields.",
        " Yeah networking I have read it, even from really good programmers, for me the only option can be online networking cause where I live there is no tech people around and even less in ML area. I just came into reddit looking for a community.",
        "This is a good answer, thank you. Applied ML is the key.",
        "they used to say the same thing about engineering in general...",
        "You have seen what I have seen..and they use sharepoint as git and don't even ignore pycache dirs. fucking morons.\nit's all a scam & hype lol",
        "Agree, although of course in the second group people should have the right environment.",
        "It's true, but some people say that to do AI applications is not that necessary to deeply know the math behind",
        "The project is about Adversarial attacks",
        "Agreed!",
        "Maybe this is my ignorance showing, but wouldn't neural nets excel at tasks that rely on intuition? If I had to pick which cognitive phenomenon or process a neural net most closely resembled, it would be a sort of subconscious/intuitive mode of thought. \n\nWhen I look at the people working on the problem of mechanistic interpretability, I get a similar feeling as I did when learning about neuroscientists' efforts to interpret unconscious thought via brain scans.",
        "Be prepared to be downvoted bro lol.\n\nI still have a math book on multivariate analysis that I have barely scratched the surface off despite being in education FT.\n\nAnd there’s people here thinking ML is going to be saturated by every other random dude.",
        "just go in the decreasing direction? lol you guys always think you are doing rocket sciece",
        "bro this math is fucking basic, I learned this shit in multivar calc in high school. The math in DL is freshman lin alg and stats AT BEST lol",
        "Thise are the prerequisites. How effectively you use them is totally up to you. You won't be hired just because you have these qualities, but they will help you build up the ML knowledge that employers are interested in.",
        "Im not sure how you quantify intuition and am also not sure of your math skill.\n\nHowever, both of these are very prized features in ML. \n\nHaving a math skill, will make things considerably easier and give you the depth of whats going on “behind the scenes”.\n \nSomething that helped me get better (im not super good at ML before someone attacks me) is doing a wide range of models and actually going into depth rather than just focusing on making a small and simple model.",
        "So my cert program was a lie?",
        "Alec Radford had none of these when he joined openai. He then went on to lead work on gpt-1, gpt-2, clip, whisper and many other non-public work.\n\nA similar case with Rohan Anil, Jeff Dean and Teknium. None of them had phd when they started working in ML.\n\nA solid understanding of high school math in, perseverance, lots of trials and failures and high attention to details is what's needed to be a good ML practitioner.\n\nThis is coming from someone with 4 years of experience in ML building cutting edge ML applications in industry.",
        "LoL",
        "From the context of OP's post, I'm taking oversaturation to mean that the number of people actively looking for or working towards a job in ML >> the number of available positions in ML.\n\nFor the purposes of this definition, the value of a person's contribution doesn't matter.\n\nOf course, there's not actually a fixed, pre-determined number of positions that we can know will be available. I wasn't suggesting there were. The numbers were just for illustrative purposes. Practically speaking, opportunities will obviously never be infinite. There will always be a limit to how many people can have employment in the ML space. Right now, more people want in than there are positions. Whether demand will expand to match supply in the future, nobody knows because nobody can accurately predict what will happen to demand or supply.",
        "This. If you judge a student on whether he would clear SAT when he is starting in std 1 then he will always fail. instead we should always encourage them and let them decide for themselves if this is something they \"like\" doing. i liked coding since i was 12, i have been coding since then and i knew this is what i like. do i look like something who understood algorithms when i was 12? i still did coding. had it been upto these people they would have told me not to code because i didn't know monte carlo method right after i was born! its so foolish.",
        "so you see maths is not the problem for you.",
        "getting a job depends on how good you present your credentials.",
        "Hindi gaali English me translate nhi hoti bhai",
        "I have thought the same, that there are tons of fake applicants",
        "That’s Applied Machine Learning. When you work as an MLE, you need to know the math behind and truly understand the concepts to effectively apply it in research or to solve any problem at hand.\nBased on the type of work you are into, the ML understanding requirement is definitely different. As a person in CV based research mostly my work is to “develop” models for different tasks. I can easily just pull a hugging face model and fine tune it but it doesn’t cater to a niche task when the data availability is low. So having a complete model development process would require fairly solid understanding of ML concepts.",
        "Empire strikes again?",
        "I think the most intuitive parts of ML have more to do with all the little decisions you make in creating your model. How to clean and frame your data, what sort of algorithm to run, etc. \n\nA neural net isn’t always the best for a given task. You might need a counter intuitive transformation somewhere, a dummy variable or something. Theres lots of little decisions that need to be made leading up to feeding data into the model that any given model will have no understanding of (until we get GPTs building models or something lol).",
        "🥲",
        "Can you give the name of the book?",
        "So ML will never be saturated then, because of the complexity in it?",
        "Yeah but you are just simplifying it to the point anyone can understand. A lot of things need to be clear to truly understand gradient descent. Diffentiability, relationship between gradient and steep ascent, partial derivatives, effect of step size etc.\n\nYou could literally say rocket science is just pointing the burning side down.",
        "There's a lot of gatekeeping and elitism in ML/DS. For 95% of use cases, domain knowledge is way more important than advanced math. You don't need to be able to derive ML algorithms. Vast majority of the time you need a general understanding of how they work.",
        "Glad it wasn’t just me who thought this…",
        "rocket science? just go up lol",
        "yeah lol I know, but it was hard for me to understand when I watched 3b1b's video.",
        "As if gradient descent is all there is",
        "Like for example, if I'm going into a topic like logistic regression I'll try to cover all the math bases first, like how it's fit is determined, t-test, approximation of the curve and other stuff. Basically math gets 1st importance then programming and implementation for me. Right now I've started ML with StatQuest and it's going great!",
        "There are gatekeepers everywhere, majorly in this sub. Don't be bothered. You can learn and practice ML knowing some high school math.",
        "No, it makes you certified in using / having basic understanding of the methods presented in that program. It does not make you a researcher, though.",
        "That's what I would like to do in the future with my startup, build ML applications that can be useful, I have a solid understanding of math cause I have a BS in industrial engineering and I have been improving my coding skills etc, later I will get into frameworks and libraries.\nDo u have any advice for me?",
        "Lol I didn't make the sarcasm obvious enough...\nInteresting to see the law of \"just post the wrong answer and people will give the right one\" hold true.",
        "So what do you think is the value that you can bring by only knowing how to code?",
        "Presenting it as “sister fucking good” is a sure winner though, HR loves that shit",
        "Its an old one called\n\nMathematical Tools for Applied Multivariate Analysis- Green",
        "These are all stuff that you can learn in any calculus 3 course, maybe pick a different example.",
        "Disagreed. Do not underrestimate the importance of learning the advanced math behind it.  \nAnd yes you don't need to derive the algorithms but being able to shows that you understand the mechanics and purpose behind it.  \nA deep understanding of the fundamentals is important.",
        "the funny part is that all maths in ML are the standard math courses in any engineering degree, I am not sure why people think it is advanced, is it because in CS they barely do any advanced math or what?",
        "Not gatekeeping. Understand the difference;\n\n1. Can you learn Basketball from YouTube videos and get decent at it? Yes.\n\n2. Are you going to be drafted for the NBA? No\n\n3. Is NBA going to get oversaturated- now that anyone can learn how to shoot? Yeah fuck no",
        "Not everyone aims to be a researcher in ML though.",
        "Build something from scratch. At least one model. Maybe try NanoGPT and code it in pytorch.  \nYou learn a lot of stuff and it won't even take you 1 week (assuming you studied the prerequisites)",
        "i was talking about you. i thought you were too. if even after learning the math you still don't get it then maybe you are not cut out for it. \n\nnow to answer some of your queries:\n\n1. \"But will you understand what the inputs/outputs are?\" - isn't this how functions work?\n2. \"Will you understand what the metrics mean?\" - the metrics is basic math not rocket science. if you understand coding it means you understand logic, and if you understand logic i see no reason why you won't understand the metrics (unless that person is you)\n3. \"Will you understand what the operations do, and what representations they work on?\" - what operations? matrix operations? mlops? if its matrix operations you are talking about then they can do that using numpy (which is basically coding), don't think they need a phd for that\n4. \"What principles they are based on?\" - here's an example. linear regression takes some samples and creates a function that gives you the output of an unknow value. the function is formed using those samples. its done using a library called scikit-learn. easy peasy? you need to know how to teach based on the capacity of the student, you don't always get to teach einstein. oh and scikit-learn is a python library, back to coding?\n5. \"Which algorithm is suitable for which data?\" - seriously? do you think all data engineers are phds? they word with data all the time and even they understand this. this is not rocket science like you think it to be\n6. \"Will you be able to \"debug\" a model that just doesn't want to learn?\" - debugging in not maths, its coding. everyone gets stuck, you are too with your knowledge of maths. you cannot stop someone from learning because they \"might\" get stuck somewhere and not know the answer. if they get stuck, and they are coders, i am sure they would know their way around to find the answer. that's the beauty of coding you see\n\ni don't think you are dumb. i think you are an alt who is trying to prove what your main account couldn't. but good try.",
        "He's right you're wrong",
        "You're being downvoted, but I think you're right",
        "All you need to know to do ml well is understand its fundamentally just distribution matching. Any edge in knowledge is just to make that more efficient and to do it properly which not even the highest paid guys can do apparently. You need more than preexisting knowledge from papers to be first and the best, you need creativity, intuition and understanding.\n\nMore on topic, a job is to make money, and to make money you have to be a slimey grifter. You need skills to grift society like make gpt wrapper app and market that to boomers not know how to write the math equation for gradient descent, what youre likely going to do is use an api for the models.",
        "I don’t know if one could say they have a ‘deep understanding’ of a mathematical idea without being able to derive it. \n\nAlso, ‘advanced math’ is a relative term. Should you have a strong grasp of basic calculus and linear algebra? Yes. But do you need extensive knowledge of say, algebraic topology or complex analysis to make original contributions to applied research? Perhaps in certain cases, but generally speaking, no.\n\nI’m saying this from the perspective of a DSP guy transitioning into applied MLSP. At least as far as I can see so far, hands on experimental in-domain experience seems to be more central to research results than knowing say, wavelet lore.",
        "Advanced is relative. \nAt university, as maths undergraduates, you would raise eyebrows at Economics students trying to rack their brains over matrix multiplication and would say it was really hard.\nIf you aren’t used to it, it’s going to be advanced from one’s own perspective. So it’s unsurprising that folk who never did maths until now are possibly going to struggle.",
        "Knowing the principles and applying them is a different matter.\n\nFormulating the optimization problem for regression into the closed form expression only works when you have a very good understand of the Linear Algebra fundamentals. And most of the time a deep understanding of the fundamentals is far harder than a surface level understanding of advanced concepts.",
        "There's a lot more ML than just the \"NBA\" tier. I don't think you even belong to the \"NBA tier \" of ML.",
        "Stop comparing apples and oranges. Stop demotivating people. I have worked with enough people to know what he said is not true.",
        "Oversaturated means that that there are more people looking for jobs than there are jobs available. So yes, the NBA is completely oversaturated, and unless you are a phenomenal player you should probably find a different career path.\n\nNot true for ML though, since there are many more jobs available at all levels of the pipeline.",
        "Indeed, which is exactly why I am saying that the program that person went through is not a lie. I am actually disagreeing with the original comment, saying that you need a PhD and all that to be good at ML.",
        ">\"But will you understand what the inputs/outputs are?\" - isn't this how functions work?\n\nFunctions in ML take float arrays as input and return float arrays as output. If that's good enough for you, cool. Personally, I would like to know what these numbers represent. What does a high number mean, what does a low number mean. Is it probabilities, logits, a word embedding, your moms phone number, an embedding in some fucked up N-dimensional latent space?\n\n>the metrics is basic math not rocket science\n\nYou consider Cross-Entropy \"basic math\"? OK i mean good for you, maybe you're a genius or something. I certainly didn't learn it in high-school, I had to take time and effort to learn what \"surprise\"/self-information is, and what minimizing this cross-entropy means and how it relates a models outputs to the data its trained on.\n\n>what operations? matrix operations? mlops? if its matrix operations you are talking about then they can do that using numpy (which is basically coding), don't think they need a phd for that\n\nWhat's a convolution? What's the attention mechanism? What's a ReLu or Sigmoid? What do they do to the data? I mean ok you can write everything down as a matrix operation, just like you can write any program down as a series of CPU instructions. but that doesn't help you understand what's actually happening inside the model you're using.\n\n>linear regression takes some samples and creates a function that gives you the output of an unknow value. the function is formed using those samples. its done using a library called scikit-learn. easy peasy?\n\nOK but why should I use that function? And when? And when should I not? And what does it actually do? And why does it sometimes produce garbage? What other functions could there be that do a similar thing?\n\nIt's only \"easy peasy\" if you're following some tutorial with nice prebaked data, IRL things are a lot more complicated.\n\n>seriously? do you think all data engineers are phds? they word with data all the time and even they understand this. this is not rocket science like you think it to be\n\nI don't know, to be honest. I just know that there A LOT of algorithms with a lot of different properties and metrics that are beyond me. You seem to think of data science as slapping together a bunch of SciPy routines until you get the plot that you want, but personally I prefer to know what I'm doing beyond a level that's just \"from scipy.stats import linregress\".\n\nPro tip from someone who's clearly not a galaxy brain as you are: asking yourself the question \"but WHY is it the way it is\" often goes a long way to gain a deeper understanding.  \n\n\n>i think you are an alt who is trying to prove what your main account couldn't\n\nAnother victim of being terminally online lmao. Go touch some grass please",
        "Not sure why some people are against learning and understanding the fundamentals.",
        "He's wrong",
        "Slimey grifters have a short job livespan.\n\nAlso it depends on the ML problem you encounter.\nA job is to make money sure but if that‘s certainly a boring limit to set yourself. You shouldn‘t just limit yourself to what the job wants you to.\n\nYou get creativity and intuition by gaining a deep understanding of fundamentals. You certainly wont get that by just applying the Models through APIs. Its also very boring.",
        "The parts about linear algebra are easy. It's when you go to continuos bayesian probability optimization that you want to kill yourself. So many hypothesis that you can wrongly assume.",
        "I dont belong to the NBA tier. \n\nNever claimed I did. OP wants to know if the field would be saturated and I dont think thats going to happen as the skillset takes time and alot of deliberate effort.",
        "Its answering a question,\n\n“Will ML be oversaturated”\n\nNo, not everyone who can pick up the subject will have the aptitude to be high level. One can progress but to work in MLR or MLE it would require ALOT of skill and time.",
        "Never mind bro, I see why you do not get Machine Learning even after knowing the math.",
        "Not against , just saying domain knowledge is often more important. Guess you probably haven't worked much",
        "In comparison to bayesian probability optimization sure. But a lot is easy when put into relation to certain topics. That doesn't mean that \"linear algebra is easy\".",
        "no its not. its not true that \"need a PhD, research experience and publications in top conferences at minimum to be good at ML\" because this is absolutely not true at all.\n\nanyone asks you can i play basketball in future, you don't tell them no because you are not going to be drafted in NBA. that itself is gatekeeping. you tell them if you play well you can. not everyone starts playing basketball thinking about being drafted in the NBA. most just like the game and they play. do they get drafted? who know what prodigy lies hiding in those curios minds. but if the first thing you tell them that you will not be drafted in the NBA so no point playing, or that you cannot learn from your local coach because that will not get your drafted in NBA so your efforts are useless then you are simply gatekeeping because you are afraid someone might take your job.\n\nits disgusting seeing this sub being full of people like you because clearly you all know nothing about machine learning or teaching.",
        "You both have different definitions of what it means to know or learn machine learning. You appear to be thinking about being able to use an existing machine-learning algorithm or pre-trained model on data. The other person seems to be thinking about understanding the concepts in much more detail and trying to think about why certain things work the way they do to the point where you are coming up with new algorithms. \n\nIt's like the difference between a machine learning engineer and a machine learning scientist. A machine learning engineer is like a software engineer who works with machine learning algorithms and models to solve problems and often deploys them in production. A machine learning scientist focuses more on theory and developing new models. Generally, ML scientist and data scientist job openings ask for Ph.D.s or publications, while data engineer and ML engineer jobs usually don't.",
        "Domain knowledge is much more easily aquired than the raw fundamentals.\nIn fact, the main issues in aquiring domain knowledge is lack of proper documentations and guidelines of processes.\n\nI worked plenty, more than you would even know.",
        "Absolutely, linear algebra is complex but almost all STEM students can handle it with some exercise. Some topics of ML would need a degree in maths, statistics or an equivalent preparation though",
        "I never said you need a PhD or publications\n\nI do believe that research experience (real time) is required to be good at ML.\n\nIf someone is worried about employment in the future, clearly it’s better to tell him there would be employment and just because there is alot of attention and spotlight on ML doesnt mean it would be saturated?"
    ]
},
{
    "submission_id": "1gruuku",
    "title": "Weird Interaction with Keras Validation Set",
    "selftext": "Hello guys!\n\nI have a question about how Keras handles validation. I have a pre-trained model (ResNet34 with a U-NET architecture) and want to train on a custom dataset for binary segmentation. I created a validation set of 60 images and when I try to set an EarlyStopping with the val\\_loss, I have weird results. The reason why I find them weird, is because I'm using the same directories for both train and validation, since the model wasn't learning and it returned full black or full white images. My main question is, even with regularization terms and drop out layer, is this behavior logical? It just looks wrong. When I trained the model without validation it seemed to work and learn. I had an accuracy of \\~80%. I'm using a library for pre-trained models called segmentation\\_models, which uses Keras framework.   \n( [https://github.com/qubvel/segmentation\\_models/tree/master](https://github.com/qubvel/segmentation_models/tree/master) )\n\nThis link is a Colab notebook for the code, [https://colab.research.google.com/drive/1jWhR3ZJJuI3YMctd8bs95YaQ5wmMkD24?usp=sharing](https://colab.research.google.com/drive/1jWhR3ZJJuI3YMctd8bs95YaQ5wmMkD24?usp=sharing)\n\n  \nThe Results:\n\n||\n||\n|Epoch 1/200 60/60 \\[==============================\\] - 32s 473ms/step - loss: 0.3068 - iou\\_score: 0.7164 - val\\_loss: 2.5515 - val\\_iou\\_score: 0.1365 |\n|Epoch 2/200 60/60 \\[==============================\\] - 28s 465ms/step - loss: 0.1274 - iou\\_score: 0.8831 - val\\_loss: 1.1177 - val\\_iou\\_score: 0.3766  |\n|Epoch 3/200 60/60 \\[==============================\\] - 27s 444ms/step - loss: 0.0852 - iou\\_score: 0.9224 - val\\_loss: 1.6045 - val\\_iou\\_score: 0.3112|\n|Epoch 4/200 60/60 \\[==============================\\] - 27s 454ms/step - loss: 0.0603 - iou\\_score: 0.9445 - val\\_loss: 0.9970 - val\\_iou\\_score: 0.4547|\n|Epoch 5/200 60/60 \\[==============================\\] - 27s 459ms/step - loss: 0.0493 - iou\\_score: 0.9545 - val\\_loss: 0.8126 - val\\_iou\\_score: 0.3551|\n|Epoch 6/200 60/60 \\[==============================\\] - 27s 447ms/step - loss: 0.0336 - iou\\_score: 0.9679 - val\\_loss: 2.2715 - val\\_iou\\_score: 0.6446 |\n|Epoch 7/200 60/60 \\[==============================\\] - 27s 445ms/step - loss: 0.0289 - iou\\_score: 0.9730 - val\\_loss: 1.9526 - val\\_iou\\_score: 0.6421|\n\n",
    "created_utc": "2024-11-15T04:04:44",
    "num_comments": 0,
    "comments": []
},
{
    "submission_id": "1grtxsi",
    "title": "what websites, resources should i use to learn DS and especially ML? sometimes have like 30 minut time bites where i dont have any work have nothing to do etc. no big articles that require my full attention. more like things that brieflt explain basics or new resesrch, models, etc. statistics is al",
    "selftext": "what websites, resources should i use to learn DS and especially ML?\n\nsometimes have like 30 minut time bites where i dont have any work have nothing to do etc. no big articles that require my full attention. more like things that brieflt explain basics or new resesrch, models, etc. statistics is also acceptable not only ML",
    "created_utc": "2024-11-15T03:05:24",
    "num_comments": 0,
    "comments": []
},
{
    "submission_id": "1grtcmt",
    "title": "The lost Reading Items of Ilya Sutskever's AI Reading List",
    "selftext": "",
    "created_utc": "2024-11-15T02:23:04",
    "num_comments": 0,
    "comments": []
},
{
    "submission_id": "1grt3et",
    "title": "Blog post: Use cases of AI in driverless cars.",
    "selftext": "",
    "created_utc": "2024-11-15T02:03:40",
    "num_comments": 0,
    "comments": []
},
{
    "submission_id": "1grrth4",
    "title": "Gemini-exp-1114 tops LMArena leaderboard",
    "selftext": "Google's experimental model Gemini-exp-1114 now ranks 1 on LMArena leaderboard. Check out the different metrics it surpassed GPT-4o and how to use it for free using Google Studio : https://youtu.be/50K63t_AXps?si=EVao6OKW65-zNZ8Q",
    "created_utc": "2024-11-15T00:24:39",
    "num_comments": 1,
    "comments": [
        "91club prediction hack "
    ]
},
{
    "submission_id": "1grrtgw",
    "title": "E2GAN Reproducibility",
    "selftext": "Hi , my name is Damiano, do u have any tips to do this paper?\nhttps://arxiv.org/abs/2401.06127\n\nThank u to all 😄",
    "created_utc": "2024-11-15T00:24:38",
    "num_comments": 2,
    "comments": [
        "Found [2 relevant code implementations](https://www.catalyzex.com/paper/arxiv:2401.06127/code) for \"E$^{2}$GAN: Efficient Training of Efficient GANs for Image-to-Image Translation\".\n\n[Ask the author(s) a question](https://www.catalyzex.com/paper/arxiv:2401.06127?autofocus=question) about the paper or code.\n\nIf you have code to share with the community, please add it [here](https://www.catalyzex.com/add_code?paper_url=https://arxiv.org/abs/2401.06127&title=E%24%5E%7B2%7D%24GAN%3A+Efficient+Training+of+Efficient+GANs+for+Image-to-Image+Translation) 😊🙏\n\nCreate an alert for new code releases here [here](https://www.catalyzex.com/alerts/code/create?paper_url=https://arxiv.org/abs/2401.06127&paper_title=E$^{2}$GAN: Efficient Training of Efficient GANs for Image-to-Image Translation&paper_arxiv_id=2401.06127)\n\n--\n\nTo opt out from receiving code links, DM me.",
        "Unfortunately one is not a code and the other one is a repo with the same name.\n\nThe authors aren't allowed to share the code."
    ]
},
{
    "submission_id": "1grnz0u",
    "title": "I am sharing Machine Learning courses and projects on YouTube",
    "selftext": "Hello, I wanted to share that I am sharing free courses and projects on my YouTube Channel. I have more than 200 videos and I created playlists for learning Machine Learning. I am leaving the playlist link below, have a great day!\n\nMachine Learning Tutorials -> [https://youtube.com/playlist?list=PLTsu3dft3CWhSJh3x5T6jqPWTTg2i6jp1&si=1rZ8PI1J4ShM\\_9vW](https://youtube.com/playlist?list=PLTsu3dft3CWhSJh3x5T6jqPWTTg2i6jp1&si=1rZ8PI1J4ShM_9vW)\n\nMachine Learning Projects -> [https://youtube.com/playlist?list=PLTsu3dft3CWg69zbIVUQtFSRx\\_UV80OOg&si=go3wxM\\_ktGIkVdcP](https://youtube.com/playlist?list=PLTsu3dft3CWg69zbIVUQtFSRx_UV80OOg&si=go3wxM_ktGIkVdcP)\n\nData Science Full Courses & Projects -> [https://youtube.com/playlist?list=PLTsu3dft3CWiow7L7WrCd27ohlra\\_5PGH&si=6WUpVwXeAKEs4tB6](https://youtube.com/playlist?list=PLTsu3dft3CWiow7L7WrCd27ohlra_5PGH&si=6WUpVwXeAKEs4tB6)",
    "created_utc": "2024-11-14T20:11:23",
    "num_comments": 0,
    "comments": []
},
{
    "submission_id": "1grnonk",
    "title": "Teaching myself to predict final score NBA",
    "selftext": "Hi gents and ladies,\n\nI am slowly teaching myself Python. I rcently scraped NBA stats for the last 12 seasons and adjusted someone elses code to use ridge regression to determine who wins the game.( He didn't show how to do future prediction). Only 65% accuracy on backtest.\n\n1. I cannot figure out how to write up the code to make future predictions using ridge regression, is it hard? Can someone give me some clues please.\n\n1. I want build another model to predict final score for home and away team. I am thinking Random Forest is a good model to try? How do I go about it? Do I create two targets? One for home score and one for away score?\n\nI am struggling to find any good guides for sports predictions, any help is appreciated.\n\nThank you",
    "created_utc": "2024-11-14T19:55:08",
    "num_comments": 4,
    "comments": [
        "I’m building a tool that can automate building prediction models for you: https://plexe.ai. We’ve got a couple of people on our discord who’ve built similar prediction models already, so if you want, you can get in touch! \nYou just need to provide your data and the goal of the prediction model, and the model will be built for you.",
        "Random Forest would be interesting, maybe turn it into a classification problem like win/lose game first and then use the binary win lose data to see if that’s a feature that is important to better predict final score? I’m solving a similar problem and curious to see what others say. I’m currently turning a random forest program I made into a classification problem to see if there’s better predictability and to generate new features",
        "Thanks, I will check it out. Not for free I assume?",
        "Currently we’re offering first model for free for a small dataset :)"
    ]
},
{
    "submission_id": "1grnat4",
    "title": "How to fetch image given the name of an item using LLM?",
    "selftext": "Newbie and non-developer trying to learn building web application using no code tool.\n\nIf I have an item say \"burrito\" and want to use a LLM to return an image of the item, which model can do it? Doing a general search suggests using unsplash api or pixabay api. But since I'm trying to learn to use a model/LLM, want to see how to do this? TIA",
    "created_utc": "2024-11-14T19:33:18",
    "num_comments": 0,
    "comments": []
},
{
    "submission_id": "1grl97y",
    "title": "Need Help with Document Verification Project Using YOLO for Text Extraction",
    "selftext": "Hi all,\n\nI’m working on a document verification project using YOLO for character recognition and text extraction but could use some guidance. So far, I’ve implemented YOLO weights to detect characters, but I’m unsure which dataset would best suit this project.\n\nAlso, any advice on what to focus on next for efficient text verification (e.g., preprocessing techniques, NLP methods) would be great. Open to suggestions and resources!\n\nThanks in advance for your help!",
    "created_utc": "2024-11-14T17:45:15",
    "num_comments": 1,
    "comments": [
        "Use paddle ocr of text detection."
    ]
},
{
    "submission_id": "1grkhtp",
    "title": "How do I figure out what mix of convolutional layers, channels and Fully connected layers?",
    "selftext": "Hi!, So my team and I are working on a CNN model to detect brain tumor thru MRI images for a class. I chose a dataset, I now don't remember it's source other than that its from kaggle. It has 4 classes. 3 tumor types and 1 no tumor. \n\nI have made a model using RELU, 4 Conv layers and 2 fully connected layers and 256 channels at the last conv layer. I get an accuracy of beyond 70%? There are just 3000 images in total in the dataset.\n\n  \nI am using the RELU activation function btw.\n\nI'll be honest. This class was more about self learning and more project based. So while I have learnt how to mimic the code, I wouldn't say I fully understand why we have conv layers and fully  connected layers. Why they are different or how different activation functions affect the outcome.\n\n  \nI do plan on reading up on the theoretical side of this during the winter break. But for now I am stuck with half knowledge.\n\n  \nI have tinkered around with a few combinations of pooling, differnet amounts of layers etc to get better accuracy. But It just gets worse every time. So my question is: is there a specific method to know what combination of the layers, pooling and other hyperparameters improve the model. And how to know when the model has ahcieved maximum accuracy above which it WILL not go.\n\n  \nTLDR: How can I achieve greater accuracy? How do I figure out the best way to code the model? I understand if there is some amount of trial and error, but I hope there is some way of determining whether a line of tries is not worth it. (I wish I could train an ML to find the best hyperparameters to train an ML)",
    "created_utc": "2024-11-14T17:06:26",
    "num_comments": 5,
    "comments": [
        "Hi, I totally understand your confusion. It's difficult to think about it this way, but getting a deeper understanding and better intuition on how to solve problems, only requires reading on the papers which solved similar problems, understanding their architectural choices and training techniques. This, in time, allows us to also come up with solutions or even use already existing solutions.\n\nWith how rapidly AI has evolved in the recent years, it is now possible to use pre-trained, more general models, for solving your particular problems. An example would be, in your case, using a pre-trained multi-modal model, to prompt it to detect and classify the tumors in the image. Another solution would be doing few-shot learning, in which you use a pre-trained object detection model to classify your data, with few samples.",
        "AFAIK there isn't an optimal way to figure it out.   I do a lot of wide and deep networks and then slowly refine them with a lot of note taking to see roughly how much and which size affects the results.\n\nI would recommend trying to learn about receptive fields.  It can at least help you avoid really bad architectures.",
        "Lots of things you can do.. from tweaking model parameters (optimizer, adjust the learning rate, increase epochs, use regularization to reduce over fitting). Don't forget data pre processing techniques on your images! (De noising, trying different contrasts, general data augmentation techniques). You can also try transfer learning like VGG. Good luck!",
        "Hey, thanks for answering!\n\n\n\nI think a team-mate of mine is doing something similar. We split our team into 2 for 2 different approaches. They called it transfer learning I think. Where we use a pre trained model and improve upon it. My side is doing the ground up version...\n\nAlso I'll definitely look up papers to learn more about the model I'm trying to make",
        "Yes, I was also referring to transfer learning. But there are other techniques as well, such as few-shot learning.\n\nIn any case, since you said you and your team are building a model from scratch, I think it would be a good idea to replicate network architectures seen in already established models. So take for example the architecture of VGG and simplify it (Remove a block or two from the feature extractor) and see where that leads! Or do the same thing with some other classification model, such as ResNet.\n\nI know that it might seem that you're technically \"cheating\" by doing this, but it really isn't, as the architectures proposed in those pre trained models (https://keras.io/api/applications/) are already proven and established. So there's no point on trying other architectures."
    ]
},
{
    "submission_id": "1grk4nk",
    "title": "What to set random Forest's min_impurity_decrease parameter?",
    "selftext": "I would like to use the \"min_impurity_decrease\" parameter while making random Forest in Python but Im not sure what a reasonable value would be? I would only like splits that are reasonably effective but I don't want to \"over-constrain\" the tree from growing effectively either. How can I find a balance? Can anyone explain how to get a good estimate for this parameter? I appreciate any suggestions/feedback. ",
    "created_utc": "2024-11-14T16:48:15",
    "num_comments": 2,
    "comments": [
        "Hyperparameter tuning",
        "Makes sense."
    ]
},
{
    "submission_id": "1grjtps",
    "title": "Leaf Disease Segmentation using PyTorch DeepLabV3",
    "selftext": "Leaf Disease Segmentation using PyTorch DeepLabV3\n\n[https://debuggercafe.com/leaf-disease-segmentation-using-pytorch-deeplabv3/](https://debuggercafe.com/leaf-disease-segmentation-using-pytorch-deeplabv3/)\n\nDisease detection using deep learning is a great way to speed up the process of plant pathology. In most cases, we go with either image classification or disease (object) detection when using deep learning. But we can use semantic segmentation as well. In some cases, leaf disease recognition with semantic segmentation is much more helpful. This is because the deep learning model can output the region affected by the disease. To know the entire process, in this article, we will cover ***PyTorch and DeepLab for leaf disease segmentation***.\n\nhttps://preview.redd.it/zicx0jofny0e1.png?width=1000&format=png&auto=webp&s=d352e386914d30f18d2c94013815db4b5fc6ccd5\n\n",
    "created_utc": "2024-11-14T16:33:32",
    "num_comments": 0,
    "comments": []
},
{
    "submission_id": "1grjk2i",
    "title": "Paper Club: Nvidia Researcher Ethan He Presents Upcycling LLMs in MoE",
    "selftext": "Hey all,  \n\n\nTomorrow Nvidia researcher Ethan He will be doing a technical dive into his work: Upcycling LLMs in Mixture of Experts (MoE). Excited to get a peak behind the curtains to see what it is like to work on models at this scale at Nvida.\n\n  \nIf you’d like to join the community tomorrow 10 AM PST we’d love to have you. We do it live over zoom and anyone is welcome to join.\n\nHere's the paper: [https://arxiv.org/abs/2410.07524](https://arxiv.org/abs/2410.07524)  \nJoin us live: [https://lu.ma/arxivdive-31](https://lu.ma/arxivdive-31)",
    "created_utc": "2024-11-14T16:20:22",
    "num_comments": 5,
    "comments": [
        "Interesting but unsure if that link is broken, shows 404",
        "View in your timezone:  \n[tomorrow 10 AM PST][0]  \n\n[0]: https://timee.io/20241115T1800?tl=Paper%20Club%3A%20Nvidia%20Researcher%20Ethan%20He%20Presents%20Upcycling%20LLMs%20in%20MoE",
        "Found [1 relevant code implementation](https://www.catalyzex.com/paper/arxiv:2410.07524/code) for \"Upcycling Large Language Models into Mixture of Experts\".\n\n[Ask the author(s) a question](https://www.catalyzex.com/paper/arxiv:2410.07524?autofocus=question) about the paper or code.\n\nIf you have code to share with the community, please add it [here](https://www.catalyzex.com/add_code?paper_url=https://arxiv.org/abs/2410.07524&title=Upcycling+Large+Language+Models+into+Mixture+of+Experts) 😊🙏\n\nCreate an alert for new code releases here [here](https://www.catalyzex.com/alerts/code/create?paper_url=https://arxiv.org/abs/2410.07524&paper_title=Upcycling Large Language Models into Mixture of Experts&paper_arxiv_id=2410.07524)\n\n--\n\nTo opt out from receiving code links, DM me.",
        "https://lu.ma/arxivdive-31",
        "Sorry about that! Correct link above"
    ]
},
{
    "submission_id": "1grg6q9",
    "title": "[Help] K-means Clustering on Football Stats: Always Getting 2 Clusters?",
    "selftext": "I'm working on a university project about unsupervised learning using football player stats (\\~2000 players, \\~50 features). One of my main tasks is to perform K-means clustering, and I’m using both the WSS (Elbow Method) and Silhouette Score to find the optimal number of clusters.\n\nHere’s the issue: no matter what I try (whether standard K-means or kernel K-means, or whether I use the whole dataset or exclude goalkeepers), I keep getting **2 clusters** as the optimal number. This feels counterintuitive because football has many positions, and I’d expect each position to roughly correspond to a different cluster.\n\nThe only time I get a different result is when I use PCA to reduce dimensionality and then perform clustering on the new dataset. But I'm unsure if that’s the right approach here.\n\nSo, I’m stuck on two questions:\n\n1. Should I go with the \"optimal\" 2-cluster solution, even if it seems too simplistic?\n2. Or is there a better way to make clustering more reflective of the different football positions?",
    "created_utc": "2024-11-14T13:46:04",
    "num_comments": 2,
    "comments": [
        "Try dbscan once",
        "Tried it and kept getting 2 clusters, 1 for goalkeepers and one for the rest. For the data without goalkeepers I got only a cluster. While focusing more on the Gap statistic I get around 4 clusters, so I guess I will not worry too much about the silhouette score here. Thanks for the advice anyways!"
    ]
},
{
    "submission_id": "1greyue",
    "title": "Need a roadmap for a project",
    "selftext": "I am supposed to be working with the YOLO-v8 architecture, the project is scheduled to begin in roughly a month, can someone please provide me a roadmap of all the things i should be well versed with before starting the project.\n\nConsider me a total beginner.",
    "created_utc": "2024-11-14T12:53:07",
    "num_comments": 3,
    "comments": [
        "Computer vision basics",
        "Just read the Ultraytics docs carefully. The app is readily packed with training, evaluating and inference script so it doesn't look like that you have to know anything about CNN architecture to use it. Learn some evaluation metrics and techniques like how to examine loss curves or how to read confusion matrix so you could examine how well your model performs. Learn some basics theory about traing neural network so you train your model correctly, and get some bits about OpenCV or PIL so you could draw annotation on the output. Thats it, no hard work required.",
        "# 1. Basic Machine Learning and Deep Learning \n\n# 2. Introduction to Computer Vision\n\n# 3. Object Detection and YOLO Basics\n\n# 4. Getting Familiar with YOLOv8\n\n# 5. Python Programming and PyTorch Basics\n\n# 6. Data Annotation and Preparation for Object Detection\n\n# 7. Understanding Model Training and Evaluation Metrics\n\n#"
    ]
},
{
    "submission_id": "1gredn9",
    "title": "Which model performs better? Help me understand the learning curves",
    "selftext": "Fraud Detection Binary Classifier with imbalanced dataset.\n\n1. Picture Learning Curve with SMOTE.\n\n2. Picture Learning Curve with balanced weights.\n\nWhat would you select and are there signs of overfitting?\n\nThanks!!\n\nhttps://preview.redd.it/hmymjc6wex0e1.png?width=1778&format=png&auto=webp&s=69e7ab779b0e98d42d8c8b6c12ac174fabf66347\n\nhttps://preview.redd.it/th5voeyxex0e1.png?width=1732&format=png&auto=webp&s=9ad6392db302ef8b14b048a9a55744be37fc31c2\n\n",
    "created_utc": "2024-11-14T12:27:58",
    "num_comments": 3,
    "comments": [
        "For the first time in all my life it appears that the model trained on SMOTE observations performs better, but gotta take that with a grain of salt because these metrics are based on CV. \n\nYou’ll discover signs of overfitting by comparing the models on a hold-out set that was not involved in the original training process. If you constructed your holdout set well then you can use it to select the better-performing model.\n\nAnd the important question is never “am I overfitting”. The important question is “what technique leads to the best performance on the holdout set?”\n\nIt’s **performance** that you care about, not some abstract concept about “how closely the model represents its training data”.",
        "thank you! What do you think is the best approach for SMOTE here?\n\nApplying SMOTE with each fold during CV (dynamic SMOTE in each fold) or creating a SMOTE train set and using it in all folds (static smote before cross-validation)?\n\nI know with a usual model evaluation the first approach is better. But what about applying SMOTE when evaluating the model performance through a learning curve?",
        "I would not use any synthetic data to estimate model performance on the final holdout set (I.e., for model comparison/selection).\n\nWhether you include SMOTE observations in the CV data that you use to finetune the model is a totally different question and the answer is that you need to treat it like yet another parameter to test. That is, do a model with SMOTE in the CV sets and then do a model without. Compare both on the final holdout set (without SMOTE) and the one that performs better will indicate which method is better.\n\nJust make sure your final holdout set is as close to *what the model will actually see during inference* as possible. Models in production will never actually see SMOTE observations so you should definitely not add them to your final holdout set."
    ]
},
{
    "submission_id": "1grcuxh",
    "title": "As an Embedded engineer, will ML be useful? ",
    "selftext": "I have 5 years of experience in embedded Firmware Development. \nThinking of experimenting on ML also. \n\nWill learning ML be useful for an embedded engineer? ",
    "created_utc": "2024-11-14T11:22:34",
    "num_comments": 27,
    "comments": [
        "If you look at the concept of Edge AI i would suppose that AI on some sort of micro edge / device edge will become a thing soon too",
        "I think ML is slowly going to get integrated into many edge devices, including (of course) robotics, communication networks and so much more. I would say yes. However, I would focus on: 1) low scale ML methods (those that fit on embedded devices and potentially distillation methods) or client-server architectures. There is much work on those directions. If you are interested, I can send you some references",
        "I think embedded AI will be one of the biggest markets. It may not get all the hype, but it will do most of the work. Similar to embedded computers compared to desktops/laptops.",
        "Check: Machine Learning Applications for Embedded Devices Using TinyML and C++",
        "I think there is an interesting crossover here that people aren't realizing. There's some niche but growing applications of ML on embedded devices. Look at Qualcomm, Nvidia, anybody who does anything in space, etc... I bet car companies will be trying more and more to push ML models to true edge devices (not these fancy Linux \"edge\" devices like the Jetson... Kids these days don't know how good they got it)",
        "Yes, there are tons of RISC-V based startups looking for ML engineers with heavy emphasis on Embedded development. Here's a recent one: [Etched](https://www.etched.com/)",
        "Lol people here giving advice without any knowledge whatsoever. Embedded engineers with AI background are in very very high demand due to low supply in areas like automotive. You dont fit an nvidia gpu in car to run ml algorithms, you need to compile these models according to the chipset available in the car.",
        "I'd have to agree with many others on here as ML/AI in embedded systems is on the upswing and is not new but is getting a lot more attention. There's actually a course (I believe from 2018 but seems to get updates) from HarvardX, on TinyML, created by professor Vijay Janapa Reddi of Harvard and some at X/Googlers (like Pete Warden who was instrumental in the creation of TFLite - now LiteRT and TFLite Micro - now LiteRT for Microcontrollers and some important datasets used in model training) that is on edX. It's 3 courses that delve into the fundamentals, the application, and the deployment of ML on edge devices, especially MCUs but also SBCs. The courses don't take long and are presented well, IMO.\n\nThere is quite a bit of discussion on how this can be applied in areas of medical devices, industrial equipment, robotics, etc. The specifics go into anomaly detection (for both medical devices and industrial equipment), keyword spotting, visual wake words, etc. and the design considerations, training, and optimizations that can and should be performed in order to minimize the model to fit and run within the confines of a microcontroller. It's also a hands-on course that walks you through deploying specific use-cases to an Arduino Nano 33 Sense BLE board that has an IMU, microphone, temperature/humidity, along with an OV7675 camera connected via a custom dev board that allows for additional sensors to be added. (NOTE: You can also get the REV2 of the Nano 33 Sense BLE with only minor updates to a couple of library installs/imports for the code examples as some components have been updated on the board.)\n\nLiteRT and LiteRT for Microcontrollers isn't viable for all SBCs/MCUs but it is definitely growing and a lot of thought was put into MCUs which provides the option for those that don't have an OS (bare metal) and for those that do like RTOS and mbedOS.\n\nThere are more and more microcontrollers/components being released that support Tensorflow and ML on board to allow for reduced reliance on a stable connection to the internet, less power consumption by allowing for these components to work while the main board is in low-power mode, potential for improved security, and the ability to design a cascading architecture for the ML whereby the on-MCU ML triggers the main system to wake to provider more advanced ML models and even send data to the cloud allowing for improved battery performance. Some of the more recent examples are the Raspberry Pi AI Camera and the AI Kit. The AI Camera comes with some models preinstalled but you can load other models within limits and with the AI Kit you can/should be able to run more complex ML models and potentially cascade from the initial processing on the camera to something more robust on the AI Kit and/or use it for other models altogether. The possibilities are starting to feel something akin to endless.\n\nThere are more SBCs/SoMs coming with AI processing on-board like AMD's Kria KR26 SoM, with Kria KR260 kits for Vision AI and for Robotics.\n\nI'd say we are still in the infancy of Edge AI but it's definitely seeing a push and lots of information has been coming out regarding the number of Edge AI devices that will be in the wild by 2030 (take with a grain of salt as there's always the question of acceptance and adoption in the long-term).\n\nThere are so many applications for ML in embedded systems (and at the Edge). I know Tandem Diabetes has been working on their next version of their main insulin pump to support ML in their algorithm(s) for, I assume, better prediction of insulin dosing to reduce spikes and valleys in blood sugar control and without necessarily requiring parameter setting on the device, kind of how the iLet Bionic Pancreas insulin pump already does.\n\nIn the end, it's up to you to determine your interest level as to whether you want to apply this type of learning to your toolset, though I do think the barrier to attain this knowledge is lower than you might think. I'd say you don't need to be an ML expert in terms of creating new algorithms as an ML Scientist would but the above course can give you a basis for using existing models and what's necessary to fit them into the embedded systems space. It can also give you a feeling for how deep you want to go into ML.",
        "AI edge model would definitely gradually go to the micro devices.",
        "Not really, but it can be stimulating and a new career path",
        "There are a lot of use cases for embedded ML in things like robotics, video & audio capture/processing, scientific instrumentation, satellite communications... the list goes on and on.",
        "I had classmates applying it to their work for inspecting chips for defects during production.",
        "Look at the company \"ARM\"",
        "no. ml and embedded don't have much in common",
        "I’m researching on Edge AI, and there’s definitely a lot of potential for it. My bet is that with all the privacy awareness, many people will want their AI to be local, and that’s where Edge AI will shine",
        "Problem is embedded systems will always be the little brother and therefor always “up and coming”. More compute is always better.",
        "Small language models on Edge devices have a lot of potential for different use cases",
        "\": 1) low scale ML methods (those that fit on embedded devices and potentially distillation methods)\" \n\nThat is really surprising to me, I assumed they would be client server. Can you give me a couple examples of these devices?",
        "Agreed! I’m already seeing it pop up as a difficult but highly valuable problem in my industry.",
        "Book?",
        "Huh? Plenty of embedded platforms include AI/ML edge processing.\n\nVersal Edge for example targets that market specifically",
        "Yeah and every single embedded device can be replaced by an internet connection and connecting to a server. Which is usually giving even more benefits to companies. \n\nOnly thing that can push them is privacy laws or something ngl.",
        "Would you be so kind to mention your industry? TinyML is what I’m working on not seeing many job postings in my local area.",
        "That falls under distributed computing not ML",
        "I work in audio DSP. It’s a pretty niche industry but a lot of companies in this space have low latency “in” device requirements. Think guitar pedals, smart tvs, cars, speakers, live broadcasting. Things like source separation have been game changing in the space and now there’s a race to optimize and miniaturize :)",
        "But said distributed computing is AI/ML workloads. You're saying it's not useful but the two things coexist in the same space. I personally know a few embedded engineers who have specialized in AI/ML.",
        "Thank you!"
    ]
},
{
    "submission_id": "1grcr12",
    "title": "Need help in creating Model for Phishing URL detection",
    "selftext": "I have come across this model which is used for Phishing URL Detection the github link is [https://github.com/pirocheto/phishing-url-detection](https://github.com/pirocheto/phishing-url-detection)  the dataset used is [https://huggingface.co/datasets/pirocheto/phishing-url](https://huggingface.co/datasets/pirocheto/phishing-url) . \n\n  \nI need this model for a project and I want to know how can I recreate this model.\n\nThanks in advance.",
    "created_utc": "2024-11-14T11:18:03",
    "num_comments": 1,
    "comments": [
        "Are you going to be using the same dataset? If so, what would the point of making your own model be?\n\nIf not, then you'd need to create your own dataset prior to worrying about your model. (Though that card tells you how they did it in the description, even gives a nifty paper)"
    ]
},
{
    "submission_id": "1grch1t",
    "title": "Understanding How LLM Works",
    "selftext": "",
    "created_utc": "2024-11-14T11:06:21",
    "num_comments": 0,
    "comments": []
},
{
    "submission_id": "1grcge7",
    "title": "Understanding How LLM Works",
    "selftext": "",
    "created_utc": "2024-11-14T11:05:33",
    "num_comments": 1,
    "comments": [
        "Just use NER?"
    ]
},
{
    "submission_id": "1grcgd4",
    "title": "Derivation of bias and variance for loss functions other than MSE",
    "selftext": "Both Deep Learning (Goodfellow) and Elements of Statistical Learning (Friedman) show that mean squared error (MSE) can be decomposed into a sum of two terms: bias and variance. This provides intuitive validation for why bias and variance combine to determine the overall loss of a model using mean squared error. \n\n[Screenshot from Deep Learning \\(Goodfellow\\) showing the explicit decomposition of MSE into a bias term and a variance term.](https://preview.redd.it/c26bqehnzw0e1.jpg?width=1706&format=pjpg&auto=webp&s=2cd18afce1bdc5ba9b1808710d4bd271d0708554)\n\nI am wondering if other loss functions (say, MAE or cross-entropy) also can be composed into explicit terms involving the bias and variance. To be clear: I understand that bias and variance can be calculated for any statistical model, regardless of the loss function chosen. However, I am curious if there is an explicit formula for the contributions of bias and variance for other loss functions.\n\nI am also wondering if I am confusing the MSE when used as a loss function (a function of the dataset, conditioned on the parameters theta), with the use of MSE in the above context (an expected value of the parameter values themselves). In this case, how can I think of the difference between the MSE of the parameter estimates, vs. the MSE of the target data points compared to model outputs?",
    "created_utc": "2024-11-14T11:05:31",
    "num_comments": 1,
    "comments": [
        "I am not sure we can get closed form equations for MAE or cross-entropy - at least I am not aware of any. If someone is, I would be happy to read. MAE isn't diff at zero and cross entropy has a nasty softmax. Maybe you could use numerical methods to understand. \n\n  \nAs for the MSE on parameters vs. target data - the first is related to model fitting while the second is to generalisation (like how we can do on unseen data). \n\n  \nHope this helps :)"
    ]
},
{
    "submission_id": "1grbwoh",
    "title": "Weird learning curve?",
    "selftext": "https://preview.redd.it/npxbbm0hww0e1.png?width=1000&format=png&auto=webp&s=3117e2339290ea9b54175a1c182240d45039f7e2\n\nWhat might be the reason of the flat 20 epochs in the image. I'm trying to train a LSTM-VAE autoregressive. It decreases almost none, then have a sharp decrease. Does this have a some kind of intuition behind?",
    "created_utc": "2024-11-14T10:42:26",
    "num_comments": 6,
    "comments": [
        "Looks like you got out of a local minimum",
        "It could maybe be that the model had a hard time making progress, maybe try different learning rate schedule. Also can you please check your gradients in that region? It could be that the VAE is struggling between reconstruction and KL term.",
        "Strange, what do you think it takes 20-30 epoch to get out there.",
        "ah make sense, thanks",
        "Relative to the curve shape it looks reasonable. I had a similar shaped elbow curve in a clustering recently, it took me a while to realize but there were actually two ”optimal” ways of clustering. Have a look at the outcome when only running 40 epochs and see",
        "Of course :)"
    ]
},
{
    "submission_id": "1grb0eb",
    "title": "Perplexity AI PRO - 1 YEAR PLAN OFFER - 75% OFF",
    "selftext": "As the title: We offer Perplexity AI PRO voucher codes for one year plan.   \n\nTo Order: https://cheapgpts.store/Perplexity\n\nPayments accepted:  \n\n- PayPal. (100% Buyer protected)  \n- Revolut.",
    "created_utc": "2024-11-14T10:04:44",
    "num_comments": 0,
    "comments": []
},
{
    "submission_id": "1gr8vxm",
    "title": "Are these skills/projects enough to land a freshman summer internship in AI/ML?",
    "selftext": "Hey everyone! I'm a freshman CS major really interested in breaking into the AI/ML field and hoping to land a summer internship after my first year. I’d love some feedback on whether the skills and projects I’m working on will be enough to land an opportunity. Here’s my plan so far:\n\n* **Python Skills**: I’ve been focusing on Python for AI/ML specifically, learning the language with an emphasis on data science and machine learning workflows.\n* **ML Libraries**: I've been studying TensorFlow and PyTorch to get comfortable with the major tools in the field.\n* **Knowledge of AI Architecture**: I have a solid theoretical understanding of architectures like transformers, including the \"under the hood\" mechanics of how they work.\n* **C++ Skills**: I know C++ isn’t particularly relevant for AI, but it’s required for my program, and I enjoy working with it to develop my problem-solving skills.\n* **Projects**: I’m working on three AI/ML/NLP projects to build experience:\n   1. **Implementing a Research Paper** – Planning to recreate a published model from scratch.\n   2. **Fine-Tuning Project** – I'll be fine-tuning a model for a specific use case, working closely with a professor as his research assistant. We’re also aiming to write and publish a paper.\n   3. **3rd Project TBD** – Not sure yet, but I want to keep it in the AI/ML space, ideally something hands-on and impactful.\n\nWould these be solid enough to land a decent internship, or am I missing anything critical?",
    "created_utc": "2024-11-14T08:35:45",
    "num_comments": 11,
    "comments": [
        "U listed only technical knowledge and none theoretical. \n\nWhat math/experimental design do you know? \nAre you aiming specifically for MLops/engineering or actually doing the science behind it?\n\nEdit- \nI’m not from the USA or a country where internships are the norm (the norm here is trying to find a part-time job in the field as a a student) so I don’t actually know what they search for in internships but I know what people look for when looking at ML/DS professionals and it is rarely a list of python packages",
        "Have you done dsa?",
        "I mean any company that knows what they're doing is going to be asking you questions about statistics and linear algebra.  So it's hard to say without you specifically talking about whether or not you have knowledge of it.  Anyone can really just sit down and train a model, they've made it extremely easy to do.",
        "ops/ eng, not the whole sci, implementing AI at companies, chatbots, agents, etc",
        "Could you elaborate on what exactly you mean by \"What experimental designs do you know?\". What are the experimental designs one should know for Machine Learning?",
        "I’m more on the research side of things but if that’s your thing I guess you should know some system design? Someone with a more suitable background might give you a better answer",
        "Maybe I should have phrased it better.\nIf someone wants to be a data “scientist” (not an engineer but even than I think it would be good) it involves a lot of similar aspects to designing a classical statistics experiment.\n\nYou need to know stuff like how to phrase your hypothesis correctly and succinctly, choosing variables, understand control groups or irregular groups in the data (skewed? Multi-class? Can we see a clear distribution merging from samples?), randomization and shuffling (e.g. stratification), how to replicate your success.\n\nAll of those stuff will help determine if people have “faith” in your model which sometimes unfortunately, is more important than the model being actually good for the companies we work for.\n\nEdit- I should add that in my experience most educators of ML from the technical side do talk about how experimental design is important and yada yada but usually don’t go into the details as it’s something for the research at hand itself usually.",
        "since you are in research, are you in the industry or academia? how are the pays in academia if you are in it?",
        "I’m doing research in a hospital so it’s kind of middle of the road between industry and academia.\n\nThe comp is just ok, I do it because I’m passionate about it. \n\nBut again, I’m not from the USA to tell you anything about the industry there",
        "well I am not there too lol, so you are in the medical technology sector? I have been actually thinking about it, how is the pay there (compared to academia)? work life balance?",
        "Oh I thought internships were a Canadian/American thing with how you phrased it.\n\nFor the last year my projects are mostly predictive modeling using ECG, it is kind of the middle between developing a “medical technology” and “clinical research”.\n\nThe pay is better than academia especially if you consider the education (most of my coworkers have a masters but in academia here you actually “work there” only after PhD).\n\nIn my experience work life balance when working in a hospital as a “technology guy” is kinda wacky, I had bosses who didn’t care where I’m at physically as long as I performed but other bosses/managers in the exact same hospital that were the opposite and controlling, so sorry this isn’t some definitive answer"
    ]
},
{
    "submission_id": "1gr7g1p",
    "title": "Where Do You Get Your AI News?",
    "selftext": "Guys, I'm looking for the best spots to get the latest updates and news in the field. What websites, blogs, or other sources do you guys follow to stay on top of the AI game?  \nPlease give me your go-to sources, whether it's some cool YouTube channel, a Twitter (X) account, or a blog that's always dropping fresh AI knowledge. I'm open to anything—the more diverse, the better!\n\nThanks a lot! 😍anything—",
    "created_utc": "2024-11-14T07:34:17",
    "num_comments": 3,
    "comments": [
        "Rundown Ai newsletter",
        "Tldr is more general but it's good",
        "Check out Ellipsis News which creates personalised AI news podcasts every morning on any topics you choose (including anything related to AI). Super efficient way to stay informed on anything relevant to your interests, whether niche or broad.\n\nPromo code ELLIPSISNOV gets you one month free by the way.\n\nhttps://apps.apple.com/us/app/ellipsis-news/id6642699957\n\n(Full disclosure, I’m the developer!)"
    ]
},
{
    "submission_id": "1gr5lqf",
    "title": "Perplexity AI PRO - 1 YEAR PLAN OFFER - 75% OFF",
    "selftext": "As the title: We offer Perplexity AI PRO voucher codes for one year plan.   \n\nTo Order: https://cheapgpts.store/Perplexity\n\nPayments accepted:  \n\n- PayPal. (100% Buyer protected)  \n- Revolut.",
    "created_utc": "2024-11-14T06:10:42",
    "num_comments": 1,
    "comments": [
        "Nice try diddy"
    ]
},
{
    "submission_id": "1gr45uw",
    "title": "Building a Chatbot from Scratch Without Using APIs – Need Guidance!",
    "selftext": "Hey everyone!\n\nI'm passionate about AI and want to take on the challenge of building a chatbot from scratch, but without using any APIs. I’m not looking for rule-based or scripted responses but something more dynamic and conversational.\nIf anyone has resources, advice, or experience to share, I'd really appreciate it!\n\nThanks in advance!",
    "created_utc": "2024-11-14T04:57:08",
    "num_comments": 2,
    "comments": [
        "[https://www.youtube.com/watch?v=l8pRSuU81PU](https://www.youtube.com/watch?v=l8pRSuU81PU)\n\n[Andrej Karpathy](https://www.youtube.com/@AndrejKarpathy)'s \"let's create GPT-2 from scratch\" seems like a great place to start if you're interested in learning how ChatGPT-style chatbots work.\n\nHe provides all the code you need to train something using data you can go download, and also the approach he describes (in great detail!) would likely scale just fine to a GPT-3 sized model if you're willing to spend the money on training it.\n\nThe one caveat is that the model he trains is a basically a text completion model, not a chatbot. To turn it into a chatbot, you'd want to train it (or more likely, fine-tune it) on conversational data. Such data, however, could probably be generated using an existing LLM (e.g. GPT-4o) just to get the ball rolling.\n\nSo yeah, my main suggest is 1) try running his code with the training data he suggests, and 2) once that works, try tuning the model on a more conversational dataset that (at first) you generate using an existing LLM",
        "What scale of training time and cost would you need to get it to work almost as well as some of the smaller open source models out there? E.g Llama 3b"
    ]
},
{
    "submission_id": "1gr3j1z",
    "title": "How Can I Best Prepare for a Career in Machine Learning During My Double Major?",
    "selftext": "Hi everyone!\n\nI’ve just started a 5-year double major in **Math & Statistics** and already know I want to pursue a career in Machine Learning (ML). I’m eager to start learning now, and I’d love your advice on how to make the most of my time and effort.\n\nHere’s a quick rundown of where I stand:\n\n# My Current Skills and Experience:\n\n   * Intermediate **Python** (200+ LeetCode problems solved).\n   * Some hands-on experience with basic **Kaggle competitions** (e.g., House Prices, Titanic), using fundamental **classification** and **regression** techniques.\n   * Knowledge of **Transact-SQL** (I regularly do SQL query challenges).\n   * Learning **ReactJS**, **TypeScript**, and **FastAPI** (planning to build a flashcards web app this January with a colleague).\n\n# My Career Goals\n\nI’m considering roles like:\n\n* Data Engineer (DE)\n* Machine Learning Engineer (MLE)\n* Quantitative Analyst (Quant)\n* Software Engineer (SWE)\n\n# My Available Time\n\n* Summers.\n* 6 hours per weekend.\n* A few weeks in January.\n\n# What I’d Like to Improve\n\nI want to build skills that will be valuable for these roles in the future, including both technical skills (programming, ML theory, system design) and professional skills (teamwork, portfolio projects).\n\n# Questions for You\n\n1. **What skills should I prioritize now to align with these roles?** Should I focus more on programming, math, or diving directly into ML frameworks like PyTorch?\n2. **What projects or challenges would you recommend to deepen my understanding of ML and data engineering?** Are there specific Kaggle competitions, open-source projects, or personal projects I should try?\n3. **How can I make the most of limited time during university?** Are there particular books, courses, or strategies that would fit into my schedule?\n\nAny advice on how to plan my journey effectively and stay consistent would be greatly appreciated!\n\nThanks in advance!",
    "created_utc": "2024-11-14T04:20:25",
    "num_comments": 1,
    "comments": [
        "take a look: https://roadmap.sh/mlops"
    ]
},
{
    "submission_id": "1gr3283",
    "title": "Exploring Shape-Restricted Models in ML",
    "selftext": "As I got deeper into machine learning for things I had to do at work, I discovered how essential it can be to incorporate shape constraints like monotonicity or convexity into models. These constraints are not just theoretical; they ensure models align with domain knowledge and produce meaningful, interpretable outputs. Think of an insurance premium model that must increase with coverage or a probability model bounded between 0 and 1. Understanding and implementing these ideas has enlightening for me, so I wanted to share what I've learned.\n\nI documented my learning experience through two detailed blog posts. They're a bit mathy, but I hope not too much. Here they are:\n\n1. [Shape Restricted Function Models](https://alexshtf.github.io/2024/10/14/Shape-Restricted-Models.html): Inspired by the paper by Ghosal et al. ([arXiv:2209.04476](https://arxiv.org/abs/2209.04476)), this post explores how polynomial models can be adapted to meet shape constraints, with practical examples and PyTorch code to get  started.\n2. [Shape Restricted Models via Polyhedral Cones](https://alexshtf.github.io/2024/11/09/Shape-Restricted-Models-Polyhedral.html): Heavily influenced by the work of Frefix et al. ([arXiv:1902.01785](https://arxiv.org/abs/1902.01785)), this follow-up goes further into using polyhedral cone constraints for models that need advanced properties like combined monotonicity and concavity.\n\nBoth posts are filled with code snippets, explanations, and runnable examples. I hope they serve as a helpful resource for anyone looking to implement shape constraints in their models or simply expand their ML toolkit. I hope learning those things will be enlightening for you, as it has been for me.",
    "created_utc": "2024-11-14T03:51:38",
    "num_comments": 1,
    "comments": [
        "Found [1 relevant code implementation](https://www.catalyzex.com/paper/arxiv:1902.01785/code) for \"Linear Inequality Constraints for Neural Network Activations\".\n\n[Ask the author(s) a question](https://www.catalyzex.com/paper/arxiv:1902.01785?autofocus=question) about the paper or code.\n\nIf you have code to share with the community, please add it [here](https://www.catalyzex.com/add_code?paper_url=https://arxiv.org/abs/1902.01785&title=Linear+Inequality+Constraints+for+Neural+Network+Activations) 😊🙏\n\nCreate an alert for new code releases here [here](https://www.catalyzex.com/alerts/code/create?paper_url=https://arxiv.org/abs/1902.01785&paper_title=Linear Inequality Constraints for Neural Network Activations&paper_arxiv_id=1902.01785)\n\n--\n\nTo opt out from receiving code links, DM me."
    ]
},
{
    "submission_id": "1gr2y7g",
    "title": "Advice for Improving the Performance of My Reinforcement Learning Model Based on Spiking Neural Networks ",
    "selftext": "Hello everyone! I am working on a project focused on training reinforcement learning agents using Spiking Neural Networks (SNNs). My goal is to improve the model's performance, especially its ability to learn efficiently through \"dreaming\" experiences (offline training).\n\n**Brief project context (model-based RL):**  \nThe agent interacts with the environment (the game Pong), alternating between active training phases (\"awake\") and \"dreaming\" phases where it learns offline.\n\n**Problems:**  \nLearning is slow and somewhat unstable. I've tried some optimizations, but I still haven't reached the desired performance. Specifically, I’ve noticed that increasing the number of neurons in the networks (agent and model) has not improved performance; in some cases, it even worsened. I reduced the model’s learning rate without seeing improvements. I also tested the model by disabling learning during the awake phase to see its behavior in the dreaming phase only. I found that the model improves with 1-2 dreams, but performance decreases when it reaches 3 dreams.\n\n**Questions:**\n\n* Do you know of any techniques to improve the stability and convergence of the model in an SNN context?\n* Do you have any suggestions or advice?\n* The use of a replay buffer could help?",
    "created_utc": "2024-11-14T03:44:23",
    "num_comments": 0,
    "comments": []
},
{
    "submission_id": "1gr2oop",
    "title": "Have you checked out newsgpt for ai news",
    "selftext": "**NewsGPT** ([https://newsgpt.ai/](https://newsgpt.ai/)) is an AI-powered news aggregation platform that uses natural language processing and machine learning to curate and summarize the latest news across various topics. It provides users with personalized, real-time updates, offering concise summaries of current events based on their interests. The platform aims to streamline information consumption by delivering relevant, digestible news content efficiently.",
    "created_utc": "2024-11-14T03:26:56",
    "num_comments": 0,
    "comments": []
},
{
    "submission_id": "1gr2lv1",
    "title": "Pytorch learning step",
    "selftext": "Hi I'm trying to get into machine learning\n\nIm familiar with it's backgrounds and I also learned basic pytorch and did some little models from the tutorials on youtube ( I want to continue in pytorch ) \nBut now I am a little confused I feel like it's better to start a bigger project than simple models but I don't know what to start with because architectures are different and each has it's own learning phase, I almost learned transformer theories but don't know what model should I try to develop \n\nPeople that are into pytorch and specially transformer and attention models what is the best practice for learning how to develop projects in this step (I mean learning to develop and also learn somehow that isn't specialized for a unique usage ) \n\nAlso if you see that I'm thinking in the wrong way please correct me",
    "created_utc": "2024-11-14T03:21:30",
    "num_comments": 0,
    "comments": []
},
{
    "submission_id": "1gr2jsl",
    "title": "just need a guidance",
    "selftext": "hey everyone i know it's not the context of the group but I need guidance. I graduated with an accounting degree for my bachelor's, and all of my passion was in coding, I did a 4-month course at a high-level university to study Java I enjoyed it so I went into the android and mobile development field and have a 1-year of experience now, the point is I want to go into AI field so I did the Microsft learning path but there are no shortcuts in AI so I am getting admission in university to get computer science masters, so do you think that I have a chance to get a good career as an AI engineer?",
    "created_utc": "2024-11-14T03:17:40",
    "num_comments": 0,
    "comments": []
},
{
    "submission_id": "1gr24s3",
    "title": "Repository issues ",
    "selftext": "How do you deal with let’s say you cloned a repo ,read the readme file carefully then realized the some files are missing .on this case the notebook file exists but the model doesn’t exist also the weights file isn’t there ",
    "created_utc": "2024-11-14T02:47:54",
    "num_comments": 0,
    "comments": []
},
{
    "submission_id": "1gr2044",
    "title": "Empowering Engineering Educators for Industry 5.0: Launch of \"Machine Learning for Engineering Teachers\" Lecture Series",
    "selftext": "As we approach the era of Industry 5.0, the transformative power of Artificial Intelligence (AI) and Machine Learning (ML) is reshaping every field of engineering. AI/ML applications are increasingly integral to diverse engineering domains, driving advancements that will redefine future industries and skill requirements.\n\nIt has become essential for engineering educators across all branches to deepen their understanding of AI and ML, as these are the foundational technologies leading the way toward generative AI. By doing so, educators can guide their students to develop skills that align with the needs of Industry 5.0—ensuring graduates are equipped to be competitive in the rapidly evolving job market.\n\nTo support this vision, I am excited to announce the launch of our new **lecture series**, *“*[Machine Learning for Engineering Teachers](https://www.youtube.com/watch?v=INB5B6zzAmg&t=1s)***.****” by* [Pritam Kudale](https://www.linkedin.com/in/pritam-kudale-90793236/) In the first lecture, we explored the broad applications of AI and ML across various engineering disciplines, identifying how these tools can be utilized to enhance project-based learning and steer academic research toward cutting-edge innovation.\n\n[Application of Artificial Intelligence in Different Engineering Fields ](https://preview.redd.it/wnvhntgwhu0e1.png?width=1773&format=png&auto=webp&s=02a6a1d95585dad4a731870866e657296f5e411a)\n\nThis series aims to equip educators with the knowledge and insights to incorporate AI/ML principles into engineering curricula, facilitating impactful, industry-aligned projects and research. Join us as we build a foundation for tomorrow’s engineers, rooted in today’s technological advancements!\n\nI highly recommend going through the link: [https://www.youtube.com/watch?v=INB5B6zzAmg&t=1s](https://www.youtube.com/watch?v=INB5B6zzAmg&t=1s)\n\n",
    "created_utc": "2024-11-14T02:38:28",
    "num_comments": 0,
    "comments": []
},
{
    "submission_id": "1gr1x3c",
    "title": "Finding string patterns that influence score values?",
    "selftext": "Hello everyone, currently working on my first proper analysis in python using ML, and I am looking for something seemingly missing in my toolbox.\n\nI have strings consisting of 8 unique symbols in total of varying lengths (programs), with summary statistics for RL agent performances on each given program. \n\nIs there an ML model that could help me identify patterns in the strings that affect the performance? I am trying to single out the “bad” programs and find what they have in common and I am hitting my head against the wall. \n\nAny help is appreciated, even just getting directed to any source that could help in this matter! It’s a big world out there in the ML field\n",
    "created_utc": "2024-11-14T02:32:04",
    "num_comments": 0,
    "comments": []
},
{
    "submission_id": "1gr0y82",
    "title": "Please recommend machine learning books/resources that follow a project based learning approach",
    "selftext": "I am looking for books that teach machine learning but use a project-based approach. The reason I say books is because I easily understand books better however any other resources that are project based learning will also be appreciated. ",
    "created_utc": "2024-11-14T01:17:13",
    "num_comments": 4,
    "comments": [
        "I really enjoyed \"Introduction to Statistical Learning\" by James, Witten, Hastie, and Tibshirani. There is a python and an R version. \n\nEach chatper ends with recaps and quizzes as well as practical exercises (it also has solutions) in the respective language.\nEven though these are not start-to-finish full projects, as a beginner the exercises really helped me getting into hands-on trying out models. \n\nThe books are availibe as PDFs on their website for free and there is also a github repository containing all the code if you just want to copy some of it.\n(I just recently joined this sub and I don't know the policy about sharing links yet but its literally the first entry on google.)",
        "Try Kaggle. They host beginner friendly competitions where you can submit your ML code, and then get evaluated against others",
        "[Hands-On Machine Learning with Scikit-Learn and TensorFlow: Concepts, Tools, and Techniques to Build Intelligent Systems](https://www.amazon.com/Hands-Machine-Learning-Scikit-Learn-TensorFlow/dp/1491962291)"
    ]
},
{
    "submission_id": "1gr00l6",
    "title": "[D] Topic modelling+sentiment on news articles",
    "selftext": "I’m working on a project using topic modeling followed by sentiment analysis on a large corpus of news articles (at least 100k). For each article, my goal is to classify the main topic and determine the sentiment as negative, neutral, or positive.\n\nI’d love to hear about your practical experiences with the following aspects, including what approaches have worked for you and what challenges you've encountered:\n\n* **Topic Modeling + Sentiment Analysis Pipelines**: Any examples of popular pipelines that combine these tasks effectively, such as LDA, NMF, KeyBERT, BERTopic, etc.?\n* **Embedding Models**: Recommendations on embedding models that perform well with different chunk sizes.\n* **Granularity of Chunks**: Insights on chunk sizes for effective topic modeling—I've seen approaches using both word counts (e.g., 50 words) and token counts (e.g., 50 tokens).\n* **Evaluation Methods**: Best practices for evaluating various architectures and hyperparameters, including metrics like perplexity and coherence.\n\nThank you all in advance! I’d be glad to share my experiences here once the project is complete.",
    "created_utc": "2024-11-14T00:03:56",
    "num_comments": 0,
    "comments": []
},
{
    "submission_id": "1gqzunn",
    "title": "Is positional encoding necessary in the Conformer model ?",
    "selftext": "   I have read the implementation of the Conformer model from Pytorch ([https://pytorch.org/audio/main/\\_modules/torchaudio/models/conformer.html#Conformer](https://pytorch.org/audio/main/_modules/torchaudio/models/conformer.html#Conformer)). \n   As I see, there is no positional encoding (PE) layer there.   \n   Does anyone know why Pytorch did not use PE here? Because the original [Conformer paper ](https://arxiv.org/pdf/2005.08100)claims that they did use PE ",
    "created_utc": "2024-11-13T23:51:19",
    "num_comments": 1,
    "comments": [
        "Found [2 relevant code implementations](https://www.catalyzex.com/paper/arxiv:2005.08100/code) for \"Conformer: Convolution-augmented Transformer for Speech Recognition\".\n\n[Ask the author(s) a question](https://www.catalyzex.com/paper/arxiv:2005.08100?autofocus=question) about the paper or code.\n\nIf you have code to share with the community, please add it [here](https://www.catalyzex.com/add_code?paper_url=https://arxiv.org/abs/2005.08100&title=Conformer%3A+Convolution-augmented+Transformer+for+Speech+Recognition) 😊🙏\n\nCreate an alert for new code releases here [here](https://www.catalyzex.com/alerts/code/create?paper_url=https://arxiv.org/abs/2005.08100&paper_title=Conformer: Convolution-augmented Transformer for Speech Recognition&paper_arxiv_id=2005.08100)\n\n--\n\nTo opt out from receiving code links, DM me."
    ]
},
{
    "submission_id": "1gqzsbi",
    "title": "Best LIVE online courses for Python/NLP/Data Science with actual instructors?",
    "selftext": "I'm in the process of transitioning from my current career in teaching to the NLP career via the Python path and while I've been learning on my own for about three months now I've found it a bit too slow and wanted to see if there's a good course (described in the title) that's really worth the money and time investment and would make things easier for someone like me? \n\nOne important requirement is that (for this purpose) I've no interest in exclusively self-study courses where you are supposed to watch videos or read text on your own without ever meeting anyone in real-time.",
    "created_utc": "2024-11-13T23:46:16",
    "num_comments": 0,
    "comments": []
},
{
    "submission_id": "1gqz9jm",
    "title": "How to install pytorch, cuda",
    "selftext": "When i put\n\n    conda install pytorch torchvision torchaudio pytorch-cuda=12.4 -c pytorch -c nvidia\n\nIt end up with\n\nhttps://preview.redd.it/wfld1f0i6t0e1.png?width=1743&format=png&auto=webp&s=1d1959ca8cae0e5e562dff576fc22a72a913b61c",
    "created_utc": "2024-11-13T23:06:28",
    "num_comments": 18,
    "comments": [
        "You've got a ton of conflicts. Are you able to just set up a new virtual environment? It also looks like you're on Python 3.13, which doesn't appear to be fully compatible with just yet. You may need Python 3.12 or lower.",
        "Download wsl and then do this on it. Windows has very bad support for ML.",
        "Use wsl2 on windows. Windows is ass for ML. Look at what python version you are using. Conda and pip wheel versions of torch are precompiled with appropriate cuda, cudnn, cudatoolkit etc so installation should be very straightforward (even on windows).",
        "Can you use Cuda on mac without boot camp?",
        "Option 1: https://pytorch.org/\n\nJust copy paste the download command based on the requirements you select. \n\nOption 2: pip install torch==2.3.0 torchvision==0.18.0",
        "Nvidia libs are crap",
        "Ok, ill try download lower version",
        "What distribution would you recommend?",
        "does wsl have GPU support?",
        "I thought Tensor Flow was the one you had to use WSL to use on windows? Pytorch has native support.",
        "[deleted]",
        "Chil brother, im just beginner",
        "Nvidia has been outstanding for CUDA and ML in general.",
        "this is the way, take a look at the docs: [https://pytorch.org/get-started/locally/](https://pytorch.org/get-started/locally/)",
        "I use ubuntu, but any is fine as you will just use the terminal.",
        "I mentioned support for ML. Didn't mention standard Python libraries.",
        "Much appreciated.",
        "[deleted]",
        "Cuda support. There will be conflicts b/w pytorch on and cuda on windows. As cuda will not support newer python/pytorch versions.\n\nWorks perfectly fine on WSL/ Mac.",
        "[deleted]",
        "Didn't say it will run poorly on Windows."
    ]
},
{
    "submission_id": "1gqyxzc",
    "title": "Non-web developers, how did you learn Web scraping? ",
    "selftext": "And how much time did it take you to learn it to a good level ? Any links to online resources would be really helpful. \n\nPS: I know that there are MANY YouTube resources that could help me, but my non-developer background is keeping me from understanding everything taught in these courses. Assuming I had 3-4 months to learn Web scraping, which resources/courses would you suggest to me? \n\nThank you! ",
    "created_utc": "2024-11-13T22:43:42",
    "num_comments": 28,
    "comments": [
        "Take Python and just code it. First try with simple http request, parse html and scrap what you need. If it doesn’t work try to run headless browser, parse html and scrap what you need. You just have to give a try, that is easiest way to learn",
        "Beautiful soup, puppeter should solve most problem(s). But writing one that handles all kind of text is difficult.",
        "I just wrote a python script for scrapy.",
        "Do a project and learn as you go. Specifically I had a motivation to scrape a particular site for rankings to publish on my news feed in my game. Beautiful soup was there for me 😌",
        "I believe the popular “Automate the Boring Stuff with Python” book is for beginners and has a chapter on web scraping.",
        "3-4 months is more than plenty if you already know python, otherwise it's just the right amount of time. Just follow along with tutorials and do as many as possible. Don't just copy and paste. When I was first learning I would put prints() everywhere to log what was happening so I could flow it all in my mind. Now, I would use logger and print but yeah",
        "Choose your target     \nTarget your target      \n???      \nProfit \n\nSince a lot moved to Ajax requests, you probably would want to open browser console and switch to network tab and find which request returns data, then  recreate it with curl. \nDon't know now, but years ago Curl had a fun feature that allowed to emit C code equivalent to command line. Maybe some python tools has the same",
        "Right click, inspect.",
        "I had to get data for a project and the only way was with selenium. Just trial and error. I'm pretty sure selenium has an IDE now, that may make learning easier. \n\nBeautiful soup may also be helpful. Also understanding requests and wget and curl will be helpful. \n\nAI will be helpful here. Feed it website structure and tell what you want to do via selenium.",
        "I once had to do it for my job, we had to scrape some data in a DotNet website, it was pretty difficult to achieve with Beautiful Soup. In the end, Selenium worked perfectly. Just choose a target and try to reverse engineer them (by looking at the HTML structure to learn what to pick and how, studying the requests, scripts).",
        "Beautiful soop or even selenium is much better: you will find tons of Tutorials on YouTube.",
        "I would try some tools that are avaliable on the Internet first that can do it for you (Kadoa, Octoparse, Browse AI). This would give you some idea what you need to look for when scraping data, but remove all that complex part of how to do it.\n\nThe I would try some understanding of how to do it. If you want code then there are many videos and open source github repositories that will help you understand the basics (Python and JavaScript is quite easy, you don't need to understand everything in how to code, you just need to go through it line by line)",
        "I approached web scraping by focusing on practical steps and leveraging my existing programming skills. Here’s how I learned it:\n\n1. **Started with Basics**: I began by understanding the fundamentals of HTML, CSS, and the DOM structure, which are essential for locating elements on a webpage.\n2. **Learned Libraries**: I explored Python libraries like **Beautiful Soup**, **Requests**, and **Selenium**. These tools make it easier to extract and interact with web content.\n3. **Hands-On Practice**: I practiced by scraping simple websites, like extracting data from tables or lists, gradually moving to more complex, dynamic pages.\n4. **Tackled Challenges**: I learned how to handle issues like JavaScript-rendered content using tools like **Selenium** or **Playwright** and managing rate limits with **proxies** and **headers**.\n5. **Followed Tutorials and Docs**: Online tutorials, documentation, and platforms like YouTube and blogs were invaluable for understanding specific use cases.\n6. **Integrated Scraping with ML**: I focused on projects that aligned with ML, such as collecting datasets for training models, which kept me motivated.\n\nThe key is starting small, practicing consistently, and solving real-world problems as you learn",
        "Well, first off... a lot of companies will sue you for doing it, hence why companies like Xhitter and Reddit changed their API rules and pricing.\n\nSecond, we have no idea what your baseline is. Do you know how to code at all? Is what you're doing something chstGPT can kick out for you?",
        "Use ChatGPT and Google Colab",
        "[deleted]",
        "I shall look this up. Thank you!",
        "If it’s publicly available in the web it’s not illegal, you might get rate limited but it’s still not illegal",
        ">Do you know how to code at all? Is what you're doing something chstGPT can kick out for you?\n\nI only know the basics. I mean, I'm learning, but I'm certainly a beginner. Not a developer or anything.",
        ">a lot of companies will sue you for doing it, hence why companies like Xhitter and Reddit changed their API rules and pricing.\n\nI see. This makes me wonder whether this happens to people who upload datasets to Kaggle, or who scrape a site, etc. for a project that they're working on. Isn't it an important skill to have for a MLE?",
        "Bro lol.",
        "Www.AutomateTheBoringStuff.com allows you to read it online for free.",
        "I should be clear, it's against those websites ToS and they can sue you for violating that ToS agreement in a civil court, but I never said it's illegal. You just agree to not do it anytime you agree to a ToS agreement.",
        "I’m not sure what level web scraping you’re looking for, but I built an app that tracked video game combat (pvp) interactions for a certain MMORPG, and I used beautifulSoup (Python lib) to do most of the heavy lifting. I built 2 versions, one that utilized the game API itself, and one that scraped the data from a 3rd party site and aggregated it. \n\nFor real. Check out that library for Python. It abstracts so much away, quite powerful iirc.",
        "To respond to both comments at once...\n\nPeople who upload those datasets will have permission or did it a few years ago when companies didn't care about web scrapping.\n\nWeb scrapping is not an important skill for an MLE. \n\nAs for your ability to build a web scrapper, it sounds like you're not quite ready to make that jump just yet and you should be focusing on your basics a little more first. You could also have chatGPT help you write it. You can have it basically hold your hand through the process. The code likely won't be perfect, but that's where you'll get your chance to improve.",
        "How is my bot supposed to agree to their TOS ?",
        "Edit: I didn’t realize there was a subsequent settlement after the 9th circuit affirmed its original decision, lol. RIP.\n\nThis isn’t entirely accurate. See hiQ Labs v. LinkedIn, on publicly accessible data. Data that requires a login to access may have different contractual obligations, but good luck trying to legally enforce a TOS against the scraping of *public* data.\n\nThis does not entitle you to infringe copyright, so what you do with the data matters. But scraping is just another way of accessing data that would otherwise be accessible through a conventional browser, regardless of whatever the TOS say. \n\nObviously, expect to be blocked if you don’t know how to throttle requests.",
        ">People who upload those datasets will have permission or did it a few years ago when companies didn't care about web scrapping\n\nI see! \n\n>As for your ability to build a web scrapper, it sounds like you're not quite ready to make that jump just yet and you should be focusing on your basics a little more first. You could also have chatGPT help you write it. You can have it basically hold your hand through the process. The code likely won't be perfect, but that's where you'll get your chance to improve. \n\nGot it. Thank you so much. I was just a little confused whether it's a necessary skill since I've read online that a mix of SWE and ML skills are needed for the MLE position.",
        "Depending on the website, it's likely just outright against their ToS. You'll need to look into each website and what they allow."
    ]
},
{
    "submission_id": "1gqx5qk",
    "title": "Deep Learning Small Project",
    "selftext": "I'm wondering what type of deep learning project I should try to level up my skill and knowledge. I'm a beginner in this aspect of technology, but I've finished learning through net what are the basics and foundation of deep learning. \n\nI would like any suggestions about CNN algorithm project, any small project that could enhance my skill. ",
    "created_utc": "2024-11-13T20:50:14",
    "num_comments": 5,
    "comments": [
        "Remember that its not about the project, but your actual knowledge and skills that advance you in the field. You could do a hundred of those CV that pulls the yolo api from ultralytics without learning any real things about CNN, so I would not recommend a project-based approach on learning these things. Here is what I would recommend:\n- First, use the basics you have learnt to built a simple classification network. MNIST, dogs vs cats, anything you wish. Learn the right way to structure, resize, normalize your inputs, how to encode and decode your outputs, etc. Get comfy with how to partition the dataset, and what to do to get resonable loss curve. Learn they way around training tools like google colab too (I wish a better tool is coming soon, colab is getting worse and worse and still its the best we got)\n- Second, examine an established classification network, like VGG, Alexnet, and if you could, build one of your own using a framework that you like. I highly recommend Pytorch since the backward incompatibility of Tensorflow drove me mad. A key take away in this is to know how to read a CNN model chart, with the correct functions accompanied with each layers: correct padding, stride, output size, with the correct activation and normalization. \n- when you master the above, then its time to move on to learn the object detection thingies. Study some of the well established models. I did it the FIFO way since I study the 2-stage detector first like RCNN, but since the 2-stage ones are getting obsolete, it would be fine to jump right to 1-stage detector like SSDs. Implement one yourself. Evaluate your model performances against the results in the models’ papers.\n- Only when you got the intricacies of these detectors, you could choose to either build an webapp using the model you got, or go the embedded way like me and get into android/ARM in general. Or you could go the HPC way and start diving into optimization. Your choice.\n\nIMO this is the only way to learn any real skills with the CNN model thingies. It would take time obviously, as it took me almost half a year till I build and tune my first model. But I promise you going deep like this would give you so much more. I wish you goodluck on this journey.",
        "binary classifier",
        "Thank you so much for this. I will definitely use this as my guide.",
        "I'd love a guide like this but more focused on NLP with transformer models, etc...\n\nOr are CNNs commonly used for NLP and other tasks too?",
        "I heard that you could use CNN with the embedded text to classify them. I don’t really know much about NLP so I would have to leave the podium to someone else more qualified than me."
    ]
},
{
    "submission_id": "1gqwu9e",
    "title": "Would the answer be D?",
    "selftext": "https://preview.redd.it/5w50cfdwos0e1.png?width=661&format=png&auto=webp&s=2ad4d967236f6f3c1c658bfdc3b4ecce412e340d\n\nI tried answering this question and arrived at D as the answer. Could someone please confirm if it's correct, and if it isn't, which one's the right answer?\n\nThanks a lot in advance!",
    "created_utc": "2024-11-13T20:31:43",
    "num_comments": 2,
    "comments": [
        "The answer D works. None of the others do with the constraints of having a maximal margin classifier.",
        "Thank you so much!"
    ]
},
{
    "submission_id": "1gqwg6n",
    "title": "How to handle non-fixed size inputs?",
    "selftext": "Hello everyone, I’m a student who’s currently run into a problem! I want to experiment with audio classification however the data I have varies wildly in size and I’d like a fixed output size. If that’s not possible then it would be fine to have a variable output size as long as it can handle variable input size. I’m aware I could chunk it or something but I was hoping there’s another way to do this! If there isn’t could you suggest the best ways to chunk my data without destroying it? Thank you so much for your assistance.\n\nMy preferred machine learning framework is PyTorch.\n\nI’m currently enrolled in high school.\n\nThank you so much for your support!",
    "created_utc": "2024-11-13T20:09:31",
    "num_comments": 2,
    "comments": [
        "I didn't know much about audio classification so I would just break up my data into similar groups using the key words of phrases they contain. Another option is probably training the model to also output the proper length of the output by using the audio sizes as both features and labels (idk if it already does this).",
        "I already mentioned chunking but I’d like to avoid it. I’m also not utilizing any pre labeled data so I’d need to label text for keywords. Currently I’m just chunking it with a RNN to generate a fixed size embedding."
    ]
},
{
    "submission_id": "1gquxul",
    "title": "How to answer this interview question on NLP?",
    "selftext": "I was asked this question in an interview. \"For a classification task, would you use an encoder or a decoder based model, if you choose one, what's the reason behind it?\" I just told them I'd use encoder model since it's attention mechanism is bidirectional, but its still not a clear differentiator.",
    "created_utc": "2024-11-13T18:48:08",
    "num_comments": 4,
    "comments": [
        "Encoder",
        "You can try to get insights from chatGPT..one question, are leetcode style coding problems asked in such interviews? Please answer..I'm terribly undecided about this thing..",
        "Idk"
    ]
},
{
    "submission_id": "1gquwgc",
    "title": "Repost: Why does my Random Forest use features that my Neural Network ignores?",
    "selftext": "Both my neural network and Random Forest have about the same accuracy (with RF slightly better) on a binary classification task. The shapley values for certain features are zero according to the Neural Network but significantly greater than zero for the random Forest. My domain knowledge tells me these features are very informative yet were not picked up by a neural network even after regularization. How could this be? ",
    "created_utc": "2024-11-13T18:46:09",
    "num_comments": 8,
    "comments": [
        "do k fold validation.",
        "Random forest and neural networks are very different models, meaning the way they process each feature is different. I suggest you look into the theory of both. \n The neural network may find some features more useful than the Random forest for your classification task. Both models could be valid, its up to you to choose the model that “captures” your use case more effectively. In my opinion , non-linearity from the neural network makes it challenging to compare shapley values, the neural network has a complex blackbox that random forest doesn’t have. \n\nLastly, if you do PCA on your dataset, you should get an insight of how much variance is explained by the components, and the feature loadings for those components should tell you how much the features contribute to the components. By doing this experiment, what I am hoping you will find is the features that explain most of the variance in the components, by comparing those to the shapley values from both models, you may get an idea of which model is actually using the features that contribute most to the variance explained by the components.",
        "[deleted]",
        "This is a solid approach I didn't think about. So I can try and use PCA to see which model is explaining more of the variance. Thanks for the feedback. I appreciate it.",
        "You can also switch to boosting/bagging which can prove more robust than random forest ( which in itself can prove more robust than decision tree) , it may give you results closer to your NN ( or not), and as others suggested, cross validation may provide you with results that you can trust more",
        "Are you saying I might not have done enough regularization and the neural network is still overfitting?",
        "No worries :)",
        "Are you measuring the generalization gap with a validation set? That’s one way to get a sense of overfitting, at least if you can assume that your validation set if sufficiently representative of the complete data distribution.",
        "Yeah they are both giving me the same accuracy on the test set (~61% accuracy)"
    ]
},
{
    "submission_id": "1gquvo8",
    "title": "How to train a CNN model to label all the facial landmarks? With n amounts of faces within the photo ",
    "selftext": "So training a CNN model to output(x,y) all the facial landmark’s locations for one face is pretty easy, but for unknown amounts of faces within the photo, I don’t know how to do it. ",
    "created_utc": "2024-11-13T18:45:07",
    "num_comments": 0,
    "comments": []
},
{
    "submission_id": "1gqtscz",
    "title": "Is the Order of Text Preprocessing Steps Correct for Twitter-based Dataset ?",
    "selftext": "Is the order correct or is there any step should be changed in order ?\n\n* Keep Only Relevant Column (text).\n* Remove URLs.\n* Remove Mentions and Hashtags.\n* Remove Extra Whitespaces.\n* Contractions.\n* Slang.\n* Convert Emojis to Text.\n* Remove Punctuation.\n* Replace Domain-Specific Terminology (given its context, airport names etc)\n* Lowercasing.\n* Tokenization.\n* Spelling Correction.\n* Stop Word Removal.\n* Rare Words Removal\n* Lemmatization\n* Named Entity Recognition (NER).\n* Part of Speech (POS) Tagging.\n* Text Vectorization.\n\n",
    "created_utc": "2024-11-13T17:49:40",
    "num_comments": 0,
    "comments": []
},
{
    "submission_id": "1gqtebn",
    "title": "Key Insight from Our Research on Lossless Compression for AI Models",
    "selftext": "**📝 Paper:** [https://arxiv.org/abs/2411.05239](https://arxiv.org/abs/2411.05239)  \n**💻 Code:** [https://github.com/zipnn/zipnn/](https://github.com/zipnn/zipnn/)  \n\n\nWe recently published a preprint, *ZipNN: Lossless Compression for AI Models*, and wanted to share one of our key findings with the community.\n\nNeural network parameters may seem random (e.g., `[0.1243, -1.2324, -0.3294...]`), but their representation in computers actually makes compression possible.\n\n# Key Insight: Floating-Point Structure Enables Compression\n\nFloating-point numbers, used to store model parameters, are structured as:\n\n* **Sign bit** (positive/negative)\n* **Exponent** (range)\n* **Mantissa** (precision)\n\nInterestingly, while the sign and mantissa bits appear random, the **exponent does not cover all values within its range**, and its distribution is skewed. As shown in the [figure](https://github.com/zipnn/zipnn/blob/main/images/exp_histogram.png), this distribution is illustrated across four different models—a pattern we observe across many models.\n\n[Histogram of exponent values](https://preview.redd.it/p0iec8x4tr0e1.png?width=792&format=png&auto=webp&s=9a4ff26cddb801daeef5d608ca4761bc294c5c11)\n\n  \n\n\n**Why?** This is due to how models are trained (see *Paragraph 3* in the paper for details).\n\n# ZipNN Library: Leveraging This Insight\n\nThis insight forms the basis of **ZipNN**, our open-source library for lossless compression, which offers improved compression ratios and faster compression/decompression speeds compared to state-of-the-art methods like ZSTD.\n\n**Storage Savings for Popular Floating-Point Formats:**\n\n* **BF16 format**: 33% space savings\n* **FP32 format**: 17% space savings\n\nWe’ve also developed a Hugging Face plugin, allowing for rapid downloading and loading of compressed models.  \n**Example model:** LLama-3.2-11B\n\nWith ZipNN, you can enable compression by adding just one line of code.\n\n🔗 [GitHub Repository](https://github.com/zipnn/zipnn/)",
    "created_utc": "2024-11-13T17:29:52",
    "num_comments": 2,
    "comments": [
        "Congratz!",
        "No relevant code picked up just yet for \"ZipNN: Lossless Compression for AI Models\".\n\n[Request code](https://www.catalyzex.com/paper/arxiv:2411.05239?requestCode=true) from the authors or [ask a question](https://www.catalyzex.com/paper/arxiv:2411.05239?autofocus=question).\n\nIf you have code to share with the community, please add it [here](https://www.catalyzex.com/add_code?paper_url=https://arxiv.org/abs/2411.05239&title=ZipNN%3A+Lossless+Compression+for+AI+Models) 😊🙏\n\nCreate an alert for new code releases here [here](https://www.catalyzex.com/alerts/code/create?paper_url=https://arxiv.org/abs/2411.05239&paper_title=ZipNN: Lossless Compression for AI Models&paper_arxiv_id=2411.05239)\n\n--\n\nTo opt out from receiving code links, DM me."
    ]
},
{
    "submission_id": "1gqrcqj",
    "title": "Exercise Solutions for the Mathematics of Deep Learning Book (De Gruyter)",
    "selftext": "Hi everyone! I'm currently studying the mathematical foundations of Deep Learning using the book mentioned in the title ([link here](https://www.degruyter.com/document/doi/10.1515/9783111025551/html)). I'm really enjoying it, but I noticed that it doesn’t seem to include solutions to the exercises—at least not in the version I have. I've tried searching online for solutions, but I haven't had any luck so far. \n\nDoes anyone here have access to the solutions or know where I might be able to find them? Thanks in advance for any help!",
    "created_utc": "2024-11-13T15:52:07",
    "num_comments": 0,
    "comments": []
},
{
    "submission_id": "1gqq6pi",
    "title": "How to deal with multi labeled text classification?",
    "selftext": "I have huge text data which is multi labelled and highly imbalanced. The task is to classify the text to their classes. The problem is I have to preprocess the text to reduce the data imbalance for the classes and choose a relevant model (transformers ..etc) to classify the text. I want some suggestions on how to preprocess the data to handle data imbalance and which model to use for the multi label classification? I have AWS g5x2 large and the training should be finished within 1 hour 30 min ( time constrain )with reasonable accuracy.",
    "created_utc": "2024-11-13T14:59:19",
    "num_comments": 0,
    "comments": []
},
{
    "submission_id": "1gqpxm8",
    "title": "How do I finetune ClipSeg?",
    "selftext": "I’m using zero-shot ClipSeg for image segmentation with a text prompt. How can I make this model provide domain-specific segmentation?\n\n  \nThanks!",
    "created_utc": "2024-11-13T14:47:45",
    "num_comments": 0,
    "comments": []
},
{
    "submission_id": "1gqp0en",
    "title": "How can I stop myself from getting 'stuck' at times?",
    "selftext": "Sometimes in my work I feel like I just get stuck, and I'm not exactly sure what to do, and I start to feel really overwhelmed and avoidant of the situation. I'm trying my best to develop skills that will help me not get \"stuck\".\n\nI think with deep learning, which is where I find myself experiencing this issue the most, I think it's sometimes because I'm not sure what to do next with a project. Usually, there's some sort of issue with it that I'm just not sure how to approach.\n\nHow can I get better at taking things such as \"my model isn't working\" to \"my model isn't working for this reason\" to \"this part of my code is the reason my model isn't working\" to \"this is how to fix the part of the code that isn't working\". Debugging. For some reason, I don't have much of a problem with debugging web apps and game dev stuff and honestly just software in general. However, with ML/DL it feels so much harder to debug and understand what exactly is going wrong. What input isn't coming out as the proper output? That sort of thing.\n\nI appreciate any insight!",
    "created_utc": "2024-11-13T14:07:00",
    "num_comments": 2,
    "comments": [
        "Maybe you'll find the following helpful.  They both cover aspects of troubleshooting ML/DL.\n\nA Recipe for Training Neural Networks from Andrej Karpathy blog:  \n[http://karpathy.github.io/2019/04/25/recipe/](http://karpathy.github.io/2019/04/25/recipe/)\n\nMachine learning yearning book by Andrew Ng:\n\n[https://info.deeplearning.ai/machine-learning-yearning-book](https://info.deeplearning.ai/machine-learning-yearning-book)  \n(I hope that's the correct link for the book.  I can't remember exactly where I downloaded it from.)",
        "Thank you! I'll check that out!"
    ]
},
{
    "submission_id": "1gqn8rs",
    "title": "Learning SFT and DPO",
    "selftext": "What are some online courses or resources to learn about SFT and DPO for LLMs? Ideally with both theory and practical exercises to run and try",
    "created_utc": "2024-11-13T12:52:25",
    "num_comments": 0,
    "comments": []
},
{
    "submission_id": "1gqn4cv",
    "title": "Is this how GPT handles the prompt??? Please, I have a test tomorrow...",
    "selftext": "Hello everyone, this is my first time posting here as I have only recently started studying ML. Currently I am preparing a test on transformers and am not sure if I understood everything correctly. So I will write my understanding of prompt handling and answer generating, and please correct me if i am wrong.\n\nWhen training, GPT is producing all output tokens at the same time, but when using a trained GPT, it is producing output tokens one at a time.\n\nSo when given a prompt, this prompt is passed to a mechanism basically same as an encoder, so that attention is calculated inside of the prompt. So the prompt is split into tokens, then the tokens are embedded and passed into a number of encoder layers where non masked attention is applied. And in the end, we are left with a contextual matrix of the prompt tokens.\n\nThen, when GPT starts generating, in order to generate the first output token, it needs to focus on the last prompt token. And here, the Q,K,V vectors are needed to proceed with the decoder algorithm. So for all of the prompt tokens, we calculate their K and V vectors, using the contextual matrix and the Wq,Wk,Wv matrices, which were learned by the decoder during training. So the previous prompt tokens need only K and V vectors, while the last prompt token also needs a Q vector, since we are focusing on it, to generate the first output token.\n\nSo now, the decoder mechanism is applied and we are left with one vector of dimensions vocabSize which contains the probability distribution of all vocabulary tokens to be the next generated one. And so we take the highest probability one as the first generated output token.\n\nThen, we create its Q,K,V vectors, by multiplying its embedding vector to the Wq,Wk,Wv matrices and then we proceed to generate the next output token and so on...\n\nSo this is my understanding of how this works, I would be grateful for any comment, and correction if there is anything wrong(even if it is just a small detail or a naming convention, anything will mean a lot to me). I hope someone will answer me.\n\nThanks!",
    "created_utc": "2024-11-13T12:47:15",
    "num_comments": 4,
    "comments": [
        "You’re on the right track, but let me iron out a wrinkle or two and crank this up a notch before your test with a little imaginary scene to think about this as, it will help later when I answer your questions directly.\n\nYou’re the captain of a starship (GPT). Your mission? Explore the vast expanse of language, one galaxy (token) at a time.\n\nTraining Phase: This is your dry run, where you chart the entire star map in advance. Your ship’s computer (the model) meticulously memorizes every route, every wormhole, every shortcut by predicting where each star (token) leads to the next. This is teacher forcing—you learn the optimal route while the map is right there in front of you. But here’s the catch: you might never fly those exact routes during the real mission, yet you’ve built intuition for the cosmos.\n\nInference Phase: Now you’re out in the wild. No complete map, just a starting star (prompt). You navigate one star at a time, piecing together a journey as you go. Each star you visit lights up nearby constellations, and as your journey deepens, the galaxy becomes richer and more vivid. You’re writing your own map in real time.\n\nAttention Mechanism? That’s your ship’s onboard AI, guiding every step. Each crew member (token) has three key tools:\n\n1. A telescope (Key): What can they see?\n\n2. A message (Value): What data are they carrying?\n\n3. A question (Query): What are they looking for?\n\n\nWhen one token asks, “What do I need to know to move forward?” the AI scans the telescopes of every other crew member and cross-references their messages. It finds the most relevant data and says, “This is what you need.” It’s not just about proximity—it’s about meaningful connections across the entire galaxy of tokens.\n\nMasked Self-Attention? This is the cosmic prime directive. You can only look back at the stars you’ve visited, never ahead to those you haven’t reached. It’s like navigating uncharted space with strict rules: no spoilers for the journey ahead. You must forge onward, star by star, reflecting only on your past.\n\nGenerating the Journey: Here’s where it gets fun. As your ship’s AI calculates the next jump (output token), it doesn’t just take the obvious route (argmax). Sometimes, it gambles—rolling the cosmic dice (sampling)—to discover new and creative paths. It’s exploration at its finest, with a touch of randomness to ensure your voyage isn’t predictable.\n\nNow, let’s transcend the mundane: Transformers aren’t just starships for language. They’re maps of thought itself. Think of every token as a historical moment, every attention layer as a web of relationships stretching across time and space. When GPT generates text, it isn’t just following a map—it’s composing an original symphony of language, one note at a time.\n\nBut here’s the kicker: you are the captain. You decide the prompts, the mission parameters, and the direction of the journey. The prompt becomes *the program*. The Ship's AI? It’s just a reflection of the universe you set it loose in—a mirror for how we humans connect ideas across vast distances of time, culture, and meaning. \n\nLet me find a few good resources and the references and for the questions answered.",
        "Ok, as promised, Elabora6ting to your questions and understanding:\n\n\"When training, GPT is producing all output tokens at the same time, but when using a trained GPT, it is producing output tokens one at a time.\"\n\nDuring training, GPT does NOT generate all tokens at the same time. Instead, it is trained to predict the next token for every position in a sequence simultaneously using teacher forcing. Essentially, each token in the input sequence has a corresponding target (the next token), and the model learns to predict these targets in parallel over all positions.\n\nAt inference time, tokens are indeed generated sequentially but the model generates one token, appends it to the prompt, and feeds the updated sequence back into itself to generate the next token.\n\nThink of training as teaching a typist to anticipate every word in a sentence based on partial context. During testing, however, the typist only gets to type one word at a time, using their learned predictive skills over their lifetime to guess what comes next.\n\n\"The prompt is passed to a mechanism basically same as an encoder, so that attention is calculated inside of the prompt.\"\n\nWell yes, while the process may *feel* similar to an encoder mechanism (especially in terms of attention), GPT does not use a separate encoder. Instead, it uses a decoder-only transformer, meaning it relies on masked self-attention. The \"masking\" ensures that each token can only attend to itself and the tokens before it, maintaining the autoregressive nature of the model.\n\nIn encoder-decoder models like BERT or T5, the encoder processes the entire input in a bidirectional way, allowing every token to see all others. GPT, however, never looks \"ahead\" in the sequence, even during training.\n\nImagine a person trying to guess a book's story while reading it linearly. They can reflect on what they’ve already read (past tokens) but can’t \"peek\" at future lines.\n\n\"For all of the prompt tokens, we calculate their K and V vectors... while the last prompt token also needs a Q vector.\"\n\nAlmost, every token has its own Q (query), K (key), and V (value) vectors, regardless of whether it’s a prompt token or a generated output token. These vectors are therefore calculated FOR EVERY TOKEN using learned projection matrices (Wq, Wk, Wv). The attention mechanism then uses these vectors to compute how much \"attention\" each token should pay to the others.\n\nKeys (K): Represent the \"features\" of each token.\n\nQueries (Q): Represent the \"questions\" a token is asking (e.g., \"What is relevant to me?\").\n\nValues (V): Contain the information to be aggregated.\n\nTherefore the last token does not uniquely \"need\" a Q vector. Instead, the Q vectors of all tokens interact with the K and V vectors of preceding tokens to compute attention scores.\n\nPicture a team of detectives (tokens). Each detective has: - A notebook summarizing their findings (K vector), - A question they’re investigating (Q vector), - Evidence they carry (V vector).\n\nTo solve the case, every detective compares their question with others' notebooks and decides whose evidence is most helpful.\n\nNow what you got right! \n\n\"Then, we create its Q, K, V vectors, by multiplying its embedding vector to the Wq, Wk, Wv matrices and then we proceed to generate the next output token.\"\n\nThis process happens dynamically at each step of the generation. For every new token added to the sequence, the model re-computes Q, K, and V vectors for the entire sequence (including the new token). However, efficient implementations like caching allow the model to reuse previously computed K and V vectors for earlier tokens, avoiding redundant calculations.\n\nThis caching mechanism is one reason transformer models can handle long sequences efficiently during inference.\n\nHere think of GPT as assembling a puzzle. For each new piece (token), it rechecks the arrangement of all existing pieces (previous tokens) to ensure the next piece fits perfectly.\n\n\"The decoder mechanism is applied and we are left with one vector of dimensions vocabSize which contains the probability distribution of all vocabulary tokens to be the next generated one.\"\n\nThis is correct. The model outputs logits of size vocabSize, which represent unnormalized scores for each token in the vocabulary. These logits are passed through a softmax function to produce a probability distribution.\n\nPro Tip: The choice of the next token doesn’t always involve selecting the one with the highest probability (argmax). Often, techniques like nucleus sampling or top-k sampling are used to add randomness and improve the fluency of generated text.\n\nImagine if GPT was a contestant in a rapid-fire quiz show. For every question (current context), it instantly computes how likely each possible answer (vocabulary token) is, then picks one based on probability.\n\nTransformers like GPT revolutionized ML by solving the \"long-range dependency\" problem, where earlier architectures (e.g., RNNs) struggled to maintain context over long sequences. The self-attention mechanism enables GPT to consider all tokens in the input context simultaneously, creating rich, contextual embeddings.\n\nHere are some good references and resources I have bookmarked over time for a refresher for this response. \n\nThe Transformer as a whole[The Illustrated Transformer-](https://jalammar.github.io/illustrated-transformer/)\n\nBased on the paper [Attention Is All You Need](https://arxiv.org/abs/1706.03762)\n\nA deep look at the paper-[A deep look at the paper-](https://www.columbia.edu/~jsl2239/transformers.html)\n\n[HuggingFace Video on Training Vs. Inference-](https://youtu.be/IGu7ivuy1Ag?si=oDlKsXtXnfCqGUpy)\n\nSo go boldly, friend. You’re not just learning transformers—you’re charting the frontier of thought itself. 🚀",
        "Thank you for your detailed and colourful response! I would like to get back at some of your comments to make sure I understand this completely.\n\n>During training, GPT does NOT generate all tokens at the same time. Instead, it is trained to predict the next token for every position in a sequence simultaneously using teacher forcing.\n\nWhen I said at the same time, I meant that the whole output sequence is generated at the same time, in a sense that there will be all of the tokens together after one pass through decoder. Contrary to one token per pass while using the decoder in practise.\n\n>Therefore the last token does not uniquely “need” a Q vector. Instead, the Q vectors of all tokens interact with the K and V vectors of preceding tokens to compute attention scores.\n\nWhen I said that K and V vectors are needed only for the last prompt token, I was refering to a time when we have already created the contextual matrix for the prompt token. And now that we are going to use these prompt tokens as already generated ones, we will not calculate their personal attentions to other tokens, so they dont need the Q vector. Because for every new token that we are trying to predict, we will use the focused token’s Q vector and the previous tokens’ K and V vectors, right?\n\nI also have another question.\n\nSo when using a decoder only, like a GPT, and training, is this how the input and output sequence work, in terms of <bos> and <eos> tokens.\n\nThe correct sequence, given to the decoder using masked attention: <bos> I am here\n\nThe expected correct sequence: I am here <eos>\n\nAnd so the job of the decoder would be to predict like this:\n\n<bos> -> I  \n<bos> I -> am  \n<bos> I am -> here  \n<bos> I am here -> <eos>\n\nSo at first, decoder has only attention for the <bos> token and has to predict token “I”, and so on...at the end it has to predict the <eos> token\n\nAnd if we use encoder-decoder and a sequence in french:\n\nJe suis ici\n\nWe would apply non masked attention in the encoder to this secuence without <bos> and <eos>, and then we would give decoder this correct translated sequence in english which he will use with masked attention, with the <bos> added:\n\n<bos> I am here\n\nAnd expect him to generate:\n\nI am here <eos>\n\nSo he will do this again:\n\n<bos> -> I  \n<bos> I -> am  \n<bos> I am -> here  \n<bos> I am here -> <eos>\n\nwhile also using all of the french sequence attention from in the cross attention blocks"
    ]
},
{
    "submission_id": "1gqmc9w",
    "title": "Video Game Ai",
    "selftext": "Would it be possible to make an AI overlay that can control game inputs that can then play the game and learn how to get really good at said game? If it is I would love pointers on where to start.",
    "created_utc": "2024-11-13T12:14:50",
    "num_comments": 8,
    "comments": [
        "You mean like the one that played Mario using simple NN in like 2017?",
        "[Here's a pointer](https://github.com/trackmania-rl/tmrl) ;)\n\nThis is focusing on vision-based real-time control though. If you want your model to become really good at the game, in most cases you will have to hack the game into a well-behaved simulator that you can step and where you can access low-level game information.",
        "I have heard there is something called reinforcement learning :D  \nI recommend you start here: [https://www.youtube.com/watch?v=2pWv7GOvuf0](https://www.youtube.com/watch?v=2pWv7GOvuf0)  \nstill a very old lecture but great. there are some blogposts or shorter videos of course. Depends on your interest in the topic.",
        "The key concept you are looking for is Reinforcement learning. Have a look at that topic and gaming. The biggest breakthrough in the field arguably was Deepmind making a new algorithm: Deep Q networks. This made the neural net be able to beat breakout.",
        "Unity has a library called MLAgents which you might like. I'm not sure if it's a great tool to learn how to do it yourself, but it is (imo) a great way to experiment with reinforcement learning and game making and have fun",
        "There are millions of these, just go on youtube and search your favorite games!\n\nAny game with more than 50,000 players probably has someone trying to make a PPO or something for it",
        "I'm trying to see if I can make a decent AI for Fortnite or Mincecraft PVP. I am a complete beginner to the coding AI world. Does this only work with Trackmania?",
        "This is a framework for real-time robots in general, but if you are a complete beginner you will have a very hard time implementing what you want to do. I suggest first learning deep reinforcement learning in simple gym environments before moving on to this type of projects, as it has several hard issues to tackle."
    ]
},
{
    "submission_id": "1gqlzzd",
    "title": "is there no comprehensive guide to machine learning that everyone recommends?",
    "selftext": "i searched around quite a lot but couldnt find anything - people recommend Pattern recognition by Bishop - but that book seems very intimidating as the first exposure.\n\n  \nHas anyone created a comprehensive list of books and resources which are high quality for say - \n\n1. Mathematics in ML\n\n2. Machine learning basics\n\n3. Deep networks\n\n4. GenAi\n\n  \netc..?\n\n  \nI would really like a post detailing all this stickied on the community so everyone can have an easy access to all these resources",
    "created_utc": "2024-11-13T12:00:31",
    "num_comments": 2,
    "comments": [
        "List of books I can't say but I have a roadmap I post Everytime someone asks the same question which was crucial for my relearning",
        "could you share it?"
    ]
},
{
    "submission_id": "1gqkxwt",
    "title": "Question - Computer Requirements for Model Fine Tuning",
    "selftext": "\nHi all,\n\nI am looking to invest in a new mid-to-long term computer to continue my NLP/ML learning path - I am now moving on to fine tuning models for use in my industry (law), or perhaps even training my own Small Language Models (in addition to general NLP research, experimentintg, and development). I may also dabble in some blockchain development on the side.\n\nCan I ask - would the new Macbook Pro M4 Max with 48GB RAM 16 core CPU and 40 core GPU be a suitable choice?\n\nVery open to suggestions. Thank you!",
    "created_utc": "2024-11-13T11:15:56",
    "num_comments": 0,
    "comments": []
},
{
    "submission_id": "1gqklft",
    "title": "Need help - making an MLP for the first time",
    "selftext": "Hi! I am a third year college student, and I am taking a machine learning and connectionist computing module. I have an assignment going on, and the professor is so laid-back and indifferent towards the class, that he does the bare minimum to help us with the assignment.\n\nI have to build an MLP from scratch (with one hidden layer trained using backpropagation) and use it to create three models - an XOR,  a model that predicts the output of sin(x1 - x2 + x3 -x4), and a letter recognition model. I am trying to do the second part, and I have no clue how to fine tune the model. I am randomly trying different values for the hyperparamters to see what works, but it is really slow and painful. I don't know well my model is learning, or where it should start. I really need someone's guidance whose done this before. I am happy to provide more details about the problem too.\n",
    "created_utc": "2024-11-13T11:01:46",
    "num_comments": 10,
    "comments": [
        "This looks interesting. The idea seems like you've already got something working, but there are some issues.",
        "Have you tried using ChatGPT to see if it can give you some basic ideas for how to go about it?",
        "What you're doing sounds really cool! How well do you understand backpropagation? I'm trying to build up to being able to attempt one of the projects you mentioned, but I'm like hardcore stuck at trying to understand how to update network weights when training. In particular, I'm trying to understand how the error from each run or batch is used to create the error function, and how the error of the output layer can be used to analyze the hidden layer. I don't understand how they are connected.\n\nWhat method are you using to calculate the error associated with each node in the network, and deciding whether to tune each weight up or down? And, how are you determining the rate of change of that error for each node in the network and creating the gradient vector?",
        "Yup, so I have the model ready, I am pretty sure it is working correctly. For the second part, I need to first generate a set of 500 vectors each consisting of 4 values (x1, x2, x3, x4) between -1 and 1. But because the numbers are so small, they many many numbers after the decimal point. So first of all, would you suggest to round these numbers to 4 or 5 decimal places?",
        "Yup, extensively using copilot, that thing has been helpful, but I feel like the extent of its answers is also limited. It said i could change the hyperparameters that I have been doing. It said I could produce a learning curve for how the error decreases in each epoch. It also had a few suggestions for which squashing functions would be helpful, like sigmoid, tanh etc",
        "So, I am using MSE (Mean Squared Error). So you don't have to calculate each component of  your gradient vector. The error on any example is simply error = (target_vector - output_vector), and then you do delta = error * f'(activation_vector) to obtain the gradient vector for the output layer. Calculating the error automatically determines whether to tune a weight up or down. \n\nMSE is doing error^2 for all examples, summing all of them, then dividing by the number of examples.",
        "You can scale them",
        "You could add a small epsilon value like 1e-8 to every value, but unless you're having issues with numerical instability, I wouldn't bother with that.",
        "Edit: Like it only helps me with very basic stuff, I have so many more questions while I am coding, that it isn't able to answer. But still it gets me somewhere, so thanks for suggesting that",
        "I’m building a tool that essentially builds an ML model for you based on problem descriptions. If you want, you can give it a shot: https://plexe.ai. It’s in an early stage right now but it could help - we generate a model code after considering various solutions, optimise them and then give you all the code and a report so you can understand what was done"
    ]
},
{
    "submission_id": "1gqkaz4",
    "title": "AI assistant for Linux OSs",
    "selftext": "I've been working on a project that can help any Linux user by executing tasks in natural language (primarily English now) using vector embeddings and NLP. I did that, but now I am trying to expand it to becoming a device assistant to help the user perform any task by just typing it out or using the STT feature (probably be using the Whisper model from HF). I'm also working on a small command generative model that can spit out commands and execute them for any task. \n\nWhy I'm not using OPENAI API or similar is because I am thinking this to make it an offline package to give absolute privacy to the user tasks.\n\nIt is similar to Google Assistant but it will be a lot powerful and offline and for Linux. I have contacted several companies for this project undertaking or collaboration like SUSE, RHEL, but got no reply from them. This can actually help them onboard more users by showing the ease of performing any task on their OS and help them save millions on tech support for companies using Linux Servers or similar. \n\nCan anyone suggest or advice how to improve my project better to sell this to a company or give insights or if you have any advice to help me out ? Would really be a huge huge help ! Thanks in advance ",
    "created_utc": "2024-11-13T10:49:59",
    "num_comments": 0,
    "comments": []
},
{
    "submission_id": "1gqk0ya",
    "title": "Need help tuning a model",
    "selftext": "",
    "created_utc": "2024-11-13T10:38:25",
    "num_comments": 0,
    "comments": []
},
{
    "submission_id": "1gqjohm",
    "title": "AI comments on reddit ",
    "selftext": "Can you suggest some tools to analyse the reddit comments and classify them as AI and human assuming i have a datset of reddit comments. I prefer python. ",
    "created_utc": "2024-11-13T10:23:55",
    "num_comments": 8,
    "comments": [
        "You have a dataset of Reddit comments and you have already labelled them as human or AI?",
        "If you share your dataset, I can build a classifier for you to do this. \nFYI - I’m building a tool that can create ML models for you from simple problem descriptions: https://plexe.ai",
        "I am thinking that too...",
        "No the comments are not classified as human or AI, not even a sample of them.",
        "Hi thanks, but I have to do it myself as it is part of a academic project",
        "Well your dataset is not very valuable until that has been done.",
        "Can you suggest some ai vs human  labelled dataset which i use to train my model?",
        "I do not know of such a thing, although I'm sure it exists."
    ]
},
{
    "submission_id": "1gqius1",
    "title": "Highest quality video background removal pipeline",
    "selftext": "",
    "created_utc": "2024-11-13T09:50:27",
    "num_comments": 8,
    "comments": [
        "This is very impressive, but posting a link to what appears to be a closed source paid service in a sub dedicated to learning seems a little off.\n\nEdit: Anyone who thinks attacking OP is the way to go, these people have produced something really cool and are sharing it. Don't be a dick.",
        "Anyone got a script to block this guy from ever coming up on my feed again?",
        "Rembg?",
        "Hey folks! We were looking for good video background removers but found that most of them sucked. Especially on complex scenes where videos would flicker or miss objects. So we built a new video background solution by combining SAM 2 (from Meta) and BiRefNet Lite (a more traditional foreground model). We use BiRefNet Lite to create an initial mask that is propagated by SAM 2.\n\nWe wrote more about it here and there’s a link to try it too: [https://www.sievedata.com/blog/high-quality-ai-video-background-removal-for-developers](https://www.sievedata.com/blog/high-quality-ai-video-background-removal-for-developers)\n\nWould love the community’s feedback :)",
        "Hey! I hear you. We're going to do a much more technical post on the inner workings on the pipeline but have a small snippet about this in the blog. Thought this part would be an interesting note to share with the community, but I understand what you're saying and hope to share slightly more education content soon.\n\n[*Segment Anything 2 (SAM 2)*](https://www.sievedata.com/blog/meta-segment-anything-2-sam2-introduction) *from Meta is an open-source model for object-tracking and segmentation. We adapted it into an automatic background removal tool by tackling key challenges, primarily SAM 2’s need for user prompts. Since background removal should be entirely automatic, we devised an approach to auto-select relevant objects in images, aiming to outperform existing image-to-mask solutions.*\n\n*Our solution combines SAM 2 with a traditional foreground model (*[*BiRefNet Lite*](https://huggingface.co/ZhengPeng7/BiRefNet_lite)*), using a hybrid pipeline. By starting with a foreground mask as a prompt, SAM 2 is iteratively guided to track specific objects across frames. To account for objects appearing in later frames, we use a comparison between tracked objects and the foreground mask; significant differences prompt SAM 2 to track new objects. This method, refined by filtering out noise from the foreground model, isolates main subjects, resulting in highly effective background removal across video frames. We plan to go into more detail about this approach in a followup technical blog.*",
        "Fuck off",
        "If someone innovates and puts in the work to build something valuable, it is up to them how to release it. \n\nSome people prefer citations. Some people prefer commercialisation of their work. \n\nML Academia is only 1/2 of the equation - ML industry exists too.\n\nMost commercial applications wouldn’t even release their pipeline or methodology and keep it completely closed off."
    ]
},
{
    "submission_id": "1gqhr91",
    "title": "Looking for help in AI (Deep Learning) project",
    "selftext": "So currently I'm taking a Deep Learning course as a part of my undergraduate degree, my professor likes to take things to the max, he made our course project off of an AI research paper he found 2 months ago and none of us have any idea where to start.\n\nIt's supposed to be an Automated Essay Scoring project, we are supposed to make it through the Encoder of a Transformer coded in PyTorch, I'd really appreciate it if somebody with more experience is willing to help guide me through this project",
    "created_utc": "2024-11-13T09:05:29",
    "num_comments": 12,
    "comments": [
        "How do you expect help if you describe the problem like this. We don't know what the problem is, what data do you have etc.",
        "Chatgpt can help you...xddd",
        "Well it's 12 pages long, I was wondering if I could get anyone to hop on a Discord call with me and such to help because the problem is kinda long",
        "I know but I'm scared I will get addicted and plagiarize everything, it's a really easy hole to sink into",
        "You can at least describe the dataset that you have.\n\nedit. Also why did you not say so in your post for discord.",
        "chatgpt is usefull to analize the paper and propose solutions.",
        "Do you have the paper online??? can show us? thanks.",
        ">You can at least describe the dataset that you have.\n\nThat's also a thing I don't understand about the project, each essay in the training set has 86 features\n\n>You can at least describe the dataset that you have\n\nWhat do you mean exactly?",
        "[https://drive.google.com/drive/folders/1tuSAmW\\_82xprXRfUa0FFga6qT0hFc4Tu?usp=drive\\_link](https://drive.google.com/drive/folders/1tuSAmW_82xprXRfUa0FFga6qT0hFc4Tu?usp=drive_link)",
        "How large is training data ? What kind of feature are there ? Is the data labelled ? Is it not labelled ? Give a general description of the dataset. \n\nAlso a simple google search yields [Build your own Transformer from scratch using Pytorch | by Arjun Sarkar | Towards Data Science](https://towardsdatascience.com/build-your-own-transformer-from-scratch-using-pytorch-84c850470dcb) Might be useful for your case",
        "Can I send you the dataset?",
        "No thanks."
    ]
},
{
    "submission_id": "1gqgcbl",
    "title": "[P] Develop Alexa like AI assistant running locally on a laptop",
    "selftext": "",
    "created_utc": "2024-11-13T08:06:58",
    "num_comments": 0,
    "comments": []
},
{
    "submission_id": "1gqeytt",
    "title": "Certifications for Transitioning from Full-Stack Development to Generative AI?",
    "selftext": " I'm a full-stack developer with 2 years of experience and am looking to transition into AI, specifically generative AI.\n Apart from building projects, building connections, and developing relevant skills, I think a recognized certification could add leverage to my profile. \nAre there any well-regarded certifications in generative AI that would be beneficial for someone in my position?",
    "created_utc": "2024-11-13T07:07:56",
    "num_comments": 0,
    "comments": []
},
{
    "submission_id": "1gqe7x5",
    "title": "Need help for project",
    "selftext": "Hey ! I am currently in cse 3rd year  . There's a thing in our clg like we have to do a mini project . As I am interested in ML , I would like to do ML based project  . It would help me if u suggest me some effective projects to do as a 3rd year cse student which will be helpful in the future . ",
    "created_utc": "2024-11-13T06:34:02",
    "num_comments": 4,
    "comments": [
        "Since you are ultra-NOOB, Train a House price regression model",
        "How about a health monitoring system ?! And I know ML",
        "What are you going to monitor? Is it a ML project or just  a dashboard ?",
        "It is a ml project . It predicts health risks and generates recommendations to improve health"
    ]
},
{
    "submission_id": "1gqd7ij",
    "title": "How to train an eye tracking neural network? ",
    "selftext": "I want to train an eye-tracking neural network that translates eye movement onto the screen, so the cursor moves to where the user is looking. The biggest issue I have right now is: what happens if there are multiple pairs of eyes? I only want to track one pair. Secondly, should I train three sets of CNNs? The first would determine if any eyes are present or if the eyes are closed, the second would locate the eyes in the image’s pixel coordinates (assuming all images are resized to 512x512), and the last would predict where the user is staring on the screen. Are there any better suggestions on how I should approach this? Also do you guys know of any databases specifically built for training eye-tracking CNNs?",
    "created_utc": "2024-11-13T05:45:14",
    "num_comments": 3,
    "comments": [
        "Can answer your question about three models. The answer is no, you train 1 model but use complex loss where you both predict whether eye position is relevant (open and look on screen) and their position. In case of close eyes second part of loss is equals to zero. I guess yolo models uses similar approach",
        "How about if there are multiple pairs of eyes on the screen, how should my AI handle it? (Assuming some of them are looking at the same position)",
        "Not completely familiar with subject but I would try 2 things: ignore everything and train model as is and then see the result, or try to do some easy preparations like carving central piece of image or just throwing an error if it is suitable solution"
    ]
},
{
    "submission_id": "1gqd514",
    "title": "Learn AdaBoost in this friendly video with code!",
    "selftext": "",
    "created_utc": "2024-11-13T05:41:51",
    "num_comments": 0,
    "comments": []
},
{
    "submission_id": "1gqcfaw",
    "title": "Confused About My Path in AI & Data Science—Is Research Experience in NLP/LLM Really Enough for ML Engineering Internships?",
    "selftext": "I recently watched a YouTuber named *The Data Janitor* talk about career paths in AI and ML, and it got me questioning my approach. He mentioned that roles in ML engineering are mainly aimed at people with a solid background in data—think data engineers and data scientists. He even gave this equation: **Data + Modeling = ML Model**.\n\nNow, this made me wonder if focusing on modeling alone is enough for me. I'm currently in an AI & Data Science degree program, and my goal is to eventually work in the AI/ML field. But here’s the thing: I’m not sure if climbing the typical “data” hierarchy (like starting as a data analyst) would work for me. In my country, there aren't really any entry-level data analyst jobs, and remote work as a data analyst doesn’t look promising either since the market is flooded with people willing to work for low wages.\n\nRight now, I’m getting ready to be a research assistant for a professor at my university. Most of our work will involve NLP/LLM projects, like fine-tuning existing models for specific applications, such as recognizing Arabic handwriting, and we’ll be publishing these findings in research papers. My question is: **Does this type of research experience boost my chances for non-academic roles, especially internships?**\n\nI’m aiming to land an internship by my freshman year. I’ve been looking at requirements for entry-level internships in ML, and some of them seem almost *too* simple. They list things like “basic knowledge of Python,” “understanding of ANN architectures,” and “some familiarity with TensorFlow” as enough. Is that really true?\n\nWould love some advice on whether research-focused experience in LLM and NLP could help me in the long run or if I’m better off pivoting to a different approach. Thanks in advance for any thoughts!",
    "created_utc": "2024-11-13T05:05:36",
    "num_comments": 5,
    "comments": [
        "You’ll be fine, I’ve seen a few of the Data Janitor’s videos as well as his earlier Quora posts. The dude is a contrarian who peddles misinformation so he can have an inflated sense of importance and sell his courses. The guy is a SQL DBA not a data engineer or ML engineer (although I’m sure he’s done some work that would normally fall into that domain as it’s common in tech to wear a lot of hats). You can check out his LinkedIn profile to confirm.\n\nThe guy also hates academics but doesn’t really understand the field beyond a few traditional ML models. He chides the idea of getting a PhD but every time he mentions somebody he looks up to in the field… they’re an AI researcher with a PhD (examples are Andrew Ng, Andrej Karpathy, and Yann LeCun). He also talks about how amazing XGBoost is and how useless researchers are. I guess the irony is lost on him that XGBoost was a research project started by a PhD student Tianqi Chen.\n\nIf I were you I would take everything he says with a grain of salt. He says a lot of things that are true, but he also says a lot of things that are wrong. Everything he says is extremely basic though.",
        "I'm just a student who's also interested in being a MLE so take it w a grain of salt, but as far as I know, MLE is more of a software/data engineering role than a machine learning role. I think research experience in ML is really good to have, but you would need to show your engineering chop to get an MLE position.\n\n\nI'm in the opposite situation! I have software engineering / infrastructure internship experience and data engineering projects, and I'm trying to get some ML related research experience and aim for an MLE internship before I graduate. I think your path is much more straightforward c:",
        "I always feel that most \"Real-World\" guys are full of crap and I did not really take him really seriously but still I kept my eyes on his words, but still, do you have a suggestion for me regarding my situation?",
        "Oop nvm I just saw that you are a freshman. Research experience is certainly only going to help, but make sure you do all the projects and leetcode like other students aiming for SWE. Expect to do some regular SWE or DE internships before you get offered MLE internships unless it's through connections or smtn",
        "CS is one of the fields where academic research and practical application (SWE, DE, etc) most align. Experience in one can often directly relate to work in the other. LLMs and NLP are also really in vogue right now, both in research and in companies of all sizes.\n\nResearch in this area is definitely helpful, even if you don’t end up training or fine-tuning models in an industry job that is still very useful experience. I work at an AI startup and my job involves reading and writing research papers, as well as all sorts of MLE and traditional SWE tasks.\n\nIn thing I would say is to spend some time understanding industry trends. RAG and agentic tool use are really big right now for example. In general, information retrieval (IR) tends to play a bigger role part in industry as datasets are very large and unseemly. Things like latency and scalability also play a bigger role. Definitely make sure to keep your programming and data skills up to snuff though, that’s one of the areas where fresh candidates tend to struggle the most in my experience. I would also recommend getting some exposure to deploying stuff on a cloud service like AWS or GCP as well."
    ]
},
{
    "submission_id": "1gqb74n",
    "title": "OCR for documents ",
    "selftext": "I’m looking to build a pipeline that allows users to upload various documents, and the model will parse them, generating a JSON output. The document types can be categorized into three types: identification documents (such as licenses or passports), transcripts (related to education), and degree certificates. For each type, there’s a predefined set of JSON output requirements. I’ve been exploring Open Source solutions for this task, and the new small language vision models appear to be a flexible approach. I’d like to know if there’s a simpler way to achieve this, or if these models will be an overkill.",
    "created_utc": "2024-11-13T03:55:03",
    "num_comments": 7,
    "comments": [
        "Pytesseract",
        "Remindme! 1 day",
        "Colbert",
        "You can use our API to do this: https://tile.run \n\nJust define the schema and we will extract according to that",
        "OcrMyPDF is based on Tesseract, open source, we’ve been using it in production for more than two years with good results.",
        "My team tested Tesseract and found that AWS was typically just better, which I guess shouldn't be surprising when comparing an open source library and a paid enterprise service. And it's far from perfect, but Textract also has functionality that's specifically for ID-like documents. I can't really get into details but there's a lot you can do with Textract plus Comprehend plus some custom pre/post processing.",
        "I will be messaging you in 1 day on [**2024-11-14 12:07:22 UTC**](http://www.wolframalpha.com/input/?i=2024-11-14%2012:07:22%20UTC%20To%20Local%20Time) to remind you of [**this link**](https://www.reddit.com/r/learnmachinelearning/comments/1gqb74n/ocr_for_documents/lwwl3bw/?context=3)\n\n[**1 OTHERS CLICKED THIS LINK**](https://www.reddit.com/message/compose/?to=RemindMeBot&subject=Reminder&message=%5Bhttps%3A%2F%2Fwww.reddit.com%2Fr%2Flearnmachinelearning%2Fcomments%2F1gqb74n%2Focr_for_documents%2Flwwl3bw%2F%5D%0A%0ARemindMe%21%202024-11-14%2012%3A07%3A22%20UTC) to send a PM to also be reminded and to reduce spam.\n\n^(Parent commenter can ) [^(delete this message to hide from others.)](https://www.reddit.com/message/compose/?to=RemindMeBot&subject=Delete%20Comment&message=Delete%21%201gqb74n)\n\n*****\n\n|[^(Info)](https://www.reddit.com/r/RemindMeBot/comments/e1bko7/remindmebot_info_v21/)|[^(Custom)](https://www.reddit.com/message/compose/?to=RemindMeBot&subject=Reminder&message=%5BLink%20or%20message%20inside%20square%20brackets%5D%0A%0ARemindMe%21%20Time%20period%20here)|[^(Your Reminders)](https://www.reddit.com/message/compose/?to=RemindMeBot&subject=List%20Of%20Reminders&message=MyReminders%21)|[^(Feedback)](https://www.reddit.com/message/compose/?to=Watchful1&subject=RemindMeBot%20Feedback)|\n|-|-|-|-|"
    ]
},
{
    "submission_id": "1gqaeu1",
    "title": "where to apply for master's in Europe?",
    "selftext": "I am currently in my final year of bachelor's in management information systems. I would like to apply to master's degree in Europe but I don't know where to start or how to choose. I will also need scholarship since the currency of my country is nothing compared to euro.   \n  \nAbout myself, I can say I have 3.5+ GPA and I had 2 months internship experience in object detection app development and currently having 3.5 months part time job experience in LLM and automated speech recognition model research and development. My main goal is to do my master's related to computer vision, object detection etc. but anything related to machine learning would also do. \n\nWhere should I apply? How can I find a program to apply? Is it possible for me to get a scholarship (tuition free + some funding for living expenses)? ",
    "created_utc": "2024-11-13T03:03:24",
    "num_comments": 0,
    "comments": []
},
{
    "submission_id": "1gqaenv",
    "title": "Good courses for model pruning and quantization",
    "selftext": "Hi!\n\nDo you have any suggestions on courses for neural network pruning and quantization? I tried searching coursera, but there were not comprehensive courses in this topic.",
    "created_utc": "2024-11-13T03:03:06",
    "num_comments": 0,
    "comments": []
},
{
    "submission_id": "1gq9ye3",
    "title": "\"Dot Product & Linear Transformation in ML\"",
    "selftext": "\n\nhttps://preview.redd.it/iias75g6cn0e1.png?width=1280&format=png&auto=webp&s=79b3d053e6e4f51b9fc0889d70ac80c1111deb74\n\nThe dot product and linear transformations are fundamental in ML and linear algebra.\n\n\n\nDot Product: Takes two vectors and outputs a scalar, measuring how much one vector \"aligns\" with another—like projecting a shadow.\n\nLinear Transformations: Multiply a matrix with a vector to change its direction, scale, or dimensionality, reshaping spaces while preserving vector relationships.\n\n\n\nThe Connection:\n\nThe dot product can be seen as projecting one vector onto another’s span. This aligns with linear projection transformations using matrices.\n\nFor example, mapping a 2D vector to a 1D line using a matrix mirrors the dot product’s computation.\n\n\n\n\n\nWhy It Matters in ML:\n\nFeature Importance: Used in algorithms like linear regression to compute weighted sums.\n\nSimilarity Measures: Quantifies feature similarity in clustering or recommendation systems.\n\nEfficient Computations: Neural networks leverage matrix-vector multiplications (batches of dot products).\n\nDimensionality Reduction: PCA uses projections to simplify data while retaining key patterns.\n\nLearn more in my lecture on Vizuara’s YouTube: [https://www.youtube.com/watch?v=47c2138lFRI&feature=youtu.be](https://www.youtube.com/watch?v=47c2138lFRI&feature=youtu.be)",
    "created_utc": "2024-11-13T02:31:05",
    "num_comments": 0,
    "comments": []
},
{
    "submission_id": "1gq9t5w",
    "title": "Fine-tuning a 8B base model with LORA",
    "selftext": "I'm trying to fine-tune the *base* version of Llama 3.1 8B.\nI'm not using the *instruct* version, because I'm teaching the model to use a custom prompt format.\n\n### What I did so far\n\n- I fine-tuned Llama 3.1 8B on 1 epoch of 36.000 samples, with the sample token length ranging from 1000 to 20.000 tokens.\n- When looking at the average length of a sample, it's only around 2000 tokens though. There are 1600 samples that are over 5000 tokens in length.\n- I'm training on completions only.\n- There are over 10.000 samples where the completion is over 1000 tokens long.\n- I'm using a 128 rank, 256 alpha.\n- My batch size is 1, while my gradient accumulation is 8.\n- I'm using the unsloth library.\n\nI actually did this training twice. The first time I used a batch size of 2 and a gradient accumulation of 4.\nI accidentally forgot to mask out the padded tokens then, so it also calculated the loss based on that.\nThe loss was much lower then, but overall the loss trens & the evaluation results were the same.\n\nThe reason I'm doing it with batch size 1 is that I don't need to pad the samples anymore, and I can run it on an A40. So it's a bit cheaper to do experiments.\n\n### Loss\n\nThe train loss & eval loss seemed to do OK.\nOn average, train loss went from over 1.4 to 1.23\nEval loss went from 1.18 to 0.96\n\nHere are some wandb screenshots:\n\n[Eval loss](https://github.com/user-attachments/assets/21981c85-e664-4019-ad8d-575c720831ac)\n\n[Train loss](https://github.com/user-attachments/assets/f0ae91d0-4a98-423a-aa23-4abaef501652)\n\n[Train grad_norm](https://github.com/user-attachments/assets/0c9f0242-3aef-4e7e-8f8b-17ee4f0bcf6f)\n\n### Testing it\n\nBut when I actually finally inference something (a sample that was even in the training data), it just starts to repeat itself very, very quickly:\n\nFor example:\n\n    I woke up with a start. I was sweating. I looked at the clock. It was 3:00 AM. I looked at the phone. I had 100 notifications.\n    I looked at the first one. It read \"DO NOT LOOK AT THE MOON\".\n    I looked at the second one. It read \"It's a beautiful night tonight. Look outside.\"\n    I looked at the third one. It read \"It's a beautiful night tonight. Look outside.\"\n    I looked at the fourth one. It read \"It's a beautiful night tonight. Look outside.\"\n    I looked at the fifth one. It read \"It's a beautiful night tonight. Look outside.\"\n    ...\n\nAnd it goes on and on.\nI can easily make it write other stories that seem fine for a few sentences, then start to repeat themselves in some way after a while.\n\nSo my questions are:\n\n- Is this normal, is it just very underfitted at the moment, and should I just continue to train the model?\n- Is it even possible to finetune a base model like this using LORA?\n- Do I maybe not have enough data still?",
    "created_utc": "2024-11-13T02:20:27",
    "num_comments": 0,
    "comments": []
},
{
    "submission_id": "1gq7rmy",
    "title": "Intro to reinforcement learning, are these still good? Other recommendation?",
    "selftext": "I'm grad school student starting my research within applying reinforcement learning to some computation problems, and totally new to reinforcement learning.\n\nI don't have much background with machine learning in general, but have a general idea from one of my courses in undergrad. My math isn't strong either.\n\nI did some research, and looking at [RL Course by David Silver](https://youtu.be/2pWv7GOvuf0?si=gI3Ehsv8CMwU4LYL), and R.Sutton's Reinforcement Learning book (translated into Japanese, as I am Japanese).\n\nWhile R.Sutton's book is 2nd edition and relatively new, David Silver's course seem pretty old. However, probably, basic things don't change and maybe it's still good introduction? Is it still a good choice? Are there better choices especially video lecture style resources?",
    "created_utc": "2024-11-12T23:43:32",
    "num_comments": 2,
    "comments": [
        "RL course by standford",
        "Found this, it's new! [https://youtu.be/WsvFL-LjA6U?si=A6\\_LptcBE--UaBGc](https://youtu.be/WsvFL-LjA6U?si=A6_LptcBE--UaBGc)\n\nthank you!"
    ]
},
{
    "submission_id": "1gq7o2y",
    "title": "What metrics should I keep track on my model to be able to debug and analyze my model.",
    "selftext": "Hi. I'm currently using weights & bias (wandb) for monitoring my model for debugging and analysis purposes. Im still new to this. I just want too ask what are metrics I should keep track to be able to debug and analyze my model. For now, I have tracked accuracy, loss and eopch of the model. I would appreciate if anyone can tell me if there's more metrics to keep track and why. ",
    "created_utc": "2024-11-12T23:36:04",
    "num_comments": 0,
    "comments": []
},
{
    "submission_id": "1gq7lhb",
    "title": "7 FREE Data Analytics Courses You Can Start Today! (No Cost, All Skill Levels) 🆓💻",
    "selftext": "",
    "created_utc": "2024-11-12T23:30:44",
    "num_comments": 0,
    "comments": []
},
{
    "submission_id": "1gq73s2",
    "title": "Which master degree is better to get a job as a Data Engineer /Machine Learning Engineer/ MLops Engineer ?",
    "selftext": "Hi, my intention is to pursue a Master Program in IT or in Data Science in the following year. But in the today market, which one is better for me to choose in order to seek for a first job in terms of Data and AI, such as Data Engineer, MLE or at least BE Developer with AI application development. I have already had a bachelor degree in Electronics and Telecommunications and 1-year experience in Backend Development (mainly .NET) and wanted to shifted my career path to the Data and AI domain.",
    "created_utc": "2024-11-12T22:55:18",
    "num_comments": 7,
    "comments": [
        "Computer science",
        "A masters in Data Science would be most apt. Many job postings ask for that.",
        "MSc in Statistics",
        "Those are three very different roles. CS would be the best since that covers a wide array of topics that touch on some topics in each of these.",
        "go in AI",
        "Hey everyone!\n\nI'm a research scientist in AI, and I’m offering a beginner-friendly course covering everything from Python basics to AI and deploying your own LLM models (like ChatGPT) — all hands-on, explained line by line in plain English.The course is $500 and spans seven months, with weekend classes and some assignments. It also includes a 3-month internship with our US-based company, where you'll apply what you've learned by working on real AI products.If you're interested, feel free to reach out at [admin@letscodeai.com]() for more details!",
        "Agree. In 2024, all of these roles are trending toward software engineering. CS is not SWE, but it’s closer than academic DS, math, or stats.\n\nSource: Am MLE."
    ]
},
{
    "submission_id": "1gq721k",
    "title": "Out of distribution data in VAEs",
    "selftext": "Is it possible to train a VAE to also be able reconstruct data which weren't in the training set. For example, let's say we leave out digots 8,9 in the mnist dataset. Is it possible to generate 8s given a sample of 8?",
    "created_utc": "2024-11-12T22:51:47",
    "num_comments": 2,
    "comments": [
        "No",
        "I'm almost certain the answer is no. I kinda see why you would think that but that's just not something that encoders (or any other ml algorithm) should be expected to be able to do."
    ]
},
{
    "submission_id": "1gq6jsr",
    "title": "𝐁𝐮𝐢𝐥𝐝 𝐋𝐋𝐌𝐬 𝐟𝐫𝐨𝐦 𝐬𝐜𝐫𝐚𝐭𝐜𝐡",
    "selftext": "“ChatGPT” is everywhere—it’s a tool we use daily to boost productivity, streamline tasks, and spark creativity. But have you ever wondered *how* it knows so much and performs across such diverse fields? Like many, I've been curious about how it really works and if I could create a similar tool to fit specific needs. 🤔\n\nTo dive deeper, I found a fantastic resource: *“Build a Large Language Model (From Scratch)”* by [Sebastian Raschka](https://www.linkedin.com/in/sebastianraschka/), which is explained with an insightful YouTube series *“Building LLM from Scratch”* by [Dr. Raj Dandekar](https://www.linkedin.com/in/raj-abhijit-dandekar-67a33118a/) (MIT PhD). This combination offers a structured, approachable way to understand the mechanics behind LLMs—and even to try building one ourselves!\n\nhttps://preview.redd.it/35sdlxdb2m0e1.jpg?width=1037&format=pjpg&auto=webp&s=dd228136fbf7cbdeeae253118ee7a46b04948c24\n\nWhile AI and generative language models architecture shown in the figure can seem difficult to understand, I believe that by taking it step-by-step, it’s achievable—even for those without a tech background. 🚀\n\nLearning one concept at a time can open the doors to this transformative field, and we at [Vizuara.ai](http://Vizuara.ai) are excited to take you through the journey where each step is explained in detail for creating an LLM. For anyone interested, I highly recommend going through the following videos: \n\n*Lecture 1: Building LLMs from scratch: Series introduction* [https://youtu.be/Xpr8D6LeAtw?si=vPCmTzfUY4oMCuVl](https://youtu.be/Xpr8D6LeAtw?si=vPCmTzfUY4oMCuVl) \n\n*Lecture 2: Large Language Models (LLM) Basics* [https://youtu.be/3dWzNZXA8DY?si=FdsoxgSRn9PmXTTz](https://youtu.be/3dWzNZXA8DY?si=FdsoxgSRn9PmXTTz) \n\n*Lecture 3: Pretraining LLMs vs Finetuning LLMs* [https://youtu.be/-bsa3fCNGg4?si=j49O1OX2MT2k68pl](https://youtu.be/-bsa3fCNGg4?si=j49O1OX2MT2k68pl) \n\n*Lecture 4: What are transformers?* [https://youtu.be/NLn4eetGmf8?si=GVBrKVjGa5Y7ivVY](https://youtu.be/NLn4eetGmf8?si=GVBrKVjGa5Y7ivVY) \n\n*Lecture 5: How does GPT-3 really work?* [https://youtu.be/xbaYCf2FHSY?si=owbZqQTJQYm5VzDx](https://youtu.be/xbaYCf2FHSY?si=owbZqQTJQYm5VzDx) \n\n*Lecture 6: Stages of building an LLM from Scratch* [https://youtu.be/z9fgKz1Drlc?si=dzAqz-iLKaxUH-lZ](https://youtu.be/z9fgKz1Drlc?si=dzAqz-iLKaxUH-lZ) \n\n*Lecture 7: Code an LLM Tokenizer from Scratch in Python* [https://youtu.be/rsy5Ragmso8?si=MJr-miJKm7AHwhu9](https://youtu.be/rsy5Ragmso8?si=MJr-miJKm7AHwhu9) \n\n*Lecture 8: The GPT Tokenizer: Byte Pair Encoding* [https://youtu.be/fKd8s29e-l4?si=aZzzV4qT\\_nbQ1lzk](https://youtu.be/fKd8s29e-l4?si=aZzzV4qT_nbQ1lzk) \n\n*Lecture 9: Creating Input-Target data pairs using Python DataLoader* [https://youtu.be/iQZFH8dr2yI?si=lH6sdboTXzOzZXP9](https://youtu.be/iQZFH8dr2yI?si=lH6sdboTXzOzZXP9) \n\n*Lecture 10: What are token embeddings?* [https://youtu.be/ghCSGRgVB\\_o?si=PM2FLDl91ENNPJbd](https://youtu.be/ghCSGRgVB_o?si=PM2FLDl91ENNPJbd) \n\n*Lecture 11: The importance of Positional Embeddings* [https://youtu.be/ufrPLpKnapU?si=cstZgif13kyYo0Rc](https://youtu.be/ufrPLpKnapU?si=cstZgif13kyYo0Rc) \n\n*Lecture 12: The entire Data Preprocessing Pipeline of Large Language Models (LLMs)* [https://youtu.be/mk-6cFebjis?si=G4Wqn64OszI9ID0b](https://youtu.be/mk-6cFebjis?si=G4Wqn64OszI9ID0b) \n\n*Lecture 13: Introduction to the Attention Mechanism in Large Language Models (LLMs)* [https://youtu.be/XN7sevVxyUM?si=aJy7Nplz69jAzDnC](https://youtu.be/XN7sevVxyUM?si=aJy7Nplz69jAzDnC) \n\n*Lecture 14: Simplified Attention Mechanism - Coded from scratch in Python | No trainable weights* [https://youtu.be/eSRhpYLerw4?si=1eiOOXa3V5LY-H8c](https://youtu.be/eSRhpYLerw4?si=1eiOOXa3V5LY-H8c) \n\n*Lecture 15: Coding the self attention mechanism with key, query and value matrices* [https://youtu.be/UjdRN80c6p8?si=LlJkFvrC4i3J0ERj](https://youtu.be/UjdRN80c6p8?si=LlJkFvrC4i3J0ERj) \n\n*Lecture 16: Causal Self Attention Mechanism | Coded from scratch in Python* [https://youtu.be/h94TQOK7NRA?si=14DzdgSx9XkAJ9Pp](https://youtu.be/h94TQOK7NRA?si=14DzdgSx9XkAJ9Pp) \n\n*Lecture 17: Multi Head Attention Part 1 - Basics and Python code* [https://youtu.be/cPaBCoNdCtE?si=eF3GW7lTqGPdsS6y](https://youtu.be/cPaBCoNdCtE?si=eF3GW7lTqGPdsS6y) \n\n*Lecture 18: Multi Head Attention Part 2 - Entire mathematics explained* [https://youtu.be/K5u9eEaoxFg?si=JkUATWM9Ah4IBRy2](https://youtu.be/K5u9eEaoxFg?si=JkUATWM9Ah4IBRy2) \n\n*Lecture 19: Birds Eye View of the LLM Architecture* [https://youtu.be/4i23dYoXp-A?si=GjoIoJWlMloLDedg](https://youtu.be/4i23dYoXp-A?si=GjoIoJWlMloLDedg) \n\n*Lecture 20: Layer Normalization in the LLM Architecture* [https://youtu.be/G3W-LT79LSI?si=ezsIvNcW4dTVa29i](https://youtu.be/G3W-LT79LSI?si=ezsIvNcW4dTVa29i) \n\n*Lecture 21: GELU Activation Function in the LLM Architecture* [https://youtu.be/d\\_PiwZe8UF4?si=IOMD06wo1MzElY9J](https://youtu.be/d_PiwZe8UF4?si=IOMD06wo1MzElY9J) \n\n*Lecture 22: Shortcut connections in the LLM Architecture* [https://youtu.be/2r0QahNdwMw?si=i4KX0nmBTDiPmNcJ](https://youtu.be/2r0QahNdwMw?si=i4KX0nmBTDiPmNcJ) \n\n*Lecture 23: Coding the entire LLM Transformer Block* [https://youtu.be/dvH6lFGhFrs?si=e90uX0TfyVRasvel](https://youtu.be/dvH6lFGhFrs?si=e90uX0TfyVRasvel) \n\n*Lecture 24: Coding the 124 million parameter GPT-2 model* [https://youtu.be/G3-JgHckzjw?si=peLE6thVj6bds4M0](https://youtu.be/G3-JgHckzjw?si=peLE6thVj6bds4M0) \n\n*Lecture 25: Coding GPT-2 to predict the next token* [https://youtu.be/F1Sm7z2R96w?si=TAN33aOXAeXJm5Ro](https://youtu.be/F1Sm7z2R96w?si=TAN33aOXAeXJm5Ro) \n\n*Lecture 26: Measuring the LLM loss function* [https://youtu.be/7TKCrt--bWI?si=rvjeapyoD6c-SQm3](https://youtu.be/7TKCrt--bWI?si=rvjeapyoD6c-SQm3) \n\n*Lecture 27: Evaluating LLM performance on real dataset | Hands on project | Book data* [https://youtu.be/zuj\\_NJNouAA?si=Y\\_vuf-KzY3Dt1d1r](https://youtu.be/zuj_NJNouAA?si=Y_vuf-KzY3Dt1d1r) \n\n*Lecture 28: Coding the entire LLM Pre-training Loop* [https://youtu.be/Zxf-34voZss?si=AxYVGwQwBubZ3-Y9](https://youtu.be/Zxf-34voZss?si=AxYVGwQwBubZ3-Y9) \n\n*Lecture 29: Temperature Scaling in Large Language Models (LLMs)* [https://youtu.be/oG1FPVnY0pI?si=S4N0wSoy4KYV5hbv](https://youtu.be/oG1FPVnY0pI?si=S4N0wSoy4KYV5hbv) \n\n*Lecture 30: Top-k sampling in Large Language Models* [https://youtu.be/EhU32O7DkA4?si=GKHqUCPqG-XvCMFG](https://youtu.be/EhU32O7DkA4?si=GKHqUCPqG-XvCMFG) ",
    "created_utc": "2024-11-12T22:16:54",
    "num_comments": 23,
    "comments": [
        "Understanding is one thing. The issue at this point in history is that you need a lot of capital for hardware to train a large language model that can surpass anything commonly openly available models can do zero-shot already. I myself know how to build a LLM architecture for training from scratch, but I can't practically train a useful one as it is unbelievably expensive.",
        "If it's for those without tech background, then it's too long and complicated. Otherwise it is good as the other offers, thx.",
        "Nice, I enjoyed your previous series.",
        "why we do it when we have chatgpt?",
        "excellent share Ambitious,\nthank you for your generosity and time to provide these resources, \ntruly outstanding of you\n\nregards,\njames",
        "Some tutorials on the infra for training and/or interference?",
        "BAHAHAHHAHAHAHQ",
        "Thanks for resources with details",
        "not just hardware, the cost for high-quality data is also very high. Openai literally employed thousands of contractors to manually label images, engineer prompts, and answer high level questions about any industry or academic discipline you can think of. This is ScaleAI's entire business model.\n\nUsing Wikipedia, github repos, and Google searches aren't good enough on their own if you want the LLM to be practical.",
        "You can currently train a GPT-2 scale LLM for under $100 reasonably.",
        "Don’t know why you’re downvoted because you’re right. No one without a tech background wants to watch 30 lectures for a single tech.\nThe order of the lectures is messed up too, like what are transformers then the architecture of GPT 3 then bird eye view of LLM’s architecture, with deep dive mixed in between?",
        "You want to build a LLM, don’t have a tech background and would prefer to do it in how many videos exactly?",
        "yes but the point stands, anything GPT-2 can do, an already existing freely licensed open model can do 10x better even in a zero-shot setting! so it will be practically useless. learning experience? sure! still, an expensive one.",
        "estimates put gpt4 at like 63 to 78 million, the computing power and the costs energy and infrastructure maintenance are all big costs",
        "Idk either but peeps on reddit can't handle critics. It's like as if they only want to hear what pleases them and always in a dreamy way. And even if I counter the negativity politely, I'm getting blocked/banned. This is the most subreddits in a nutshell.\n\nYou are absolutely right btw. People are obviously not interested in quality anymore. They have a lot of (good) intentions but often wasting their time reinventing the wheel for the good. But that's rather sad. \n\nI'm with you bro.",
        "Even one video can be sufficient, e.g. https://youtu.be/kCc8FmEb1nY?si=sK40PriWMZpKK1R0\n\nBut as qu3tzalify already mentioned, no one with no tech background is willing to watch 30 videos for one tech.",
        "i respectfully disagree,\nif one seeks competency in the knowledge and the how-to, they will sit through several orders more than thirty videos...",
        "Then don't watch them ? Not like most have any intent or building a llm if your goal is to use llm with API calls...\n\nAre you suggesting : let's build an os from scratch should only be a single 5 minute video?\n\n Karpathy's videos are freaking awesome but they assume a ton of prior knowledge.. unless your goal is to just copy his code and assume you understand 1% of what's going on",
        "I appreciate your respect and my reply is: it heavily depends on the context of the person. Some just want to get their feet wet by watching a first introduction video or touching the basics by reading an article or tutorial. For all of those who seek a deeper understanding: you definitely have to spend more time in general (besides being a talented genius). The mission somehow determines the time and energy spent by an individual and I'm not the one who claims to have a general solution for all people who likes to get to know the building process of a LLM from scratch.",
        "I don't watch them, thanks for the clarification. In case you missed it, OP's post is about building a LLM from scratch.\n\nNo, I'm not suggesting that. That's only your imputation against me.\n\nAnd you either way have to spend more time to gain a deeper understanding of the topic. I'm not claiming one video is sufficient enough to grasp the basics in one take.\n\nBut it doesn't take 30 video lectures to teach that topic. That's for sure. \n\nYou Sir should better be silent or contribute something more useful to the discussion than this next time. Just an advise.",
        "🤙🏽",
        "Yeah... It feels quite obvious to me that you are a bit clueless if you think you can cover all the material/knowledge required in less than that, it fact, it's probably a lot more than that, but whatever... you do you!",
        "Well, hold on. I guess your feels are kinda required to be fixed by a professional if you seriously assume I'm not into the topic enough to be a sufficient part of this discussion with my comments *big sigh*. \n\nApologies for being honest, but please be ashamed for writing such gibberish multiple times. You have no idea what you are talking about. It rather seems you are overwhelmed with the potential of the developments happening in tech, but also guess what. I'm not responsible for bringing you back to reality kid, so could you please contribute something more useful next time rather than questioning my words with your annoying doubt? (the last time I'm asking). Simply lost."
    ]
},
{
    "submission_id": "1gq5b2f",
    "title": "Microsoft Magentic One: Autogen's extension for Task Executions (Multi AI Agent)",
    "selftext": "Microsoft released Magentic-One last week which is an extension of AutoGen for Multi AI Agent tasks, with a major focus on tasks execution. The framework looks good and handy. Not the best to be honest but worth giving a try. You can check more details here : https://youtu.be/8-Vc3jwQ390",
    "created_utc": "2024-11-12T20:59:35",
    "num_comments": 0,
    "comments": []
},
{
    "submission_id": "1gq52p4",
    "title": "Last year of CS undergrad: Analysis or Optimization course series?",
    "selftext": "I am in my last year of undergrad, trying to decide between two course series. I would like to get as proficient as possible in applied data science prior to graduation. My end goal is to work in neuroscience research (non-university), but any data science job will do for now.\n\nThe course series and descriptions are as follows: \n\n1. a) Linear optimization: The optimization of linear functions subject to linear constraints. Linear programming, duality theory, sensitivity analysis, applications\n\nb) Nonlinear optimization: Nonlinear optimization with emphasis on basic theory (including Lagrange multipliers and the Kuhn-Tucker conditions), algorithms for numerical solution of problems, and applications. Introductory dynamic programming, with emphasis on applications and algorithms.\n\nOR\n\n2. a) Numerical computation: Computer arithmetic, solution of nonlinear equations and optimization in a single variable; matrix factorization; matrix iterative techniques.\n\nb) Numerical analysis: Polynomial interpolation including splines, orthogonal systems of functions and least squares approximation; numerical differentiation and integration; solution of systems of nonlinear equations and unconstrained optimization.\n\nComputation is a prerequisite for analysis. I could take linear optimization and computation instead of one of the series as well.\n\nAny input is appreciated, thank you",
    "created_utc": "2024-11-12T20:45:41",
    "num_comments": 0,
    "comments": []
},
{
    "submission_id": "1gq4zga",
    "title": "x.infer - Framework agnostic computer vision inference.",
    "selftext": "I spent the last month creating x.infer to simplify computer vision inference. \n\nTLDR features:  \n✅ Run inference with >1000+ models in 3 lines of code.  \n✅ List and search models interactively.  \n✅ Launch a Gradio interface to interact with a model.  \n✅ Serve model as a REST API endpoint with Ray Serve and FastAPI.  \n✅ OpenAI chat completions API compatible.  \n✅ Customize and add your own models with minimal code changes.\n\nLoad any models from the supported backends like TIMM, Transformers, vlLLM, Ollama, Ultralytics. Currently supports over 1000+ models combined.\n\nhttps://i.redd.it/fyylos1xhl0e1.gif\n\nList all models\n\nhttps://i.redd.it/jf4x4vy4ll0e1.gif\n\n\n\nLoad a new model by changing one line of code\n\nhttps://i.redd.it/n8yfgv6kil0e1.gif\n\nYou can easily add your own custom model by subclassing the `BaseXInfer` model\n\n    @register_model(\"my-model\", \"custom\", ModelInputOutput.IMAGE_TEXT_TO_TEXT)\n    class MyModel(BaseXInferModel):\n        def load_model(self):\n            # Load your model here\n            pass\n    \n        def infer(self, image, prompt):\n            # Run single inference \n            pass\n    \n        def infer_batch(self, images, prompts):\n            # Run batch inference here\n            pass\n\nThe x.infer API is easy to use and remember.\n\n    import xinfer\n    \n    model = xinfer.create_model(\"vikhyatk/moondream2\")\n    model.infer(image, prompt)         # Run single inference\n    model.infer_batch(images, prompts) # Run batch inference\n    model.launch_gradio()              # Launch Gradio interface\n\n\n\nRun inference and visualize on Gradio.\n\nhttps://preview.redd.it/jcc5mvfzil0e1.png?width=750&format=png&auto=webp&s=4fe9d00620035be35ed8ae7dc436c9f8e29cbc54\n\nServe a model with a FastAPI endpoint with Ray Serve as the model serving backend. Bonus - OpenAPI Chat Completion API compatible.\n\nhttps://preview.redd.it/bbdjqt57jl0e1.png?width=924&format=png&auto=webp&s=8662275c10290ed61a0c45b1d877864e79ef162f\n\nRepo - [https://github.com/dnth/x.infer](https://github.com/dnth/x.infer)\n\nColab quickstart - [https://colab.research.google.com/github/dnth/x.infer/blob/main/nbs/quickstart.ipynb](https://colab.research.google.com/github/dnth/x.infer/blob/main/nbs/quickstart.ipynb)\n\n\n\nWhy did I make this?\n\nIt's mostly just for fun. I wanted to practice some design pattern principles I picked up from the past. The code is still messy though but it works.\n\nAlso, I enjoy playing around with new vision models, but not so much learning about the framework it's written with.\n\nI'm working on this during my free time. Contributions/feedback are more than welcome! Hope this also helps you (especially newcomers) to experiment and play around with new vision models.",
    "created_utc": "2024-11-12T20:40:23",
    "num_comments": 0,
    "comments": []
},
{
    "submission_id": "1gq4w1j",
    "title": "What environment do you recommend?",
    "selftext": "I have been trying to dowload one of the quantized llm models from The HuggingFace to retrain and evaluate on a dataset. The issue is the amount of GPU available in the free environments. I need at least 20, and I will need to rerun that process a few times. \n\nCan you recommend me a free/ relatively cheap environment where this could work? I tried  GoogleCollab Pro+ but it was not enough, and I do not want to buy the premium option. I am  a beginner and still an undegrad trying to learn mroe about ML. Thanks for any suggestions!",
    "created_utc": "2024-11-12T20:34:45",
    "num_comments": 0,
    "comments": []
},
{
    "submission_id": "1gq4k3d",
    "title": "Binary classification error rate vs confidence",
    "selftext": "Hi! So I am a beginner self-learning machine learning and I'm currently dealing with a binary classification problem. I made a binary classifier with a basic neural network and I did some experiments with the error rate and confidence interval. It is not a very good classifier, so I want to have a better idea of how much I should trust the result given by the model. Is it a good idea to calculate the possibility of error based on the following result? (Like during inferencing, if my model predicted class 0 with 95% confidence, I can deduce the possibility of error is around 30%) Or this there a better way to approach this? Since It seems a little bit weird to me that somehow 85% confidence has a lower error rate than 95%. Thanks!\n\nhttps://preview.redd.it/tacbh9l0gl0e1.png?width=989&format=png&auto=webp&s=303b6152d7803bedec52188497e9d14eefbd7616\n\nhttps://preview.redd.it/jkmny851gl0e1.png?width=989&format=png&auto=webp&s=7730321d23746d5a1242362743ab5e3f1d95f348\n\nhttps://preview.redd.it/cf9fs6r1gl0e1.png?width=996&format=png&auto=webp&s=9b0f1a8bcad74c133dd50a9c3b0be206136a3187\n\n",
    "created_utc": "2024-11-12T20:15:49",
    "num_comments": 4,
    "comments": [
        "A network $f$ outputting a score $f(x)$ between $0$ and $1$ given the input $x$ is by itself not a classifier. In order speak of the confidence of a prediction you must first make a prediction. That is, you need a classifier which maps $x$ to a class and not a number between 0 and 1.\n\nThat is, you must fix a threshold theta such that x is classified in the positive class if f(x) > theta and in the negative class otherwise f(x) < theta.\n\nOnce you have a classifier it now makes sense to speak of the confidence of a prediction. That is, how certain can you be that something predicted as positive is actually positive ? How certain can you be that something predicted as negative is actually negative ?   To answer these questions you just compute the precision of the classifier.\n\nsource : me.",
        "Thanks for your reply! I actually took 0.5 as the threshold and used abs(possibility - 0.5)*2 to infer a 0-100% confidence score for each class. I wanted to know how reliable that confidence score is, so I plotted the figure above. It just seems weird to me that the error rate of 90% confidence in class 0 is even higher than that of 80%.",
        " Why would this abs(possibility - 0.5)\\*2  give a confidence score of a class ? What is the reasoning for this number",
        "My idea is to split the 0 - 1 space into 2 portions. When it gives 0.3, the value is far away from 0, so it will be 40% confidence for class 0 (0.2 \\* 2). When it gives 0.7, the value will be 40% for class 1 (also 0.2 \\* 2). Both 0.3 and 0.7 deviates from 0.5 to the same extend, and thus will result in the same confidence rate. It is just an idea I came up with to make it easier for me to see the confidence rate for each class. I am a beginner so perhaps there's a better way to do this."
    ]
},
{
    "submission_id": "1gq47bn",
    "title": "Why does my Random Forest use features that my Neural Network ignores?",
    "selftext": "Both my neural network and random Forest have about the same accuracy (with RF slightly better) on a binary classification task. The shapley values for certain features are zero according to the Neural Network but significantly greater than zero for the random Forest. My domain knowledge tells me these features are very informative yet were not picked up by a neural network even after regularization. How could this be? ",
    "created_utc": "2024-11-12T19:55:53",
    "num_comments": 4,
    "comments": [
        "Maybe the information represented by the feature is also present in different features and RF just picked a different one. Try to check for correlation or retrain the RF without the feature and compare the shaply values and the performance.",
        "As the other comment mentioned, NNs could be rolling several features into one behind the scenes.\n\nYou can try surrogate models both ways and see how things react. Might uncover the reason, or at least give more info to continue.",
        ">Maybe the information represented by the feature is also present in different features and RF just picked a different one.\n\nI literally didn't think of that. That's actually a real possibility. So maybe some form of \"confounding multicolinearity\" if that term makes sense?",
        "Not a term I would use to describe it but yes :D \nIn real world datasets you have this all the time. On its own it is not a big problem. Most ML algorithms can handle this on their own."
    ]
},
{
    "submission_id": "1gq2t0u",
    "title": "Advice and Resources Needed: Starting AI/ML for Physics Applications as a Physics Undergrad",
    "selftext": "Hello everyone, \nI'm an undergraduate student majored in physics and mathematics. I have some programming experience (Java and C++ from high school, and Python during my undergrad studies). I'm interested in building a career in physics with a focus on integrating AI/ML applications. I have a solid foundation in linear algebra and statistics, and I'm looking for guidance on where to start learning AI/ML with an emphasis on physics-related applications. I have a 2-month break and would like to cover all the essential topics needed to build a strong foundation in the subject, so I can continue learning at a steady pace afterward. \nAny recommended resources, advice, or study strategies would be greatly appreciated!\nThanks in advance.",
    "created_utc": "2024-11-12T18:42:22",
    "num_comments": 1,
    "comments": [
        "Physics informed deep neural networks"
    ]
},
{
    "submission_id": "1gq2ntn",
    "title": "Open Source project recommendations",
    "selftext": "Hi\n\nI've been spending some time working on ML models and I want to contribute to an open-source project to strengthen my skills and to have some fun with ML. Not sure where to start to looking for one. Anyone willing to help?",
    "created_utc": "2024-11-12T18:34:56",
    "num_comments": 11,
    "comments": [
        "Let me know if you find anything, looking for the same thing.",
        "Go on Kaggle.com, they have a bunch of open source datasets, competitions and courses.",
        "U hu r",
        "Let me know if you wanna work together",
        "Same for me. But I like the kaggle idea. I think I will have a look there :)",
        "Wanna collab",
        "https://github.com/prayas7102/NodejsSecurify check this if you van contribute.",
        "I've used Kaggle and I guess what would be a good place to start. However, I want to learn to contribute to something bigger and learn on the way if that makes sense.",
        "I'd genuinely be down",
        "Unsure how much I can bring to the table, but sure",
        "very interesting. I will try. Thank you!"
    ]
},
{
    "submission_id": "1gq2dbl",
    "title": "Is it possible to convert the Hazard ratio from cox proportional hazard model into a survival function to estimate survival probability?",
    "selftext": "[](https://www.biostars.org/u/146338/)\n\nHello, I'm Interested in making a ensemble machine learning model for survival analysis using Random forest, Gradient boosting and Cox proportional hazard model by averaging the survival curve to obtain the survival probability. But while modelling i encountered the issue where the cox model's output is different from the other two models. I want suggestion regarding **how can i transform the hazard ratio output from a cox model into a survival function** if possible. Any suggestions regarding alternative models and the exclusion of the cox model would also be appreciated. I'm new to this field please feel free to point out if there any mistakes in my approach, Thankyou.\n\nAdditional context: I used **CoxPHFitter** from **lifelines** to fit the model",
    "created_utc": "2024-11-12T18:20:14",
    "num_comments": 0,
    "comments": []
},
{
    "submission_id": "1gq1b69",
    "title": "Math for Machine Learning",
    "selftext": "Hi All,\nI am trying to set up a structured learning path for myself to learn the math behind machine learning more fully. \n\nI am taking this course: “Mathematics for machine learning and data science” ( Coursera course)\n\nhttps://www.coursera.org/specializations/mathematics-for-machine-learning-and-data-science\n\nI am also reading the “Intro to Statistical Learning.” \n\nI was wondering if there was anything else I could besides supplementing with KhanAcademy. I would really like to pre-plan all the materials so I’m not bouncing around everywhere (which is I’m currently doing). Does anyone have any advice? \n\n",
    "created_utc": "2024-11-12T17:27:17",
    "num_comments": 0,
    "comments": []
},
{
    "submission_id": "1gpz3ki",
    "title": "Need advice for data gathering? ",
    "selftext": "\nHello everyone. I am trying to do a project for which I need a huge corpus on EU TRAFFIC LAW specifically speed limit, signage, general regulations, offenses, penalties. I did find the resources like Eur-lex, european commision etc but I am having a hard time collecting data manually. Can someone suggest or help me in automating this process or maybe tell me some resources where I can get this data more easily. ",
    "created_utc": "2024-11-12T15:43:26",
    "num_comments": 0,
    "comments": []
},
{
    "submission_id": "1gpxm37",
    "title": "[OFFER][EU] Renting 6x RX 580 8GB GPUs - Mining Expert, DevOps Background, Crypto Payments Accepted 0,03$/h",
    "selftext": "Hey everyone,\n\nI'm offering my professionally maintained GPU setup for rent from Europe:\n\n* 6x RX 580 8GB with server-grade cooling\n* Pricing: $0.03-0.07/hour per GPU (negotiable based on workload)\n* Payment in crypto after job completion\n* EU-based servers with excellent connectivity\n* Willing to upgrade RAM/CPU if needed\n* Can expand the rig based on requirements\n\nBackground & Expertise:\n\n* Extensive mining rig building & maintenance experience\n* Professional Backend/DevOps developer\n* Hardware optimization specialist\n* Custom rig assembly expert\n* Thermal management experience with server-grade cooling solutions\n\nTechnical Capabilities:\n\n* Docker container setup\n* Secure, anonymous access\n* Custom environment configuration\n* Technical support 24/7\n* Real-time monitoring\n* Thermal optimization\n* Performance tuning\n* EU datacenter-grade internet connection\n\nThe setup can be used for:\n\n* ML/AI training\n* Rendering\n* Computing tasks\n* Mining\n* Other GPU-intensive workloads\n\nFlexibility is key - I'm open to:\n\n* Long-term arrangements\n* Project-based rentals\n* Custom configurations\n* Hardware upgrades for the right project\n* Scaling solutions\n\nAll GPUs are:\n\n* Professionally maintained\n* Server-grade cooling system\n* Stable 24/7 operation\n* Monitored for optimal performance\n* Temperature controlled environment\n\nEuropean Advantages:\n\n* Low latency for EU clients\n* GDPR compliant if needed\n* Reliable power infrastructure\n* Professional hosting environment\n\nDM me",
    "created_utc": "2024-11-12T14:35:59",
    "num_comments": 0,
    "comments": []
},
{
    "submission_id": "1gpwhx7",
    "title": "The Regression Revolution: Supervised Algorithms Explained 🚀📈📊",
    "selftext": "#Python, #DataScience, #ArtificialIntelligence, #MachineLearning, #DeepLearning, #AI, #DataAnalytics, #BigData, #PythonProgramming, #ML, #DataEngineer, #DataScientist, #DataVisualization, #AIForGood, #NLP, #ComputerVision, #NeuralNetworks, #DataScienceCommunity, #DataDriven, #PythonDeveloper, #Statistics, #DataScienceTips, #AICommunity, #PythonCode, #AIResearch, #MLModels, #DataMining, #PredictiveAnalytics, #SupervisedLearning, #UnsupervisedLearning, #PythonScripts, #DataWrangling, #AIApplications, #PythonForDataScience, #AIProjects, #MLProjects, #DataStorytelling, #PythonLibraries, #DataPreparation, #DataCleaning, #ModelEvaluation, #AITrends, #DataScienceTools, #Kaggle, #DataScienceLife, #MLAlgorithms, #PythonCommunity, #AIInnovation, #Analytics, #DataScienceJourney",
    "created_utc": "2024-11-12T13:48:09",
    "num_comments": 0,
    "comments": []
},
{
    "submission_id": "1gpwh8v",
    "title": "🚨 Master Lasso Regression: Simplifying L1 Regularization 📈",
    "selftext": "#Python, #DataScience, #ArtificialIntelligence, #MachineLearning, #DeepLearning, #AI, #DataAnalytics, #BigData, #PythonProgramming, #ML, #DataEngineer, #DataScientist, #DataVisualization, #AIForGood, #NLP, #ComputerVision, #NeuralNetworks, #DataScienceCommunity, #DataDriven, #PythonDeveloper, #Statistics, #DataScienceTips, #AICommunity, #PythonCode, #AIResearch, #MLModels, #DataMining, #PredictiveAnalytics, #SupervisedLearning, #UnsupervisedLearning, #PythonScripts, #DataWrangling, #AIApplications, #PythonForDataScience, #AIProjects, #MLProjects, #DataStorytelling, #PythonLibraries, #DataPreparation, #DataCleaning, #ModelEvaluation, #AITrends, #DataScienceTools, #Kaggle, #DataScienceLife, #MLAlgorithms, #PythonCommunity, #AIInnovation, #Analytics, #DataScienceJourney",
    "created_utc": "2024-11-12T13:47:18",
    "num_comments": 0,
    "comments": []
},
{
    "submission_id": "1gptifb",
    "title": "Seeking advice on using AI for screening job applications (first ML project)",
    "selftext": "Hey!\n\nI'm looking for some guidance with a project I've been working on for a while. This will be my first ML project, so expect some beginner questions :)\n\nIn the non-profit organization I belong to, we collect personal details, an optional cover letter, a CV, and answers to a few open-ended questions from each applicant. The challenge is that we often receive around 500-600 applications per position, and it would be great to use AI for some initial screening to reduce the need for manual review.\n\nIdeally, the model would allow admins to input suggestions for important values, features, or skills for the position. The output should be a list of applications with a score, allowing us to sort and focus on those candidates who are the best fit.\n\nI know this falls under the domain of NLP, but I'm not sure what steps to take next. I have extensive programming experience, so I'm open to either building a tool from scratch with available libraries or using an existing solution.\n\nMany thanks in advance for any suggestions on how to tackle this problem!",
    "created_utc": "2024-11-12T11:44:27",
    "num_comments": 0,
    "comments": []
},
{
    "submission_id": "1gpsrzk",
    "title": "Want to get 5 hours of your week back with AI? I can help!",
    "selftext": "Hey all! I’ve been diving deep into AI lately—not just the tech side but how it can actually help us save time and boost productivity. After experimenting with different tools, I realized that a lot of people don’t know where to start with AI, or they think it’s just for hardcore techies.\n\nThat’s why I started Reclaim.ai, a weekly newsletter that shares practical, no-fluff ways to leverage AI and reclaim your time. Every issue focuses on actionable tips, from AI-driven scheduling and task management to automating repetitive work. The best part? It’s geared toward anyone who just wants to be more efficient, not necessarily AI experts.\n\nEach week, you’ll get\n\n\n•\tA Productivity Prompt: One quick tip to improve how you work.\n\n\n•\tA Long-Form Story: A short dive into an AI tool or trend that can make a difference.\n\n\n•\tAn AI Tool Spotlight: A tried-and-tested tool recommendation that’ll help you cut down on the busywork.\n\n\nIf you’re curious about getting more done with less effort, check it out! Let me know if you have any questions about AI and productivity—I’m happy to share what I’ve learned.",
    "created_utc": "2024-11-12T11:14:42",
    "num_comments": 0,
    "comments": []
},
{
    "submission_id": "1gprodr",
    "title": "Sample Images for Image Deduplication algorithms.",
    "selftext": "I'm currently working on a project that requires a lot of nearly simillar images that will be the input of the program based on ImadeDedup libary.  \nHow can I find the free image source for free ?",
    "created_utc": "2024-11-12T10:29:53",
    "num_comments": 0,
    "comments": []
},
{
    "submission_id": "1gpr64l",
    "title": "Help! Feels Stuck, I have completed 2 chapters of ISL,it took me 20 days, till now going weel, whenever i stuck i grind and understood every theory well, like p-value, R^2 etc. but can't grasp its LAB assessment, it feels like my 20 days of hardwork goes just vanished. Suggest something pls",
    "selftext": "I know its a very basic question, but i am yet starting. In the lab they just started importing functions and started fitting. Is it the ML? aren't we suppose to write every formula/functions?",
    "created_utc": "2024-11-12T10:09:34",
    "num_comments": 9,
    "comments": [
        "Did you watch the videos as well?",
        "You should use ChatGPT. Idk what the rest of your background is, but I personally find it very difficult to read a stats book and just understand it with one go. And I don’t find rereading the same thing particularly useful in many cases. Like, I didn’t understand it so rereading it will not magically make me understand it. \n\nBut with LLM you can just ask a specific question and it is a good place to get an explanation and you can ask it to be more intuitive or give more specific examples. \n\nYou should also just google and rely on multiple sources too. LLM are not perfect and they do not always give correct answers, so if you’re a complete newbie then it will be hard for you to determine if it’s wrong or not. IME it is really effective with explaining stats and ML concepts.",
        "No",
        "Yeah i would have never reach so far without gpt, there were so many things that's doesn't make sense, i used gpt as a teacher it really helped a lot, my issue is , In the lab they started linear regression with functions/methods without explaining methods. I think for time being i should just learn as it is",
        "I would start there, maybe the videos will make more sense for you",
        "Officially one from the ISL book?, because i guess in the videos they mostly just dictate the book(maybe I am wrong here). or should i watch other creators videos?",
        "https://edx.org/learn/statistics/stanford-university-statistical-learning\n\nhttps://www.edx.org/learn/python/stanford-university-statistical-learning-with-python\n\nThese are the videos, you can watch them for python or r. It’s completely free.",
        "Oh thanks, i was feeling very low",
        "it look like, covering this book, alongside of videos and labs will be very helpful, by this i can learn faster"
    ]
},
{
    "submission_id": "1gpqmmy",
    "title": "Newbie Self-Studying Data Science - Need Guidance on the Right Learning Path",
    "selftext": "Hi everyone! I'm new to data science and self-studying. At first, I wanted to become a Data Analyst, but after diving deeper, I discovered data science, which made my career goal a bit unclear. I’m hoping for some guidance from professionals to make sure I'm on the right path.\n\n**Where I'm at so far:**  \nI started with SQL, practiced it, and then moved on to Python, focusing on pandas and numpy. Right now, I’m learning linear algebra and calculus, and after that, I plan to study statistics and probability.\n\n**Where I need help:**  \nSince I’m self-studying, I don’t have a clear path and feel a bit lost. There are so many resources out there, but I’m not sure which ones to focus on.\n\n* What should be my learning path?\n* Are there other languages or tools I should learn?\n* What skills or projects should I work on to land my first internship?\n\n**My goal:**  \nIn the long term, I’d like to work in data science within the finance industry.\n\nThanks in advance for any advice or guidance! I’d really appreciate help finding a clear path.",
    "created_utc": "2024-11-12T09:47:48",
    "num_comments": 5,
    "comments": [
        "Step 1) use the search bar",
        "This is a question I would ask gpt 😂",
        "https://virgili0.github.io/Virgilio/",
        "idk but i just got accepted into nvidia's new startup platform and the shit they have coming out soon and want people to take their courses and help with is absolutely insane! They have like their own whole university now its crazy over my head but i can see its gonna be huge",
        "Hi all! My name is Ryan, and I am an experienced data scientist working in the financial sector. I have been studying data analysis and machine learning for over two decades, and currently use Python as my main programming language. I would like to expand my knowledge by exploring more advanced topics such as deep learning and machine learning algorithms."
    ]
},
{
    "submission_id": "1gpovsl",
    "title": "Retrieval augmented generation-based chat application",
    "selftext": "",
    "created_utc": "2024-11-12T08:36:11",
    "num_comments": 0,
    "comments": []
},
{
    "submission_id": "1gpou4l",
    "title": "A complete guide on how to extract text from a board or on paper",
    "selftext": "",
    "created_utc": "2024-11-12T08:34:13",
    "num_comments": 0,
    "comments": []
},
{
    "submission_id": "1gpoj8r",
    "title": "I have been applying for AI/ML internships and co-op roles for months now. Didn't get a single response. Can you please give me some feedback. Thank you.",
    "selftext": "Hi,\n\nI have been applying for AI/ML/Data science intern roles. Haven't been able to get a single interview. Is there something wrong with my resume or Is it switch from Cloud to AI that is causing problems?\n\nAny feedback would be highly appreciated.\n\nThank you!\n\n[https://imgur.com/a/iepUgWE](https://imgur.com/a/iepUgWE)\n\n\n\nhttps://preview.redd.it/chi6ibmkxh0e1.jpg?width=902&format=pjpg&auto=webp&s=921ff55c3f983843b9ffc33c2c1ea51ed34cf8c2",
    "created_utc": "2024-11-12T08:21:40",
    "num_comments": 11,
    "comments": [
        "Honestly think you might be better fit for entry level roles rather than internships with this resume",
        "Dm me I’m interested in hiring you",
        "Are you an international student?",
        "Not work at an AI startup in California",
        "That could be a reason but I'm applying for internship roles because of my student status. Its frustrating tbh.",
        "Are you a recruiter?",
        "Yes.",
        "That's probably the reason.",
        "You mean they won’t give jobs to international students?",
        "Yeah, the market is terrible, and international students need sponsorship. This makes it way harder for you. It's the truth. Just keep applying and changing your resume based on JD",
        "You are right about the market but I already have 4 months co-op work permit and I will legally get my work permit from the government as soon as my program has ended. I don’t think sponsorship is required."
    ]
},
{
    "submission_id": "1gpoa1m",
    "title": "What should be my pathway from this point in life?",
    "selftext": "Hello everyone I am a first year college student planning to major in math with a minor in data science. Since about two years now I have been interested in the field of Ai/Ml and want to make a career where I work as an Ai/Ml engineer in some capacity.\n\nCould you guide me by answering these few questions-\n\n1) I guess I decided to major in math because I think the rigour it will provide me will be helpful in the field and Ml is very math heavy anyways - is there something I should focus on the next four years of my math degree like taking extra stat courses or something like that?\n\n2) I am planning to pick up coding on my own meanwhile and I am planning to follow Cs50 Harvard course for introduction in coding. What can I do after that ?\n\n3) What should I plan to master in after graduating? Ms in stats? Ms in computer science? Ms in Ai-ml which skills will help me stand out?\n\n4) Even though I am learning coding how to make myself competitive and stand out in the ai/ml market especially as a non cs student? Like after learning coding should I apply for ml internships? Focus on online ai ml data science course?\n\n5) Any other advice would be appreciated!!",
    "created_utc": "2024-11-12T08:11:16",
    "num_comments": 0,
    "comments": []
},
{
    "submission_id": "1gpnvru",
    "title": "Have you run into the issue of hallucinations in LLM models?",
    "selftext": "How do you manage their impact on model reliability? What detection and monitoring methods do you use, and how do they help improve the accuracy and reliability of your models?",
    "created_utc": "2024-11-12T07:54:18",
    "num_comments": 0,
    "comments": []
},
{
    "submission_id": "1gpmp7r",
    "title": "Do  your professors help you with your code?",
    "selftext": "",
    "created_utc": "2024-11-12T07:03:00",
    "num_comments": 2,
    "comments": [
        "i assume this question was asked in context of machine learning being used in science. no, a professor will typically advise you on how to perform research, what literature to read, what research questions to work on, publishing ... programming is on you.",
        "That’s what I was asking! Thanks"
    ]
},
{
    "submission_id": "1gpm34a",
    "title": "Explain this am new to this topic",
    "selftext": "",
    "created_utc": "2024-11-12T06:34:58",
    "num_comments": 7,
    "comments": [
        "What have you tried and which part is confusing?",
        "Alright friend. \n\n  \nYou have real data, results (like win/loss - what happened in real life) and you have your ML model that has learnt some patterns and can make predictions based on the data to which it was fit.\n\n  \nIn ML terms, you have: \n\n1) TP, ActualWin+PredictionWin (True Positive; means the model gave you 1 ('yes', 'positive' etc) and it is TRUE because the real data is True as well), and the actual data/result is 1 as well.\n\n2) TN ActualLoss+PredictionLoss (True Negative; means the model gave you 0 ('no', 'negative' etc) and it is TRUE because the real data is True as well), and the actual data/result is 0 as well.\n\n3) FP ActualLoss+PredictionWin (False Positive; means the model gave you 1 and it is FALSE because the real data is 0).\n\n4) FN ActualWin+PredictionLoss (False Negative; means the model gave you 0 and it is FALSE because the real data is 1).\n\n  \nTo calculate these, you need to do this: divide each number by the 'whole' (sum of all); you'll have:\n\nTP = 65/sum = 65/(65+3+8+24)=0.65=65%\n\nTN = 24/sum = 0.24 = 24%\n\nFP = 3/sum = 0.03 = 3%\n\nFN = 8/sum = 0.08 = 8%",
        "How many predictions did you get right? How many did you get wrong? The left side tells you what you predicted. The top side tells you what the actual result was. Look at where they intersect. There were 65 instances where the prediction was a win and the actual result was a win. Conveniently this is out of 100 total examples, so the number you got right/wrong is equal to the percentage you got right/wrong.",
        "Instead of writing prediction there, they should've written prediction win, prediction loss. That'll make this matrix super intuitive. \n\n\nHow many of your predictions are actuals, how many actuals did you miss ..etc come in error analysis. \n\nIf everything is diagonal then it's good (prediction = actual) else there's errors.",
        "Confusion matrix is just 4 things, true positives, false positives, true negatives, false negatives. \n\nLook up the definitions for those and use the numbers in the table to compute them.",
        "We're not ChatGPT",
        "Maybe confusion matrix is confusing (pun intended)"
    ]
},
{
    "submission_id": "1gplfby",
    "title": "Do I need to use OpenAi for a chatbot to answer questions ONLY from a dataset given to it ",
    "selftext": "\nI'm making a chatbot that takes as dataset all courses offered by a college (1M tokens all together) and then answers questions about those courses that are available in the dataset (such as \"what psychology courses are available on Monday\") though it will become expensive.. Is there an alternative to achieve the same goal? Maybe another model? I don't need it to be smart or know things outside of the dataset, just answer thse types of questions ",
    "created_utc": "2024-11-12T06:03:31",
    "num_comments": 6,
    "comments": [
        "Just put it in a table and run queries. As long as the columns are labeled, you can show whatever kinds of filters you want.",
        "Would retrieval augmented generation work for your desired outcome? RAG would pull directly from your dataset and could limit responses to just that dataset",
        "Just basic questions about what courses are available at what times as such. The hardest it could get is just asking it to make you a schedule for example \"I want to register in thr following classes and only want them on Monday Wednesday while avoiding time conflicts between courses\"\n\nIf it'll help I can split up them into majors and courses so that it doesn't have to read the entire course offering for everything and only for the ones asked.",
        "If you want to build RAG, you can run open LLMs locally or through Hugging Face",
        "I would recommend you to check the [RASA](https://rasa.com/) framework. As you said, you don't need a dynamic agent, only one that knows how to answer specific questions.\n\nWith RASA you can create a chatbot that handles different flows in your conversation, catching intentions and parameters. For example:\n\n> user: tell me which courses are available on Monday \n\nThe intention is `[consulting]`, with `monday` and `courses` as the parameters. Then, once you got the intention and parameters from the message, you can execute a query and return the requested info. \n\n> chatbot: Sure, the courses are: `{SELECT * FROM courses WHERE ...}`\n\nStill a NLP problem where you train a model, but not a heavy one. Besides, you remove the expensive calls to external APIs.\n\nIf you need something just send a DM, I'll be glad to help",
        "Why not just have your chat process the input as if it’s going to fill out a sql query, and the run your checks on the query to run it and give the response back to the student?\n\nI wouldn’t give my dataset directly to the bot because they aren’t really accurate reading from a table but you can have it fill out the sql query correctly like 99% of the time.\n\nThis opens up your code to sql injection vulnerabilities but nothing that can’t be overcome."
    ]
},
{
    "submission_id": "1gpkj87",
    "title": "Time Series Forecasting book from Manning",
    "selftext": "Hey there,  I just started at Manning Publishing, and I wanted to share a new book with you, and ask a question: Would you like Manning to post discount codes in here, and ask for technical reviewers?\n\n  \nUnlock the power of time series forecasting with Marco Peixeiro's new book. Dive into advanced techniques using foundation models to predict future trends and make data-driven decisions. Perfect for Data Scientists and Machine Learning Engineers eager to stay ahead.\n\n[https://mng.bz/YDxN](https://shortener.manning.com/YDxN) \\--or-- [https://www.manning.com/books/time-series-forecasting-using-foundation-models](https://www.manning.com/books/time-series-forecasting-using-foundation-models)\n\n\n\n",
    "created_utc": "2024-11-12T05:19:36",
    "num_comments": 6,
    "comments": [
        "Gonna read it for sure, I started a new job into forecasting and I really need this kind of content.",
        "I'll buy it with a discount.  But I'm so backed up with Bayesian time series material to learn ATM.  IDK when I'll get to it.",
        "Thank you",
        "Sure",
        "Sure. I was looking into time series forecasting for a project.",
        "pfft just read James Hamilton or Box & Jenkins \nhttps://github.com/MatthewK84/Time-Series-Textbooks"
    ]
},
{
    "submission_id": "1gpjqet",
    "title": " How to Approach Check-Worthiness Classification for Textual Claims?",
    "selftext": "Hello, r/learnmachineLearning community!\n\nI’m currently working on a text classification problem related to **check-worthiness** estimation. The goal is to determine whether a statement or claim is **worthy of fact-checking** or if it’s simply a **non-check-worthy** statement. This task is particularly important in the context of **misinformation detection**, **fake news analysis**, and general **text validation** in public discourse.\n\ndata : [https://gitlab.com/checkthat\\_lab/clef2024-checkthat-lab/-/tree/main/task1/data](https://gitlab.com/checkthat_lab/clef2024-checkthat-lab/-/tree/main/task1/data)\n\nhttps://preview.redd.it/and70x1ltg0e1.png?width=594&format=png&auto=webp&s=2ce342ccfd72d34571dbb01993773a18f4c54094\n\n# Problem Overview:\n\nGiven a collection of statements, the task is to classify them as either:\n\n* **Check-Worthy** (i.e., a claim that should be fact-checked), or\n* **Not Check-Worthy** (i.e., a statement that doesn’t require verification).\n\n# Background:\n\nI’ve been exploring different ideas for solving this problem, but I’m still unsure about the best approach. Some of the challenges I’ve encountered include:\n\n1. **Feature Extraction**: What are the most relevant features to capture in text? Do you rely on traditional NLP features, or is it better to use pre-trained models like BERT for embeddings?\n2. **Labeling & Data Quality**: For this task, I have a dataset of claims paired with labels. However, the labeling process for check-worthiness can be subjective. How do you approach this issue, and do you recommend any methods for improving the quality of labeled data?\n3. **Model Selection**: I’ve considered a variety of models, from classical machine learning (like Random Forest, SVM) to modern deep learning models (like BERT, RoBERTa). Do you think one type of model is better suited for this kind of task? Is it better to fine-tune a pre-trained transformer model for this task?\n\n# Questions:\n\n1. **Feature Engineering**: In your experience, what types of features work best for this type of classification problem? Should I focus on specific linguistic features, sentiment scores, or perhaps contextual embeddings from pre-trained models?\n2. **Model Choice**: Given the text classification nature of this problem, what machine learning or deep learning models do you think would be best suited to classify check-worthiness? Any suggestions on how to handle data imbalance or improve model performance?\n3. **Evaluation Metrics**: What metrics would you suggest for evaluating the effectiveness of a check-worthiness classifier? Accuracy, F1-score, or maybe something specific like **Precision at k** for ranked predictions?\n4. **Challenges and Pitfalls**: What are some common pitfalls in working on this type of task? Any advice on how to handle ambiguous statements, or scenarios where human judgment might differ?\n\n# Additional Context:\n\n* I have a background in **Data Science**, and I’m using tools like **Python**, **scikit-learn**, and **Hugging Face Transformers** in my approach.\n* I’m also considering using **text-based features** like **TF-IDF**, **word embeddings**, and **sentiment analysis** as part of my feature extraction pipeline.\n\n# Conclusion:\n\nIf anyone has worked on similar problems (like fake news detection or fact-checking classification), I’d love to hear your suggestions or approaches! I’d greatly appreciate your thoughts on how to effectively tackle this check-worthiness classification task.\n\nThank you in advance for your time and help!",
    "created_utc": "2024-11-12T04:36:45",
    "num_comments": 0,
    "comments": []
},
{
    "submission_id": "1gpjlsb",
    "title": "Build Ecommerce Applications with Marqo's New Ecommerce Models That Outperform Amazon by up to 88%!",
    "selftext": "We are thrilled to release two new foundation models for multimodal product embeddings, [Marqo-Ecommerce-B](https://huggingface.co/Marqo/marqo-ecommerce-embeddings-B) and [Marqo-Ecommerce-L](https://huggingface.co/Marqo/marqo-ecommerce-embeddings-L)!\n\n* Up to 88% improvement on the best private model, Amazon-Titan-Multimodal\n* Up to 31% improvement on the best open source model, ViT-SO400M-14-SigLIP\n* Up to 231% improvement over other benchmarked models (see blog below)\n* Detailed performance comparisons across three major tasks: Text2Image, Category2Image, and AmazonProducts-Text2Image\n* Released 4 evaluation datasets: GoogleShopping-1m, AmazonProducts-3m, GoogleShopping-100k, and AmazonProducts-100k\n* Released evaluation code with our training framework: Generalized Contrastive Learning (GCL)\n* Available on Hugging Face and to test out on Hugging Face Spaces\n\n  \nThese models are open source so they can be used directly from [Hugging Face](https://huggingface.co/collections/Marqo/marqo-ecommerce-embeddings-66f611b9bb9d035a8d164fbb) or integrated with [Marqo Cloud](https://www.marqo.ai/cloud?utm_source=reddit&utm_medium=organic&utm_campaign=marqo-ai&utm_term=2024-11-12-12-00-utc) to build search and recommendation applications!\n\nTo load with Hugging Face transformers:\n\n    from transformers import AutoModel, AutoProcessor\n    \n    model_name= 'Marqo/marqo-ecommerce-embeddings-L'\n    # model_name = 'Marqo/marqo-ecommerce-embeddings-B'\n    \n    model = AutoModel.from_pretrained(model_name, trust_remote_code=True)\n    processor = AutoProcessor.from_pretrained(model_name, trust_remote_code=True)\n\nBlog (with benchmarks): [https://www.marqo.ai/blog/introducing-marqos-ecommerce-embedding-models?utm\\_source=reddit&utm\\_medium=organic&utm\\_campaign=marqo-ai&utm\\_term=2024-11-12-12-00-utc](https://www.marqo.ai/blog/introducing-marqos-ecommerce-embedding-models?utm_source=reddit&utm_medium=organic&utm_campaign=marqo-ai&utm_term=2024-11-12-12-00-utc)\n\nHugging Face Collection (models, datasets and spaces): [https://huggingface.co/collections/Marqo/marqo-ecommerce-embeddings-66f611b9bb9d035a8d164fbb](https://huggingface.co/collections/Marqo/marqo-ecommerce-embeddings-66f611b9bb9d035a8d164fbb)\n\nGitHub: [https://github.com/marqo-ai/marqo-ecommerce-embeddings](https://github.com/marqo-ai/marqo-ecommerce-embeddings)",
    "created_utc": "2024-11-12T04:29:47",
    "num_comments": 4,
    "comments": [
        "Cool, also share it on twitter to gather reach and potential  validation",
        "It is really 88%?",
        "Yep! We benchmarked these models!\n\nYou can find the benchmark results for these models in our latest blog: [https://www.marqo.ai/blog/introducing-marqos-ecommerce-embedding-models?utm\\_source=reddit&utm\\_medium=organic&utm\\_campaign=marqo-ai&utm\\_term=2024-11-12-22-00-utc](https://www.marqo.ai/blog/introducing-marqos-ecommerce-embedding-models?utm_source=reddit&utm_medium=organic&utm_campaign=marqo-ai&utm_term=2024-11-12-22-00-utc)\n\nAnd you can even replicate the benchmarking for yourself: [https://github.com/marqo-ai/marqo-ecommerce-embeddings#:\\~:text=12%2C%205.2173e%2D12%5D-,Evaluation,-Generalised%20Contrastiove%20Learning](https://github.com/marqo-ai/marqo-ecommerce-embeddings#:~:text=12%2C%205.2173e%2D12%5D-,Evaluation,-Generalised%20Contrastiove%20Learning)",
        "Oh, cool, thanks."
    ]
},
{
    "submission_id": "1gpj7ni",
    "title": "So I Have A Data Product... Now What?",
    "selftext": "",
    "created_utc": "2024-11-12T04:07:19",
    "num_comments": 0,
    "comments": []
},
{
    "submission_id": "1gpj3bi",
    "title": "Product between two neural networks",
    "selftext": "I am starting to study a bit of ML and as HomeWork I have to respond to these questions: Is the sum of two neural networks still a neural network? Is the product of two neural networks still a neural network? Is the composition of two neural networks still a neural network?\n\nI think that the key part of the solution is to recognize how linear/non-linear components of neural networks interact in the three cases but I am not sure.\n\nCan someone explain to me which are the right answers and why?\n\nThank you very much",
    "created_utc": "2024-11-12T04:00:16",
    "num_comments": 7,
    "comments": [
        "Composition can be thought of as connecting a second neural net to the output of the first neural net, therefore as a whole this is also a neural network.\n\nSum is trivial. In neural nets, neuron values are sums of the neuron values from the previous layers. So if these are sums, you can separate the neurons, because addition is associative.\n\nFor the product operation I don’t have any cool facts, but if you multiply the outputs of a neural net, this is still a differentiable function, because a derivative of a product is trivial.",
        "No because that’s literally your homework for you to do.",
        "A neural network is just a complicated non-linear function. Is the sum of two functions still a function? And so on …",
        "Thank you! Anyway for the product I have found a result by professor Josef Teichmann in which is said that In general NO unless you choose the nonlinear function for this purpose",
        "So the sum and the composition are still neural networks but the product is not?",
        "Could you post a link to that research, please? I’d like to learn more about his results.",
        "I'm sorry, unfortunately I have found only a solution sheet (https://metaphor.ethz.ch/x/2022/fs/401-3932-19L/ex/sol1.pdf) which responses at the question without any detailed demonstration. Anyway, you can certainly find some interesting stuff directly on Teichmann website: https://people.math.ethz.ch/\\~jteichma/"
    ]
},
{
    "submission_id": "1gphyjv",
    "title": "Simple Intuition Behind Cross-Entropy Loss",
    "selftext": "https://preview.redd.it/tbg0c04r9g0e1.png?width=1280&format=png&auto=webp&s=c0ee823e215e023543d15c3769c9cc4ace3b2c9d\n\n\n\nHow do you measure the performance of a classification model? Cross-Entropy Loss is the go-to metric.\n\n\n\nImagine showing a stick figure to 3 people, asking them to classify it as a dog, cat, tiger, or lion. If it’s actually a cat, their guesses might look like this:\n\n\n\nPerson 1: 20% dog, 29% cat, 31% tiger, 20% lion (uncertain and wrong).\n\nPerson 2: 97% dog, very low for others (confident and wrong).\n\nPerson 3: 97% cat, very low for others (confident and correct).\n\nCross-Entropy Loss penalizes errors logically: rewarding confidence when correct but imposing heavier penalties for confident mistakes. It focuses on the true class and scales penalties with logarithms.\n\n\n\nIntuition:\n\nModel A (4-class task, loss = 1.38): Random guessing yields a loss of -ln(0.25) = 1.386. With a loss of 1.38, this model has learned nothing meaningful.\n\nModel B (1000-class task, loss = 1.38): Random guessing gives -ln(0.001) = 6.907. A loss of 1.38 is significantly better, meaning the model has learned patterns and predicts closer to true labels.\n\nModel C (MNIST, accuracy = 0.1): Random guessing results in -ln(0.1) = 2.30. If the loss is near 2.30, the model is guessing; a lower loss indicates some learning, while a higher loss shows confident mistakes.\n\nCross-Entropy Loss encapsulates how well a model aligns predictions with reality, guiding AI systems to make better decisions.\n\n\n\nHere is a lecture I published to give you a better intuition: [https://www.youtube.com/watch?v=wTTYHM\\_DMxw&feature=youtu.be](https://www.youtube.com/watch?v=wTTYHM_DMxw&feature=youtu.be)",
    "created_utc": "2024-11-12T02:44:45",
    "num_comments": 0,
    "comments": []
},
{
    "submission_id": "1gpgqze",
    "title": "Understanding Mixture of Experts (MoE): A Deep Dive into Modern LLM Architecture",
    "selftext": "[https://skillupexchange.com/understanding-mixture-of-experts-moe-a-deep-dive-into-modern-llm-architecture/](https://skillupexchange.com/understanding-mixture-of-experts-moe-a-deep-dive-into-modern-llm-architecture/)  \nis this a good blog post to understand MoE(mixture of experts) model ???\n\nIf not then what blog or articles or book should i read\n\n[View Poll](https://www.reddit.com/poll/1gpgqze)",
    "created_utc": "2024-11-12T01:13:16",
    "num_comments": 0,
    "comments": []
},
{
    "submission_id": "1gpg55c",
    "title": "Hello. Im a grade 11 student studying IB. I want to start learning machine learning as I am interested in it and think it would help me in the future as I want to go into finance. Can someone please share a roadmap and recommend courses to learn machine learning.",
    "selftext": "",
    "created_utc": "2024-11-12T00:25:24",
    "num_comments": 1,
    "comments": [
        "roadmap.sh"
    ]
},
{
    "submission_id": "1gpg2y8",
    "title": "What we learned building RAG systems for 100+ technical teams like Docker and CircleCI",
    "selftext": "Hey r/learnmachinelearning! I'm one of the founders of [kapa.ai](http://kapa.ai) (YC S23). We've helped teams at Docker, CircleCI, and Reddit implement RAG systems in production, and I wanted to share some key technical lessons we've learned along the way. \n\nThe biggest technical challenges we consistently see: \n\n1. Data curation matters more than volume - companies often try to dump their entire knowledge base into RAG \n2. Refresh pipelines need to handle incremental updates \n3. Evaluation frameworks catch different issues in production vs POC \n4. Security considerations are often overlooked until too late \n\nI've written up a detailed [technical breakdown here](https://www.kapa.ai/blog/rag-best-practices) covering implementation patterns that actually work. \n\nHappy to discuss specific RAG challenges you're facing. What issues have you encountered moving RAG systems to production?",
    "created_utc": "2024-11-12T00:20:46",
    "num_comments": 7,
    "comments": [
        "This is helpful!",
        "We have documentation of company process PDFs that contains text and pdf\nWe want to rag based qna chatbot \n\nCan you help me with that\n\nI am facing a challenge in making a chatbot to  understand the screenshots and text as those are sequential steps",
        "Helpful indeed. Wishing you continued success.",
        "I bought it",
        "hi is this good for non english language dataset. i want to build a RAG base not sure where to start i am not ML engineer.",
        "Sure! Ping me on [emil@kapa.ai](mailto:emil@kapa.ai) and I'd be happy to help out. :)",
        "Can i get in on it too?"
    ]
},
{
    "submission_id": "1gpfv17",
    "title": "DataCamp Black Friday 50% OFF Everything",
    "selftext": "",
    "created_utc": "2024-11-12T00:04:11",
    "num_comments": 1,
    "comments": [
        "Amazing"
    ]
},
{
    "submission_id": "1gpfryv",
    "title": "[Blog] Metrics for Table Extraction",
    "selftext": "Table extraction is challenging, and evaluating it is even harder. We went through various metrics that give a sense of how good/bad is a model when we are extracting data from tables and here are our insights -\n\n* Basic Metrics: They are easy to code and explain, but usually you need more than 1 to give a sense of what is going on. Example row-integrity can tell if the model missed/added any rows, but there's no indication of how good are the contents in the rows. There is no exhaustive list of simple metrics, so we have provided around 6 such metrics.\n* However, tables are inherently complex, and embracing this complexity is essential.\n* TEDS views tables as HTML, measuring similarity via tree edit distance. While well-designed, it feels like a workaround rather than a direct solution.\n* GriTS tackles the problem head-on by treating tables as 2D information arrays and using a variation of the largest common substructure problem to calculate cell-level precision and recall.\n\nOverall, it's recommended to use GriTS for table extraction as it is the current state-of-the-art metrics.\n\nI've explained GriTS and TEDS in more detail, with diagrams here -\n\n[https://nanonets.com/blog/the-ultimate-guide-to-assessing-table-extraction/](https://nanonets.com/blog/the-ultimate-guide-to-assessing-table-extraction/)",
    "created_utc": "2024-11-11T23:58:00",
    "num_comments": 0,
    "comments": []
},
{
    "submission_id": "1gpcayi",
    "title": "Help understand where the neck network of Segment Anything coming from",
    "selftext": "Hi,\n\nI am trying to read the encoder part of the [segment anything](https://arxiv.org/abs/2304.02643) (SAM from FAIR). I found the encoder has a neck network, which I didn't find from the paper or the citation in VitDet paper. I wonder if someone could point to where that portion is coming from, or why it is needed?\n\nCode link: [https://github.com/facebookresearch/segment-anything/blob/dca509fe793f601edb92606367a655c15ac00fdf/segment\\_anything/modeling/image\\_encoder.py#L88](https://github.com/facebookresearch/segment-anything/blob/dca509fe793f601edb92606367a655c15ac00fdf/segment_anything/modeling/image_encoder.py#L88)",
    "created_utc": "2024-11-11T20:11:42",
    "num_comments": 2,
    "comments": [
        "Found [9 relevant code implementations](https://www.catalyzex.com/paper/arxiv:2304.02643/code) for \"Segment Anything\".\n\nIf you have code to share with the community, please add it [here](https://www.catalyzex.com/add_code?paper_url=https://arxiv.org/abs/2304.02643&title=Segment+Anything) 😊🙏\n\nCreate an alert for new code releases here [here](https://www.catalyzex.com/alerts/code/create?paper_url=https://arxiv.org/abs/2304.02643&paper_title=Segment Anything&paper_arxiv_id=2304.02643)\n\n--\n\nTo opt out from receiving code links, DM me.",
        "To the code bot: this is not the question i am asking for."
    ]
},
{
    "submission_id": "1gpapbk",
    "title": "More than 500 job applications but coudnt get a call. what's wrong with my resume? would really appreciate some critical feedback",
    "selftext": "",
    "created_utc": "2024-11-11T18:45:47",
    "num_comments": 222,
    "comments": [
        "If you have to read 100 of this... Do you think it would be easy or hard?\n\nShorts answer, hard, make it easier to read.\n\nLess is more\nOptimize it",
        "I know people are poopooing your resume, but I actually think it's pretty good as an ML hiring manager. Its a little dense, my eyes cross a bit trying to read it. Streamline the most important parts. Make the intro more about you and your career goals, it doesn't need to be stuffed with buzzwords. Put those in the list of skills, we'll see them there. In the experience section, focus on the value add. If you need to cut to expand on experience, cut your projects. They don't have even half the impact as real job experience. \n\nThe biggest thing you're going to face is bias against your foreign status. This costs companies money and there is also (real talk) straight up racism and cultural barriers to deal with in this process. Maybe an unethical pro life tip, but maybe downplay this on your resume, or remove altogether. You legally can't be discriminated against in the hiring process, but you can be rejected for literally any made up reason just because they don't want to entertain the option. So if you can just get an HR person to call you, you're already halfway there. Think about what you can do to not get auto rejected by these screeners. Just dont lie",
        "Most of the comments here are geared around the content/quality of your CV, but I kinda disagree and think you shouldn't get hung up on that too much. There's nothing wrong with your profile in my view and most recruiters can see that you have a good set of skills that would be useful in the majority of organizations. A couple of years back you'd have a lot of companies lining up to hire someone like you.\n\nUnfortunately right now the market isn't really amenable to newly minted grads or international students. I get a lot of people pinging me on LinkedIn in your same exact position.\n\nMy suggestion would be find a plan B to juggle with your plan A of getting a job... it might look like pulling a team together and giving YC a shot. Or exploring PhD opportunities. Anything. Just don't remain stationary for too long and leave gaps in your resume. Keep it moving.",
        "It's a solid resume, but your bullet points are way too long, making it hard to read. Try to highlight achievements and concrete metrics. Don't detail the actual work; that's for the interview.",
        "Being international makes it hard because it’s expensive for companies to hire an applicant and sponsor them when another applicant is that much cheaper. I see you chose Northeastern, a lot of international people tend to choose US institutions but it doesn’t change the above fact. So you’ll have a struggle, sorry to say.\n\nNext, your resume is too wordy. Trim each of your projects down to 2 lines max. The experience can use some cleaning up too.\n\nLastly, on the real, job market is harsh for ML right now. Not because no one is hiring, but because a lot of companies are not gaining revenue and they keep doing freezes. You’ll get there, keep at it!",
        "What non-LLM and non-Generative AI experience do you have?",
        "Honestly, your resume is great. I would instead focus on self marketing. Creating blog posts, LinkedIn spam, cool visualizations for your projects. Even if you are very technical, recruiters still want something shiny to look at.",
        "For an ml guy, you didn't bother to use ML. Anyways, pretty impressive so far. But the 2024 market is shit and especially for foreign grads. Especially in 2025 when he gets into the White House. Here's a cleaned up LLM version. \n \n\n---\n\nProfile\nMachine Learning Engineer with 3+ years of experience delivering Generative AI, MLOps, and end-to-end ML solutions on cloud platforms. Skilled in building and deploying RAG systems and scalable ML pipelines. Known for balancing technical expertise with practical implementation, delivering high-accuracy models and optimized workflows in Python and PyTorch.\n\nCore Skills:\nGenerative AI, MLOps, RAG Systems, CI/CD, A/B Testing, Test-Driven Development, AI Safety\n\nTechnical Stack:\n\nLanguages: Python, SQL\n\nML Tools: PyTorch, LangChain, scikit-learn, LlamaIndex\n\nBig Data: PySpark, Databricks\n\nCloud & MLOps: AWS SageMaker, MLflow, Azure DevOps, Docker, Git, Jira\n\nDatabases: PostgreSQL, MySQL, Neo4j (Graph Databases)\n\n\n\n---\n\nExperience\n\nMachine Learning Engineer Intern\nLowell, MA\nSep 2023 – Jan 2024\n\nLed the development of a Computer Vision model using YOLOv8 and PyTorch, achieving 91% accuracy and reducing inference time by 30% through optimized pre-processing and augmentation techniques.\n\nIntegrated LOFTR matcher in PyTorch to improve damage tracking, leveraging Homography for enhanced viewpoint consistency and accuracy.\n\nImproved object detection by 40% on a custom dataset of 5,000+ images through fine-tuning and data augmentation.\n\nSet up an automated monitoring system using Weights & Biases and MLflow, reducing model error by 15% and increasing reliability.\n\n\nData Scientist\nJune 2019 – May 2022\n\nBuilt and deployed recommendation models using XGBoost and collaborative filtering for 15 partner colleges, boosting prediction accuracy by 25% and serving over 500+ students per semester.\n\nDeveloped a SQL-based analytics pipeline to track engagement for 12,000+ users, enabling real-time insights and data-driven improvements in recommendation systems.\n\nScaled infrastructure using PySpark and AWS, increasing daily pipeline efficiency by 40% to handle larger data volumes.\n\n\n\n---\n\nEducation\n\nNortheastern University\nBoston, MA\nMaster’s in Applied Machine Intelligence, Minor in Finance | GPA: 3.62 (Expected May 2024)\n\nBachelor of Technology in Electrical and Electronics Engineering\nAug 2015 – May 2019\n\n\n---\n\nProjects\n\nAuctionMark - Text to SQL Converter\n\nDeveloped a Text-to-SQL Converter using SQLCoder-7B and LlamaIndex, achieving a 73% accuracy rate in converting natural language to SQL through prompt engineering and LangChain.\n\nBuilt a responsive frontend in React with a Supabase backend, handling 2,000+ matches with automated quality monitoring and validation.\n\n\nMovie Recommendation System Using Knowledge Graphs and LLMs\n\nBuilt a recommendation engine using Neo4j and LangChain, utilizing knowledge graphs and LLMs to generate personalized movie recommendations.\n\nDeployed a LLaMA-based interface for interactive movie discovery, providing intuitive, natural language search capabilities.",
        "I think it’s a little weird that there is a switch from data scientist to intern but u are a student so it’s forgivable, I think mainly just lack of experience, for most ml jobs at the moment this is crucial. Also I personally think it’s a little heavy on GenAI for an ML masters student I would expect more traditional ML modeling in your projects which imo is a lot more difficult to produce a compelling portfolio piece than with GenAI where pretty much any CS grad can throw together a project that sounds cool.",
        "Honestly, this is a boring sheet of ‘words’ with a lot of filler. If I was in HR, this would go in the trash and I'd reach for the next resume that was more engaging, easier to understand, and is less ambiguous.",
        "If you ask me its just too overloaded. Noone cares and most people reading this wont understand what your numbers mean and honestly thats normal given you talk about things but dont specify anything. Improving some model on a \"custom dataset\"? Might be a super impressive thing or really not impressive at all and something anyone could have done.   \nLet me ask you this. If you read your \"Machine Learning Engineer Intern\" part for example. What would your takeaway be as a guy who wants to hire?  \nBecause I read it and I dont have any.  \nTone it down, cut at least 60% of the words. Focus on the important things. Dont repeat yourself.",
        "Is there a reason you are hiding one location and not the other? Being h1b visa holder can be tough for job markets. But I'm guessing that's not the case here.",
        "i’m so fucked",
        "Nobody is getting call backs these days",
        "You need a Github with project examples. Explain what problem you were trying solve, show the code and what was thw good and bad",
        "As has been said, being an international student is part of the problem (sad reality, but it’s a reality). \n\nThe other issues are as follows (this is coming from someone who just had to hire someone for an ML role):\n \n1.) it reads as having done a lot of tutorials and common projects, with little actual model development\n\n2.) it’s so heavy on pre-trained models and LLMs, it feels like it’s not relevant for real business use cases \n\n3.) it’s too dense; this could be reduced significantly and would be much more readable. Also, nobody cares about the bolded numbers - just describe how it would help in a business use case. It’s always business use cases. \n\n4.) it needs more variety than just ML stuff; highlighting proficiency in general HPC, Linux, data management, data engineering, and feature engineering will get you a lot further as it shows you are well rounded and can go from “crappy data” to “nice model”\n\n5.) projects completely outside ML show you have transferable skills, which this resume doesn’t highlight very well. \n\n6.) maybe add in some math skills, such as statistics, linear algebra. \n\n\nBest of luck!",
        "I've been a hiring manager for DS and ML people, including managers. Here is my take on your issues in order of priority, aside from the visa status, which has been covered in the thread. \n\nYou seem like someone who could be brought in for an interview, once I read more deeply. But at first glance, I would likely reject:\n\n1) Seemingly lacks experience on scaled up deployments: I look for impact on a resume. What I see are a few hundred to a few thousand people using your work deliverables. At least that's what you highlight. Why not emphasize the project that saw millions of daily hits? That is the most impressive bullet point-- lead with that. Put that in your summary.\n\n2) Perception of lack of experience: This is a tough market for entry level people. Despite saying you have 3+ years experience, having your most recent experience be an intern makes you seem too green on first glance. Seeing the transition from full employee to 'intern' is jarring. Maybe leave off the 'intern' in the role title, and explain it in the bullets.\n\n3) One way to fill in the gap is to start doing freelance consulting gigs with real clients. Or to turn the AuctionMarkt into a real business buy growing a paid customer base. Even if the money is low, these actions could set you apart from other candidates.",
        "are you international?",
        "from a design standpoint it's sorta hard to parse the information. \n\nexample: all the same size font, taking up the same amount of space, no varition, little seperation. Having the dates far to the right requires almost a full head swerve to read. Some information barely matters, like GPA, projects could be simplified a bit, I'm not even sure if the profile section matters either (some would debate this). \n\nThings need more space and room to breath. Relevant information should be grouped more together, categories of information should be distinct and easily noticed. \n\nThis is kinda like when a store shelf is overloaded so much you can't find what you're looking for.",
        "The reason is your resume must talk about solving business problems and improving business outcomes. \n\nIf you don't have that, nobody will care and give you a second look.\n\nOn top of this, your resume is not readable/understandable by the average corporate recruiter.  It means nothing to them.  Make it easy for them to read, find what they want and take that summary to the hiring manager.",
        "You lack experience. People with experience are in the market and are willing to accept a lower or the same salary. Unfortunately, this is the current situation.",
        "whats the business impact. you have alot of ML related words, but if you are an HR person, what'd you improve with that fancy algorithm?",
        "Your resume looks great! I don’t think the issue is with your qualifications. But let me ask—where are you applying for jobs?\n\nIf you’re mainly relying on LinkedIn, you should know that around 90% of the job listings there have recently been fake. A few months back, a developer seeking remote work shared their experience on Reddit (you can read it [here](https://www.reddit.com/r/RemoteJobseekers/comments/1fdpeg2/how_i_landed_multiple_remote_job_offers_my_remote/)). He applied for jobs on LinkedIn for five months without any success. Eventually, he took a different approach: he found hundreds of companies on Google Maps, sent his resume in bulk, and secured multiple offers. This could be an effective strategy for you, especially if you’re aiming for a remote position.\n\nIf your goal isn’t a remote job, you can easily adapt this method. Let’s say you’re looking for a role as a “bartender”—use Google Maps to search for “bars” or “pubs” in your area, compile contact information in a spreadsheet, and send your resume to them directly. This way, you’re actively reaching out to potential employers nearby, giving yourself a much higher chance of getting noticed. Keep going; with this targeted approach, your perfect opportunity is closer than you think!",
        "First thing I think of when I see this is there’s wayyy too much text",
        "It just looks like you know how to use a bunch of AI tools, which is great but maybe not what employers are looking for. There is no “story” on your resume, no theme no clear purpose; by that I mean your transitions, what’s the story behind doing the internship how did the data’ scientist position help in your next role etc",
        "The only thing I find weird is that you move from data scientist to ML intern",
        "Hey , same as others are saying . Just a bit of a visual overload .  \n\nI was a ops manager for cex for 8 years and now work in the home office hiring .\n\nIf you want you can send me your resume and I'll run it through my kickresume account and format it for you nicer .",
        "I scrolled the comments for a few minutes and gave your resume a read. I agree with the majority of the comments re streamlining and not to get to down on yourself.\n\nHave you connected with your alumni society and your schools job board? I get pushed hard from my alumnus to hire more from them.\n\nAlso, I’m not a fan of your experience section. The gap of a year without work, and listing a 2-month job with another year without work? Neither of these things are great, and it’s worse that your first line is “3+ years experience” \n\nI would retarget your descriptions to show that you  understand value of your work and add it; while also showing that you know how to work on a team  in a modern engineering environment. \n\nFirst job out of advanced programs are hard, I would really lean on your social and academic network. Best of luck!",
        "Hiding that you are Indian while asking for honest feedback is funny lmao",
        "No experience. Student.  The market doesn’t want fresh blood.",
        "I’d be more concerned that potential employers might miss out on your value simply because they can’t fully grasp the depth of detail in your resume.",
        "Tbh, try to keep the bullet points short, like just 1-2 lines.",
        "You should keep it simple and too the point\n\nThis resume is filled with data which puts a bad impression",
        "I don’t know if this will solve your problem — but here’s an idea: for your internship and work experience rewrite it to focus on the business impact and goals.  It just reads like a bunch of factoids or tasks that you’ve checked off on your way to a masters.  I know a lot of folks write resumes like this, but maybe it’s time for a change. \n\nFor example, you’ve blanked out the company names but I should still be able to tell what the company does based on your work.  That’s totally not the case here.  The point of doing that is to target the companies you apply to and show you have immediately applicable skills.",
        "Depends what ur applying for",
        "Can I ask you what tools you used to make it? look really clean I’d like to have mine like yours",
        "As someone who doesn’t know shit about the space it sounds like you know what’s up. Sent you a DM to see if you would be interested in a side project for my company",
        "Anybody that spends time reading the resume should assume he is a very good candidate",
        "I always find the people who \"put in 100s of apps, no replies\" are targeting a specific job, the answer is that line of work is In such low demand it's super competitive, a wage expectation is fine, but you would have greatly better odds if you weren't dead set on a specific ideal position",
        "write less and make it easier to see?",
        "I'm a data scientist and your resume looks fine to me. In fact I'd say it's quite similar to mine. Perhaps a little wordy which you can try to cut down a little and quantify biz impact if possible, like what others said.\n\nTake some of the negative comments saying they don't understand your resume with a pinch of salt. A quick check of their profile shows that they're not even in a related industry, so no wonder they don't understand. I'd think that HR and the hiring manager should have a basic understanding of the ML lingo/packages you're using.",
        "TBH it's not that bad at first glance.\n\nHR etc won't understand a word of it - but if it ever reaches the hiring manager it's acceptable.\n\nPersonally, I would like to see a very brief couple of lines initial summary saying (your version of) something like:  \n*\"Senior software engineer specialising in safety-critical real-time systems for the oil industry\".*  \nHiring managers have a problem to solve, generally quite soon.  \nThey also also spend only a couple pf seconds looking at a *resume* unless something leaps out at them.  \nIf your capsule description matches their problem - bingo!\n\nThat said, maybe others here would recommend a generalist approach to widen your scope?\n\n*Note: I suspect that, as others have said, your (deliberately hidden?) international status will not help at all.*",
        "Of those 500 applications, what proportion were preceded by some kind of direct one on one human connection.  \n\n Try taking out some hard details and replace with fluffier wording, then just have the keywords in a keyword section at the bottom? The hiring manager doesn’t care that you used PyTorch and FANCY_ACRYNOM to achieve 91% accuracy, for example….they care that you “Eliminated 9000 hours of manual data entry by using modern ML technologies while workin in a team environment.”\n\nHonestly though, it’s a good resume. I’d hire you as my coworker if we were hiring! ",
        "I like it a lot",
        "Are you on a visa?",
        "Your resume isn't to get you hired, it is to get you an interview. \n\nIf it has the essentials to qualify you for the job then what is left? Only reasons to not give you the interview. \n\nWhen I am scanning through 50 resumes I only need to find one reason on each one to chuck it off the pile. \n\nIf I want to find out more about some cool piece of tech you built then great. I'll have to interview you to find out. When I read it on your CV I'm likely to be disappointed and it won't sound as good. \n\nIdeally you would have half as many words on your one page, and they would be easier to read. I want to know (a) you are qualified for the role and (b) something that will make me want to believe that you could do the role. Anything else on there is a reason to not interview you and so it shouldn't be on there. \n\nMost of the applications will either not fit the role, or have an easy reason to not bother to interview them so getting past those two filters is key. After that you can work on your interview technique.",
        "Vai india jaa ke startup suru karle",
        "Your CV is overwhelming imho. I think I have very similar experience as you, but I've managed to `.squeeze()` it to much less rows.",
        "I’ve interviewed a lot of people & read hundreds of resumes. Your resume does not speak to whomever will be reading it & I doubt they’ll even finish it. Nobody is going to call your employer to verify any stats, & we/they don’t care to read them. Bullet point only your skill sets & leave out ALL that extra clutter. In the interview if asked about a bullet point be prepared to explain how that skill will apply to the position you seek.",
        "I've seen hundreds of CVs. I would bet money this isn't even being read unfortunately. \nSay the same things, in 1/3 of the current text.\nDoes everything need to be there? is it valuable to the company you're sending it too? \nGive more space/text to the best and most significant projects/experiences (but not more than 3), reduce at the min the rest using a single sentence.",
        "Jesus man, your cv its literally \"to much text dont read\",. You really need to change the format; less is more. I don’t see a LinkedIn or portfolio link. I hope you don’t code like this—look for a cleaner format.",
        "From my perspective, this CV lacks consistency: in the top section you use fancy gen ai buzzwords, and the experience is about previous generations of ML tools like YOLO and xgboost. This experience is valuable but barely matches the headline.",
        "It's too dense which makes it hard to read quickly.  I would stretch it out to 2 or 3 pages, add some whitespace and bullet points.",
        "I'd flag two things as a manager:\n\n\\- Very limited experience overall (which is fine if you are going for junior enough roles)\n\n\\- A sizable portion of your \"Core Competencies\" doesn't seem to be vetted via actual work",
        "I think it's good, but I personally won't highlight the 3.62 GPA",
        "This is wild to me. This is like me just ~2 years ago and I haven’t even had to apply to get jobs, recruiters are banging on my doors \n\nAnyway no advice, great skill set. I’d hire you.",
        "My eyes go right to the center of the page where there are more bolded words, then I can't really focus on anything in particular. I have to force myself to start reading, and its so dense. I'm sure it'll hit all the points on one of those robo screeners.\n\n  \nI view resumes like like any other form of written content that is trying to capture your attention. Social media has spent billions to understand that the layout, and the amount of attention you capture in the first few seconds is what matters. Not the quality of the content. That idea has been proven again and again and again. Unqualified people get hired over qualified only because they communicate better. \n\n  \nDo you want to read a newspaper or watch a couple of videos? What do you think the HR person wants to do? The first person that looks at your resume is most likely not the person who understands the skills needed for the job. Meaning they aren't engineers and don't operate like them.",
        "HR has no ideal what you are talking about.l and the ML/AI guys know you are just stating that you know the basics with really fluffed language. Focus more on the impacts and outcomes of your contribution ",
        "HR has no ideal what you are talking about and the ML/AI guys know you are just stating that you know the basics with really fluffed language. Focus more on the impacts and outcomes of your contribution ",
        "Make it easier to read.",
        "\n\nMaybe you are just a too good engineer so good that skipped the core basics of working with data.\nI see a lot of mentions to tools ( even javascript )\nBut no single mention to pandas, Polars , numpy or any data manipulation tool (the closest you get is spark but you mention that is for some pipeline)? You even do xgboost but no mention to handle data. \nThen you claim to do MLOpsish practices  in all your experience even using MLflow since the beginning \nThen after doing data science + MLE+ MLOps and then you decided to be an intern while simultaneously studying.\nSo in my experience is one of these two, or you don't know what are you talking about or you, or you are claiming experience you really don't have. I might be wrong and misjudged but I would just discard a candidate based on my hunch and lack of consistency.",
        "I hire for this role and I like your resume.",
        "Don’t list accuracy metrics in your descriptions",
        "I have looked at hundreds, possibly thousands of resumes personally over the years. I have hired or not hired many people. Rarely because of the resume, but in this case, it is overly complicated. No one is going to look at all that. They will just assume you are overly complicated and perhaps disorganized.  \nSince you are good with AI, I would show it to Gemini Advanced, and have it fix it. It will take a few seconds.\n\nI did exactly that with the image of your resume, and it did a very nice job. I would have posted it here, but Reddit gives me no way to do that.",
        "500 is rookie numbers. I applied to Amazon alone 60 times, when I was looking for a job.",
        "I would hire you, not that it matters but, are you working on anything?",
        "+1 too busy overall but nothing specifically wrong. \n\nConsider: (Resume)\n-  using GenAI to help accelerate tailoring your resume to specific jobs\n- reduce use of bold\n- consolidate project detail\n- move profile to the end\n- add a quick overview of your elevator pitch \n\nAlso you can try to \n- network (attend free conference, do hackathons for good, join online groups.) - getting a referral is the most straightforward way\n-  look at the JD then identify-your proficiency with skills and ensure you address any gaps with transferable skills\n\nI have applied to this volume of jobs before but most jobs came from the latter approach. \n\nGood luck.",
        "The fact that you are not a US citizen lol",
        "You need honest answer? Because it looks like, a lot of information is due to current trends, try to only put things which you did and not everything. Honesty is your friend!",
        "Slim it down. The content is good, but when reviewing CVs, we typically skim read. This means readers only take away a few points. If you choose your words carefully, you get to decide what points get taken away.",
        "\"text to SQL converter\" sounds lame, I almost skipped over it until I read part of it and saw it's a good project.\n\n  \nMaybe find another name like \"Natural Language SQL Querier\" or \"Query with Natrual Language\" or something like that.\n\n  \nIt's better than my resume though by quite a bit.",
        "You have very little real experience and the market is full of people who are looking for work.  When we post a listing we get flooded with resumes now, even for much higher level AI/ML roles.  Too much hype right now.",
        "One thing jumps out to me is that you have an EE degree but no stated experience in C, C++ or Assembly. That would have me questioning your undergraduate degree. \n\nAlso, you have a masters degree, where are your papers?",
        "OP, I think your resume is great for a general ML engineer. The issue in today's job market is that these type of positions are primarily given to PhD holders, given there are some many PhDs with ML algorithmic skills and publications. The way you can stand out is focusing on a niche domain. Think about about what niche you can focus on and market your resume accordingly. Also, I do not think it's the foreign visa thing which is the issue as much as the other factors that I have mentioned.\n\nTLDR: I personally will not spend time formatting the resume but will focus on highlighting a niche that will make the resume standout.",
        "A talent person is always the first one to read your resume. There are a bunch of comments in here that are spot on around market conditions and visa status, but you don't control those. Two things stand out to me that you can change that might hold you back right now -\n\n  \n1. Many companies no longer consider internships in calculating your years of experience. Seeing that you factor it in to your calculation may read as mildly dishonest and rule you out.\n\n  \n2. The bullets you have written here are comprehensible to an engineer but not a recruiter, consider your audience a bit more",
        "Just one question. Do you have US citizenship? Green Card or long term work visa? Do you currently reside in the US?",
        "Instead of applying to 500 generic jobs with one generic resume, spend one hundred times as long on each of 5 jobs. Find 5 jobs for which you’d be a perfect fit. Really research each company, hiring manager, etc.  Tailor your resume specifically to highlight what a good fit you’d be for each job (not just a cover letter, a modified resume for each company). When you spam apply to 600 jobs, realize that 500 other yous are doing the same thing for each of those jobs. You will not stand out.  Instead, break the mold and do the extra work to actually match yourself with the right job.",
        "you probably havent heard anything back because your name and contact info are blocked out.\n\n/s aside,\n\nless project bullets and more work experience bullets. save the project work in your back pocket to bring up in interviews. \n\nformatting wise, separate the skills section out vs cramming it into the profile. give it some breathing room so its easier to identify.\n\ni feel like there are so many things bolded that my eyes don't know where to start or focus on.",
        "I mean this with all sincerity — you’ve blacked out your foreign undergrad university which indicates to me you may require a visa or sponsorship?  If that is true, then there’s your answer.",
        "Why are you giving hiring managers a redacted resume?",
        "my eeeeyes!",
        "I Know people with half of these skills and they had find nice jobs very quickly",
        "I hire data scientists and data engineers a lot for my org. I would usually get a lot like this. What I’m looking for is yes, technical skill, but more importantly, do you know how to apply your ML knowledge to real world problems? So as great of is that you learned mlflow, JIRA, and build a RAG chatbot and movie recommendation, about 100 other applications will have it. Focus on real-world problems. Did you deploy any ML projects to production? What has been the ROI from your models on a product or process? How has your MlOps work help a model adapt to new circumstances over time? Again I would lean more business in the experience part. Whoever hires you wants to make sure you can use ML using their data and their environment and have a defined ROI because of the work.",
        "My company was recently hiring for a date engineer position where half of the 600+ applications looked very similar to this format. Foreign college, US Masters, 1 yr work experience in the States 3 years with experience abroad. The formatting of the resumes were also very similar. In the first batch of interviews we called a number of those applicants but they generally performed poorly on the initial round of interviews/tests. I think that led to us down weighing similar resumes in future batches.\n\nMay I also ask where you were taught to format your resume this way?",
        "I agree with most others that your resume is looking great. One advice I give to my friends who is in the job hunt with visa requirements is finding the right companies that would sponsor visa and apply. Some suggestions in the Reddit community is using H1Bdata.info site and filter companies who has sponsored visas in your application domain. This targeted approach might help to get your resume in the right hands. All the best!",
        "Just a visual tip, I'd try to shorten bullet points that are a word or two onto the next line to be more concise. I would also keep all text of a similar type (headers, bullet points, etc) consistent. The different size fonts bothers my eyes. \n\nWishing you the best of luck! I've found reaching out on LinkedIn with a catchy message to be really productive. Time it the recruiters schedule when they're not busy and have free time. (Tuesday -> Thursday) and either early morning to be fresh in the queue or EOD to be fresh in the queue for the last look. \n\nCheers",
        "Bro I don't know about how to Improve it but that's and Impressive resume man, loose up and prioritize network referrals over blind form filling.",
        "I think nothing wrong with you, it is just a market. I was applied to big tech with referrals and was rejected multiple times even on internship positions.",
        "Northwestern is a very prestigious school with a lot of connections. Did you build a network with classmates that may have leads for you? Can you access the alumni network to look for job leads? I highly recommend leaning on your network in this market. Referrals will go a long way in the current market. If you don’t use your network, then you’re playing a numbers game and competing with > 20 people (on the low end) for all the jobs you’re applying to.",
        "must be H1B US market is terrible right now, may need to work on some certifications, what is applied machine Intelligence",
        "Maybe, you forgot to put your phone number.",
        "Customize your resume for each job you apply to. Go for quality over quantity. Position yourself as the perfect candidate for each role.",
        "Number of applications actually is not enough - if ur international ive seen people need thousands before. It took one of my friends 2500+ apps for 1 position. Just keep applying... wish you luck",
        "Most job postings are fake, made by companies so they can appear to be growing so they can keep up the con that keeps investors investing or to keep a leash on current employees to prevent them from demanding better pay.\n\nOf the real job posts, more and more are auto filtered by LLMs, which means if you don't format your Resume/CV 'just right' and have SEO spam levels of keywords, chances are your resume just gets binned without a human ever seeing it. \n\nFind a recruiter and be ready to interview with companies you have no interest in working with, or go old school, pick up the phone and start calling up companies you want to work with directly.",
        "May 22 - Sep 23 gap may be a put off, a cover letter could be used to explain the gap in employment",
        "Honestly bro, this is just a shitty time to find a job",
        "Some sparse of the resume will help a lot, I was in your shoe. Get ride of the Profile section(way too much ward, just keep what ever the skills the job looking for). For the experience part, simple it. you are showing to someone who coming from a non-stem background.",
        "One as a person that works, you have 30 seconds to make an impression. \n\n1. Add a nice picture.\n\n2. Better paper with borders.\n\n3. If online then copy and paste words in description of the job.\n\n4. If you get an interview bring donuts as a surprise and dress nice.",
        "I’m so cooked bruh",
        "Damn, I am so cooked. I'm an international freshman in CS at U South Florida and half of my resume is blank. If yours cannot get you a job, I can't imagine what will happen to me",
        "Try using one of the resume  products novoresume/kickresume.  Currently it's too hard to read. Try a vertical section that can list your skills (just list the skills, and keep the list organized ) and get through any of the ATS systems. \n\n(BTW I have run teams of ML engineers) Reading through your resume, I am not saying I don't believe you, but it's a lot of skills for about 4 years of experience. I would prioritize a fewer skills. You should target 4 lines with about 10 -11 words per line for experience. What were the outcomes of your work - for example recommendation system, what was the improvement to your employer. Think about each line what was it and what was the value. \n\n If you have projects, link to your Github.  You need to indicate you just graduated. Its easier to explain why you went from FT to Intern.",
        "I recently was hiring for a Lead Data Scientist position (the position is closed), and within 2 days we had received just under 150 applications, 90% that looked very close to yours.  By that I mean very little actual work experience, only what was done as an intern/part time while going to school for data science.\n\nWhen going through the pile they were sorted into \"set up interview\", \"not no\", and \"no\".  10 or so made it into the interview, and about 40 made it into \"not no\".  I would put yours in the \"not no\" category.  \n\nPositives: You do have some real work experience (in between schooling) along with project work, and have used a broad range of tools.  We also wanted some MLOps help, and you mentioned that.  \n\nNegatives: I don't believe you \"Led\" any projects as an intern in only a year while finishing your masters.  You mention in your profile and Core Competencies \"Generative AI\" first, but none of your work experience reflects that.  Your one project building a RAG system doesn't warrant that level of emphasis.",
        "Too congested and long winded. Whoever looks at it is not going to take more than 30 seconds to scan and the only thing that stands out under experience is intern. You have good information but need a better way of delivering it. Also, it's obvious you changed the margins and font size to make things fit.",
        "That looks so hard to read",
        "Maybe delete your GPA? You only mention your GPA in master so I would assume your GPA in undergrad is lower than 3.62.",
        "Believe it or not, there are many jobs posting fake jobs online to make themselves seem like they’re hiring.",
        "Because everyone can see straight through your lies",
        "Check r/engineeringresumes for the google docs template. Simplify the CV. Seems good",
        "Less is more. Read job requirements for which you are applying. State experience tailored to those requirements. Keep it brief. You want them to be interested in learning more. Don’t use jargon associated with prior projects, use more general terms. A major accomplishment at each gig is plenty.",
        "It looks like you used AI to write it.  Which...I mean I get it, thats the JOB youre looking for, but...yeah no.  All the bolding as well?  \n\nYoure also in a tough place, with few job openings that fit you perfectly, and many of those you are probably being out experienced on I suspect.\n\n  \nThats my guess.  In the end its a numbers game, just keep putting in applications.  Good luck!",
        "Data scientist to machine learning intern is weird",
        "brother, i have applied to 1000 jobs but no interview. my resume: [https://drive.google.com/file/d/1p1cI0RAzQ4dnXM26JwI3sMFggr\\_VgPPv/view?usp=sharing](https://drive.google.com/file/d/1p1cI0RAzQ4dnXM26JwI3sMFggr_VgPPv/view?usp=sharing)",
        "You 4 months on your last role, then you 1.1 years in data science \n\nYou went from one role to another role with nothing done.\n\nFor example. \n\nYou worked on a team of 4 developers. Feature specific instead of the stack. This way you're not chatting geek to the recruiters.\n\nTech skills are stack specific to hiring managers.\n\nTrims the fat.\n\nVery easy to read.\n\nThen when you are on a call share the knowledge with someone who can help build you in a role the new company can fit you in.\n\nSo much going on with so little experience says you can't do anything for n the job! \n\n\nYap yap Yap... \n\nSkills on linkedIn have previous ppl you worked with giving you a thumbs up.\n\nMaybe saying things like you were a benefit to have a round for the year.\n\nOr maybe the intern role was to be a part of a feature for a startup where budget was an issue and you gave time followed up by...\n\nSo much going on.\n\nYet. No experience. No features. Maintenance jargon, buzz words, and nothing to show for it if you read it like I did.\n\nSimple. Is more...",
        "[deleted]",
        "Omfg burn it and use fewer words. My resume for meta and google literally is one sentence per each experience and the rest is in bullet points.",
        "Change font to times new Roman bro",
        "Your projects are not very impressive. I’m m in my 5th semester of my bachelors degree and one of my friend is also developing semester project on a movie recommendation system. Work on your projects and build more expertise in Ml algorithms. I’m working on a project related to the data analysis if you like we can work together it will be beneficial for both of us.",
        "Take a look at canva. You can make some pretty nice CV templates on that",
        "Here are some things I would change as an engineer that almost always gets called when I apply somewhere:\n\n1. Put education under the projects section, this field doesn't care about education most of the time.  \n2. Move the buzz word / technologies list that you have (currently over the Experience section) underneath the education (at the bottom).  \n3. Make it less dense (maybe a bit more line spacing). It is totally ok to have 2 or even 3 page resumes as you gain experience btw.  \n4. Don't bold the buzzwords in your bullet points this isn't an SEO article.  \n5. Don't list skills but rather list accomplishments that demonstrate skills (you are doing a good job of that already just mentioning it because its a mistake a ton of people make).",
        "You’re international. No matter how good you are on paper, you will have an extremely hard time right now getting callbacks. Give up and go home, companies in the US are starting to offshore to India anyways.",
        "are you from NITW?",
        "it's racism my friend. get used to it",
        "I actually think this is pretty good from a content and brevity perspective.  I wish more resumes were this concise.  I pick up immediately that this person at least has the knowledge to put on paper, so I would say, \"why not spend 1/2 hour talking with them?\"",
        "THIS SHOULD BE UPPP",
        "Absolutely agree with this. Also a hiring manager, across the board companies are supporting visas less and especially in ML, there is more xenophobia, historically, but also recently amped up given the uncertainty after the election. As for the template and format, pretty sure my resume is in the same template.... But I wrote it originally almost 20 years ago. It's fine, the information comes across, but it's definitely dated, however I don't think it's holding you back",
        "Yeah I had to stare at the resume pretty hard to find anything obviously wrong with it and I couldn't.  My guess is that \n\n  \n1. The job market sucks\n\n2. OP shouldn't ship the same resume to every job application.  Are you modifying it based on the position and going further to write a cover letter about why the company is great? Because of 1., you gotta do more work (I did too when applying)\n\n3. Companies are reluctant to deal with visas because they suck.",
        "Yeah I was gonna say, good content, but too much information density. \n\nMy recommendation is to go over to Canva and pick an aesthetically pleasing cover page that summarizes the TLDR and gives an overview at a glance. You could easily feed your resume into your favorite LLM and ask it to help you layout a cover page to make a compelling case.",
        ">Maybe an unethical pro life tip, but maybe downplay this on your resume, or remove altogether\n\nMost companies straight up ask as part of the CV submission \"will you require visa assistance right now and in the future?\". Unless you straight up lie here, there's no way to downplay the status",
        "Where do you all see that they are foreign. Just curious.",
        "I agree that sometimes its just racism, but also its hard because of you have a thick accent and poor grammar skills (because English isn’t your first language) than i also kind of get it. I dont like having to ask people to repeat themselves 100 times. I feel like communication worries is a legitimate concern with foreigners (from any country)",
        "I use long bullet points as well. I haven't found a way to get all the necessary stuff on one page other than long bullet points",
        "What are the job opportunities for  a ML guy having only experience building LLM apps using langchain and RAG?",
        "Thanks so much for this. Very much appreciate it",
        "I like this version",
        "wait did u use ML to edit his resume? what did you use? share the wealth!!",
        "the switch is bcs I did my job back in India and internship is during my masters.",
        "Thank you. Appreciate the feedback",
        "nah, you are talking shit. This proportion is good for this kind of expertise.",
        "No reason tbh. Not h1b yet. Graduated this May and yes international student",
        "why",
        "Lmfao. Don’t give up bro we got this",
        "HR isn't going to go through your github. They give a resume 15 seconds max. Either they like it in those 15 seconds or they throw it in the trash.",
        "I think you’ve nailed it completely. Those projects, while sounding impressive, are one day tutorials you can find on the langchain and llama index web page.  If he’s not after entry level jobs, the guy is inflating his ability and anyone in the field can see it instantly.",
        "An intern is too green and an employee requires too much experience. Then how the hell do you bridge the gap between intern and employee?",
        "> Or to turn the AuctionMarkt into a real business buy growing a paid customer base. Even if the money is low, these actions could set you apart from other candidates.\n\nHey I have a question for myself in regard to this. If he/she turned Auctionmarkt into a business, would it be good to put it under \"work experience\" even if its just a sole proprietorship?",
        "The other weird issue is that according to this he began to work as a DS when he just finished his bachelor of eng electronics, it looks like he made up a few stuff.",
        "Yes. Graduated this may",
        "Are you applying for jobs in the United States? If yes, without citizenship, residency, green card or at least a 5 year long term work visa, you will have zero chance and I mean zero.",
        "It’s that he’s international. If he wasn’t then this would be getting hits right now.",
        "nothing to do with experience in here. He has multiple YoE in industry and even a master degree in USA",
        "This has nothing to do with experience and everything to do with being international.",
        "You can’t fucking read better than a 1st grader can you? He has internship experience and a masters degree. What’s really holding him back is his citizenship status or lack thereof.",
        "**WARNING:** Do not trust this commenter. They use a network of bot accounts to promote a scam service through their link. They manipulate different subreddits with these bots to boost their comments to the top. It's a scam designed to steal your information. I've been warning others for about two months, this person has numerous accounts. You can check my comments for proof",
        "Am I the only one who noticed random things in bold? I feel like it looks tacky and makes it really hard to read.",
        "I did my masters in that gap period. Internship was in masters as well. But yeah I need to connect to my alumni more",
        "Yes F1 student",
        "He is Indian, US companies won’t go through visa process. Zero chance.",
        "Portfolio and LinkedIn, github is in the header section I blurred out but yeah I need to optimise more",
        "He is literally an Indian applying for US jobs without having citizenship, residency, green card or a long term work permit. There is zero chance.",
        "Just the project I’ve listed on there. It’s basically a cricket stats website where you can query stats through natural language. In the backend the text gets converted or sql, executes and returns the results. I use langchain agent to orchestrate this process. It’s basically like statmuse.com but for this sport called cricket.",
        "It’s a latex template called “jake’s resume”.  Commonly used in tech fields",
        "I would also suggest to move your skills to the bottom as a good closing statement. Let your summary give context for you, your experience validate that, then remind them what your good at the end. Same tip with formatting. I would use ChatGPT to plug those into categories that visually fill the space. Also asking what are high in demand skills in the field to see if there's any other ones you forgot you had experience with.",
        "And what would that lied be? I’m really curious",
        "What gives that vibe from the resume bcs I most certainly did not lie. Asking so that I can make changes accordingly",
        "He is Indian. Doesn’t have citizenship, residency, green card or a long term work permit, there is literally zero chance that he gets in.",
        "There's (more) xenophobia in ML? That sounds horrifying!",
        "The difference is, highlighting it on your resume gets you automatically rejected when being screened manually. The answers to these questions usually go into a different place in an applicant tracking system, which they would check after deciding your resume looks interestin. They legally cannot automatically filter on your status, so putting any barrier between them looking at your resume and noticing your foreign status is worth a shot.",
        "they have a “bachelor of technology” degree, which is mostly only a thing in south asian colleges",
        "maybe use some ML to write for you",
        "None, it isn't ML. Any software dev can do that stuff",
        "Are you open to Europe or just the US?",
        "Also make sure your resume is optimized for ATS since most companies are using ATS to screen applicants",
        "Yes I did ironically right? All I did was download the resume picture and upload it to chat Gpt . My prompt was basically: make this resume infinitely better and clearer. Make it easy to read and hireable for a mid level engineer and at least get an interview by a ML hiring manager.",
        "I'm not talking about the amount of things he conveys or the expertise, I am talking about the lengthy way he describes things some parts being repetitive. People reading loads of applications a day wont even read through many longer parts which is why compressing information and focusing on the key aspects is important. The content doesnt even matter at that point.",
        "Problem is being an international student in 2024",
        "Hmmm that would make things harder ngl.  Market is rough",
        "100% you are indian",
        "No reason? So why did you?",
        "100% you are indian",
        "100% you are indian",
        "Because OP is also fked, and his resume is already above average, if not good.",
        "That’s something I’m thinking myself but wasn’t sure so put it in projects",
        "If you are spending full time on it, and making real progress, it's OK. But since companies do background checks for your employment history, you should be upfront that this was a self-driven startup, not a company that employed you.",
        "I'm just gonna tell you this right now. It is literally like 10 times worse if you're international. No companies want to sponsor. Look for opportunities in your home country. Because unfortunately right now the economy is just like this",
        "Thank you based hero",
        "I almost fell for the scam. Thank you.",
        "Dude it’s wild because the account checks out. It’s 17 years old, with a lot of diverse activity.\n\nBut on the flip side, I believe you since I’ve read essentially this exact same comment in multiple threads, from different users. What’s the story here?",
        "I don't know man. You're telling me that sending your resume to companies in your area is not a good strategy? I know some people who never received responses from Indeed and all other job boards, but got hired by using this method.",
        "Dude, are you crazy? Why am I being called a bot for commenting on a post just once? Did I tell you to pay for the service? I saw a good technique and shared it—what's wrong with that? Also, which part of the service is a scam? Every time someone shares a link, someone else jumps in to call it a \"scam.\" I guess that’s the rule of Reddit now—anything that involves a fee or a link automatically gets labeled as a \"scam\". LOL. \n\nIt’s unbelievable that you really have nothing else to do in life and spend your time on this and calling people bots.",
        "I can agree with you that it may look like a promotional post, but calling it a \"scam designed to steal your information\" might be the funniest thing I've heard in a long time—especially considering that much of your personal data is already on the deep web. Get a life, mate.",
        "Northeastern 100% has an office to help new grads craft their resume and connect with local orgs to find their first job.",
        "that is the real reason",
        "Got it. In any case, this CV does need some work regardless.",
        "Oh yeah…. That will make a difference",
        "All of them",
        "Do you know any good resources?",
        "ATS also cannot apparently handle anything in columns. \n\nOP, you have columns.",
        "Not even just America, in the UK we have a massive amount of masters students from different countries we can't hire. \n\n\nCosts a company £7k for a licence to sponsor then another few k per actual sponsored person. \n\n\nYou gotta ask yourself, \"Am I worth an extra 10k to this company?\"",
        "damn why you saying it like you caught me in a lie😭",
        "I believe the person behind r/RemoteJobseekers created it a few months ago to push low quality, potentially fake job postings to people in desperate need of remote work. If you look at the moderators’ accounts and the accounts promoting the subreddit, they were all created over ten years ago, yet only started posting actively about 3-4 months ago. It seems they realized they could use AI tools like ChatGPT to help people with resumes and then began promoting a questionable product. They’re likely making significant money off people in need. It’s honestly disturbing.",
        "Don’t twist my words or create a false equivalency. I never said sending resumes to local companies was a bad idea. What I said is that what the bot is promoting is a scam, two entirely different things. Why are you defending a literal bot?",
        "14 year old account, starts posting 3 months ago. Nothing else to say, bot",
        "He is now using a different bot to make comments. You can't make this up lmfaooo. The devil works hard, but you work harder",
        "Bit weird to accuse people when u dk shit abt them",
        "Maybe ask someone who is proficient in AI platforms such as openai or anthropic",
        "Can't they work for you using the 2 year government backed graduate visa?",
        "You really don't need to hide it.",
        "I read through the entire comment history and really tried to understand you. You might be right; it could actually be a promotion, i just don't understand why does this bother you soooo  much, even if it is a promotion? Honestly, I’m starting to thnk you do something similar and  feeling jealous about it.",
        "You, who talk about 10-year-old accounts posting for the last 2-3 months, have only been a Reddit member for 3 months. Lol.",
        "What the fuck?  \nYou just came, attacked a poster/bot, and you didn't even elaborate on the content, which means it only makes sense to assume you don't agree with what they said.",
        "it's very funny because your account 3 months old :)",
        "this guy is going crazy. He calls every comment he doesn’t like a bot. HahahaHahahaHahaha. bro don't do it yourself. really.",
        "Meh. I’ll just use long bullet points",
        "Yes, but when you recruit someone you put time money and effort into them, so whilst you could, you put yourself into a situation where you are gonna need to recruit again in the long term, but what also happens alot with student visa's is job hopping. \n\n\nI see 200 of these CVs a day. The people who do get jobs start looking 3 months into getting a new role. They have a finite time to either - \n\n\nA - Find a company willing to sponsor them (which is only fair) \n\nOr \n\nB - Make as much money as humanly possible in that 2 year period to take back with them. \n\n\nAgain, it's only fair, but to an employer paying £5.8-7k for the recruitment fee, it's too much of a risk. \n\n\nAlso.remember they are now only entitled to that visa if they earn a certain salary now, too.",
        "Because you are taking advantage of people desperately looking for remote work in a hard economy. Do you not feel shame?",
        "So for anyone reading this, here’s a quick guide on how to spot a bot. They often post in subreddits like r/CelebPortraits places with low entry barriers but where it’s easy to get upvotes. You’ll notice they have near perfect punctuation and tend to repeat the same comments. Take this user, for instance. If you check their comment history, they’ve also promoted that sketchy resume site. Makes you think, right? And when you call out their scam, they usually start attacking repeating the same lines over and over lmao",
        "My brother in Christ, please reread my original comment. Nowhere did I say I agree or disagree with sending resumes to local companies. I was specifically talking about their link. I’m not sure where you got that idea. I also mentioned you could check my comments for proof, where you’ll see other bots making identical posts. So again, I have to ask why defend a literal bot? It has an account for over 10 years, start posting 3 months ago.",
        "Yeah, not 14 years old and randomly reactivated 3 months ago :)",
        "Don't do what?",
        "Here's a sneak peek of /r/CelebPortraits using the [top posts](https://np.reddit.com/r/CelebPortraits/top/?sort=top&t=all) of all time!\n\n\\#1: [Alexandra Daddario](https://i.redd.it/oqlo6ek8ewyd1.jpeg) | [74 comments](https://np.reddit.com/r/CelebPortraits/comments/1gjg1bz/alexandra_daddario/)  \n\\#2: [Anne Hathaway ](https://i.redd.it/uo9u77u7cwxd1.png) | [48 comments](https://np.reddit.com/r/CelebPortraits/comments/1gfmnne/anne_hathaway/)  \n\\#3: [Alexandra Daddario ](https://i.redd.it/hqsbx1p28pvd1.png) | [38 comments](https://np.reddit.com/r/CelebPortraits/comments/1g76v4n/alexandra_daddario/)\n\n----\n^^I'm ^^a ^^bot, ^^beep ^^boop ^^| ^^Downvote ^^to ^^remove ^^| ^^[Contact](https://www.reddit.com/message/compose/?to=sneakpeekbot) ^^| ^^[Info](https://np.reddit.com/r/sneakpeekbot/) ^^| ^^[Opt-out](https://np.reddit.com/r/sneakpeekbot/comments/o8wk1r/blacklist_ix/) ^^| ^^[GitHub](https://github.com/ghnr/sneakpeekbot)",
        "[deleted]",
        "Even if you’re right, I really don’t understand why you’re putting yourself through this. Why are you doing this to yourself? You joined Reddit just 3 months ago, and when I check your account, all you do is attack other people. Please take care of your mental well-being; I truly don’t understand the motivation behind it.",
        "So this is another one. Proof: [https://www.reddit.com/r/jobsearchhacks/comments/1fhg1d4/comment/ln9yoy6/](https://www.reddit.com/r/jobsearchhacks/comments/1fhg1d4/comment/ln9yoy6/) and [https://www.reddit.com/r/graphic\\_design/comments/1fhdvp7/comment/ln9vxtv/](https://www.reddit.com/r/graphic_design/comments/1fhdvp7/comment/ln9vxtv/)\n\nAccount 10 years old, started posting 2 months ago. Like moths to a flame",
        "The motivation is calling out someone who is purposefully taking advantage of people. Is that so hard to understand? And here is a word of advice, if you don't want to seem like a bot, stop posting in subreddits like, r/celeportraits. It is a dead give way. Another piece of advice is the near perfect punctuation. Good luck next time"
    ]
},
{
    "submission_id": "1gpa0q6",
    "title": "Business Review Language Model",
    "selftext": "https://reddit.com/link/1gpa0q6/video/9nhj5qyfmd0e1/player\n\nOne year into teaching myself machine learning, I created a language model to write business reviews and predict a review's 1-5 star rating! The model is a 109M parameter generative transformer trained from scratch on 689M tokens (actually three epochs of 229M tokens from the yelp dataset). Even with the limited model size/data, the model had acquired the ability to recall characters' names in the prompt. The training code and the model checkpoint are here: [https://huggingface.co/gabe00122/sentiment\\_lm](https://huggingface.co/gabe00122/sentiment_lm)",
    "created_utc": "2024-11-11T18:11:10",
    "num_comments": 1,
    "comments": [
        "\\* Edit sorry I meant million tokens not thousand"
    ]
},
{
    "submission_id": "1gp9uo4",
    "title": "How do you feel about the role of exclusivelyai algorithms in the creative process? ",
    "selftext": "Are there specific examples of [AI-generated works that you find impressive](https://exclusivelyai.uk/) or thought-provoking? Let’s dive into this discussion and explore the nuances of creativity in the age of artificial intelligence!",
    "created_utc": "2024-11-11T18:02:48",
    "num_comments": 0,
    "comments": []
},
{
    "submission_id": "1gp8vg6",
    "title": "Why isn't federated compute not a service?",
    "selftext": "I've been doing some research on federated ML recently and I am extremely surprised to find that there exists no service where people are paying for and selling compute services in a decentralized way.\n\nThe way I envision this is that you, an ML engineer, can submit a job to this service and pay some amount per TFLOP. Other users can sign up and accept jobs, and get paid some amount per TFLOP. The jobs they can accept will be scoped to the hardware they have available. Then just do federated ML. As far as I can tell, there is no way for your data to remain private, so that's an issue, but there also many cases for ML where this may not matter.\n\nWhy isn't this a thing? What am I missing? Obviously this isn't better than renting out a huge AWS cluster but it could definitely be cheaper (have you seen the margins on AWS).",
    "created_utc": "2024-11-11T17:14:50",
    "num_comments": 7,
    "comments": [
        "I have several years of experience working adjacent to (but not directly on) this kind of technology.\n\nA public network of this type is *extremely difficult* to get right. There's also a relatively small market that meets the following criteria:\n\n - Needs enough compute to warrant paying for it out of pocket\n- Does not need quick task completion speed badly enough to pay cloud computing prices\n- Does not care about data privacy (as you noted yourself)\n- Is willing and able to put in the development work to parallelize their compute tasks for a specific platform\n\nThe individual compute jobs that can be done on a system like this are also somewhat limited. Jobs need to:\n\n- Be highly parallelizable\n- Not require much inter-process communication\n- Have small parts which individually require enough processing work to justify several seconds of time lost to IO\n\nFederated learning fits the bill, but is not a mature field yet, has high IO overhead, and is almost always limited by data privacy concerns.\n\nThe market itself is also a problem. You would need to manage a market where an individual person is paying hundreds or thousands of different people to do work for them at different speeds and with different hardware capabilities (GPU availability, memory footprint, etc). On cloud services you're paying for minutes of compute time on fixed hardware. How do you translate that to be hardware-agnostic without either encouraging slow processing speeds or punishing poor workload estimation? Tracking TFLOPs is not trivial, and can actually slow down processing massively if done with enough accuracy to justify payment systems. Also, what if you need very little compute but massive IO? Or have huge memory needs relative to your actual processing needs. This isn't a market where you can set a single price and go.\n\nA platform like this would also come with security concerns for the compute provider. Remotely executing arbitrary code is a massive risk, and the alternative is sandboxing your execution environments (reducing performance and limiting what kind of code can be run, both of which are counter to your market needs).\n\nAnd another thing is the difficulty of validating work done by untrusted agents. Unlike inherently checkable programs like crypto mining, the results of arbitrary work are impossible to validate without running the program a second time. Either you open yourself up to fake compute providers returning garbage results, or you add massive overhead in the form of work duplication.\n\nI could go on, but the bottom line is that it's really hard to do because of a ton of different roadblocks, and nobody has solved all of them well enough to make a marketable product yet.",
        "Perhaps you can blame Bitcoin mining?",
        "I'm not aware of anything fitting your spesifics, but there are several services allowing you to rent out GPUs on a common marketplace where the end user is provided with a virtual machine to work with.\n\nBut my experience is that while you can get some really cheap offers as a user, reliability can be a concern if you're not renting from a data centre.",
        "Hey thanks for this comment. You confirmed what I suspected which is that there's a lot of \"what about...\"\n\nI actually think reinforcement learning works pretty well for this, because you don't actually need to send data, just a simulation environment. And you could combine genetic algorithms with RL to get a system that can operate in a more or less decentralized way.\n\nBut I think any one of the issues you mentioned makes this a deal breaker. Namely, remote code execution. The only solution I can think to this is that code needs to be open source, but ideally this platform should be accessible to non-ML engineers.",
        "Can I just pop in to thank you for taking the time to lend us your expert insight in this topic? Contributors like you make Reddit great!",
        "I'm guessing your point is that mining bitcoin is more profitable?",
        "Don’t have a strong opinion or point, but I would imagine btc mining offers a means to monetize federated compute resources without needing to attract, service, or support demand.\n\nHaven’t thought too much about this, that was just my first impression.\n\nAcademic institutions use shared clusters, but the privacy issue is clearly a significant driver of your observation as well.\n\nWas only partially serious in my response."
    ]
},
{
    "submission_id": "1gp6d6s",
    "title": "can anyone recommend a course or a video about python for ai development?",
    "selftext": "so I have been watching a ton of courses and videos about AI LLMS, Huggingface, creating models from scratch, but I cant make out anything!, he just imports pytorch, what is even that?, whats tensorflow? whats keras? I know these may sound like fundemental knowledge, but I cant figure it out myself, how to write proper code for it?",
    "created_utc": "2024-11-11T15:18:42",
    "num_comments": 8,
    "comments": [
        "Maybe start with intro to python.",
        "There is no good video for “ai development” you need enough knowledge to build what you want.  The fact that you don’t even know python means you need to walk before you run.\n\nIt’s like you’re asking for a video on how to build a house but you don’t know how to plug in a tool",
        "Take a look at a course like AI programming with Python (Udacity). It’s a beginner course that starts by giving you an intro first into Python and then introduces the core packages required for data analysis and deep learning (plus a little maths). It in no way covers LLMs, but you have to cover off the basics first. \n\nNB I’m not saying do this exact course but something with a similar syllabus.",
        "I think that, if you want to get more into ai development, you should start with machine learning basics first. And also, math for ML/AI.\n\nAfter that, you’ll understand most of the things imported and used in AI. Do not forget also about AI fundamentals (neural networks, transformers, etc).",
        "Don't think AI is your immediate concern my man. Learn Python and basic programming first.",
        "From the way you describe your question it sounds like you don't know tha basics of the fundamental of what you are looking for like what /AppropriateSpeed is saying.\n\nI try not to post on Reddit for a number or reasons, one of which is that ChatGPT gives me most of what I want in answers. Have you  tried havinga meaningful conversation with that yet?\n\nI use learning.oreilly and Udemy for most of the \"courses\" that I take and Google university (or what ever its called) and the university consortiums are out there, but I get what you are asking.",
        "I know how to code in python, its not like that, anyone with any tutorial on python for ai development just imports and does 70% of work like its common knowledge, I am asking where I can learn that, How to work with pytorch or tensorflow",
        "Then the next step is to break what you want to do down into manageable steps and work your way through it."
    ]
},
{
    "submission_id": "1gp5i9m",
    "title": "Meta Onsite interview",
    "selftext": "I recently had a phone interview with Meta, and I’m optimistic about moving to the next stages. Could you recommend some key references or resources to study in preparation for the next stage? I’m applying for a **Software Engineer, Machine Learning** position, and would also appreciate any insights or experiences shared by others who have gone through the process. Thank you!",
    "created_utc": "2024-11-11T14:42:09",
    "num_comments": 0,
    "comments": []
},
{
    "submission_id": "1gp5h8z",
    "title": "Help with a machine learning project ",
    "selftext": "I need to do a multimodal biometric project which uses facial recognition and keyboard dynamics. I have publicly available datasets for both which I am supposed to use. However, since these datasets contain different users\r\nand potentially varying numbers of users, I will need to simulate a unified multimodal dataset where each individual is represented across\r\nmultiple modalities. \nAny idea on how that could be done? ",
    "created_utc": "2024-11-11T14:40:57",
    "num_comments": 0,
    "comments": []
},
{
    "submission_id": "1gp4x4z",
    "title": "Evaluating Reasoning in Abstract Strategic Games (I)",
    "selftext": "The project started with the basic premise understanding if an LLM can play n x n board games starting with simple 3. x 3 Tic Tac Toe to 64 x 64 type games.\n\nThe initial problem was how to encode the game so an LLM could understand it. I chose to select each position as a (i,j) tuple on the n x n matrix. Where all available positions are handed over to the player as an array. Then, I needed to select a framework. In this case, I used Transformers Agents that implement the ReAct pattern. As a result, each move of an LLM returned a selection of a tuple from the provided array.\n\nFor the selection of the LLM, I initially explored local Llama3.2 and Mistral Nemo, but I landed at Qwen2.5 Instr 70B through the free API since it provided the most robust response at no cost. All responses were prompted to be a JSON dict.\n\nFor the opponents, I used a Random Player and a Reinforcement Learning type Player.\n\nIn this screenshot, you see the visualized result of two Qwen Agents playing against each other.\n\nhttps://preview.redd.it/bpbxuetbjc0e1.jpg?width=960&format=pjpg&auto=webp&s=054a4bd06ce74d01edae05c84332a0509f366b56\n\nThe full project can be found on [Substack](https://jdsemrau.substack.com/p/evaluating-consciousness-and-reasoning)\n\nIf you have any questions, let me know",
    "created_utc": "2024-11-11T14:17:25",
    "num_comments": 0,
    "comments": []
},
{
    "submission_id": "1gp4bsp",
    "title": "Simulating chess games: Random Player vs LLMs - Leaderboard",
    "selftext": "",
    "created_utc": "2024-11-11T13:52:47",
    "num_comments": 0,
    "comments": []
},
{
    "submission_id": "1gp3drs",
    "title": "[D] How to report without a test set",
    "selftext": "\nThe dataset I am using has no splits. And previous work do k-fold without a test set. I think I have to follow the same if I want to benchmark against theirs. But my Val accuracy on each fold is keeping fluctuating. What should I report for my result? ",
    "created_utc": "2024-11-11T13:13:53",
    "num_comments": 2,
    "comments": [
        "How about reporting the mean of validation with fluctuations, that is, with error bars. So that we can know if fluctuations are very wide or narrow?",
        "Good idea"
    ]
},
{
    "submission_id": "1gp1x0t",
    "title": "Starting an Energy + AI Startup with Zero Tech Background",
    "selftext": "I’m exploring the energy & AI space to build a startup that tackles grid flexibility and forecasting. But I’m stuck on where to begin with no tech background.\n\nShould I:\n\n1. **Start with web dev** so I can build a prototype early on, OR\n2. **Jump into ML** and tackle web dev later when I need it?\n\nAnyone with similar experience? Which path would set me up best for the long run? Appreciate any tips, resources, or personal stories! 🚀\n\nThanks!",
    "created_utc": "2024-11-11T12:15:15",
    "num_comments": 8,
    "comments": [
        "I'm gonna be honest, this sounds like a tool that energy companies will already have with forecasting. In terms of grid flexibility, they purposefully don't want that to maximize their profits.",
        "What do your customers want first?",
        "Web dev",
        "I know there are exceptions but generally when looking to do a startup someone involved has extremely deep knowledge of the field. You will find it extremely difficult to compete in the ML space or even in the energy space without a pretty sophisticated background in it. It's heavily regulated.\n\nAre you an experienced web developer? Do you know it like the back of your hand?\n\nDo you know how to engage with the industry to find customers so you can understand the gaps in the market and define your niche? \n\nYou should probably start with the basics.",
        "I am really exploring the space rn, I have not started working on any project as such. so I just want to know what path would be better for me in this space.",
        "thanks you for your advice but could you help me with why I should go for web dev and not ai/ml",
        "Web dev is considerably easier and requires way less background. ML is not really a field you can jump into without a decent amount of foundational knowledge. \n\nIf you have zero tech background you should probably focus on developing your idea first. Read some research papers, figure out what kinds of models you might need, maybe some higher level architectural stuff about your web platform, your pricing model, etc. Then find someone to partner with who can fill in some of the gaps in your expertise. \n\nI suggest reading the book Running Lean by Ash Maurya. Follow the steps, develop a lean canvas, elevator pitch, etc. Focusing your energy here will be much more relevant to your success than attempting to go from 0 to web dev and ml expert.\n\nEthos: Masters in big data systems (Ai and ML), currently working as full stack developer for a startup.",
        "This is really helpful. Thanks a lot"
    ]
},
{
    "submission_id": "1gp1s45",
    "title": "Is there any way to fine-tune using completely unlabeled data?",
    "selftext": "Say I want to fine-tune llama to answer questions about medical data and just want to use a medical textbook, the text book states facts but has no question answer matterial that looks like the text I wan't my chatbot to replicate.\n\n\n\nIs this possible? Or do I need to have it in a question answer format?",
    "created_utc": "2024-11-11T12:09:50",
    "num_comments": 1,
    "comments": [
        "You can use an existing LLM to create questions from facts that you find. Give it a fact, and prompt it to generate questions based on that fact.\n\nFor example, I asked perplexity.ai : Create a question that has the corresponding answer: \"The mitochondria is the powerhouse of the cell. Its primary role is to generate ATP, the chemical that most biochemical reactions relies on for energy.\"\n\nThe question it gave me was: \"What is the primary function of mitochondria in a cell?\"\n\nYou can of course use local LLMs to do this too, I just don't have access to mine at the moment :c\n\nAnyways, if you accumulate enough of this semi-synthetic question-answer data, it will help with the fine-tuning process."
    ]
},
{
    "submission_id": "1gp02gl",
    "title": "Using KitOps to deploy ML with Dagger.io – Dagger.io community call",
    "selftext": "",
    "created_utc": "2024-11-11T11:00:26",
    "num_comments": 0,
    "comments": []
},
{
    "submission_id": "1gozwi9",
    "title": "Logging Hyperparameter Tuning Using Parallelized Hyperparameter Tuning",
    "selftext": "Have a pretty intensive data project I'm hyperparameter tuning using Hyperopt and SparkTrials. Running into an issue where it is crashing sometimes midway through, and I have to start all over from the beginning. The model is just a tree model (xgboost). Can't really find anything on the Hyperopt docs for how to log the trials with parallelization, so it can pick up where it left off if it crashes. If anyone has any suggestions or other Hyperparameter Tuning options that have this built in, would be super helpful to know. Probably am missing something.",
    "created_utc": "2024-11-11T10:53:52",
    "num_comments": 0,
    "comments": []
},
{
    "submission_id": "1goz5j6",
    "title": "Need help to create an AI Saas ",
    "selftext": "Hi guys, I am new to AI and ML and very excited to learn this .\nI am thinking to create an ai integrated SaaS which help a enhance efficiency and reduce wastage of resources.. Can anyone please share their insights on how do I learn to create and make an ai saas and where do I start from and what resources which will be precise,can help me to get through this ? \nYour help is greatly appreciated!!\nThanks for it !\n",
    "created_utc": "2024-11-11T10:24:03",
    "num_comments": 3,
    "comments": [
        "Question is poorly phrased and not specific",
        "Okay sorry for that ..just asking how do I start to learn about making an ai integrated software. What knowledge of ML and AI and software making will I need .. please tell from basics like what topics or lectures it requires.\nThanks for it",
        "DM me, I’m building a tool that can help you build an ML model and integrate into your software"
    ]
},
{
    "submission_id": "1goy5de",
    "title": "Introducing Speakr – A Privacy-First, All-in-One Voice AI Solution",
    "selftext": "",
    "created_utc": "2024-11-11T09:44:32",
    "num_comments": 0,
    "comments": []
},
{
    "submission_id": "1goxqn6",
    "title": "Is there any simple ai model that can remove reflections from images?",
    "selftext": "I've been working on a project involving images taken from inside an autonomous vehicle, but I often run into issues with reflections showing up in my photos due the glass windows. I'm looking for an AI model or some type of automated tool that can help remove or reduce these reflections effectively.",
    "created_utc": "2024-11-11T09:28:10",
    "num_comments": 1,
    "comments": [
        "not sure an AI model would be needed here, wouldn't context-aware fill work?"
    ]
},
{
    "submission_id": "1govrld",
    "title": "How to learn Machine learning and related stuffs",
    "selftext": "# Can all of you help me to learn machine learning from scratch and find freelancing projects or part-times jobs related in this field. This would be very helpful to build my career as I am an undergraduate in Computer Engineering",
    "created_utc": "2024-11-11T08:08:41",
    "num_comments": 3,
    "comments": [
        "Learn machine learning first",
        "By using the search bar \n\nLmao at part time jobs",
        "can you give me any resources to learn"
    ]
},
{
    "submission_id": "1govhhe",
    "title": "How to learn Calculus the proper way?",
    "selftext": "I was not a good student in school, and never paid much attention to learn Mathematics. However, I am planning a career as a MLE and I know that I need to learn Mathematics for a successful career in Machine Learning. I have planned to study Mathematics for Machine Learning the hard way, start from the beginning, and then move all the way to learning Calculus. This is because learning Calculus requires prerequisites and I am not sure what I will be missing if I try to pick and choose topics.\n\nMy question is, if I speedrun the following syllabus from the beginning till the end, will it be enough for me to start learning Mathematics for Machine Learning? [https://www.khanacademy.org/math/in-math-ncert](https://www.khanacademy.org/math/in-math-ncert)",
    "created_utc": "2024-11-11T07:57:16",
    "num_comments": 14,
    "comments": [
        "MIT OCW. They have Calculus 1,2, and 3. I learnt it comprehendively from there and there has been no looking back after that. Try looking for Single variable and then proceed to Multi-variable Calculus. Its well worth the time and effort.",
        "Those lectures here could be of use: \n\n  \n[https://www.youtube.com/watch?v=NtBb5PdHkWg&list=PLcPXq\\_8xLMgHYYjeL8VjiLHWz05rNDL\\_2](https://www.youtube.com/watch?v=NtBb5PdHkWg&list=PLcPXq_8xLMgHYYjeL8VjiLHWz05rNDL_2)\n\n[https://www.youtube.com/watch?v=OaWRB86hcws&list=PLcPXq\\_8xLMgGcqI7Z4tnVoXNsEa0yrQdE](https://www.youtube.com/watch?v=OaWRB86hcws&list=PLcPXq_8xLMgGcqI7Z4tnVoXNsEa0yrQdE)\n\n[https://www.youtube.com/watch?v=NtBb5PdHkWg&list=PLcPXq\\_8xLMgHupm4NZNgVqJcsdiFO1RE1](https://www.youtube.com/watch?v=NtBb5PdHkWg&list=PLcPXq_8xLMgHupm4NZNgVqJcsdiFO1RE1)\n\n  \nFeel free to check them out.",
        "It's good one.",
        "1.  Get Stewart Calculus.  There are two versions w/ transcendentals.  I can't remember which version is correct.  Stewart is the top book.  But MIT as suggested by others is fine.\n\n2.  Go to youtube, find lectures that correspond to the book. like Professor Leonard https://www.youtube.com/@ProfessorLeonard.  There are other calc prof like him.\n\ndo the lecture, read the chapter, do EOC exercise, check your solutions.\n\nUse Khan Academy or other sites (/r/learnmath), when you get stuck.",
        "In terms of calculus, all you need to know is what a derivative is, how to calculate it (which is often trivial when compared to integrals), what a partial derivative is and how to calculate it. Honesty that’s all you need, Linear Algebra is the more important math to fully understand IMO",
        "I have a list of courses here which includes some math courses: [https://github.com/duncantmiller/ai-developer-resources](https://github.com/duncantmiller/ai-developer-resources)\n\nThis is the one I like for Calculus: [Calculus for Machine Learning and Data Science](https://www.coursera.org/learn/machine-learning-calculus) by [DeepLearning.AI](http://DeepLearning.AI)",
        "Math is kind of awesome in that you do not have to be smart to become good at it. The more you practise the easier it gets. Practising problems over and over again until it's burned into your brain goes a long way.",
        "“Math for machine learning” online book, free\n\n“Practical deep learning,” online course+book, free https://course.fast.ai/\n\n“Math ML Foundations” course by John Kron, hybrid free on GitHub paid on O’Reilly https://github.com/jonkrohn/ML-foundations",
        "100% agree with this. I'm 12 years out of my PhD and still use MIT OCW often",
        "That's not \"all\" you need. Integration and multivariable calculus are applied extensively in probability, and probability is used in ML.\n\nLinear algebra (beyond the basics), doesn't appear too often in ML. E.g. images, kernels, Cayley-Hamilton theorem, etc.",
        "Calculating derivatives over functions is a mechanic and simple operation.\n\n\nReally understanding what's going on, requires a good teacher and textbook. \n\n\nLook for MIT OCW for the Calculus lectures. Do all the problems sets, textbook and exams. There's no shortcuts.",
        "Absolutely! It’s a goldmine for self learners..",
        "Singular value decomposition/principal components is very good to know\n\nComplex eigenvalues are important in time series analysis. \n\nTo get started you just need to multiply matrices but the full intro sequence is kind of expected for a proper machine learning engineering background",
        "yeah man. I completed the series when it was offered via edX and damn it was awesome. and yes there are no shortcuts here."
    ]
},
{
    "submission_id": "1gouojm",
    "title": "Does block size matter beyond training? ",
    "selftext": "So I have trained a basic transformer model with the block size parameter set to 30 (tokens). Now I'd like to pass sentences through the transformer (i.e., pass every token contained in the sentence and aggregate embeddings), yet most sample sentences contain more than 30 tokens. I would have thought that this hyper-parameter only matters for the training exercise and less for out-of-sample \"prediction\" but I seem to get an error whenever a sentence has more than 30 tokens and I'm not sure whether I'm just simply performing a bad forward-pass through the transformer or whether I have to somehow account for a variable token length when performing the forward pass.\n\nEDIT: It seems that the error occurs due to the tril mask which, during training is defined to be a tensor of size block\\_size x block\\_size which causes an error when token size > block\\_size. ",
    "created_utc": "2024-11-11T07:23:25",
    "num_comments": 0,
    "comments": []
},
{
    "submission_id": "1gou65g",
    "title": "Using Multiple LLMs and a Diffusion Model Together",
    "selftext": "",
    "created_utc": "2024-11-11T07:01:41",
    "num_comments": 9,
    "comments": [
        "You could benefit from using routeLLM: https://github.com/lm-sys/RouteLLM. Although the project is different as it routes between a “weaker” model and a larger, more expensive model but the concept of routing could potentially apply to",
        "Hi there; I've been experimenting with running multiple models together in one app and it's been pretty promising. I'm jokingly referring to this setup as MoM (Mixture of Models). Note, this is more targeted at beginners / devs, not research / academic level.\n\nMy goal for this was a technical / engineering exercise (to explore / experiment). There are existing tools / UIs out there that are mature and do similar things, so this isn't meant to launch another UI, just explore concepts. \n\nMost recently, I've used llama 3.2 3B, llama 3.1 8B and Stable Diffusion 1.5 together. What each model is doing:\n\n* llama 3.2 3B: sits in front and is used to classify a user message into text or image responses needed buckets\n   * Additionally a JSON schema is used for this step to constrain the LLM response\n* llama 3.1 8B: generates the responses and optionally generates a prompt for the image model based on the user's message\n* SD 1.5: image generation\n\nNotes:\n\n* Why two different language models?\n   * The larger model could do everything, but I wanted the classification step to happen as quick as possible. Using a smaller model is noticeably quicker.\n   * Also, right now, llama.cpp can't hot swap models, so they're run in parallel instances\n* What about MoE?\n   * I'm actually going to revisit this. I found the phi MoE family a bit lack luster when I tried them, but maybe a different family would be more compelling or maybe I just need to look at phi more.\n   * On paper MoE should be the way to go, and would be more memory efficient, though in my setup adding the smaller LLM didn't didn't make or break my VRAM limits\n* Why classification instead of regex or string matching?\n   * It's true classification vs something like regex is pretty heavy handed for this, however, I was surprised at how quick the classification was, all things considered, and I think classification is the more powerful approach so I wanted to explore it (going back to the experimentation goal)\n* Why SD 1.5?\n   * Good enough for testing purposes and LCM makes it very quick for image gen (compared to say Flux)\n* My first pass just had a single LLM and the image model with different endpoints and you'd have to active the image gen using a slash command.\n   * The new classifier approach means the default response path will detect what response is needed and generate the appropriate response\n\nWhy this might be useful:\n\n* Exploration of running multiple models together for different tasks / optimizations\n* Example using JSON schema for structured output\n\nHere are the resources:\n\nGitHub repo: [https://github.com/matthewhaynesonline/ai-for-web-devs/tree/main/projects/6-mixture-of-models](https://github.com/matthewhaynesonline/ai-for-web-devs/tree/main/projects/6-mixture-of-models)\n\nYouTube tutorial: [https://www.youtube.com/watch?v=XlNSjWSag0Q](https://www.youtube.com/watch?v=XlNSjWSag0Q)\n\nTech setup note: I'm running this on an AWS Linux EC2 because my laptop (an old Intel Mac) doesn't have an NVIDIA GPU, but it can be run on anything that supports docker, etc.\n\nDiagram (sorry mobile users)\n```\n                                       +------------------+\n                                       | Default Message  |\n                                       | Path             |\n                                       +------------------+\n                                                |\n                                                v\n                                       +------------------+\n                                       | Small LLM:       |\n                                       | Classifier       |\n                                       +------------------+\n                                          /            \\\n                                 Needs Image         Needs Text\n                                       /                \\\n                                      v                  v\n    +------------------+    +------------------+     +------------------+\n    | Image Message    |    | Large LLM:       |     | Large LLM:       |\n    | Path             |    | Image Prompt     |     | Text Response    |\n    +------------------+    | from User Message|     +------------------+\n                        \\   +------------------+\n                         \\ /\n                          v\n                +------------------+\n                | Image Model:     |\n                | Pipeline         |\n                +------------------+\n```",
        "PS I’m not associated to the routeLLM project in any way. Just found it interesting and thought it might be helpful :)",
        "Not sure if it'd tank performance but what if you used a smaller model (i.e. finetuned Bert) to classify as text/img response? Could potentially be faster in response time too (Bert-large is 300mil params iirc as compared to 3B in your Llama)? Not super well versed so may be wrong but just a thought",
        "how do you guarantee the Large LLM sends the correct formatted prompt to the Image model? Like how do you guarantee formatting?",
        "Hey that's very cool, thanks for sharing! I'm curious to see how their routers work, so I'm going to take a peek. Glancing, looks like the LLM and bert routers are closest to what I have, but maybe some of the other ones have better speed vs quality trade offs.",
        "Great question and it's actually something I want to revisit, along with MoE. \n\nMy intuition is that BERT would probably be the best set of trade offs, with regex being on one end of the spectrum (fast, but worse / brittle) and an llm on the other (slower but more robust), but in my case for this proof of concept, the small llm was fast enough to not impact the UX and I also didn't have to worry about fine tuning with a dataset. Mind you, finetuning BERT for this simple classification should be trivial, but I had wanted to tinker with JSON schema for LLM as well, so I was already down the rabbit hole.\n\nIf I were to optimize the setup for a real application, I would imagine that BERT would be the best bet.",
        "At least for my proof of concept, prompting alone was sufficient. The 8B model did well enough even without examples or fine tuning, though more rigorous prompting would likely be needed for production work loads. But yeah, I could just directly feed the output of the 8B model into the diffusers pipeline\n\nHere is the prompt for the 8B model that is used to generate the image gen prompt: \n\nhttps://github.com/matthewhaynesonline/ai-for-web-devs/blob/main/projects/6-mixture-of-models/app/services/prompts/diffusion_prompt_from_message.j2\n\nNote that this will allow the LLM to fill in details, but I actually wanted that as I could still directly prompt the image model using a /image command.\n\nAs a counter example, the 3B model did need more guidance via prompting to reliably classify messages\n\nhttps://github.com/matthewhaynesonline/ai-for-web-devs/blob/main/projects/6-mixture-of-models/app/services/prompts/message_classifier.j2",
        "No problem! When I was tinkering with it, the router using matrix factorisation seemed to be the best in terms of cost-performance tradeoff"
    ]
},
{
    "submission_id": "1gou0ec",
    "title": "License Plate reader - how to implement in python?",
    "selftext": "Hello, was wondering how to leverage ML and AI to make a program that can read the license plate of an image and detect and use the data of the numbers for something else?\n\nThanks in advance ",
    "created_utc": "2024-11-11T06:54:40",
    "num_comments": 2,
    "comments": [
        "First, take your big problem and dissect it to little problems: \n\n\\-You need to detect cars (YOLO will be useful for this)\n\n\\-You need to detect a licence plate inside those car rectangles (YOLO)\n\n\\-Finally, use some OCR to translate the image into numbers and letters.",
        "Any libraries or tools for the last part?"
    ]
},
{
    "submission_id": "1gotr9r",
    "title": "Customized learning path for ML AI",
    "selftext": "Hi All, I am a part of observatbility team where I take care of monitoring our own and customers infrastructure. We use multiple tools and technologies to do the same. Now by mid next year I have to integrate all my tools to machine learning. For example nagios, graphite data source, grafana , AWS cloudwatch and x ray , azure log analytics, splunk and the list is endless. So I am just trying to figure out if should start learning ML from scratch or do I need some customized training plan or should I just focus on some aspects of it. Basically we have a lot of monitoring data sourceslike , cloudwatch, graphite, SQL, Loki, Prometheus, telegraf etc . Please suggest ",
    "created_utc": "2024-11-11T06:43:13",
    "num_comments": 7,
    "comments": [
        "What do you mean by “integrate all my tools to machine learning”?",
        "I’m building a tool that can help! You can use your data, provide a simple problem description and I can create an ML model for you which you can easily integrate into your code. Feel free to check us out: https://plexe.ai\nWe’re building an SDK that will make your ML model available for you to call in few lines of code. :) Happy to chat further!",
        "It means that 99% of what they're trying to do has nothing to do with ML and the remaining 1% shouldn't be done using ML.",
        "What I mean by integrating our tools with machine learning is to leverage predictive analytics and anomaly detection across our observability stack. The goal is to enhance proactive monitoring by spotting trends or potential issues before they impact systems. For example, I’d like to explore how we might use historical data from CloudWatch, Grafana, and other monitoring tools to build predictive models that alert us to potential performance degradations or unusual patterns.\nI realize ML might not be the only or even the primary tool for all use cases, but with the directive to implement ML in our monitoring processes, I’m trying to get a sense of where it could add value without adding unnecessary complexity. Any advice on how best to prioritize my learning for this kind of applied ML in observability?",
        "For now yes it has nothing to do with ML, but by mid next year it will be",
        "CloudWatch and Splunk already have some built in AI/ML “insights”. The others probably do too. Are you wanting to combine metrics from all of those sources and build an anomaly detection model based on the combined metrics?",
        "Yes correct, and maybe predictive analysis"
    ]
},
{
    "submission_id": "1goqen8",
    "title": "Help me with on-premise ML model deployment for batch predictions ?",
    "selftext": "I need to deploy a `.pkl` model for batch predictions in a setup where code is pushed to GitLab, SQL/pyspark is used for data, and cron jobs handle scheduling. Docker, Kubernetes, and cloud are not allowed. this is on-premise setup. What are some best practices or approaches for this kind of deployment?",
    "created_utc": "2024-11-11T03:49:59",
    "num_comments": 0,
    "comments": []
},
{
    "submission_id": "1gop4g3",
    "title": "Introduction to Machine Learning in Julia",
    "selftext": "As we dive deeper into ML using Julia, I am excited to share the latest lecture in our \"Introduction to Machine Learning in Julia\" series on. **Here is the lecture:** [https://youtu.be/N2Jur-SHyaI?feature=shared](https://awd0j.r.a.d.sendibm1.com/mk/cl/f/sh/1t6Af4OiGsF30hUBRhPM827erXn4FS/YUyXgIwSOHk2)\n\nhttps://preview.redd.it/g6jd8buvy80e1.png?width=1280&format=png&auto=webp&s=b39007b5c54320d0b7f0894b79d183eba33a8efc\n\nThis session focused on mastering the building blocks of Julia programming, which are essential as we progress toward more complex topics.  \n  \n**Here is what I cover:**  \n 1) Conditional Statements – Writing if-else structures to make decisions in your code.  \n 2) Loops – Using for and while loops to automate repetitive tasks.  \n 3) Functions – Creating reusable blocks of code, including anonymous functions for quick operations.  \n 4) Data Structures – Exploring arrays, tuples, dictionaries, and sets, and how they make data handling efficient in Julia.  \n  \nWe also discussed set operations, variable scope, and common operations like push, pop, and delete on arrays.  \n  \n These concepts might seem basic, but they are crucial for building confidence and fluency in Julia. If you are coding along with me, you are laying the groundwork for the data science, machine learning, and deep learning topics that are coming soon.  \n  \n What’s Next?  \nIn the next lecture, I will introduce you to Pluto Notebooks, Julia's interactive and intuitive notebook environment. It is packed with features and is perfect for visualizing and sharing your work.  \n  \n **Find the full lecture here:** [https://youtu.be/N2Jur-SHyaI?feature=shared](https://awd0j.r.a.d.sendibm1.com/mk/cl/f/sh/1t6Af4OiGsFVLPgbq3VHIIBUFMlnxW/lr8oAN9gTUuu)  \nJulia may feel niche compared to Python, but its power in scientific computing and machine learning is unmatched. If you have made it this far, you are already on the path to mastering Julia!  \n  \nThis course is aimed at both students exploring ML and industry professionals looking to build impactful ML applications. Julia may be niche, but for anyone serious about scientific ML, it is a powerful tool to add to your skill set.",
    "created_utc": "2024-11-11T02:22:27",
    "num_comments": 3,
    "comments": [
        "Why tho?",
        "Julia is cool but I've been wondering what can it that python can't? The autodiff though llvm code seemed compelling, but I  couldn't think of or find any uses to it that I couldn't do with pytorch",
        "It’s also hardly ever used in the industry compared to Python"
    ]
},
{
    "submission_id": "1gooypq",
    "title": "Need suggestion regarding splitting a small data set and tunning hyperparameters for building a risk prediction/detection model",
    "selftext": "Hi everyone, I am using a small data set of 500 instances to build a predictive system. I am confused about whether I should split the the data set into three sets as I also want to tune hyperparamters of the ml model on the validiation set. My goal is to build a reliable model which provide good prediction performence. I am comparatively new in ML and mostly learning on my own so I seek your valuable guidance.\n\nSo my queries are-\n\n1. what is the best approach to split a small data set?\n2. Should we always do hyperparameter tunning? Also should we only perform hyperparameter tunning on the validiation set?",
    "created_utc": "2024-11-11T02:10:43",
    "num_comments": 3,
    "comments": [
        "Your dataset is too small to do a triple-split, I think.\n\nLook up nested cross-validation for hyperparameter tuning? It’s a two-layer process that separates hyperparameter tuning from model evaluation to avoid data leakage. \n\nThe outer loop splits the data into k folds, with each fold acting as an independent test set while the remaining k-1 folds serve as training data. \n\nWithin each training set of the outer loop, an inner loop of m folds is used to tune hyperparameters, selecting the combination that performs best on inner folds. \n\nThe model is then retrained on the full outer training set with the best hyperparameters and evaluated on the outer test fold.\n\nRinse and repeat across all outer folds, average.",
        "Thank you",
        "Actually I forgot mention that I have done feature selection on the training set and have done hyperparmeter tuning on the selcted features using Grid Search , is this approach wrong?"
    ]
},
{
    "submission_id": "1gonupt",
    "title": "maths for machine learning",
    "selftext": "I'm an a levels graduate, and I'm very interested in learning machine learning, but even on the first lecture of Andrew Ng, I have already stumbled upon some maths that I haven't learned, and since I have a half year break before my university starts, Im willing to learn, however I want to avoid learning too many unnecessary details of the maths as my main focus here is machine learning, do you guys have any recommendations? ",
    "created_utc": "2024-11-11T00:48:10",
    "num_comments": 23,
    "comments": [
        "It depends on what you’re interested in; let’s say you are keen on Neural Networks: in this case to perform simple tasks you need to know the concept of gradient of a function, how to perform derivatives (partial and not). If you want to build a simple NN I think this is sufficient. \nThe math behind NN isn’t that complex imo, the methods to regularize, propagate errors, building networks are much more important to understand; this is what I think :) \n\nSVM (support vector machine) for example needs a little more complicated math concept that are Lagrange multipliers. \n\nI don’t know how deep your math knowledge is, but if you want to build a simple ML algorithm try to search about K-nn: may be a good way to start.",
        "https://mml-book.github.io/",
        "Imperial college’s Mathematics for Machine learning on Coursera is a great place to start. It should also help for your computational calculus and lin alg classes in university. Also the book “all of statistics” by wasserman is a good initial reference to stats.\n\nYou might want to try this by Andrew Ng and the deep learning.ai team: https://skills.workera.ai\n\nIt will recommend resources to fill any gaps in knowledge.",
        "Learn linear algebra, statistics and calculus",
        "i would say linear algebra, matrix operation, and numerical optimization",
        "Maybe those could be of use: \n\n[https://www.youtube.com/watch?v=NtBb5PdHkWg&list=PLcPXq\\_8xLMgHYYjeL8VjiLHWz05rNDL\\_2](https://www.youtube.com/watch?v=NtBb5PdHkWg&list=PLcPXq_8xLMgHYYjeL8VjiLHWz05rNDL_2)  \n[https://www.youtube.com/watch?v=NtBb5PdHkWg&list=PLcPXq\\_8xLMgHupm4NZNgVqJcsdiFO1RE1](https://www.youtube.com/watch?v=NtBb5PdHkWg&list=PLcPXq_8xLMgHupm4NZNgVqJcsdiFO1RE1)",
        "You should consider the IITM s youtube lecture on Machine learning practices by Dr Arun Rajkumar. \n\n\nhttps://youtube.com/playlist?list=PLZ2ps__7DhBbA_e6_G3FI-BA1f7lCINUu&si=lfa09mWjogDmXWsi",
        "I'm on the same path and am currently doing [this specialization](https://www.coursera.org/specializations/mathematics-for-machine-learning-and-data-science) of DeepLearning, it has three courses: linear algebra, calculus and probability and statistics. After that I plan to complete the other courses of DeepLearning like the ML course",
        "See john cron ytube channel for maths stuffs",
        "I want to be able to at least understand the maths used in Andrew Ngs lecture, I've watched first two lectures so what I have encountered so far is like Hessian matrix (which I have no idea wut is that), and various subtle linear algebra identities that were used in the derivation, I think I get partial derivative, basically just with respect with whatever u are differentiating, and for my maths level, just I assume that I understand whatever thats in single variable calculus, and I have a basic understanding of probability, binomial normal, poisson and so on",
        "And he did use a probability density function of gaussian distribution, I know what's a probability density function, but idk how the PDF of gaussian is derived from",
        "I see thx for the recommendation 😃",
        "Studying the Hessian matrix is a way to discover if a stationary point (a point in which gradient of the function is 0) is a max, min or a saddle point. This is a concept, as well as Lagrange multipliers, belonging to calculus 2. I suggest to learn it, math is not complex in NN (and other models) but it’s used frequently, learning very basics concepts of Calc 2 will not be a loss of time. For example the part on differential form can be skipped, as the one on integration I guess, at least for building simple ML algorithms.",
        "Fine, I've found this: (https://www.youtube.com/watch?v=mnhrQxdzHqE). I've checked the given PDF very quickly, and it seems well structured. Unfortunately, there is a lot of math here, but it's not something you can avoid if you're interested in ML, especially if you want to build models on your own. Otherwise, you can use libraries that contain already-done models (see Keras for example).\n\nEdit: The book is also more advanced than required for you at the beginning. However, I can't find a book with only the basic knowledge...  \nConsider also asking ChatGPT for better or further explanations on math concepts, it can be very clear and helpful: the possibilities of getting wrong answers are near to 0 for this topics!",
        "what about all the linear algebra stuff 😭",
        "hey thx man, I rlly appreciate you going out of your way to search for suitable resources for me, I hope both sides of your pillow are cold tonight, have a great day man, I will check out the stuff u sent",
        "Ah yes, you may need also linear algebra, basic concepts such as hyperplanes or in general vector spaces. May sounds difficult at the beginning, especially if you’re not science graduate, but there are many videos in YouTube that help you to understand this concepts 😁.",
        "You're welcome! I needed to search for the meaning of that slang because I didn't know its meaning haha (English isn't my first language).   \n  \nIn any case, feel free to ask!",
        "bro do you have a list of what I need to learn I'm seriously lost, I'm sure I will encounter more maths that I do not know when I learn ML 😭🙏🏿",
        "hahahahahaha",
        "I learned Machine Learning during my master’s degree, I followed a course in my university so I don’t have particular experience or suggestions in YouTube channels about that. However I can search and come back here with some ideas (furthermore, surely someone will answer to this post with great suggestions on YouTube channels). \nDo you prefer channel suggestions on ML or math?",
        "I would prefer stuff on maths related to ml, but I don't think it has to be YouTube, it can also be books",
        "you'll probably want to have something with excercises (so I'd bet on books) - math is not a spectator-sport (no really: you need to work it out)"
    ]
},
{
    "submission_id": "1gomscn",
    "title": "Need Help for a ML project ",
    "selftext": "Hi there. I'm new to machine learning. I am in my final year of engineering in EC department.I have to make a major project on \"Phishing URL detection using ML and Deep learning\" I did some research on how to proceed with ML projects but I am not finding any success with my project. My dataset is huge with 235794 observations and 56 columns. This dataset is given by my project guide and unfortunately I can't change it. I have a project phase upcoming in 3 days. I want someone to help me do this project and give me the code. I know i am asking it for free but a little help would mean the world to me.🙏🙂 The link for the dataset is given above.",
    "created_utc": "2024-11-10T23:28:13",
    "num_comments": 12,
    "comments": [
        "You can hire someone on a freelancing platform.",
        "I can guide you on how to do it, but can't do it for you.",
        "Dare I say ask chatgpt. Not to build it for you of course, but to give you a few ideas to start from.\n\nI hope it goes without saying do *not* get it to write any of it for you.\n\nDon’t risk getting caught for plagiarism by getting someone / something to do it entirely for you",
        "I can help you, DM me!",
        "Good luck.",
        "Okay no problem just guide me. I will do the rest.",
        "Yuppp that's true. But atleast guide me. I am facing overfitting problem for my decision tree algorithm.",
        "Do you have text data?\nIf so you can start with TF-IDF and follow sklearn for binary classification. Start simple (logistic regression). Try to come up with features, like what helps you identify a phishing URL and try to create a feature based on that.\n\nThe next step could be BERT and family, if you don't get good results.\n\nAlso I can't see any link to your data",
        "https://archive.ics.uci.edu/dataset/967/phiusiil+phishing+url+dataset here's the dataset. I have a CSV file and I have jupyter notebook installed and I have to work in that IDE. I have done data cleaning process and I'm stuck in exploratory data analysis. I have plotted histogram of all the features and I don't know what to do further. I just know that some of the graphs are either left or right skewed. Just guide me what to do further.",
        "Upload the notebook to Google colab and share it with me. I can have a look.",
        "Okay wait. I will share it."
    ]
},
{
    "submission_id": "1goluq3",
    "title": "Need help. Model does not predict specific class ",
    "selftext": "https://preview.redd.it/rgektw33t70e1.jpg?width=2000&format=pjpg&auto=webp&s=d7be75b62d19210c309e375fb2cc26e32b53aa96\n\nHi. I'm a computer engineering student doing my thesis. My model utilizes DD-Net (Double Feature Double Motion Network) for Skeletal Action Recognition. By retrieving the key points of the person from YOLO-POSE I wanted to predict different actions based on the key points.  I am currently training my model on the training data that I have.\n\nThe results of the training produces this confusion matrix. I am confuse that I was able to all of the class except for two specific classes, which is \"push-ups\" and \"lateral shoulder raises\". I've tried debugging it but I still was not able to understand why the model predicted this way.\n\n  \nI would be grateful if any knows the answer or even some advice.",
    "created_utc": "2024-11-10T22:24:06",
    "num_comments": 15,
    "comments": [
        "Well I can explain lateral shoulder raises being confused with Jumping jacks, it needs some more information.\n\nBut the model confusing between pushups and sit-ups is extremely weird, can I take a look at the dataset?",
        "depends what is your dataset , maybe the dataset is imbalanced ie you need to add information in the dataset for those classes",
        "Are you sure you haven’t accidentally shifted labels in your training data set?",
        "Hi, I am a long time lurky and pretty new in doing stuff. What are the information I can provide. Also how do I give you the dataset? Is it an image or or through kaggle? Sorry for being inexperience and thanks for helping!",
        "Hi. These are the counts of how many instance of each classes that I have in my dataset. \n\n    bicep_curls: 538\n    dumbbell_rows: 580\n    dumbbell_shoulder_press: 541\n    jumping_jacks: 502\n    lateral_shoulder_raises: 502\n    lunges: 562\n    pushups: 583\n    situps: 556\n    squats: 569\n    tricep_extensions: 570\n\n\n\nRegarding the information of the dataset what kind of information do I need I simply followed the code that was shown on the DD-Net's repository.\n\n  \n[https://github.com/fandulu/DD-Net](https://github.com/fandulu/DD-Net)",
        "I've double checked it. Labels weren't shifted during or before training",
        "For example, if you are doing pose estimation on a single image, jumping jacks look really similar to lateral shoulder raises. Maybe adding a temporal element could help identify the difference. For pushups being mistaken as situps, I need to look at the images and your output of poses.",
        "lets see , maybe you could play around with the hyperparameters during training , and the evaluate , have you set up seeds for reproducing results ? , alot of this is empircal and i would suggest to log your experiments agressively ( wandb ) or comet or a csv file would do",
        "Hi, sorry for the late response, the algorithm that I'm using uses multiple pose estimation on an action. For example jumping jacks, the frames of the jumping jacks video is captured  and converted to pose estimations and it is then placed on DD-Net.\n\nHere is the visualization example of the data DD-Net is using.\n\nJumping Jacks: [https://i.imgur.com/MhSBt2B](https://i.imgur.com/MhSBt2B)\n\nLateral Shoulder Raises: [https://imgur.com/Ei2qwpO](https://imgur.com/Ei2qwpO)\n\nPushups: [https://imgur.com/EfFmriv](https://imgur.com/EfFmriv)\n\nSitups: [https://imgur.com/a/Kb4r3Fz](https://imgur.com/a/Kb4r3Fz)\n\n\n\nMaybe you are right. I just noticed it now that there are similarities between lateral shoulder raises and jumping jacks. This might cause the model to be inaccurate? How can I implement temporal element. \n\n  \nThe dataset that I'm using for training is this MM-Fit Dataset: [https://mmfit.github.io](https://mmfit.github.io) to be specific the 2D Pose Estimations only.",
        "I haven't really tried playing around with the hyperparameters. I'll do that to see if there's some changes on the testing and evaluation of the model. I'll try to log the details of the model. I haven't tried wandb or coment so I'll be learning on that. Thank you so much for the advice!",
        "Something is off about the jumping jacks, how is the y component of the feet almost always the same? Can you recreate all the animations with fixed x and y lims?\n\nThe temporal element also seems to be correct, but if you look at situps and pushups, there is this static point (probably a problem with the pose estimator), might cause problems.\n\nYou can also add a couple more features based on the angles between each two lines. For example, the angles of the knee bending can differentiate between situps and pushups with very high accuracy if they the model gets confused between them both only.\n\nYou can maybe add optical flow element as a list of vectors instead of a list of points. This can show where the movement is happening and help the model understand that instead of leaving the model understand movement itself.",
        "Will do. I'll recreate the animation with fixed x and y lims. Is this possible reason on why the model is inaccurate?",
        "Lots of reasons can lead to inaccuracies, try adding this angle thing and fixing the static point in the images and let me know if that helps. If it is doesn’t (i honestly am sure it should help), we can further discuss that. You can contact me on LinkedIn, I reply faster there. Here’s [my LinkedIn profile](https://linkedin.com/in/ziad-amerr/).",
        "Thank you so much for the help. Really having a hard time with this.",
        "Good luck, I am here to help :)"
    ]
},
{
    "submission_id": "1golog2",
    "title": "Extend graduation for ML experience or graduate ASAP?",
    "selftext": "I’m a CS major set to graduate in May 2025, but I’m considering extending to Dec 2025 to gain more ML experience. My ideal goal is to work as a Machine Learning Engineer or in Data Science, or to start a company with those skills, so I plan to eventually apply for an ML-focused Master’s degree since it seems to be the minimum for these roles.\n\nMost of my internships, coursework and projects so far have been in software and data engineering or systems programming. I've been exploring ML independently - e.g building a language model from scratch in PyTorch, reading research papers, and experimenting with LLM agents. But I know this likely won’t carry as much weight without related internships or research experience.\n\nMy dilemma:\n\n**1. Graduate in May 2025:**\n\n* **Pros**: Enter the job market sooner, likely in a role closer to SWE given my background.\n* **Cons**: Limited ML-specific experience might make it harder to stand out for ML roles or master’s programs - unless I can find ML opportunities while working as an SWE.\n\n**2. Extend to Dec 2025 for an AI/ML Internship or Research:**\n\n* **Pros**: Extra time could help me gain specialized ML experience to strengthen my profile for ML roles and grad school.\n* **Cons**: No guarantee I’ll secure any relevant positions, so it could just delay my entry into a full-time role.\n\nAny advice from people who’ve been in a similar situation? Should I prioritize gaining ML experience now, or would graduating sooner and entering the workforce be better? Open to other suggestions, too!\n\nThanks!",
    "created_utc": "2024-11-10T22:13:16",
    "num_comments": 5,
    "comments": [
        "Do you have a current job offer? Are you in current loops that are promising for ft or internships?",
        "If possible, I would wait until the last possible moment to make this decision.  My only reason being is that this field changes so much from month to month, and I think you will know exactly what to do when you get to that point.  <--old person's advice lol.  Unless you \\*have\\* to make the decision now...",
        "If you want to extend it, extend to get research experience. It puts you in prime place to go either for a masters or for a job as well.",
        "Don't have a job offer yet but I am in the loop for a couple of jobs. I'm also in loop for some ML or data science related internships.",
        "If you get a job offer I would take it because you never know if you will get one later. If you get a good internship offer maybe extend even with an offer if you know the company has good conversion rate"
    ]
},
{
    "submission_id": "1golfl2",
    "title": "Which of the following model is better and why??",
    "selftext": "[model 1 - predicting win percentage](https://preview.redd.it/pcjq05puo70e1.png?width=1864&format=png&auto=webp&s=b5e4858695bd07b2c776f99de5127b2cf52f54fe)\n\n\n\n[predicting total number of wins](https://preview.redd.it/3o2uuqcxo70e1.png?width=1885&format=png&auto=webp&s=2e980624459f4678d9470947f1b30448e5e28b56)\n\nI did this because in the dataset, for year 1981, there were less number of games played than other years. \n\nAlso, does it make sense to keep L (# of losses) in the model??\n\nOh forgot to mention, this is Lahman's baseball dataset, teams.csv. for years from 1947 - 1989.\n\nI was trying to predict total number of wins, or most important variables for increased chances of winning.\n\nI want to know, which of the above two models is better than the other, if that's the case? Which model should I prefer over the other?  \n",
    "created_utc": "2024-11-10T21:58:19",
    "num_comments": 1,
    "comments": [
        "Wouldn't MAE, MAPE be better for this case to evaluate performance of model"
    ]
},
{
    "submission_id": "1goknp5",
    "title": "I am beginner and I want to get started with machine learning ",
    "selftext": "There are lot of resources online, so I am confused which one should I start can someone suggest me resources.",
    "created_utc": "2024-11-10T21:11:00",
    "num_comments": 21,
    "comments": [
        "Post 1 million like this. Try searching for the others",
        "For beginners in machine learning, start with free resources like Google’s Machine Learning Crash Course and Kaggle, which offer interactive tutorials and hands-on projects. Coursera provides high-quality courses from universities, such as Andrew Ng’s ML course on Coursera, which is highly recommended for foundational understanding. For practical applications, DataCamp and StrataScratch offer focused, project-based learning. Additionally, YouTube channels like StatQuest and 3Blue1Brown provide accessible explanations. These platforms cover a range of learning styles, from theory to hands-on practice, so you can find one that suits your goals and budget.",
        "Roadmap.sh and good luck 🍀",
        "There are plenty of resources out there, try to google or ChatGPT some options.  \n  \nIf you are really interested in generative AI, you can learn a lot from playing with ComfyUI and Stable Diffusion - and different workflows. It will teach you things from the outside without the math. You will see how you can influence the model output, and you might even train you own small additional models (LoRa) to be able to generate characters looking like you.  This is a very different approach - But will give you lots of insight before you delve into the deeper theoretical stuff.",
        "Can you search on reddit? I think it's a pretty useful feature.",
        "Ask chatgpt",
        "3blueonebrown - maths  \nstatquest - statistics",
        "\"Hands on Machine learning with Scikitlearn kears and Tensorflow\" book is all you need",
        "take any machine learning book.\ntake the index page. \nsearch for topic on the internet, you will find links to good articles on medium.com or youtube videos (like statquest). \nafter learning the concept, do a hands on using real world data maybe from kaggle competitions.",
        "yeah this hsa been posted a million times on here.",
        "I have been working on a list of [AI courses, books and tutorials](https://github.com/duncantmiller/ai-developer-resources) that might be helpful. Most of them are free so they could be a good way for you to get a sampling of what is out there and where you want to focus your energy. Its an open source repo so please submit any new resources you find!",
        "Start with free resources such as Google or YouTube to start your machine learning journey.",
        "What is your background? Are you familiar with programming and algebra?",
        "My dog want to learn machine learning too",
        "ISBN 3636363636\n\"How to talk to your dog about Machine Learning\"",
        "Yeah I did that it was great",
        "Highly recommended",
        "This ^",
        "isn't tf going obsolete? Asking as a beginner too.",
        "As a beginner that’s simply not your problem to worry about. It’s like wanting to learn stock trading but being worried that intel stock is down.",
        "Thank you for clarification."
    ]
},
{
    "submission_id": "1gokiqv",
    "title": "AI algorthim that would be best suited for this tile game (rules in comments)",
    "selftext": "",
    "created_utc": "2024-11-10T21:02:58",
    "num_comments": 3,
    "comments": [
        "This probably doesn’t require a machine learning algorithm to solve, you can just use some kind of back tracking algorithm.      \nThe first step before using machine learning is to ask yourself if you NEED to use machine learning",
        "This is a tile game. The goal of the game is to remove all the tiles on the board. The tiles are arranged in a row and col as well as half rows and half columns. (This creates a pyramid/brick like stacking and more complexity to the game, since tiles can be half or quarter covered by tiles above them). You click visible tiles and add them to a bank that can hold up to 7. Once three matching tiles are in the bank they all get removed. If you fill the bank without making a match you loose. Tiles are visible if there are no tiles overlapping them from above. There's a special section called stacks and stacks are stacks of tiles face down and the last tile is face up. When you move the top tile to the bank of a stack it automatically reveals the next tile underneath. There is a multiple of three tiles of each color. Colors are how you match each tile.\n\nI'm trying to create an AI that can learn advanced strategy or optimal probability to solve this game. I was looking into Alphazero as I think it would well suit here, but it seems very complicated so I wanted confirmation that this is the right track to go down before I implement it",
        "RL"
    ]
},
{
    "submission_id": "1gojuvh",
    "title": " Using sub-classes as anchors and classes as positives and negatives  in a Siamese network with triplet loss?",
    "selftext": "I’m experimenting with a Siamese network using triplet loss to categorize sub-classes into broader classes. My setup differs from traditional triplet loss models: It involves using the sub-class as the anchor and the broader class as the positive (where the sub-class fits) and a different class as the negative (where it doesn’t fit). The goal is to position each sub-class embedding closer to its relevant class and farther from unrelated classes. Would this architecture make sense for capturing context-dependent relationships between sub-classes and classes? Are there any limitations I should be aware of?\n\n",
    "created_utc": "2024-11-10T20:25:06",
    "num_comments": 0,
    "comments": []
},
{
    "submission_id": "1gojun5",
    "title": "GenAI Interview Questions: RAG framework, part 4",
    "selftext": "In the 4th part, I've covered GenAI Interview questions associated with RAG Framework like different components of RAG?, How VectorDBs used in RAG? Some real-world usecase,etc. Post : https://youtu.be/HHZ7kjvyRHg?si=GEHKCM4lgwsAym-A",
    "created_utc": "2024-11-10T20:24:46",
    "num_comments": 1,
    "comments": [
        "Lolwut!!!!\n\n> \"What is ***THE*** rag framework?\"\n\n?!?!?\n\nThe only right answer should be \"There are many - which one do you have in mind\""
    ]
},
{
    "submission_id": "1goi46h",
    "title": "💡 How to evaluate LLMs and identify best LLM Inference System",
    "selftext": "📜 User experience and therefore the performance of LLM model in production is crucial for user delight and stickiness on the platform. Currently, LLMs are evaluated using metrics such as TTFT (Time to first Token), TBT (Time between Tokens), TPOT (Time Per Output Token) and Normalized Latency. Introducing a Etalon for evaluating optimal runtime performance. The summary of the research paper by authors of Etalon is in the article below:\n\n🔗 Link: [https://vevesta.substack.com/p/choose-llm-with-optimal-runtime-performance-using-etalon](https://vevesta.substack.com/p/choose-llm-with-optimal-runtime-performance-using-etalon)\n\n💕 Subscribe to my newsletter on substack ([vevesta.substack.com](http://vevesta.substack.com/)) to receive more such articles",
    "created_utc": "2024-11-10T18:48:29",
    "num_comments": 0,
    "comments": []
},
{
    "submission_id": "1gohc3l",
    "title": "Personalized AI advisor",
    "selftext": "**GoalAdvisor**\n\n**Are you learning ML or trying to skill up in ML, and looking for personalized advice?**\n\nI'm excited to share [GoalAdvisor](https://goaladvisor.app/), a tool am developing to help you break down your goals, stay organized, and track your progress with AI-powered advisor. Whether you're getting into AI/ML, growing your expertise, advancing your career, or just managing personal growth, GoalAdvisor is here to help!\n\n* **Personalized AI-driven roadmaps**—tailored to your goals and milestones.\n* **Breakdown & task organization**—we help you split complex goals into actionable tasks.\n* **Progress tracking**—visualize your journey and stay motivated along the way.\n* **Built by an ML Applied Scientist**—my focus is to help more people dive deeper into AI/ML and reach their full potential.\n* 🚀 **Free early acces**s—the first 10 people who join the waitlist will get exclusive early access!\n\n[Join Waitlist](https://goaladvisor.app/)\n\nI’d love for you to give it a spin and share your thoughts!\n\nPS: Have been an Applied Scientist at a FAANG company myself, and would be happy to chat and guide you in your journey through AI/ML/CV.",
    "created_utc": "2024-11-10T18:07:29",
    "num_comments": 10,
    "comments": [
        "Thanks. I am interested and signed yp",
        "Sounds cool! I signed up.",
        "I signed up, Looks promising to me. I would really like to know the tech stack you have for this tool",
        "Signed up! Looks cool. I would like to know more about the tool's backend engine.",
        "Very interesting. I signed up for it, can’t wait to try it out.",
        "Awesome! Thanks for signing up. Will reach out to you soon, and share access to GoalAdvisor!",
        "Awesome! Thanks for the sign up.  \nWill share the access soon. Would love to hear any feedback.",
        "Thanks for the sign up, and encouraging words. Will share access soon, and would love to hear any feedback you have.",
        "Thanks for the sign up.  \nAccess coming to you soon!",
        "Awesome! Thanks for signing up.   \nWill share access soon, and would love to hear any feedback."
    ]
},
{
    "submission_id": "1gogh7d",
    "title": "“Seeking 1:1 ML/Data Science Mentor from India or Pakistan for Project Guidance”",
    "selftext": "“Looking for a 1:1 ML/Data Science mentor from India! I’m facing challenges in building my project portfolio and need guidance from an experienced trainer to mentor me and help improve my ML skills through hands-on projects. Any recommendations or contacts would be greatly appreciated!”",
    "created_utc": "2024-11-10T17:22:46",
    "num_comments": 1,
    "comments": []
},
{
    "submission_id": "1gobtis",
    "title": "When does one have too little data for a model ",
    "selftext": "Hello!  \n\nI am currently doing a project on a data set. The original data set has 244 data points, with 14 features. Its a binary prediction, so just one feature. There seems to be a decent bit of differentiation for one of the categorical features, Region after doing some expirimenting with T-test and Chi-sqaured. So i decided to split them into two schemes, one for each region. Issue is, now I have 122 points for each. The models do really really well (90+ accuracy and 0.92 AUC) after stratifiedkfold and cross val. Other tests (i am trying different ML methods) are literally hitting 1.00 AUC each time, so I'm a bit concerned that I have too little data. Want to keep using the data set since I already did a fair amount of research into it, so any advice? ",
    "created_utc": "2024-11-10T13:44:52",
    "num_comments": 2,
    "comments": [
        "For something like regression analysis, the general rule of thumb is to have 10 data points per predictor variable, at minimum.",
        "Go Bayes and check your models with cross validation."
    ]
},
{
    "submission_id": "1gobejy",
    "title": "Model-dependent preprocessing - how to set up configs?",
    "selftext": "I'm trying to test out a variety of model types/architectures on some datasets.\n\nI've been using hydra to manage configs and pytorch-lightning to set up the training/test runs.  The issue I'm running into is that there are preprocessing steps that would be best handled by the dataloaders (on the CPUs) but which depend on the model.  Sometimes details of these steps are variable (can be considered model hyperparameters).\n\nI want to save this config as part of the model config so that when I load a trained model, I'm guaranteed to automatically load the necessary parameters (would be bad if, say, I ran test data through a model and accidentally did different normalization than was done during training).\n\nHowever, I want these transforms to be applied by the dataloader.  The solution I've come up with at the moment is to do something like this:\n\nIn the training loop:\n- initialize the datamodule based on data config\n- initialize the model based on model config and pass in the datamodule\n    - the model init code then sets various options / adds transforms to the datamodule\n\n\nI think this can work, but I wanted to ask people more experienced whether there is a different (more standard?) way to approach this problem?",
    "created_utc": "2024-11-10T13:26:42",
    "num_comments": 0,
    "comments": []
},
{
    "submission_id": "1go9vrz",
    "title": "Probabilistic Timeseries Classification Question",
    "selftext": "I've been working on a little project with data I recorded from my FitBit. I've been labelling the activities walking, running, driving, working out, and stationary and collecting pedometer step count, heart rate, and moment statistics of the accelerometer every 30 seconds.\n\nI implemented a little classification model by converting pedometer step count and heart rate into p(feature | activity) histograms and using multinomial logistic regression to learn a model of p(activity | accelerometer features). Then using the logistic regression as a prior and updating the probability with the step count and heart rate assuming those to be conditionally independent. This works well enough, but only achieves around a 90% accuracy due to some hard to differentiate activities. In particular driving and stationary. I get near 100% if I treat it as a time series and only swap activities if the past 2 activity predictions were the same, but I would like to learn a more principled approach. I am constrained by the FitBit hardware to use simple models so I would like to stay within the realm of simple probabilistic and linear models. My only experience with light weight models for time series prediction are with HMMs, which seem appropriate except for the input of the logistic regression which does not map neatly to an emission probability.\n\nAm I on the right track with pursuing an HMM solution to this? Is there another type of model I should be considering?",
    "created_utc": "2024-11-10T12:21:33",
    "num_comments": 0,
    "comments": []
},
{
    "submission_id": "1go9baz",
    "title": "MAE vs RMSE/MSE",
    "selftext": "I am deciding on evaluation metrics for a dataset related to house prices to answer the research question \"Can we predict house prices based on given features?\". Am I correct in deciding to use MAE over RMSE/MSE because:\n\n1. MAE produces an error in the same units as the original output, i.e. GBP m\\^-2. MSE does not at all, while RMSE slightly scales it which is still different in the end.\n2. MAE does not exacerbate errors; given that the scenario of house prices is fairly risk-free, emphasising larger errors by giving them a greater penalty is not very beneficial.\n\nI am also planning to use R\\^2 and R as a general way of measuring the models' goodness-of-fit in order to evaluate my model selections on a high-level.",
    "created_utc": "2024-11-10T11:57:11",
    "num_comments": 0,
    "comments": []
},
{
    "submission_id": "1go7pl4",
    "title": "Implemented AlphaZero and created the ultimate X and Os playing agent with Godot",
    "selftext": "I used the AlphaZero algorithm to train an agent that would always play X and Os optimally. You can check out the code on my GitHub [here](https://github.com/K3dA2/AlphaZero/tree/main). I tried to make the code as modular as possible so you can apply it to any board game you want. Please feel free to reach out if you have any questions or suggestions 🙏🏾",
    "created_utc": "2024-11-10T10:48:14",
    "num_comments": 17,
    "comments": [
        "It's missing possible winning moves though...",
        "Bazooka to kill a housefly. I love it! Great learning project!",
        "It should have won the 5th game, but it didn't play correctly.",
        "AlphaZero for tic tac toe?",
        "Nuked the ant but it's still alive",
        "But can[ it do](https://jdsemrau.substack.com/p/evaluating-consciousness-and-reasoning) [5x5, 8,8, 16x16](https://jdsemrau.substack.com/p/unlocking-insights-from-11-million)?",
        "Shall we play a game?",
        "Isn’t tic tac toe impossible to lose if you play optimally",
        "How did you do",
        "I did something similar with a feed-forward network and actor-critic self-play https://gabrielkeith.dev/projects/tictactoe. It's not using Godot though.",
        "If I remember right, AlphaZero is a MCTS based algorithm. Missing wins will come from things like small sample size or just unlucky sampling. The only one who can tell us is OP.",
        "Thanks 🫡",
        "Just noticed that. You’re right. Guess it still needs to train longer",
        "Yes it's a solved draw",
        "[deleted]",
        "Even a really small neural network can approximate minmax tic-tac-toe without search.",
        "Chess hasn’t been solved yet, maybe someday tho",
        "[deleted]",
        "You're not interpreting this correctly. Zermelo's theorem, when applied to chess, simply tells you that one of those three outcomes must occur for any given game state, depending on whether a given position is clearly advantages (or not) for either player. \n\nIt does *not* satisfy the conditions for a solution where, given any game state, you have to be able to outline a concrete optimal strategy to force a particular outcome or, at the very least, be able to definitively tell whether a particular game state yields a win, draw, or loss with respect to whichever player (see the differences between a strong, weak, and ultra-weak solution to a game). \n\nChess is only partially solved in that, for certain positions or relevant variants, we have what might qualify as a solution - however, we are currently unable to verify a solution in any mathematical sense from all possible board states."
    ]
},
{
    "submission_id": "1go6tq7",
    "title": "Homemade GPT JS - a minimal (<300 lines) TensorFlow.js re-implementation of Karpathy's minGPT (Generative Pre-trained Transformer)",
    "selftext": "",
    "created_utc": "2024-11-10T10:10:15",
    "num_comments": 0,
    "comments": []
},
{
    "submission_id": "1go5b4d",
    "title": "Is my bayesian network correct?",
    "selftext": "This is the bayesian network I created for my assignment. The give question was *Diabetes can influenced by several factors, including obesity, and physical activity levels. Additionally high blood sugar level are a direct symptom of diabetes, while both diabetes and high blood sugar can lead to increate thirst*\n\nhttps://preview.redd.it/ueydrwcuv30e1.png?width=341&format=png&auto=webp&s=fdb66d9235b0ad167473e1c7e376d0fe7c77b2d3\n\n",
    "created_utc": "2024-11-10T09:05:22",
    "num_comments": 2,
    "comments": [
        "I think this makes sense given the question. One thing though:\n\n> both diabetes and high blood sugar can lead to increate thirst\n\nDoes this mean: \n\n* diabetes can lead to increased thirst in the absence of high blood sugar\n* high blood sugar can lead to increased thirst in the absence of diabetes, but diabetes cannot lead to increased thirst in the absence of high blood sugar.\n\nYou've graphed the former. If that's not an accurate interpretation, then to graph the latter you'd just eliminate the `(diabetes) -> (thirst)` relation, i.e. so diabetes' influence on thirst is moderated through its effect on blood sugar.",
        "Yes I think diabetes to thirst should be removed. I made a mistake. Thank you so much"
    ]
},
{
    "submission_id": "1go4jhs",
    "title": "How I Turned AI Magic into Weekend Savings: $300 and 20 Hours, Poof!",
    "selftext": "",
    "created_utc": "2024-11-10T08:32:23",
    "num_comments": 1,
    "comments": [
        "Congratulations! These are the types of personal success stories I love to see! Free, open-source technology empowering individuals to save time and money!"
    ]
},
{
    "submission_id": "1go36fu",
    "title": "I need beginner help getting started with LM Studio - the first models I downloaded won't load",
    "selftext": "I have a new M4 Pro Mac Mini (24GB), I installed LM Studio, and the LLM that it sets up as a demo (`Llama-3.2-3B-Instruct-4bit`) runs great! But I want to see what else it can do.\n\nFirst questions: How do I know what models will run well on this computer? Which models should I be looking at as the most interesting and the most capable for chatting, image generation, and maybe help with creative writing? Do I need to worry about a model from huggingface having a trojan in it?\n\nI created a HuggingFace account and looked at the 'Trending' models. I decided to try a 'text-to-image' model, so I downloaded `stable-diffusion-3.5-large-gguf` and tried to load it ... but it failed with `llama.cpp error: 'error loading model architecture: unknown model architecture: 'sd3'`\n\nSo next I downloaded `FLUX.1-dev-gguf` and tried to load it, and that failed with `llama.cpp error: 'error loading model architecture: unknown model architecture: 'flux'`\n\nBefore I waste more gigabytes of bandwidth downloading stuff I can't use - is there a way I can get these to run in the latest LM Studio? Or is there something I should be looking for that will tell me when I can't run something?\n\nThank you for your help!",
    "created_utc": "2024-11-10T07:31:21",
    "num_comments": 2,
    "comments": [
        "I have same errors. Don't know what's going on with LM Studio",
        "I've been told that LM Studio isn't meant to handle text-to-image models, and that I should use ComfyUI or Forge instead. Those programs will load a model and let me type in a prompt to generate images. Someone specifically recommended ComfyUI to me because it provides the most control, and because most new model architectures work with it first, but it can be complicated and daunting unless I watch a beginner tutorial first. (I haven't done that yet.)\n\nThe person also recommended a tool named Pinokio to help get the programs set up.\n\nThat's all I know so far! Good luck!"
    ]
},
{
    "submission_id": "1go2gtp",
    "title": "Learning from fitting polynomials with Linear Regression",
    "selftext": "Hi all,\n\nI am trying to deepen my understanding of Machine Learning by playing around with basic examples. I did some experiments and thought it might be interesting to share, and possibly have some discussions about the observations. I have some background and experience already, but there's most likely some errors in my assumptions below, so take it as a base for discussion instead of a textbook tutorial.\n\nMy starting point was trying to fit a linear regression model to noisy data of the function sin(x) using standard gradient descent. Nice to look at, but not very interesting:\n\nhttps://reddit.com/link/1go2gtp/video/xt31zzd7l10e1/player\n\nNext, let's try a quadratic function, a2 \\* x\\^2 + a1 \\* x + a0 (in case you're wondering: yes this is still linear regression, as the weights are still linear; got surprised by this at first too):\n\nhttps://preview.redd.it/c64ysvnzl10e1.png?width=640&format=png&auto=webp&s=e72542867b63e03382722cad41bedf727330c1e6\n\nIt completely diverges. Why? Well, thinking about it, it must be because in the new model function the new term causes the derivative to be much higher, which in turn massively increases the gradients for the parameter updates. Let's try a smaller learning rate to compensate.\n\nAs it turns out, I had to decrease the LR from 0.05 to 0.002 to get it to converge, a factor of 25. That's a big jump! Considering the derivative of x\\^2 is 2x and x goes up to 6 here, I expected a change of maybe 12x.\n\nhttps://reddit.com/link/1go2gtp/video/uq33ej1gn10e1/player\n\nStill, the loss doesn't get as low as in the linear case. Let's try a polynomial of degree three, which should naturally fit much better to one period of the sine function.\n\nThis time I had to decrease the LR only to 0.0001 (factor of 20) to get it to converge.\n\nhttps://reddit.com/link/1go2gtp/video/4x99fjaos10e1/player\n\nLook, the wiggle shows how excited the model is to learn! Just kidding, it's probably because the LR is just below the threshold to divergence, where the parameters jump between opposite sites of a valley.\n\nAt first, it looks like it converges to a bad solution in a local minimum, and that's what I assumed in the first run. However only after increasing the training length by a lot, you can see, that it eventually changes its shape to match the sine. \n\nBut it's so painfully slow! What can we do? I tried changing the initialization. So far, I deliberately chose zero for all parameters, to keep it as simple as possible. Changing it to the default Kaiming initialization, we get a much better start (most of the time):\n\nhttps://reddit.com/link/1go2gtp/video/i6jg5kr9z10e1/player\n\nBut after that, it's still as slow. I tried reducing the learning rate, to prevent the wiggling, so that it jumps down the valley more effectively. However, it only sped up the beginning it seems.\n\nThen, when I looked at the gradients, it became clear. The magnitude of the gradients was very quickly decreasing. This indicates that the loss landscape is very steep in the beginning (as the diverging and wiggling also show), but when it gets to the roughly correct area, it flattens by a lot, slowing down the progress.\n\nhttps://preview.redd.it/uv7m7x3ku20e1.jpg?width=640&format=pjpg&auto=webp&s=25761b84568319ba3a8e7b689d6904c77ac59bf9\n\nWhat can we do? What if we use a low learning rate in the beginning and then increase it? This sounds like a job for a learning rate scheduler, which is usually used to do the opposite, to decrease the learning rate in later stages to find a good minimum in spiky, \"roughly correct\" areas of the landscape.\n\nSurprisingly, I couldn't get an improvement with learning rate schedulers. Even when using a simple rule to update the LR at a fixed iteration step after the oscillating stopped (here: 0.0001 to 0.0005 @ step 150), a few steps later the gradients blow up.\n\nhttps://preview.redd.it/3qv8bqaap20e1.png?width=640&format=png&auto=webp&s=713b2f4488ae77846b7f15bea732cde958f06825\n\nI have no explanation for this, I assumed the landscape should be pretty flat and smooth at this point.\n\nThere is one last thing I didn't do yet: Normalize the features. Doing a simple \\[-0,5,0.5\\] scaling, gradient descent stabilized by a lot. I could turn up the LR to 0.7. However, there was one gradient still far from zero, that only slowly (but still a lot faster) converged to the best fit at zero. This was when I realized how important normalizing is in this case: Because we use a polynomial, there is a change in scale in the parameters in the order of value\\_range\\^degree\\_of\\_polynomial; so for the x\\^3 part we have a change of scale of 6\\^3=216. That's huge! So even when we scale to \\[-0,5, 0,5\\] there is a big change of scale left. Let's scale to \\[-1, 1\\]:\n\nhttps://reddit.com/link/1go2gtp/video/38u4eaqgx20e1/player\n\n Finally!!! Comparing to the initial attempt with 10k iterations, we get a visually almost perfect result after only \\~200 iterations, and even a complete convergence after \\~520 iterations. And some wonky numeric instability after that I guess ;)\n\nI hope you enjoyed this journey! There is nothing groundbreaking here, but maybe you learned a thing or gained a new perspective. For me, it really helped to internalize how and why we do certain things. One thing I realized is how hard it is to steer the training in the right direction. In this example, it was always obvious that the model can fit much better, and we could visually see how off it is and imagine how it needed to change. But in deep learning you cannot visualize anywhere near that clearly. The loss function barely tells anything in these cases; here, I additionally learned how useful it is to look at the gradients. But even that gets much more complicated with deep layers and tons of parameters. Still, I hope these simple experiments can give a bit more intuitive insight on how to practically train deep networks.\n\nAlso, [here is the (extremely) messy code of the final version](https://gist.github.com/ptoews/9752c22457ae97795b019285f7e8f01f). If you want to experiment yourself, I recommend starting from [this earlier version](https://gist.github.com/ptoews/0f1ddcf5ec0b70e5c9f51c7dec056f69) instead. The final version includes lots of stuff for the video animations and other automations because I was lazy.\n\nFinally, I would like to hear your thoughts. Maybe there are things I glanced over and you are wondering about, that we can discuss or continue to experiment. Some of you can probably explain the reasons for some of the observations. Also, there are probably some errors in my explanations, so I am looking forward to being corrected ;)\n\n\n\n",
    "created_utc": "2024-11-10T06:58:55",
    "num_comments": 0,
    "comments": []
},
{
    "submission_id": "1go24cp",
    "title": "Looking for Research Collaborations ",
    "selftext": "Hi,  \n  \nI’m seeking mentorship and collaborative research collaboration in AI, particularly in areas like Multimodal Machine Learning, AI agents, RAG, Knowledge Graphs, and Reinforcement Learning. With over five years of experience and 2 masters degree, I’m looking to contribute actively to projects aligned with my research interests and commit meaningful time to the work. I am eager to transition into research from industry\n\nI recently applied for PhD programs but didn’t get in, likely due to a lack of published work a gap I’m eager to fill through collaborative projects. I’m especially interested in memory-augmented models, reinforcement learning for adaptive agents, and enabling AI to perform structured, experience-driven reasoning across various data types.\n\nIf you’re working on projects in these areas or know someone who might be interested, I’d love to connect. I’m ready to contribute, discuss ideas, and start with small, focused tasks that could grow into impactful work.\n\nThanks for reading",
    "created_utc": "2024-11-10T06:42:07",
    "num_comments": 1,
    "comments": [
        "Hey I am on the same boat , I want to pursue PhD but I am well aware of the research gap in my resume , I am into multimodal ML and RL. We can prolly chat and see if we match and can setup a time line for publishing and research?!"
    ]
},
{
    "submission_id": "1go0yum",
    "title": "New to Machine Learning – Looking for Advice on Getting Started!",
    "selftext": "How did you get started with machine learning, and what do you wish you’d known earlier?",
    "created_utc": "2024-11-10T05:44:26",
    "num_comments": 1,
    "comments": [
        "Main thing is to focus on doing more projects… like more… of all scales"
    ]
},
{
    "submission_id": "1go0twf",
    "title": "[Dataset Request] Looking for Animal Behavior Detection Dataset with Bounding Boxes",
    "selftext": "Hi everyone,\nI'm a college student working on an animal behavior detection and monitoring project. I'm specifically looking for datasets that include:\n\nPhotos/videos of animals\nBounding box annotations\nBehavior labels/classifications\n\nMost datasets I've found either have just the images/videos without bounding boxes, or have bounding boxes but no behavior labels. I need both for my project.\nFor example, I'm looking for data where:\n\nAnimals are marked with bounding boxes\nTheir behaviors are labeled (e.g., eating, running, sleeping, hunting)\nPreferably with temporal annotations for videos\nLike in the photo given.\n\nHas anyone worked with such datasets or can point me in the right direction? \nAny suggestions would be greatly appreciated!\nThanks in advance!",
    "created_utc": "2024-11-10T05:37:03",
    "num_comments": 2,
    "comments": [
        "That's definitely some research to get around the 'Are you a human' checks to detect bots.",
        "What are you on about?\nI just asked for help finding some good dataset."
    ]
},
{
    "submission_id": "1gnzqty",
    "title": "I dunno what to do next",
    "selftext": "\nHi , I'm a clg student seeking to learn ml then dl. \nI know python , numpy, pandas, matplotlib and maths. I also did Andrew ng's ML specialization course. \n\nNow I'm stuck at this point where i dunno what should i do next. \nShould I learn eda , preprocessing or start learning ml algorithms?\nIf so, where and how can l learn to do these? \nI need your guidence guys. Please help me out.\nThanks in advance!\n\n( Edit : give some upvotes and make this post floating in top because it would be helpful for ppl like me)",
    "created_utc": "2024-11-10T04:37:27",
    "num_comments": 25,
    "comments": [
        "Immediately start \"Python for data analysis by Wes Mckinney\" - try to get the latest edition as many changes have been made. \n\nSince you already know numpy, pandas & matplotlib, not only it'd be a revision of all of those things from the creator of pandas himself but you'd also learn EDA techniques... \n\nthen you can do a comprehensive ML course like Uwaterloo intro to ML, or eecs 189 (recommended book is Deep Learning by Bishop which also covers little ML so make sure you're following it as well) or MIT 6.390 along with CS229 (Classic). Supplement this with books like PRML (Bishop), Pattern Classification by Duda & Hart, Understanding Machine Learning - From theory to Algorithms by Shai Ben-David. And if you still need more mathematical grounding then Probabilistic Machine Learning - Murphy. Remember you don't have to read all the books cover to cover as it'd take years, just study & learn what you need (keep them as reference).\n\nAll of the above would be theoretical + practical (as long as you're also following their practice sets & projects to implement). For a more practical treatment + if you aren't able to make projects yet, then take ML Zoomcamp by Datatalksclub (Join their slack & watch them on yt you'll understand everything).\n\nNow, time for deep learning - but one thing to note here is that although you can build some basic models as projects, you can't still deploy them. Before deep learning, you can do MLOps as well to cover that weakness. \n\nYou can still do Deep Learning if you're excited to dive into it but keep learning MLOps as well. For MLOps - \n\n1) MLOps Zoomcamp by Datatalksclub\n\n2) Made with ML by Goku Mohandas \n\nDo supplement the above choices with the book Designing Machine Learning Systems by Chip Hyuen (GOLDMINE). \n\nFor deep learning, MIT's intro to deep learning is freely available. So is CMU 11785 (personal favourite). Some books - Deep Learning (Ian Goodfellow), Understanding Deep Learning (Simon JD Prince) - especially for diffusion models & d2l.ai \n\n\nI feel now you won't feel as if you don't have enough things to do & you won't ask the question what to do next 😉",
        "Read cook books of ml and dl, build projects,  post them on github and keep learning about most used algorithms.\nI would suggest not to follow any yt channel as the field is very new and changes very fast any channel doesn't give that good Explanation",
        "Follow campux 100 days of ml, only this can save your ass",
        "i was in the same spot...\n\nsince you already know those libraries, try to sharpen them in platforms like kaggle. also try to learn some more libraries that deals with real ML/DL stuff life TF or pytorch. after doing some projects on kaggle try to follow some crash courses in YouTube. Ng's courses are good in a way that they provides a good idea about the concepts but lacks practical exposure. so try some youtube crash courses on deep learning. you can also follow the [book](https://udlbook.github.io/udlbook/) which beautifully explains everything in DL. But don't hang to the ML part for too long and expose yourself to NLP and transformers.",
        "CS229 is amazing if you miss Andrew’s lectures and wanna dive a bit deeper into ML. https://youtube.com/playlist?list=PLoROMvodv4rMiGQp3WXShtMGgzqpfVfbU&si=iKjKanCUGFH6v7BI",
        "exactly what i've been going through, i started a 100 days deep learning playlist but i feel like i don't know even know half of ML properly, and i'm like scared, i haven't even gotten a proper like good paying internship yet which scares me even more",
        "Depends what you would like to learn; if you'd like to understand ML, maybe you could look into the following book: \n\n  \n1. Machine Learning and Pattern Recognition (Bishop) \n\n2. Probabilistic Machine Learning An Introduction (Murphy)\n\n3. Deep Learning (Goodfellow et al.)\n\n4. Introduction to Reinforcement Learning (Sutton and Barto)",
        "you said you completed the specialization , but you want to start learning ML algorithms??",
        "Learn python",
        "Thoose were some brilliant suggestions bud. Thanks for that. I am wondering if you have a similar comment for someone building DL models on Genomic data. Thansk in advance!!",
        "I really can't thank you enough for sharing such a organised plan man!",
        "What do you mean by cook books? Like any specific book.\nAnd where can i learn the concepts ( algorithm, how to do ml projects, eda)",
        "Maybe it'll help me out. Thanks man!",
        "Yeah ok.\nAnd there are much notebooks and problems in kaggle.\nSo which one would be good for a beginner",
        "So what you do now",
        "Brother. Read…",
        "Don't have too much idea about genomics as I've personally never worked on them. For building deep learning models, the best course I've found is CMU 11785, also once you have a good understanding of the inner workings of CNN, RNN, transformers etc. from courses & books, you can check out these -\n\n1) https://pmc.ncbi.nlm.nih.gov/articles/PMC10649223/\n\n2) https://academic.oup.com/bib/article/25/3/bbae138/7640738\n\n3) For more resources - https://www.researchgate.net/publication/329192400_A_primer_on_deep_learning_in_genomics\n\n\nThere are similar papers you can find upon a google search + arxiv, I don't think any courses are that specific in nature considering your use case so you'd need to rely on research papers, that's my 2 cents!!",
        "All the best!! Happy Learning ✨",
        "Cook books are the books form which we learn how to code ml algorithms like the famous book hands on machine learning by aurelien geron and if you want to learn concepts of ml algorithms then you will have to prefer text  books like neural networks and deep learning by michael nielsen,  etc , Cook books will teach you how to do ml projects.",
        "He said he knows numpy, pandas, matplotlib but doesn't know EDA. So I thought he doesn't know anything properly",
        "Thanks man!! Glad you replied bud 😊",
        "Understood",
        "Read the first 3 words of the second sentence.",
        "All good 🙌",
        "I don’t wanna be cocky btw, just realized how this might come over…"
    ]
},
{
    "submission_id": "1gnz619",
    "title": "Structured outputs in LLMs: don’t put the cart before the horse",
    "selftext": "",
    "created_utc": "2024-11-10T04:02:29",
    "num_comments": 2,
    "comments": [
        "Good article but it belongs in r/LLMDevs instead of a subreddit about training machine models.",
        "Thank you. I didn't know that subreddit existed."
    ]
},
{
    "submission_id": "1gnz3nk",
    "title": "Fine-Tuning vs. Transfer Learning in Voice Synthesis - INGOAMPT",
    "selftext": "",
    "created_utc": "2024-11-10T03:58:23",
    "num_comments": 0,
    "comments": []
},
{
    "submission_id": "1gnyztx",
    "title": "Can data analyst skills along with AI/ML knowledge land me a role like data scientist in future? ",
    "selftext": "I currently work in a service based company and initially I had no guidance nor knowledge of the vast domains of data, so I ended up choosing a project as a fresher which I am not very satisfied with right now. \nIts a tool called Salsify, its a PXM ( Product Experience Management) tool. Basically an ETL tool only, but more advancements on catalogs, sites and insights reports. \n\nI was selected to help in a usecase for a hackathon on GenAI in the initial few months and that changed the whole course of my career. I fell in love with creating and applications and learning about the world of genai and ML. Since then, I continued to work in various GenAI hackathons and Pocs while working on Salsify. \n\nMy 18 months are over and now I have liberty to switch in the next 6 months and I am starting to prepare for interviews and make and CV. But I also am facing a dilemma.\nMy current billable project which is Salsify and my other works like GenAi are clashing in techstack. One is ETL and another AI. \nSo I looked into upskilling from PXM and found Power BI and data visualisation to be the best fit. It also seems I am gaining interest in data visualisation as well. \n\nSo ultimately, I will end up with skills or knowledge in AI and Power BI and data visualisation. \nWhat role fits this skillset the best? Data analyst or Data scientist? And what is the payscale difference in both? ",
    "created_utc": "2024-11-10T03:51:19",
    "num_comments": 3,
    "comments": [
        "Data analyst. Dabbling with LLM APIs isn't what data scientists or machine learning engineers do.",
        "Data analysis is a good stepping stone towards those careers when paired with a proper education. Data Analyst + self-learning ML probably will not get you there.",
        "No"
    ]
},
{
    "submission_id": "1gny2gs",
    "title": "Local image classification in the browser? ",
    "selftext": "I want to create a chromium extension, one of the main components of the extension is classifying images (think dynamic content filtering, a few different categories, one of which is recognizing inappropriate content).\n\nOriginally I wanted to use a multimodal llm to classify images, because they tend to do quite well at classifying images with little dev effort, but it seems like it won't be possible to my knowledge to get a local model working with the Chrome extension, and an api call for each image will be too expensive as my goal is for it to be free to use. \n\nSo next I looked into tensorflow mobile net, and tried this specific example:\n\n[https://github.com/tensorflow/tfjs-examples/tree/master/chrome-extension](https://github.com/tensorflow/tfjs-examples/tree/master/chrome-extension)\n\nAnd while it worked, it seemed to do poorly on most things(except tigers, it seemed to consistently recognize them well). ​Accuracy was far too low.\n\nAnyways I would like to hear opinions of people who are more knowledgeable in this field, what's the best solution to do a rough, but accurate classification of images with the least dev effort and runnable on a browser? Should I invest time experimenting with other tensorflow mobilenet models, or should I expect fairly low accuracy in them too? (I would like to as much as possible avoid investing in the effort of training my own custom model at this stage) ",
    "created_utc": "2024-11-10T02:48:44",
    "num_comments": 0,
    "comments": []
},
{
    "submission_id": "1gnwghj",
    "title": "Get access of almost all Machine Learning Courses from Coursera at $239 for one year.",
    "selftext": "**Offer Details:**\n\n* **Offer Dates:** Nov 7, 2024 — December 12, 2024 (26 days)\n* **Offer Discount:** 40% Off Coursera Plus Annual Subscription ($160 off)\n* **Limitations**: excluding IN, DE, spanish speaking LATAM\n\nStarting today, Coursera is offering a 40% discount on our annual Coursera Plus subscription. You can gain unlimited access to over 7,000 courses, including Professional Certificates from top industry leaders like Google, Meta, Microsoft, IBM, and more — all for just $239 (regularly $399) for 12 months. [Read Main article](https://medium.com/p/0dbed9b06f78).",
    "created_utc": "2024-11-10T00:50:26",
    "num_comments": 0,
    "comments": []
},
{
    "submission_id": "1gnvird",
    "title": "Questions for practice ",
    "selftext": "I have just completed mathematics for machine learning book and I am unable to find resources to practice questions. Can anyone suggest me some resources?",
    "created_utc": "2024-11-09T23:41:40",
    "num_comments": 1,
    "comments": [
        "Find undergraduate assignments and end exam papers related to subjects what you have learnt"
    ]
},
{
    "submission_id": "1gnv3x8",
    "title": "Epoch for GAN training",
    "selftext": "Hi, so i want to try learning about GAN. Currently I'm using about 10k img datasets for the 126x126 GAN model. How much epoch should i train my model? I use 6k epoch with 4 batch sizes because my laptop can only handle that much, and after 6k epoch, my generator only produces weird pixels with fid score of 27.9.\n\n",
    "created_utc": "2024-11-09T23:11:42",
    "num_comments": 23,
    "comments": [
        "these are way too noisy\n\nwhich probably means that your generator function is completely overwhelming your discriminator, and your discriminator loss is probably really high compared to the generator loss.\n\nyou should probably either step up your discriminator with a few more layers or test with a few dropouts in your generator\n\nafter that, it is all about hyperparameter-based testing, fiddling around with different combinations of parameters (like increasing/decreasing epochs, batch size, changing the learning rate for both the generator and discriminator individually, don't assign them the same variable right away) should yield you some coherent outputs. checkpoint your progress as you've done in this. the key is to find the balance between the discriminator and generator so that neither overpowers the other and both their losses are minimal.\n\nalso, why train locally for such a large dataset for such a large amount of epochs? (I'm just estimating it took you a long time, no clue what your GPU is but you said Laptop so I figured) \n\nif you wanna do it for so long why not just upload it on Kaggle and let the notebook run on its own online? meanwhile, you can do something else.",
        "sorry im a newbie, whats GAN?",
        "With lower batch size, use lower learning rate.",
        "What kind of GAN? DCGan? 128x128 is upper limit of DCGan afaik. Maybe try to make it work on 64x64 first.",
        "Your epoch and batch size is a hyper parameter, so you should be testing that as well and picking the best performing one",
        "This reminds me of a similar issue I had when I learnt how to train GANs for the first time. The problem is definitely your discriminator. So there’s a whole empirical study on how to make good discriminators, I’ve forgotten the exact name of the paper but it’s the dcgan paper. Once I used the results from that study to build my models they started working properly. I could give you my repo where I implemented DCGAN so you could check out how to implement a good discriminator if you want.",
        "ouch what a mess x)",
        "HAHAHAHAH I ONCE TRIED THIS BSSSS , SUCH A WASTE OF TIMEE!! START WITH SIMPLER MODELS IF YOU ARE A BEGINNER .",
        "I see, thank you for ur suggestions, im going to try it now. I was using Google Collab, but it took too much time to collect the data because i use duckduckgo for downloading the datasets. So i prefer local, I'll try using Kaggle next time. Thank you again for ur help",
        "I always wondered how to do hyperparameter testing with a huge dataset, a huge model, friggin long learning periods, huge number of epochs/steps... I mean, do I really have to wait a week or two to see if my added layer has any effect?",
        "[Generative Adversarial Networks](https://en.m.wikipedia.org/wiki/Generative_adversarial_network)\nIn this case, you get a model that tries to produce an image (generator) and a model that tries to distinguish whether the image is a real or a fake (discriminator) and train them by feeding the discriminator both the images produced by your generator and your dataset.\nWith time, your generator will end up producing images that look more real in its attempt to fool the discriminator, and also the discriminator will end up getting better at distinguishing real and fake images.",
        "Yes, it's a DCGAN. I was experimenting to create 64x64 and 128x128",
        "Thank you, I'll try to read the paper later.",
        "Simpler models such as ???",
        "Why not just download the dataset to collab directly? Use the Kaggle CLI or Python requests or something. No need to get public data to the server via your laptop",
        "probably work on a smaller subsection i guess.\n\ni don't have any experience like that since I don't work on enormous datasets and only a chunk, but once you figure out which epoch your loss skyrockets for the function you can kinda just work your way up to rectifying that epoch and look further.",
        "In that case I recommend using the same dataset from one of the implementations to get the same result with your implementation before switching to your own dataset.  ",
        "Among generative models, VAEs & Diffusion models are easier to train. You can use them as a kind of sanity check if you still intend to use GAN in the end.",
        "simpler ones where there is no concept of two models fighting against each other .. you can try segmentation , object detection , or something else from that zone...",
        "It's my first time trying to learn gan, so i thought why not collecting the data too. My bad",
        "What a bad advice and a shitty attitude (addressing your first comment). In what way does detection help with understanding generators-discriminators? These are completely different models. If OP wants to learn about SVMs, telling them to go back to logistic regression is nonsense - yes, one is simpler than the other, but learning one helps very little with learning the other. OP made a mistake and that's ok, it's part of learning.",
        "Two models fighting against each other is extremely common in unsupervised learning, so I'd actually recommend to learn them after learning how ML works in the first place. Stuff like DDPG (and it's children), GAN, Adversial VAE, (and the list goes on) use two (or more) models fighting against each other. Even if you can do it without two models some scenarios (like VAE) benefit a lot from a adversial loss instead of normal MSE.",
        "I said 'beginner' for a reason .\nAlso it's not that if he is training a model for detection, segmentation, generations the training methodology is gonna change .. at the beginner level we mostly just throw data at the model and hope for the loss curves to change . \nThe thing is that beginners don't know what a loss curve should look like , how does a nn behave .. and when there are two adversarial networks it all goes to shìt .. moreover Gans are known to be finicky to train .\n\nNow, what projects have you made ? Why don't you give better advice to the op, instead of correcting me ?"
    ]
},
{
    "submission_id": "1gnuwmo",
    "title": "[Help Needed] Looking for a Machine Learning Dataset",
    "selftext": "Hi everyone! I'm a student working on a machine learning project and I'm in need of a dataset. Ideally, I’m looking for a dataset that has a few thousand samples with around 15 features that I can preprocess and then use for training ML algorithms. I’ve received suggestions on general sources for datasets, but I’m looking for particular datasets that are well-suited for hands-on learning and experimentation. Any specific recommendations would be very appreciated!\n\nThank you in advance!",
    "created_utc": "2024-11-09T22:57:45",
    "num_comments": 3,
    "comments": [
        "Have you’ve tried hugging face?",
        "Kaggle",
        "For datasets, you could check out options like Kaggle, UCI Machine Learning Repository, or Google Dataset Search. They’ve got a good variety, and you might find something that fits the size and structure you’re looking for. Another idea: look into Interview Query’s [Machine Learning Projects](https://www.interviewquery.com/p/machine-learning-projects), where you'll find tailored projects that not only come with datasets but also help you build relevant ML skills."
    ]
},
{
    "submission_id": "1gnq81z",
    "title": "[Help] LSTM seq2seq generating same sequence ",
    "selftext": "[Kaggle Notebook](https://www.kaggle.com/code/dineshpabbi10/notebook9a205a8e55/edit)\n\nI am trying to implement seq2seq model in pytorch to do translation. The problem is model generating same sequence. My goal is to implement attention for seq2seq and then eventually moving to transformers. Can anyone look at my code (Also attached kaggle notebook) :\n\n    class Encoder(nn.Module):\n      def __init__(self,vocab_size,embedding_dim,hidden_dim,num_layers):\n        super(Encoder,self).__init__()\n        self.vocab_size = vocab_size\n        self.embedding_dim = embedding_dim\n        self.hidden_dim = hidden_dim\n        self.num_layers = num_layers\n        self.embedding = nn.Embedding(self.vocab_size,self.embedding_dim)\n        self.lstm = nn.LSTM(self.embedding_dim,self.hidden_dim,self.num_layers,batch_first=True)\n    \n      def forward(self,x):\n        x = self.embedding(x)\n        output,(hidden_state,cell_state) = self.lstm(x)\n        return output,hidden_state,cell_state\n    \n    \n    class Decoder(nn.Module):\n      def __init__(self,vocab_size,embedding_dim,hidden_dim,num_layers):\n        super(Decoder,self).__init__()\n        self.vocab_size = vocab_size\n        self.embedding_dim = embedding_dim\n        self.hidden_dim = hidden_dim\n        self.num_layers = num_layers\n        self.embedding = nn.Embedding(self.vocab_size,self.embedding_dim)\n        self.lstm = nn.LSTM(self.embedding_dim,self.hidden_dim,self.num_layers,batch_first=True)\n        self.fc = nn.Linear(self.hidden_dim,self.vocab_size)\n    \n      def forward(self,x,h,c):\n        x = self.embedding(x)\n        output,(hidden_state,cell_state) = self.lstm(x)\n        output = self.fc(output)\n        return output,h,c\n    \n    \n    class Seq2Seq(nn.Module):\n      def __init__(self,encoder,decoder):\n        super(Seq2Seq,self).__init__()\n        self.encoder = encoder\n        self.decoder = decoder\n      \n      def forward(self,X,Y):\n        output,h,c = encoder(X)\n        decoder_input = Y[:,0].to(torch.int32)\n        output_tensor = torch.zeros(Y.shape[0],Y.shape[1],FR_VOCAB_SIZE).to(device)\n        # output_tensor[:,0] = Y[:,0] # Set same start token which is \"<START>\"\n    \n        for i in range(1,Y.shape[1]):\n          output_d,h,c = decoder(decoder_input,h,c)\n          # output shape : (batch_size,fr_vocab_size)\n          decoder_input = torch.argmax(output_d,dim=1)\n          # output shape : (batch_size,1)\n          output_tensor[:,i] = output_d\n    \n        return output_tensor # ouput shape : (batch_size,seq_length)\n    \n    \n    class Seq2Seq2(nn.Module):\n      def __init__(self,encoder,decoder):\n        super(Seq2Seq2,self).__init__()\n        self.encoder = encoder\n        self.decoder = decoder\n      \n      def forward(self,X,Y):\n        output,h,c = encoder(X)\n        decoder_input = Y[:,:-1].to(torch.int32)\n        output_tensor,h,c = self.decoder(decoder_input,h,c)\n        return output_tensor\n    \n    encoder = Encoder(ENG_VOCAB_SIZE,32,64,1).to(device)\n    decoder = Decoder(FR_VOCAB_SIZE,32,64,1).to(device)\n    model = Seq2Seq2(encoder,decoder).to(device)\n    \n    lr = 0.001\n    optimizer = torch.optim.Adam(model.parameters(),lr=lr)\n    loss_fn = nn.CrossEntropyLoss(ignore_index=0)\n    epochs = 20\n    \n    for epoch in range(epochs):\n        running_loss = 0.0\n        progress_bar = tqdm(train_dataloader, desc=f\"Epoch {epoch+1}\", leave=False)\n    \n        for X, Y in progress_bar:\n            Y_pred = model(X, Y)\n          \n            # Y = Y[:,1:]\n            # Y_pred = Y_pred[:,:-1,:]\n            Y_pred = Y_pred.reshape(-1, Y_pred.size(-1))  # Flatten to (batch_size * seq_length, vocab_size)\n            Y_true = Y[:,1:]\n           \n            Y_true = Y_true.reshape(-1)  # Flatten to (batch_size * seq_length)\n           \n            loss = loss_fn(Y_pred, Y_true)\n            optimizer.zero_grad()\n            loss.backward()\n            optimizer.step()\n            \n            # Update running loss and display it in tqdm\n            running_loss += loss.item()\n            progress_bar.set_postfix(loss=loss.item())\n    \n        print(f\"Epoch {epoch+1}, Loss = {running_loss/len(train_dataloader)}\")\n      ",
    "created_utc": "2024-11-09T18:14:57",
    "num_comments": 6,
    "comments": [
        "Just to provide more context :\n\n\\- The loss does go down from 10 to \\~2.0 in 20 epochs but the outputs make no sense\n\n\\- An example output :\n\n  \nThank you ever so much  :  je de ton bouffe de sur fois fois fois fois",
        "Would help me a lot. Dealing with similar issue",
        "Train more epoch and use more data, use more wider and deeper model",
        "You should first check that the repetitive ending applies to like a few words or is it across a wide range of words. If the first case, maybe check your training corpus bc a repetitive ending means that your language model gives high prob for certain words probably due to their natural high frequency. Then maybe try removing those words and see if the same thing happens.\nTwo things you can try when it comes to seq2seq models are teacher forcing and attention.",
        "I have seen the same behavior with llama 3.2 models",
        "Architecturally and in pre-processing. Do you see any issues ?"
    ]
},
{
    "submission_id": "1gnnjkh",
    "title": "Best Resources & Advice for Getting Started in Machine Learning?",
    "selftext": "I’m planning to learn machine learning, but I’m at the start of my computer science degree and feeling a bit overwhelmed with all the options out there. I’d love some guidance on where to begin, especially since I want to do a masters in machine learning and I want to be a stand out applicant.\n\nSome questions I have:\n\n1. Should I focus on learning the math first, or dive into practical ML and learn the math as I go?\n2. What online courses or resources would you recommend?\n3. How can I improve my chances of enrolling onto a good Machine learning masters.\n\nThank you for your answers in advance :)",
    "created_utc": "2024-11-09T15:55:41",
    "num_comments": 3,
    "comments": [
        "Look.  \nThis question gets asked every, every day. Did you do a search to see other peoples responses to actually better crafted questions?  \n\n\nNo.   \nPut the big boy/girl pants on and do a little research yourself.",
        "It's probably a preference thing. I prefer to learn the theory first, then practice. Andrew Ng's machine learning specialization course on coursera puts things in simple words, which is the most useful approach for me. You can find other courses too. for the 3rd question I have no answer.",
        "Check out my list of online AI courses here, I've been keeping this as I've been learning over the past few years: [https://github.com/duncantmiller/ai-developer-resources](https://github.com/duncantmiller/ai-developer-resources)"
    ]
},
{
    "submission_id": "1gnjobu",
    "title": "Algorithm suggestions for tracking noisy measurements?",
    "selftext": "I'm tracking a value in time with noisy measurements and am interested in knowing both the estimate of the underlying value at a given instant in time, as well as the estimated error in that value at each instant in time. (Essentially, value plus-minus error, both as a functions of time).\n\nFor example, if the real value did a step function in time, the measured value would have some transition as it jumps to the new value, and during that transition, the error would spike.\n\nI've been trying a Bayesian linear dynamic system with a Kalman filter. (It's possible that I've implemented this wrong) but it seems to get increasingly certain, even when horribly wrong. Any suggestions for good algorithms to use for this type of problem?\n\nAlso, the measurement noise is gaussian and I know about what its distribution is if that helps at all.",
    "created_utc": "2024-11-09T12:52:46",
    "num_comments": 0,
    "comments": []
},
{
    "submission_id": "1gnja1p",
    "title": "Correlation of columns ",
    "selftext": "",
    "created_utc": "2024-11-09T12:34:34",
    "num_comments": 0,
    "comments": []
},
{
    "submission_id": "1gnj6ss",
    "title": "compare two roles",
    "selftext": "hi guys, i got an offer from a small insurance company which is data scientist, which will be working on predict customer behavior and fed into risk equation(will include deployment and monitoring) but i think this role is lack of work life balance. My current role is machine learning engineer and mainly working on research and proof of concept using genAI like GPT, big insurance company with great work life balance. The offer i got is about 10% higher than my current role, please share some advice as I'm struggling to make a wise decision",
    "created_utc": "2024-11-09T12:30:17",
    "num_comments": 2,
    "comments": [
        "Your current job sounds like more fun too.\n\nTake the offer to your current manager and ask if they can match it. If they won't at least make a counter-offer then you can assume you're not going to go any higher where you are and you can either take the DS job, or hang around and casually look for somewhere else and take the better job when it comes along.",
        "Comp is temporary, skills and career trajectory is forever."
    ]
},
{
    "submission_id": "1gniryc",
    "title": "What does a volatile test accuracy during training mean?",
    "selftext": "While training a classification Neural Network I keep getting a very volatile / \"jumpy\" test accuracy? This is still the early stages of me fine tuning the network but I'm curious if this has any well known implications about the model? How can I get it to stabilize at a higher accuracy? I appreciate any feedback or thoughts on this. ",
    "created_utc": "2024-11-09T12:11:05",
    "num_comments": 47,
    "comments": [
        "I think it’s volatile only in comparison to train accuracy. 0.02 is not that much of a difference in my opinion. Maybe it’s jumpy because your test sample is much smaller than train, so the noise is more apparent",
        "Actual question for somebody with more ML engineering experience than me: is it actually jumpy? It is jumpy relative to training which to me intuitively makes sense. But just as an absolute independently looking at test accuracy it looks like 63.5 +-1.5%, which is not that jumpy imo.",
        "Your entire graph spans 0.05, this is actually quite stable",
        "Please start ALL your graphs at 0.",
        "Try using a smaller learning rate. You could also add some dropout/regularisation. Lastly, Don't forget to use data augmentation, if you can.",
        "How many samples in your test set? You can expect to see more volatility with a small test set.",
        "Do yourself a favor and use AUROC along with AUPRC instead of Accuracy. Accuracy is a hard metric to diagnose.",
        "Can you draw the standard deviations of both? Maybe draw the training as a boxplot? Might just be that your test-set is rather small. The \"jumps\" are within 2%, depending on how much data you have I wouldn't call this jumpy.",
        "Accuracy will always be jumpy, because it is not continuous. What does the loss look like?",
        "because you have your graph super zoomed in",
        "“Jumpy” could mean stuck at local optima\nIf training is smooth but Val is jumpy, then maybe your model is at a local optima that has low generalizability. Overfit \n\nHave you implemented CV?",
        "Gradient clipping",
        "Size of test set? Small means more volatile. \nAre you using CV, if so over your many folds? Maybe you're making too large changes over small validation sets. \nHow large is your learning rate? Stable training accuracy says likely not an issue, but just to be sure. \nPotentially overfitting? The better training accuracy with unstable test could be a sign of this. \nBalanced classes? If they're not then accuracy isn't the metric to look at.\nGood split of classes between test and train? Make sure you're evaluating data points sampled independently from the same distribution (as much as possible).",
        "It is actually good it is not jumpy, the graph is misleading",
        "I'm confused how your test accuracy could be higher than your train accuracy in the beginning. May be worth a look",
        "A perfect example of \"look at your axes carefully\" :)",
        "The only anser one can give is that without proper context it is impossible to really tell.  \nThat being said as others have stated as well this is not a high volatility unless you for some reason zoom in like this on purpose.",
        "It is a bit jumpy. Try normalizing your data and adjust the learning rate (try lower learning rate or even a dynamic one). This chart alone doesnt provide enough insight on how to improve the performance. But these 2 mentioned reasons are the most common.",
        "Possibly you’re underfitting, I’d check the loss curve to further investigate that. \nYour training or test accuracy is not volatile as y is between .6 and .65. Always set the y from 0 to 1 for accuracy plots.\nLastly, it’s not a good practice to make use of the test data while training. You should create a validation set and use that to validate your training approach and make changes. At the very end you can use the test set for the final evaluation.",
        "This means your model is probably under-fitting, which can happen due to a number of reasons: insufficient data, model too simple/complex (in case of small datasets), choice of parameters, etc. If you know you don't have a large amount of data you can do data augmentation to increase the amount or make your model a bit less complex by reducing the number of layers in the Neural Network. Some other suggestions also mentioned in the comments here like adjusting the learning and adding dropout/regularization might help. Usually it's recommended to start with a relatively small learning and adjust it along with other hyperparameters as you go.",
        "i think you have some other issues too .. other than the jumpiness , the test acc. is essentially the same till the end ... also the train accuracy isn't really showing a huge diff ...\n\ni have a feeling , that your test data isn't really a good representation of the entire data .. other wise you would expect to see the test acc increase slightly at least .. or maybe the classes are imbalanced?",
        "Good observation so it may be more stable than I'm thinking.",
        "Assuming the graph was on a scale of 0-1 and looked the same as above, what would that mean / what would the implications be?",
        "also are they window or lifetime NEs?",
        "It isn't jumpy. op said he has 1200 samples in the validation set.. That's like 20 samples.",
        "Yeah thats what I'm realizing now. Gonna expand window.",
        "Noted",
        ">add some dropout/regularisation\n\nThat was gonna be my next step. Wasn't sure if this type of volatility could help point in the right direction in terms of what type of regularization to try.",
        "About 1200 samples in the test set. About 10,000 in the training.",
        "I'm confused on how to interpret AUROC. Accuracy is easier to interpret but I'll definitely look into it. Thank you.",
        "Surprisingly much more stable and smooth. How should I interpret that?",
        ">This means your model is probably under-fitting,\n\nThats a good thing then right because this implies that my accuracy can potentially go up?",
        "Yeah good point. Test accuracy should be improving, but it’s relatively constant. OP should try more iterations and see if the test accuracy improves",
        "It is more stable than you think. Plot the graph with the loss going from 0 to 100 and you will see the test dataset is kind of “tracking” the train dataset",
        "Set your ylim between 0 and 1.",
        "You just need to return to tradition (qualitative tests)",
        "a 1% variation in your accuracy between your training and validation isn't crazy. Considering that your validation is 1200 samples, that's 12 examples on average.",
        "That number of observations could be pretty much. But also not that much.\n\nIt also depends on the number of features and their complexity, so their distribution / number of unique values. With few unique values I mean features with nominal scale for example (vs. continuous).\n\nAlso the scale of your learning curve graph is also a little misleading here, like others already said.",
        "You can think of auroc as a class balanced rank order. A bigger number means that you're more likely to properly categorize groupings if 0 and 1 given your continuous predictive method. Accuracy requires a cutoff where is auroc does not.",
        "You can go [there](https://en.wikipedia.org/wiki/Precision_and_recall). My favorite wikipedia page. Extremely clear about all the ratios and scores that you can compute with a 2 x 2 tables with Labels (0 or 1) and Prédiction (0 or 1).\n\nI gave some explanations in another comment that was commenting one of your comment.",
        "If loss is stable, then the training process is stable. The issue with accuracy is that small changes around the threshold will lead to large jumps in accuracy (moving from 0.49 to 0.51 with a 0.5 threshold will have maximum impact on accuracy). You could add more data to the test set for accuracy to stabilize. Beyond that I would not worry about the accuracy jumping. ",
        "Under-fitting and Over-fitting are never good for models, however like you said the accuracy can go up if you can find and fix the cause why it's happening.",
        "What's a qualitative test?",
        "To complete your answer, (I am sure you know but to explain to OP), when computing the ROC curve, your script will compute the Sensitivity (a.k.a Recall, a.k.a True Positive Rate) and the Specificity (a.k.a True Negative Rate) for every possible thresholds and plot the TPR vs the False Positive Rate (1 - TNR). The area under the ROC curve gives you a way the interpret the global discriminative power of your model. Usually the best threshold is the one corresponding to the most upper left part of the curve.\n\nThe PR curve will compute the Recall (a.k.a Sensitivity, a.k.a TPR) and Precision (a.k.a Positive Prédictive Value) for every possible threshold and plot them.\n\nThe point is the ROC curve isn't influenced by your class imbalance. Whereas the PR curve is. What is the best threshold to use ? Well it depends on your problem.\n\nWhat is the most important thing to you ?\n If you want to detect all positive cases, meaning 0 false negatives, then you need to have a low threshold, but you will have a high number of False Positive.\nIf you want as few False Positve as possible, then you want a high threshold,  you will have a high number a False Negative though. But the few that you predict Positive will be quite confident.\n\nPlease compute also the F1 score and/or the Matthews correlation coefficient. Extremely easy to do on sklearn, which I guess you are using. The wikipedia page and Precision Recall is extremely well done.",
        "Looking at the actual model outputs for the test set and seeing if it makes sense to you. IMO simple auto metrics like loss and accuracy are mostly just good as a sanity check to make sure your model is training correctly.",
        "Thank you for the breakdown",
        "You are welcome. I understand it can be confusing at first and a bit overwhelming but it is quite easy to implement. \nI also advice you to look at confusion matrix. Extremely easy to compute and plot with sklearn. It will give you more details about your numbers of True Positive, True Negative, False Positive and False Négative. But you need to specifiy a threshold for that. \n What I do is that I use a for-loop to compute the F1-score, Matthews-score etc. for 100 thresholds: from 0 to 1 with a 0.01 step. And for each score I select the threshold value maximizing that specific score. And then I plot the confusion matrix for this score. Gives you a better understanding of your model's performances."
    ]
},
{
    "submission_id": "1gnik86",
    "title": "Back propagation correct step",
    "selftext": "# \n\nHello I am trying to figure out the formula of the partial derivatives of loss of MSE (mean square error) with the parameters of a single layer multi perceptron network. There are m input neurons, n hidden neurons. Weights of the hidden layer is defined as m x n matrix. bias of the hidden layer n- dimensional vector. weights of the output matrix n x k matrix. bias of the output neuron is a k dimensional vector.\n\nGiven the above dimensions, I define a true output matrix Y which is a matrix that holds true value of Y for all samples. I have N samples and the number of output neurons is n, thus resulting in an n x N matrix. The same can be developed for Ypred which represents our prediction values.\n\nDefine a loss function based on MSE as the sum of (Yi - Ypredi)\\*\\*2.\n\nOne can easily see that derivative d (MSE) / d (Ypred) = 2/ N (Ypred - Y).\n\nNow in trying to find d(MSE) / d(Wo), where W0 represents weights of the output layer. I tried to use chain rule to simplify d (MSE / d(Wo) = d(MSE) / d(Ypred) X d(Ypred) / d(W0). And Ypred = W0 \\^ T \\* Xh + B0.... where Xh is output of the hidden layer and B0 represents the bias which includes the value b0 vectors as column N times. However I am stuck here, because derivative of a Matrix (Ypred) with a matrix W0 is a tensor right. How do I simplify the above relationships and continue with the other parameters.\n\nAny help even with only the answers will be appreciated thanks....",
    "created_utc": "2024-11-09T12:01:26",
    "num_comments": 0,
    "comments": []
},
{
    "submission_id": "1gnfb6o",
    "title": "Math behind Diffusion Models",
    "selftext": "Does anyone have any good resources that explains the math math behind diffusion models crystal clear? ",
    "created_utc": "2024-11-09T09:34:57",
    "num_comments": 11,
    "comments": [
        "If you understand PyTorch, here’s a good tutorial explaining how it works along with code:\nhttps://huggingface.co/learn/diffusion-course/en/unit1/3\n\nOr here’s a pretty good source with some math explanations:\nhttps://michaelwornow.net/2023/07/01/diffusion-models-from-scratch",
        "Basically this.\n\nhttps://arxiv.org/abs/2208.11970",
        "Imo the [flow matching ](https://arxiv.org/abs/2210.02747) interpretation is easier to understand than the [sde](https://arxiv.org/abs/2011.13456) version. For a time discrete version [this](https://arxiv.org/abs/2208.11970) is as good it gets.",
        "I read many resources when I was learning about diffusion and for me the best introduction is this one: https://arxiv.org/abs/2208.11970\n\nIt derives everything from scratch and makes a connection to variational autoencoders",
        "For math and intuition behind diffusion models, I can't recommend \"Understanding Deep Learning book by Simon JD Prince\" - It has a freely available pdf, look into diffusion model sections. Personally, I haven't yet seen such a comprehensive & intuitive approach to this subject, definitely worth checking out!!",
        "Imo read Lilian Wengs blog post.",
        "**I think you already understand the concepts and just want the math...**  \n  \n**1. Forward Diffusion Process**\n\nThe forward process gradually adds Gaussian noise to the data over TTT time steps. Starting from real data x0∼q(x0)\\\\mathbf{x}\\_0 \\\\sim q(\\\\mathbf{x}\\_0)x0​∼q(x0​), the process is defined as:\n\nq(x1:T∣x0)=∏t=1Tq(xt∣xt−1),q(\\\\mathbf{x}\\_{1:T} | \\\\mathbf{x}\\_0) = \\\\prod\\_{t=1}\\^T q(\\\\mathbf{x}\\_t | \\\\mathbf{x}\\_{t-1}),q(x1:T​∣x0​)=t=1∏T​q(xt​∣xt−1​),\n\nwhere each transition is a Gaussian distribution:\n\nq(xt∣xt−1)=N(xt;1−βtxt−1,βtI),q(\\\\mathbf{x}\\_t | \\\\mathbf{x}\\_{t-1}) = \\\\mathcal{N}(\\\\mathbf{x}\\_t; \\\\sqrt{1 - \\\\beta\\_t} \\\\mathbf{x}\\_{t-1}, \\\\beta\\_t \\\\mathbf{I}),q(xt​∣xt−1​)=N(xt​;1−βt​​xt−1​,βt​I),\n\nwith βt∈(0,1)\\\\beta\\_t \\\\in (0, 1)βt​∈(0,1) being a variance schedule.\n\nAn important property is that xt\\\\mathbf{x}\\_txt​ can be directly obtained from x0\\\\mathbf{x}\\_0x0​:\n\nq(xt∣x0)=N(xt;αˉtx0,(1−αˉt)I),q(\\\\mathbf{x}\\_t | \\\\mathbf{x}\\_0) = \\\\mathcal{N}(\\\\mathbf{x}\\_t; \\\\sqrt{\\\\bar{\\\\alpha}\\_t} \\\\mathbf{x}\\_0, (1 - \\\\bar{\\\\alpha}\\_t) \\\\mathbf{I}),q(xt​∣x0​)=N(xt​;αˉt​​x0​,(1−αˉt​)I),\n\nwhere:",
        "**3. Training Objective**\n\nThe model is trained to minimize the variational bound on negative log-likelihood:\n\nEq\\[−log⁡pθ(x0)\\]≤Eq\\[DKL(q(xT∣x0)∣∣p(xT))+∑t=1TDKL(q(xt−1∣xt,x0)∣∣pθ(xt−1∣xt))−log⁡pθ(x0∣x1)\\].\\\\mathbb{E}\\_{q} \\\\left\\[ -\\\\log p\\_\\\\theta(\\\\mathbf{x}\\_0) \\\\right\\] \\\\leq \\\\mathbb{E}\\_{q} \\\\left\\[ D\\_{\\\\mathrm{KL}}(q(\\\\mathbf{x}\\_T | \\\\mathbf{x}\\_0) || p(\\\\mathbf{x}\\_T)) + \\\\sum\\_{t=1}\\^T D\\_{\\\\mathrm{KL}}(q(\\\\mathbf{x}\\_{t-1} | \\\\mathbf{x}\\_t, \\\\mathbf{x}\\_0) || p\\_\\\\theta(\\\\mathbf{x}\\_{t-1} | \\\\mathbf{x}\\_t)) - \\\\log p\\_\\\\theta(\\\\mathbf{x}\\_0 | \\\\mathbf{x}\\_1) \\\\right\\].Eq​\\[−logpθ​(x0​)\\]≤Eq​\\[DKL​(q(xT​∣x0​)∣∣p(xT​))+t=1∑T​DKL​(q(xt−1​∣xt​,x0​)∣∣pθ​(xt−1​∣xt​))−logpθ​(x0​∣x1​)\\].\n\nHowever, this objective can be simplified. Ho et al. (2020) proposed a training loss that focuses on predicting the noise added to x0\\\\mathbf{x}\\_0x0​:\n\nLsimple=Et,x0,ϵ\\[∥ϵ−ϵθ(xt,t)∥2\\],L\\_{\\\\text{simple}} = \\\\mathbb{E}\\_{t, \\\\mathbf{x}\\_0, \\\\boldsymbol{\\\\epsilon}} \\\\left\\[ \\\\left\\\\| \\\\boldsymbol{\\\\epsilon} - \\\\boldsymbol{\\\\epsilon}\\_\\\\theta(\\\\mathbf{x}\\_t, t) \\\\right\\\\|\\^2 \\\\right\\],Lsimple​=Et,x0​,ϵ​\\[∥ϵ−ϵθ​(xt​,t)∥2\\],\n\nwhere:\n\n* ttt is sampled uniformly from {1,…,T}\\\\{1, \\\\dots, T\\\\}{1,…,T},\n* ϵ∼N(0,I)\\\\boldsymbol{\\\\epsilon} \\\\sim \\\\mathcal{N}(\\\\mathbf{0}, \\\\mathbf{I})ϵ∼N(0,I),\n* xt=αˉtx0+1−αˉtϵ\\\\mathbf{x}\\_t = \\\\sqrt{\\\\bar{\\\\alpha}\\_t} \\\\mathbf{x}\\_0 + \\\\sqrt{1 - \\\\bar{\\\\alpha}\\_t} \\\\boldsymbol{\\\\epsilon}xt​=αˉt​​x0​+1−αˉt​​ϵ,\n* ϵθ\\\\boldsymbol{\\\\epsilon}\\_\\\\thetaϵθ​ is the neural network predicting the noise.\n\nThis loss function trains the model to predict the noise component ϵ\\\\boldsymbol{\\\\epsilon}ϵ given the noisy input xt\\\\mathbf{x}\\_txt​ and the time step ttt.",
        "**4. Variance Schedule and Parameterization**\n\nThe variance schedule βt\\\\beta\\_tβt​ controls how much noise is added at each step. Common choices include linear or cosine schedules.\n\nThe mean μθ\\\\boldsymbol{\\\\mu}\\_\\\\thetaμθ​ is parameterized in terms of ϵθ\\\\boldsymbol{\\\\epsilon}\\_\\\\thetaϵθ​:\n\nμθ(xt,t)=1αt(xt−βt1−αˉtϵθ(xt,t)).\\\\boldsymbol{\\\\mu}\\_\\\\theta(\\\\mathbf{x}\\_t, t) = \\\\frac{1}{\\\\sqrt{\\\\alpha\\_t}} \\\\left( \\\\mathbf{x}\\_t - \\\\frac{\\\\beta\\_t}{\\\\sqrt{1 - \\\\bar{\\\\alpha}\\_t}} \\\\boldsymbol{\\\\epsilon}\\_\\\\theta(\\\\mathbf{x}\\_t, t) \\\\right).μθ​(xt​,t)=αt​​1​(xt​−1−αˉt​​βt​​ϵθ​(xt​,t)).\n\nThis parameterization simplifies training and sampling, as the model predicts the noise directly.\n\n**5. Sampling Procedure**\n\nTo generate new samples, start with xT∼N(0,I)\\\\mathbf{x}\\_T \\\\sim \\\\mathcal{N}(\\\\mathbf{0}, \\\\mathbf{I})xT​∼N(0,I) and iteratively apply the reverse transitions:\n\nFor t=T,T−1,…,1t = T, T - 1, \\\\dots, 1t=T,T−1,…,1:\n\n1. Predict ϵθ(xt,t)\\\\boldsymbol{\\\\epsilon}\\_\\\\theta(\\\\mathbf{x}\\_t, t)ϵθ​(xt​,t).\n2. Compute μθ(xt,t)\\\\boldsymbol{\\\\mu}\\_\\\\theta(\\\\mathbf{x}\\_t, t)μθ​(xt​,t).\n3. Sample xt−1\\\\mathbf{x}\\_{t-1}xt−1​ from N(xt−1;μθ(xt,t),σt2I)\\\\mathcal{N}(\\\\mathbf{x}\\_{t-1}; \\\\boldsymbol{\\\\mu}\\_\\\\theta(\\\\mathbf{x}\\_t, t), \\\\sigma\\_t\\^2 \\\\mathbf{I})N(xt−1​;μθ​(xt​,t),σt2​I).\n\nThe variance σt2\\\\sigma\\_t\\^2σt2​ can be fixed or learned, but often a fixed variance simplifies the process.\n\nGood luck!",
        "* αt=1−βt\\\\alpha\\_t = 1 - \\\\beta\\_tαt​=1−βt​,\n* αˉt=∏s=1tαs\\\\bar{\\\\alpha}\\_t = \\\\prod\\_{s=1}\\^t \\\\alpha\\_sαˉt​=∏s=1t​αs​.\n\n**2. Reverse Diffusion Process**\n\nThe reverse process aims to reverse the noising process by modeling:\n\npθ(x0:T)=p(xT)∏t=1Tpθ(xt−1∣xt),p\\_\\\\theta(\\\\mathbf{x}\\_{0:T}) = p(\\\\mathbf{x}\\_T) \\\\prod\\_{t=1}\\^T p\\_\\\\theta(\\\\mathbf{x}\\_{t-1} | \\\\mathbf{x}\\_t),pθ​(x0:T​)=p(xT​)t=1∏T​pθ​(xt−1​∣xt​),\n\nwhere p(xT)=N(xT;0,I)p(\\\\mathbf{x}\\_T) = \\\\mathcal{N}(\\\\mathbf{x}\\_T; \\\\mathbf{0}, \\\\mathbf{I})p(xT​)=N(xT​;0,I).\n\nEach reverse transition is parameterized as:\n\npθ(xt−1∣xt)=N(xt−1;μθ(xt,t),Σθ(xt,t)).p\\_\\\\theta(\\\\mathbf{x}\\_{t-1} | \\\\mathbf{x}\\_t) = \\\\mathcal{N}(\\\\mathbf{x}\\_{t-1}; \\\\boldsymbol{\\\\mu}\\_\\\\theta(\\\\mathbf{x}\\_t, t), \\\\Sigma\\_\\\\theta(\\\\mathbf{x}\\_t, t)).pθ​(xt−1​∣xt​)=N(xt−1​;μθ​(xt​,t),Σθ​(xt​,t)).\n\nIn practice, the covariance is often fixed, and the model focuses on learning the mean μθ\\\\boldsymbol{\\\\mu}\\_\\\\thetaμθ​.",
        "Instead of copy and pasting from ChatGPT, why not just say “Ask ChatGPT”? I agree that ChatGPT can massively help with understanding math and machine learning, but copying its output including the LaTeX math formatting that clearly doesn’t render on Reddit is useless."
    ]
},
{
    "submission_id": "1gnel4g",
    "title": "SOTA architecture to build image classifiers which depend on text shown in pictures",
    "selftext": "I am currently trying to build a simple multi class image classifier. I want to use a pretrained model for image embeddings. However, to reliably differentiate the classes of my task, the model also needs to take into context the text/numbers displayed in the image. The number of texts per image to be classified is not fixed in size.\n\nMost vision encoders have a fairly small input size, which makes text intelligible for the model, requiring the need to extract the required text using a different approach, for example using OCR tools.\n\nMy idea would be to run a detection + recognition OCR tool and then embed the recognized text, using a text encoder and then add positional embeddings based on the bounding box location in the image.\n\nHowever, given the \"n\" embedded texts + the embedded image, what would be the best way to combine them then and feed them into a classifcation head, for example?\n\nIn general, is the approach I am trying to take feasible or are there any other ones which I can apply which ensure that the text in the image is taken into account, in addition to the general image structure?\n\n  \nThank you guys in advance!\n\n",
    "created_utc": "2024-11-09T09:02:32",
    "num_comments": 0,
    "comments": []
},
{
    "submission_id": "1gndqic",
    "title": "The dynamics of SGD",
    "selftext": "Hello,\n\nI have a background in pure mathematics, and I would like to understand better the dynamics of stochastic gradient descent (SGD), for example speed of convergence, guarantees of convergence, continuous approximations of SGD... but in the stochastic case, that is, not just classical convex optimization where the objective function is fully known.\n\nWould you have any recent references to get up to date? I would prefer recent papers. Thank you very much",
    "created_utc": "2024-11-09T08:24:32",
    "num_comments": 0,
    "comments": []
},
{
    "submission_id": "1gnbk4n",
    "title": "Have troubles with classificator, need an advice ",
    "selftext": "Hello everyone.\n\nI have a question. I am just starting my journey in machine learning, and I have encountered a problem.\n\nI need to make a neural network that would determine from an image whether the camera was blocked during shooting (by a hand, a piece of paper, or an ass - it doesn't matter). In other words, I need to make a classifier. I took mobilenet, downloaded different videos from cameras, made a couple of videos with blockages, added augmentations and retrained mobilenet on my data. It seems to work, but periodically the network incorrectly classifies images.\n\nQuestion: how can such a classifier be improved? Or is my approach completely wrong?",
    "created_utc": "2024-11-09T06:43:02",
    "num_comments": 4,
    "comments": [
        "Training with larger dataset will help",
        "Do you have to use a neural network? I don't know how difficult the task is as that depends on when you consider the camera to blocked, but traditional approaches could be sufficient and maybe even more stable, assuming that your training dataset is too small.\n\n\nYou could try some threshold based approaches based on the edge density. I would assume that blocking the camera would reduce the edge count.   \n\n\nAlternatively, you could simulate blocked images to expand your dataset. For example, crop out blocking objects, like hands, to create masks. Then, overlay these masks onto a diverse dataset like ImageNet, but also include the original unmasked images. This approach would allow you to create a large, varied dataset with both blocked and unblocked images, all without needing additional labeling.",
        "I try to use classic approaches but they do not work well and sometimes they are sensitive to scene. I am using some augmentation to extend the dataset",
        "\"Some augmentations\" could be anything and differ a lot from creating a synthetic dataset."
    ]
},
{
    "submission_id": "1gnbgta",
    "title": "Want to learn ML",
    "selftext": "It's almost 120 days into ML. I only learned basic terminology and basic statistics and am applying the ML library to do projects, but I want to learn ML properly(Math).\n\nMy teacher suggested me this course( [https://www.coursera.org/specializations/machine-learning-introduction?utm\\_medium=sem&utm\\_source=gg&utm\\_campaign=b2c\\_india\\_machine-learning-introduction\\_stanford-deeplearning.ai\\_ftcof\\_specializations\\_arte\\_sep-23\\_dr\\_sem\\_rsa\\_gads\\_lg-all&campaignid=20594446971&adgroupid=161332312026&device=c&keyword=andrew%20ng%20ml%20coursera&matchtype=p&network=g&devicemodel=&adposition=&creativeid=698085043093&hide\\_mobile\\_promo&gad\\_source=1&gclid=Cj0KCQiArby5BhCDARIsAIJvjISxaKzKNoepqcBXjoLGFuXsvW6Nfky5zfVANjFwQ87GDmxKwy18eUQaAmzOEALw\\_wcB](https://www.coursera.org/specializations/machine-learning-introduction?utm_medium=sem&utm_source=gg&utm_campaign=b2c_india_machine-learning-introduction_stanford-deeplearning.ai_ftcof_specializations_arte_sep-23_dr_sem_rsa_gads_lg-all&campaignid=20594446971&adgroupid=161332312026&device=c&keyword=andrew%20ng%20ml%20coursera&matchtype=p&network=g&devicemodel=&adposition=&creativeid=698085043093&hide_mobile_promo&gad_source=1&gclid=Cj0KCQiArby5BhCDARIsAIJvjISxaKzKNoepqcBXjoLGFuXsvW6Nfky5zfVANjFwQ87GDmxKwy18eUQaAmzOEALw_wcB) )\n\nwill it be worth it?\n\nAnd please provide any other resources.\n\nThank you",
    "created_utc": "2024-11-09T06:38:18",
    "num_comments": 11,
    "comments": [
        "No, it doesn't teach any math",
        "Andrew Ng  ML course from Stanford will help you",
        "From Where should I learn",
        "CS229... any newer versions will do as well, don't need the andrew ng one (it's from 2018)",
        "They also offer mathematics for machine learning course",
        "Look for MLT 2022 by Arun Rajkumar in YouTube. It is a foundation course on machine learning focused on math.",
        "If want to be better，then choose a direction,  and read the paper and blog。for example ，if you want to know more about recommendation ，you can start with google's paper and dig deeper",
        "You think 2018's ML course is outdated?",
        "the classic ml method will nerve be outdated i think,  the math under the algorithms are common,  including grandient descent and more",
        "If you are a begineer, my best advice is Andrew ng or hsuan-tien lin",
        "no ofcourse not considering CS229 goes into the nuts & bolts behind ML, it's obviously great because even after 5 years more it'll not be outdated!!\n\nPersonally I prefer the latest versions that's why I said what I said."
    ]
},
{
    "submission_id": "1gnb61v",
    "title": "Is my logistics regression model good(concerned about the true values)data is cleaned and balanced ",
    "selftext": "Logit reg -\nAccuracy: 0.90\nConfusion Matrix:\n[[67472   499]\n [ 6679   511]]\nTrue Positive: 511\nTrue Negative: 67472\nFalse Negative: 6679\nFalse Positive: 499\nSensitivity: 0.07\nSpecificity: 0.99\nPositive Predictive Value: 0.51\nNegative Predictive Value: 0.91\nClassification Report:\n              precision    recall  f1-score   support\n\n           0       0.91      0.99      0.95     67971\n           1       0.51      0.07      0.12      7190\n\n    accuracy                           0.90     75161\n   macro avg       0.71      0.53      0.54     75161\nweighted avg       0.87      0.90      0.87     75161\n-------------------------------------------------------------",
    "created_utc": "2024-11-09T06:23:31",
    "num_comments": 2,
    "comments": [
        "You said data is balanced but you have 7190 positive examples to 67971 negative examples?  That's not very balanced.",
        "Okk."
    ]
},
{
    "submission_id": "1gnb1mv",
    "title": "Frequent Pattern Mining question",
    "selftext": "I'm performing a Frequent Pattern Mining analysis on a dataframe in pandas.\n\nSuppose I want to find the most frequent patterns for columns *A*, *B* and *C*. I find several patterns, let's pick one: (*a*, *b*, *c*). The problem is that with high probability this pattern is frequent just because *a* is very frequent in column *A* per se, and the same with *b* and *c*. How can I discriminate patterns that are frequent for this trivial reason and others that are frequent for interesting reasons? I know there are many metrics to do so like the *lift*, but they are all *binary* metrics, in the sense that I can only calculate them on two-columns-patterns, not three or more. Is there a way to to this for a pattern of arbitrary length?\n\nOne way would be calculating the lift on all possible subsets of length two:\n\nlift(*A*, *B*)\n\nlift((*A*, *B*), *C*)\n\nand so on\n\nbut how do I aggregate all the results to make a decision?\n\nAny advice would be really appreciated.",
    "created_utc": "2024-11-09T06:17:20",
    "num_comments": 3,
    "comments": [
        "you need to know the reason you want to use frequent pattern mining，to solve what kind of problem.",
        "Support, confidence and lift are not binary metrics. You could have a rule {A, B, C} -> {D} with high lift and/or confidence.",
        "What do you mean?\n\nLike I have to figure out what relationship I'm interested in based on the problem to focus on that? For example I know that the relation I want is (A,B) -> C and so I focus on searching patterns which lift is maximum for that specific relstion?"
    ]
},
{
    "submission_id": "1gnarbb",
    "title": "Newbie asking how to build an LLM or generative AI for a site with 1.5 million data",
    "selftext": "I'm a developer but newbie in AI and this is my first question I ever posted about it.\n\nOur non-profit site hosts data of people such as biographies. I'm looking to build something like chatgpt that could help users search through and make sense of this data.\n\nFor example, if someone asks, \"how many people died of covid and were married in South Carolina\" it will be able to tell you.\n\nBasically an AI driven search engine based on our data. \n\nI don't know where to start looking or coding. I  somehow know I need an llm model and datasets to train the AI. But how do I find the model, then how to install it and what UI do we use to train the AI with our data. Our site is powered by WordPress.\n\nBasically I need a guide on where to start.\n\nThanks in advance!",
    "created_utc": "2024-11-09T06:02:43",
    "num_comments": 25,
    "comments": [
        "You don't have to train your own LLM. That would take way too much time and money.\n\nLook up techniques like Retrieval Augmented Generation and Text2SQL. How is your data stored right now, and what sort of questions do you want to answer? That influences the techniques you should go for.\n\nIn my experience, for quantitative questions like \"how many...\" it's best to get your data into a database and have the large language model construct queries to answer the user's question. Look up tool use or function calling for this.\n\nIt's easier to get started using a provider like OpenAI's API (which is paid), but if you want to do this completely for free look up open source models like Llama and Mistral.",
        "Since you are a newbie to AI, I would recommend you to first start with learning a few concepts. I'm working as a data scientist for a fortune 500 and learned these things 2 years back. \n\nYou don't need to go deeper into the technicalities but would recommend these topics: \nIntro to LLM (https://youtu.be/5sLYAQS9sWQ?si=sHkEZTsGgsFR9hTQ) or search intro to llm by andrej karpathy but it might be a little heavy. \n\nHow to run a model locally (https://youtu.be/yPphKQp1fqE?si=4_Zj3psdpbvxH7S6), \n\nHow to use openai api (https://youtu.be/xP_ZON_P4Ks?si=N53ZO2ef9Rg3lbmL), \n\nbasics of RAG, TextToSql (https://youtu.be/03KFt-XSqVI?si=98Vcsn9o2Rz3ywLS)\n\nRead this blog from swiggy (https://bytes.swiggy.com/hermes-a-text-to-sql-solution-at-swiggy-81573fb4fb6e)\nLet me know once you are done with this, I'll help you on the further steps.",
        "Building on what a few other people have said in this thread, if you are trying to query structured data, you are effectively looking for the LLM to write SQL.\n\nIf this is the case, the challenge you are going to face is an accuracy one.\n\nI run a startup in this space (fluenthq.com) and one of our big learnings has been that unless you are using some version of a semantic layer to define business logic and constrain the kinds of things the LLM can answer, you are only going to get to ~80% accuracy on answers generated.\n\nHappy to share learnings if you want to chat.",
        "Hey maybe you could collaborate with an open-source community of LLM  experience people to develop something for a good cause",
        "What you want is known as \"semantic search\". At work, one thing we use for this is Qdrant",
        "AI is just the new buzzword for everyone.. just like \"internet\" or \"web3\" or \"virtual reality\" were all meant to be over the years.. don't focus on that as the solution to your problems.\n\nYou are missing quite a lot of basic information for help here.. e.g. where does your non-profit get it's money from? Just because it's non-profit does not mean it's non-charge..  is it a genealogy site or a marriage record lookup site or ???   Because obviously you want to focus on the area that the majority of your customers are interested in. \n\nAI is to a certain extent just a lot of really smart IF/ELSE statements..  as other people said you just want to put your data into a database with correctly formatted fields and data and then setup SQL queries to access that data.   \n\nWeirdly the only one suggestion you gave sounds more like a government dept search query..\"how many people died of covid and were married in South Carolina\" ??    I mean what normal people are interested in that kind of query and what's the point of it??    You need to provide more useful information before you get a useful response. :)",
        "As other people have mentioned, I would highly recommend RAG, retrieval augmented generation. You don't want to train your own AI unless you have a very specific use case, and yours is not that.",
        "Text2sql is mentioned by other people. I don’t recommend going that route. Just go the simpler route “RAG” route with vector database that contains your data in a chunked format. It’s harder to create accurate queries with text2sql as opposed to just chunking the information as vectors into a vector database and retrieving it using semantic similarity search. A good place to start is Jason Liu - learn to improve your rag & free 6 day rag course.",
        "I know a little about this, u can make a chatbot using existing LLMs such as Llama 3. I use langchain + streamlit and groq to build my chatbot, and with rag, u can turn ur data into knowledge-based. So every time someone asks ur chatbot about something that is related to ur data, the LLm will respond based on that data. Try groq or hugging face models for exploring LLM",
        "Easy solution is to have Claude API generate db queries for user requests using the database schema and run the queries on your db.",
        "You don't need generative ai for that.\n\nYou just need to reformat all the biographies into a SQL-esq database then use a pretrained model/pay for one to do the actual look up part of things.\n\nAlternatively tokenise each biography and have separate columns for identifying stuff like 'cause of death', date of death, marital status, gender etc so your lookup function can search columns rather than try to understand language, id potentially use NLP for that if I were you.\n\nIdk for this task it sounds like AI is just being used to make a fancy version of what you could do in SQL, reinventing the wheel so to speak.\n\nIf I were you I'd make a process to put the data into SQL in a way that can be processed and build your search function to work with that, then after that's been built if I wanted to incorporate some form of AI I'd build that on top of the already built data",
        "I think you could literally ask chatgpt this question and get a better answer",
        "1.5 million data is small",
        "Do you mean Meta Llama? It has a download button on their site. What am I downloading? Do I download a software that I send my data to and to be processed? Or download python code?\n\nI guess my question is what does model look like? Is it a json file? Or a python code?",
        "I will keep you posted, Sir",
        "Thanks",
        "Yeah. But where do I start?",
        "Please don’t take this as disrespect, but based on your questions, you have a long way to go. You can start by learning what an LLM is and how to use them. There are many different ways, from implementing them locally (through HuggingFace, Ollama, etc) or connecting via an API (ie, Claude, Cohere, OpenAI). You also need to learn about agents (for your text to SQL use case). If you have any cloud experience, you may find it easier to use their services/tools. Or look into a platform such as DataBricks or DataRobot.",
        "I think you're missing the most important part: \"It's easier to get started using a provider like OpenAI's API (which is paid)\"\n\nI know non-profits like to squeeze out every penny but you're *already* wasting their money by using your salary to try to download something you don't know how to run on hardware that wasn't designed to run it. Stop wasting their money and follow the advice the poster above gave you: \"It's easier to get started using a provider like OpenAI's API (which is paid)\"\n\nIf you do decide to continue down the hard and expensive path of doing it yourself then this is the wrong subreddit to ask questions. You aren't training a model so you don't need advice from machine learning practitioners. You should go to r/LocalLLaMA  to learn how to do it the hard way, or r/llmdevs to learn how to do it the easy way.",
        "Look at llamaindex, you can use milvus as vector store that’s for searching anything in unstructured text.\n\nFor structured data you  must look at function calling from llm",
        "We’ll be glad to help. I sent you a DM.",
        "he did say he was a newbie lol",
        "Every journey begins with a single step...",
        "You are not a scammer are u?",
        "Very true"
    ]
},
{
    "submission_id": "1gn9luq",
    "title": "Title: How to Start Preparing for a Job in Machine Learning as an MCA Passout Fresher?",
    "selftext": "Hi everyone,\n\nI recently completed my MCA (Master of Computer Applications) and unfortunately, I wasn't able to secure a placement during campus recruitment. I’m now feeling a bit lost, as many of my peers have already landed jobs, and I’m concerned about the impact of this study gap on my job prospects. I’ve decided to focus on building a career in machine learning, but I’m not sure where to start, given that I’m a fresher without prior experience in this field.\n\nCould anyone guide me on how to begin my journey in machine learning from scratch? What are the essential skills I need to acquire, and what resources (books, courses, projects) would be helpful for a beginner like me?\n\nAdditionally, in the current job market conditions, do you think it’s realistic to land a job in machine learning? Are there specific strategies I should adopt to stand out in this competitive job market?\n\nAny advice or personal experiences would be greatly appreciated!\n\nThanks in advance!",
    "created_utc": "2024-11-09T05:00:53",
    "num_comments": 0,
    "comments": []
},
{
    "submission_id": "1gn9bmc",
    "title": "Daniel Bourke is the goat for leanin",
    "selftext": "That's it, he explains so well practical concepts, andrew ng is good too but mostly for theoretical ",
    "created_utc": "2024-11-09T04:45:07",
    "num_comments": 11,
    "comments": [
        "well he kinda is i guess....at the very least, he has a successful startup which might, i think, prove that he's definitely someone who knows his job.",
        "I am tired of seeing these Daniels promotional campaigns. Guys don't post too often. 😂😂",
        "Where can I find some of his content?",
        "haha yes",
        "Is it profitable or is he funding it with YouTube $?",
        "yt afaik ...",
        "Best buy, Walmart",
        "not sure about that lol",
        "Was looking for it at ToysRUs. No luck."
    ]
},
{
    "submission_id": "1gn9a1p",
    "title": "Beating the dinosaur game with ML - details in comments",
    "selftext": "",
    "created_utc": "2024-11-09T04:42:31",
    "num_comments": 13,
    "comments": [
        "Small project I made with python from scratch(no ML libraries) to beat the dinosaur game. More details on my blog: [https://matthew-bird.com/blogs/Dino-Game-ML.html](https://matthew-bird.com/blogs/Dino-Game-ML.html)\n\nGitHub Repo: [https://github.com/mbird1258/Dino-Game](https://github.com/mbird1258/Dino-Game)",
        "wow this is great....is it an RL agent? very interesting.",
        "Great work! You should try programming this with a CNN. You are giving the network access to game's internal object positions. Try using a CNN to just use an image of the game and see what it does.",
        "if want to learn about ML where should i learn from ?",
        "Someone did exactly the same and posted it here like a month ago, including the neural net visualizaion",
        "Wow congrats ! it looks cool asf !",
        "What resources you followled to learn.want to learn.your help will be appreciated",
        "Yep, it’s an RL agent using deep q learning and gradient descent. More details on the GitHub page if you’re interested.",
        "If you want to learn the math behind it to create your own projects without TensorFlow or other libraries, I'd recommend 3blue1brown's machine learning series. However, it does require some calculus and can be hell to troubleshoot, so if you just want to make a quick and easy project I'd recommend looking into how to use some of the machine learning libraries out there. \n\nVid: [https://www.youtube.com/watch?v=aircAruvnKk&list=PLZHQObOWTQDNU6R1\\_67000Dx\\_ZCJB-3pi](https://www.youtube.com/watch?v=aircAruvnKk&list=PLZHQObOWTQDNU6R1_67000Dx_ZCJB-3pi)",
        "If you want to learn the math behind it to create your own projects without TensorFlow or other libraries, I’d recommend 3blue1brown’s machine learning series. However, it does require some calculus and can be hell to troubleshoot, so if you just want to make a quick and easy project I’d recommend looking into how to use some of the machine learning libraries out there.\n\nVid: https://www.youtube.com/watch?v=aircAruvnKk&list=PLZHQObOWTQDNU6R1_67000Dx_ZCJB-3pi",
        "Don’t shy away from the math. It makes hell of a difference knowing what goes on under the hood.",
        "Backprop nearly killed me.  It was hard to determine when something was working correctly, especially with my god awful math skills",
        "Same with me :( \n\nLong iteration time and so many steps that can fail for reasons beyond my comprehension gave me a mental breakdown. I'm thinking I might try implementing [automatic differentiation](https://en.wikipedia.org/wiki/Automatic_differentiation)(like TensorFlow does I believe) in a future project instead to avoid all this trouble."
    ]
},
{
    "submission_id": "1gn8b22",
    "title": "Great Roadmap with machine learning🎖️",
    "selftext": "New to Machine Learning? Start Here with a Beginner-Friendly Roadmap!😌\n\nMachine learning can seem daunting, but with the right roadmap, anyone can get started. This post lays out a clear, beginner-friendly plan to help newcomers navigate the world of ML. From understanding basic algorithms to working with Python and PyTorch, you’ll find resources to start building and deploying your own models. Say goodbye to confusion and hello to actionable steps toward ML mastery.\n\nReady to begin your ML journey? Head over to r/learnmachinelearning and start with this guide! 👇🏽",
    "created_utc": "2024-11-09T03:41:54",
    "num_comments": 0,
    "comments": []
},
{
    "submission_id": "1gn80t9",
    "title": "Pytorch diverges although numpy converges with same data + parameters",
    "selftext": "I implemented basic gradient descent for linear regression first in numpy and then using pytorch. However, with the same data, parameter initialization and learning rate, one converges (numpy, left) while the other diverges (pytorch, right)\n\nhttps://preview.redd.it/xz80dqlxzuzd1.png?width=1274&format=png&auto=webp&s=808142943d9ecf323a4a7933fef95b2bb7532de7\n\nHere is the code for each:\n\nNumpy:\n\n    import math\n    \n    import matplotlib.pyplot as plt\n    import numpy as np\n    \n    \n    n = 50\n    np.random.seed(1)\n    x = np.linspace(0, 2*math.pi, n)\n    y = np.sin(x)\n    y += np.random.normal(scale=0.1, size=len(y))\n    \n    alpha = 0.15\n    m = 0\n    b = 0\n    losses = []\n    fig, axs = plt.subplots(2)\n    while True:\n        axs[0].plot(x, m*x+b)\n        axs[0].scatter(x, y)\n        axs[1].plot(losses)\n        plt.draw()\n        plt.waitforbuttonpress()\n        for ax in axs:\n            ax.clear()\n    \n        b -= alpha * 1/n * sum(b + m*x[i] - y[i] for i in range(n))\n        m -= alpha * 1/n * sum((b + m*x[i] - y[i]) * x[i] for i in range(n))\n    \n        mse = sum((y - (m*x+b))**2)/n\n        losses.append(mse)\n\nPytorch:\n\n    import math\n    \n    import matplotlib.pyplot as plt\n    import numpy as np\n    import torch.nn\n    \n    n = 50\n    np.random.seed(1)\n    x = np.linspace(0, 2*math.pi, n)\n    y = np.sin(x)\n    y += np.random.normal(scale=0.1, size=len(y))\n    x = torch.from_numpy(x)\n    y = torch.from_numpy(y)\n    x = x.reshape(-1, 1)\n    y = y.reshape(-1, 1)\n    \n    alpha = 0.15\n    m = torch.zeros(1, requires_grad=True)\n    b = torch.zeros(1, requires_grad=True)\n    loss_fn = torch.nn.MSELoss()\n    optimizer = torch.optim.SGD([m, b], lr=alpha)\n    losses = []\n    fig, axs = plt.subplots(2)\n    while True:\n        y_est = m * x + b\n        loss = loss_fn(y_est, y)\n        losses.append(loss.item())\n    \n        loss.backward()\n        optimizer.step()\n        optimizer.zero_grad()\n    \n        axs[0].plot(x, y_est.detach().numpy())\n        axs[0].scatter(x, y)\n        axs[1].plot(losses)\n        plt.draw()\n        plt.waitforbuttonpress()\n        for ax in axs:\n            ax.clear()\n\nEven when I drop the LR to 0.1 they still behave the same, so I don't think it's a small rounding error or similar.",
    "created_utc": "2024-11-09T03:22:28",
    "num_comments": 2,
    "comments": [
        "Ok, I found out why. The parameter update in numpy stems from the calculation from Andrew Ng's Coursera course. There, he defines the loss starting with 1/2m instead of 1/m for the general MSE. Therefore, the gradients are half as large in the numpy case, which effectively halves the learning rate.",
        "[https://discuss.pytorch.org/t/pytorch-convergence-out-of-the-box/144542/2](https://discuss.pytorch.org/t/pytorch-convergence-out-of-the-box/144542/2)\n\n  \nyeah this is it.\n\n  \nUPD: you've found it yourself nice!"
    ]
},
{
    "submission_id": "1gn5vnw",
    "title": "Cosine similarity activation function",
    "selftext": "I've read that the cosine similarity activation function isn't usually used in practice as an activation function and I wonder why that is. Specifically if the use case is training for similarity?\n\nI am currently training a sentence transformer neural net using linear activation functions but assessed against labelled cosine similarity scores based on doc2vec vectors so the match doesn't seem to great as output values can be quite outside the bounds of the cosine similarity function.",
    "created_utc": "2024-11-09T00:45:01",
    "num_comments": 10,
    "comments": [
        "Because it's not a good activation function [https://datascience.stackexchange.com/questions/19010/using-the-cosine-activation-function-seems-to-perform-very-badly](https://datascience.stackexchange.com/questions/19010/using-the-cosine-activation-function-seems-to-perform-very-badly) it is good as a distance metric though",
        "What do you mean by cosine similarity activation function?\nDo you mean that y=cossim(wx), or that y=cos(wx)? \nIn the first case, your function is a dot product between w and x both normalized, these are vectors.\nIn the second case, you have a periodic element-wise function that I expect to perform worse.",
        "Right now I train my neural network the following way:\n\nStep 1: Sample 2 random sentences (Sentence A and Sentence B) from the training set\n\nStep 2: Encode each sentence using transformer model embeddings and mean pooling (Sentence A = Vector A and Sentence B = Vector B)\n\nStep 3: Create the neural net input vector by concatenating Vector A, Vector B and the absolute difference between Vector A and Vector B\n\nStep 4: Define the training label as the cosine similarity (sklearn function) between the doc2vec vector of Sentence A and Sentence B. As a result the training label will always be in between -1 and 1. \n\nStep 5: Start training by passing the neural net input vector (Step 3) through 3 Linear Layers (i.e., Linear Activation Function). \n\nI am uncertain as to the consistency between using cosine similarity as a training label and the output from the neural net but I am not entirely sure how else I should train on similarity.",
        "So you are not using Cosine Similarity as an activation function.\nYou are trying to predict cosine similarity of two vectors with a linear layer.\nThis seems pointless, whenever you have those vectors you can directly compute the true cosine similarity, and you are not training the transformer, but a \"neural network\" that is actually a single linear layer (if you stack several linear layers you get just what you'd have with 1).\nYou're teaching a linear layer to approximate cosine similarity calculation.\n\n\nIf you are trying to teach your transformer encoder to maximize or minimize cosine similarity of sentences, this task makes sense but you're not actually solving it this way.",
        "Correct, atm I am not using cosine similarity as an activation function. \n\nThe transformer model itself is a given and each sentence is initially encoded as a mean-pooled vector of its word embeddings. Those vectors aren't very great though and my goal is to add an additional training layer (the neural net) so that similar sentences will ultimately result in similar vectors. This is why I'm using cosine similarity as a label so that the optimization should take note of similar sentences, or at least that's the theoretical thought in my head.\n\nThe goal of the neural net is not necessarily to predict cosine similarity but to predict the vector of a sentence after passing it through the neural net that has been optimized on cosine similarity. Does that make any sense?",
        "If you are explaining correctly what you are doing, I think you're not doing what's needed to achieve your goal.  As of now, for what I understood, you are training a linear layer to predict cosine similarity based on the 2 sentence vectors and their difference (which again is a linear operation that you shouldn't need to do for the network).\nYou'll get a linear layer that does that, and your transformer will still do \"bad\" embeddings and you won't know how to use your linear net to create \"good\" embeddings of whole sentences so that similar sentences have similar embeddings. ",
        "Correct me if I'm wrong but I seem to have understood from online resources that, in practice, similarity training is done by concatenating a vector A and B and their difference. Am I missing something? \n\nTo be clear this operation is done to produce the input vector which then passes through the linear layer of the neural net whose output is compared to a similarity label. My understanding would be that, by using a linear layer, training should be least constrained to optimize based on existing similarity labels.\n\nExisting labels have been obtained by applying cosine similarity on the same concatenation of 2 vectors with the sole difference that those vectors are obtained through doc2vec for now for testing purposes (to speed up initial testing, as compared to manual labeling).",
        "But you must train is the embedding model too, in this case a transformer.\nThe linear head is just to set the framework up.\nIf the transformer is frozen (which is what I got from your first reply, but maybe I got it wrong), you are not doing similarity training.\nOverall, I think you don't even need the linear layer, you can just have the transformer embed a batch of sentence and use a similarity loss so that similar sentences are embedded similarly and negative matches are embedded orthogonally",
        "I see why I might cause some confusion. The transformer model itself is also trained from scratch but in a classic transformer model way (e.g. predict next word based on context).\n\nSo when I want to create sentence vectors, what I essentially do is encode each word in a given sentence based on passing them through the trained transformer. So a sentence of say 20 words will result in a 20,600 tensor (600 being the embedding dimension per word in the transformer). This tensor is then mean pooled and serves as an input to the neural network.\n\nThe reason I am using a neural network after the transformer is based on my understanding so far that sentence vector representations based on word embeddings alone is of poor quality and most resources online seem to suggest that a neural network should be used to further train the mean pooled vectors in order to get better representations of sentences. As far as I understand it that additional training step can be done with respect to different labels, similarity being one of them.",
        "Either you use a neural network / linear layer to transform the pooled vectors and train it with the cosine similarity loss, or you backprop through the transformer too.\n\n\nAnyways I don't think you need this concatenation step, if you want the sentence pooled embeddings to reflect sentence similarity just teach the network to maximize similarity.\nYou want a new transformer and a nonlinear head that transforms the set of tokens from a sentence into a single token (you did averaging linearly, now use a linear layer, an MLP or another transformer block), so that similar sentences have similar single tokens.\nYour current linear layer is not learning this and is not forcing the transformer to learn this, your linear layer is only learning to tell you if you're achieving this or not."
    ]
},
{
    "submission_id": "1gn5fxb",
    "title": "If Gradient Descent is really how the brain \"learns\", how would we define the learning rate?",
    "selftext": "I came across a recent video featuring Geoffrey Hinton where he said (I'm paraphrasing) in the context of humans learning languages, \"(...) recent models show us that stochastic gradient descent is really how the brain learns (...)\" and I remember him comparing \"weights\" to \"synapses\" in the brain. If we were to take this analogy forward - if weights are synapses in the brain, what would the learning rate be? ",
    "created_utc": "2024-11-09T00:12:43",
    "num_comments": 23,
    "comments": [
        "It's not how the brain learns though.",
        "Different tasks (e.g., language learning vs. motor skill acquisition) requires different “learning rates,” which the brain modulates dynamically through factors like attention, effort, and the release of neuromodulators like dopamine.",
        "That talk has always confused me. Wasn't it established years ago that deep learning and the adjacent stuff is actually much further from humans than we thought? I thought Hinton was also on that train, I don't know why he's suddenly been saying all of this stuff recently.",
        "This is an interesting question. While trying to ponder this, I tried to think in material terms since you mentioned Hinton\\* made the analogy of weights to synapses. \n\nSince learning rate is just λ * dL/dw or the derivative of the loss function (\"L\") wrt the weights, the best I can think of is that learning rate represents how these synapses are strengthened or reinforced overtime. I think neuroscientists would call that plasticity, but I could be using that term too broadly. *Basically learning rate is some sort of mechanism that reinforces a signal at the synapse.* \n\nA (non-material?), psychological concept might be something like dosage of effective practice. But that also assumes that dosage of effective practice is non-monotonic in function to really square away the analogy.\n\nEdit: added that learning rate (λ) is a hyper-parameter multiplied in conjunction with dL/dw.",
        "\"Do the brain perform something similar as GD\"?This is a very contentious topic in neuroscience and most of the time the needle points towards No. ",
        "Every new tech becomes a metaphor or philosopher’s theory of the human mind. Whether our minds actually work that way is a totally different question.",
        "I thought Hinton had said that the brain probably does not use SGD. Please link the video.",
        "Varying with external factors such as energy, focus etc",
        "The irony is, we still dont fully understand how the human brain works...",
        "Geoffrey Hinton’s analogy suggesting that the brain may learn through a process resembling stochastic gradient descent (SGD). If we extend this comparison, the \"learning rate\" in the brain could be seen as:  \n\n1. Learning Rate as the Speed of Synaptic Adaptation: In machine learning, the learning rate determines how quickly a model adjusts its parameters (or \"weights\") in response to the error (or \"loss\") it encounters. If we liken this to the brain, the learning rate could represent how quickly synapses strengthen or weaken in response to experiences or new information. This adjustment happens through processes like *synaptic plasticity*, which includes long-term potentiation (LTP) and long-term depression (LTD), where synapses either strengthen or weaken over time.\n2. Learning Rate and Dopamine Regulation: Another perspective might be to associate the learning rate with the brain’s *dopamine system*. Dopamine signals often reflect \"prediction errors\" in the brain, essentially signaling when outcomes differ from expectations, which helps adjust learning. Higher or lower dopamine levels could influence the \"learning rate,\" making the brain more or less responsive to changes. For instance, in high-stakes or highly emotional situations, the brain might boost its learning rate, adapting more rapidly to ensure survival or success.\n3. Learning Rate and the Speed of Habit Formation or Skill Acquisition: In more practical terms, learning rates in the brain might also differ depending on what we’re learning. For example, rapid adjustments might be made for language acquisition in early childhood (a high learning rate), whereas adult language learning is often slower (a lower learning rate). This could also reflect the diminishing plasticity with age or the brain’s efficiency in filtering what it considers important to retain versus discard.\n\nIn short, We don’t have a single \"learning rate\" knob in the brain, the closest equivalent would likely be a combination of *synaptic plasticity*, *dopamine-driven error signaling*, and *contextual factors* that modulate how rapidly or slowly we learn.",
        "Yeah frankly I don't know why Hinton has been saying a lot of hot takes in the past couple of years.",
        "He said it’s an analogy…",
        "That’s what I thought too, maybe more research has changed things?",
        "Correct me if im wrong, but isnt dL/dw the loss wrt weights? Not the learning rate?\n\nThe learning rate is more of how impactful the loss wrt to the weights should be, something like a \"multiplier\" to it",
        "I would think it would be contentious! Curious to go down a rabbit hole on this one. \n\nI don’t really know too much about modeling learning outside of the addiction literature (which I guess is dysfunctional learning?) and a few friends who did whole PhDs based on modeling neurons that learn.",
        "https://youtu.be/n4IQOBka8bc?si=cH4o102kNfIVhyTK\n\n27:20",
        "it's not a very good analogy.  it's comparable to when people try to explain time as a river. neither are true.",
        "If it did then I think we would have heard about it by now. I don't want to sound like a Negative Nancy but over the years all that I see from Hinton is a guy who's not even from NLP trying to jump on the LLM hype train. He's one of the best researchers and writers (his papers read more like novels than papers) out there but his takes on LLMs have been questionable at best.",
        "Yes! That’s right! It’s more of a hyper-parameter multiplied in conjunction with dL/dw. Looked at an old presentation to jog my memory about this and noticed the presenter even had that piece wrong (😱). Thanks!\n\nI think my analogy is still apt in this case, even with the edit. Hard to distinguish between the two terms biologically!",
        "I don't think in that interview he said that the brain uses SGD. He said that the brain, like an ANN, can learn a lot from data rather than having a lot hard-coded into its architecture.\n\nBut through Google I did find him saying: \"\n\n>",
        "Actually I think a good analogy might be λ * dL/dw as a whole represents plasticity but dL/dw represents something like up/down regulation of whatever proteins mediate plasticity at the synapse and λ represents the effect size for a certain protein or something. Fun to think about!",
        "I think you forgot to paste his quote. Could you please do that? I'm interested to see what he's said.",
        "You might as well read it in-context:\n\n[https://www.kdnuggets.com/2014/12/geoff-hinton-ama-neural-networks-brain-machine-learning.html](https://www.kdnuggets.com/2014/12/geoff-hinton-ama-neural-networks-brain-machine-learning.html)"
    ]
},
{
    "submission_id": "1gn4nfh",
    "title": "What next ?",
    "selftext": "I just finished neural network zero to hero by andrej karpathy. I am trying to again revise it as it is so much information dense.\n\nWhat other course should I take?\nI was looking forward to fast ai ?is it good or should I go for cs231n? Or what should I do ?",
    "created_utc": "2024-11-08T23:14:46",
    "num_comments": 12,
    "comments": [
        "I think you should read a book called neural networks and deep learning by michael nielsen",
        "What are prerequisite for studying nlp",
        "Well, you can start cs231n now. Now is the time to specialize, find a niche field and read papers on them continuously.",
        "Seconding this",
        "Where can I get this.is it available online ?",
        "Some linear Algebra, \nNlp is not part of machine learning so you can directly learn it but you should study ml and some dl to actually use nlp",
        "Okayy so no fast ai ?",
        "Yes it's a free book you can only get it online there is no hard copy of it.",
        "Thnku",
        "Well, there is no hard and fast rule. Fast.ai can be good as well. But you're good enough to jump on cs231n",
        "Ok thanks",
        "Ok thanks"
    ]
},
{
    "submission_id": "1gn2skp",
    "title": "How is Fast.ai helpful?",
    "selftext": "I have tried learning from it multiple times and from multiple versions of it. I just don't get how there are some people going on to work at big tech AI labs who attribute [Fast.ai](http://Fast.ai) for their success. I understand my learning style could be different from the intended audience, but I'd like to know the people it benefited. \n\nFirstly, the notebooks/book have little to do with the videos. Secondly, there is so much abstraction that it kind of doubles your work, as you need to look up how something is actually implemented in PyTorch. Thirdly, everything is a notebook, and I am not a fan of notebooks.",
    "created_utc": "2024-11-08T21:11:20",
    "num_comments": 14,
    "comments": [
        "I had similar experience with fast.ai. I have attempted the course 3 separate times. But, never got to finish it. The course sells as practical DL. However, IMHO, it falls pretty short. The theory is lacking and everything is about using higher level abstractions to build apps. This approach didn’t stick for me and looking back, I don’t think I got much from the course. I had a better experience with courses like cs231, cs224, etc. I was thinking about giving fast.ai another shot. But, having second thoughts after hearing your experience.",
        "The reality is taking a couple of online courses isn't enough to land an ML engineer role or really understand whats going on. \n\nInstead, I think fastai demos what ML can do hands on to inspire you to truly learn the fundamentals and do real projects across many other frameworks. At least for me, it gave a good reference point and framework to think of how i can make my training process smoother, and some good intuition for model results.",
        "Former college professor here:\n\nUltimately, any type of self learning is going to a number of factors long before it comes down to quality of the content. Stuff like learning style, capacity to commit and stick with it, etc. \n\nI personally thrived with fast.ai, but I was someone who was already academic research and publishing quantitative work in the life sciences. I didn't need or want foundational theory. I wanted to quickly learn how to execute a neural network or a set of decision trees on my data set. And it was fantastic for that.",
        "Dude just avoid that shit. There are way better resources for free.",
        "Did you use their fastai book? I think videos alone is not enough",
        "The course is pretty good imho, but you definitely need another one or three to compliment that one. I think learning fast.ai and looking at the code under the hood made me a better programmer.",
        "It got me interested enough to try Kaggle and a bunch of books several years ago. I’m not a researcher but my resume says DS now.",
        "I believe the guy who founded fast.ai mentioned his style of top down teaching is not for everyone. I tried this course as well as the Andrew ng deep learning course. The fast.ai course helped with the big picture of whats needed to train a model end to end while Andrew's course helped me understand the mathematics behind it all. Another thing that helped me was looking at the source code for the abstractions used in the fast.ai library. With all that being said I do believe that course is meant mostly for people who want to quickly get things up and running",
        "I agree with that view. However, what surprises me is the testimony of people saying the role [Fast.ai](http://Fast.ai) played in them getting an ML research/engineer position at a hot startup/big tech lab.",
        "Can you recommend me some?",
        "Yeah I agree, I always sort of thought that was marketing fluff or encouragement for students. I do think making some projects with fastai is so damn fast and easy it's pretty useful to slap on a resume and seem more impressive while you build up other key ML knowledge over time.",
        "I have found [d2l.ai](http://d2l.ai) to be an exhaustive learning resource.",
        "For sure. I will make a detailed write up soon.",
        "See I learn better when there's video content along with paper resources. I found Sebastian Raschaka's course pretty interesting. He also has the book published. I would reccomend going about it this particular way. \n\nStart with NumPy, Pandas and Matplotlib.  \n[https://sebastianraschka.com/blog/2020/numpy-intro.html](https://sebastianraschka.com/blog/2020/numpy-intro.html)\n\nThen I would touch base on the Math concepts before deep diving into DL.   \n[Tubingen DL](https://uni-tuebingen.de/en/fakultaeten/mathematisch-naturwissenschaftliche-fakultaet/fachbereiche/informatik/lehrstuehle/autonomous-vision/lectures/deep-learning/). They have mentiioned the pre-reqs in clearly.\n\nPost that I would mix and match these three resources,  \nTubingen DL: [https://www.youtube.com/playlist?list=PL05umP7R6ij3NTWIdtMbfvX7Z-4WEXRqD](https://www.youtube.com/playlist?list=PL05umP7R6ij3NTWIdtMbfvX7Z-4WEXRqD)\n\nS.Raschka DL: [https://www.youtube.com/playlist?list=PLTKMiZHVd\\_2KJtIXOW0zFhFfBaJJilH51](https://www.youtube.com/playlist?list=PLTKMiZHVd_2KJtIXOW0zFhFfBaJJilH51)  \n[https://sebastianraschka.com/teaching/stat453-ss2021/](https://sebastianraschka.com/teaching/stat453-ss2021/)\n\nI would definitelys start with the Raschka one and then move to the Tubingen one.\n\nFor Books:\n\n1. Deep Learning: GoodFellow  \n2. Understanding Deep Learning: Simon Prince  \n[https://www.youtube.com/playlist?list=PLmp4AHm0u1g0AdLp-LPo5lCCf-3ZW\\_rNq](https://www.youtube.com/playlist?list=PLmp4AHm0u1g0AdLp-LPo5lCCf-3ZW_rNq)\n\n3. Raschka's Book: Machine Learning with PyTorch and Scikit-Learn\n\n4. [http://d2l.ai](http://d2l.ai)\n\n5. Deep Learning: Foundations and Concepts by Bishop of the Machine Learning fame.\n\n  \nThis I think would be one hell of a comprehensive learning experience if one follows them sincerely. Please do let me know your thoughts on my plan and if you would change anything."
    ]
},
{
    "submission_id": "1gn2mzu",
    "title": "Spotify Music Recommendation Project",
    "selftext": "Hi, I'm working on a Flask project that uses the Spotify API to recommend songs. I'm curious about the feasibility of implementing a hybrid approach **(content-based filtering + collaborative filtering)** to generate recommendations for long-time Spotify users who have extensive user data (top tracks, playlists, followed artists, etc.).\n\nAdditionally, I'm considering using **content-based filtering** to provide recommendations for new users based on the artists they input in an HTML form.\n\nHere, I'm wondering if building a ML model on a dataset (https://huggingface.co/datasets/maharshipandya/spotify-tracks-dataset) would be beneficial for making more accurate recommendations.   \n\nIs it necessary to build separate machine learning models for the hybrid approach with long-time users and the content-based approach with new users?  \n  \nHas anyone attempted a similar project? Any insights or suggestions would be greatly appreciated.",
    "created_utc": "2024-11-08T21:02:04",
    "num_comments": 1,
    "comments": [
        "I haven’t built something like this in the past, but if you use https://plexe.ai, you can generate a quick custom ML model with your dataset and try it out quite quickly"
    ]
},
{
    "submission_id": "1gn2k4e",
    "title": "Generative AI Interview Questions on basic concepts ",
    "selftext": "In the 2nd part of Generative AI Interview questions, this post covers questions around basics of GenAI like How it is different from Discriminative AI, why Naive Bayes a Generative model, etc. Check all the questions here : https://youtu.be/CMyrniRWWMY?si=o4cLFXUu0ho1wAtn",
    "created_utc": "2024-11-08T20:57:45",
    "num_comments": 1,
    "comments": []
},
{
    "submission_id": "1gn1sqv",
    "title": "How to train my own real time object detection model?",
    "selftext": "In recent weeks, I've become interested in creating my own video object detection model. I don’t want to build it entirely from scratch but would like to train it using my own dataset. However, I’m unsure where to start. Could someone guide me on where to begin, what tools I can use to prepare my dataset, and what trainable models are available? Any advice would be greatly appreciated.",
    "created_utc": "2024-11-08T20:13:04",
    "num_comments": 6,
    "comments": [
        "Well, it depends a lot on your dataset and your requirements. For a general case, YOLO variants can suit. You can train a YOLO model using ultralytics. To prepare a dataset, you can use roboflow or cvat.ai. there are other alternatives like label studio too.",
        "Well, it depends a lot on your dataset and your requirements. For a general case, YOLO variants can suit. You can train a YOLO model using ultralytics. To prepare a dataset, you can use roboflow or cvat.ai. there are other alternatives like label studio too.",
        "Use cvat for annotations",
        "Click train button",
        "Look into transfer learning with pre-trained object detection like YOLO. \n\nAlso, keep in mind that a video is just a series of images.",
        "You can try using https://plexe.ai to build your model with your own dataset"
    ]
},
{
    "submission_id": "1gmywhs",
    "title": "Suggestions - pattern and neural network learning ",
    "selftext": "Can anyone recommend the best video resources for learning the concepts in Pattern Recognition and Machine Learning? I'm finding the book a bit overwhelming to tackle on its own. \n\nI'd really appreciate suggestions for beginner-friendly videos that cover all the key topics from the book. \n\n“ Pattern recognition and machine learning by Christopher Bishop’s “\n\nThanks!\"",
    "created_utc": "2024-11-08T17:34:57",
    "num_comments": 1,
    "comments": [
        "Many people suggested me with caltech coursework by Abu-Mostafa does it cover all the topics from the book ??"
    ]
},
{
    "submission_id": "1gmy36x",
    "title": "I forget what I’ve learnt very easily",
    "selftext": "Hello, \n\nI’ve been learning about machine learning for a while now mostly through videos, books and coding. Yet I forget very easily how things work, I only know how to implement them in code since there are lots of ML libraries. Learning how to code was way easier since it’s less theoretical? \n\nWhat do you suggest? Coding really from scratch without using any libraries? \n\nEdit : Thanks for your inputs! I’ll try all of the things you’ve recommended and will report back if I find out the best method (maybe this can help someone in the future even though we all learn differently). ",
    "created_utc": "2024-11-08T16:54:12",
    "num_comments": 18,
    "comments": [
        "Videos and books are great, even some learning platforms are great for basics, but may be fill in the blank. I’ve had a number of students who have the same concerns as you. I tell them: bring up your coding environment and play with the code, change it, see if it does what you expect. Even for math—you can just use numpy with small tensors and understand dot products and a bunch of other stuff just by playing with small examples.",
        "Take notes as if you’re gonna use it to teach someone",
        "In my experience it's impossible to learn anything math adjacent without exercises. For ML techniques, what helps me is little \"meta\" problems like I'd ask myself how a particular function would be represented/learned by a method, or some stats analysis, something that wouldn't be prohibitively hard, but still would require me to think for a bit.\n\n\n As the most basic example possible, I have been asking myself e.g. if I have Y that is completely explainable by speed, and my X is time and distance, what would different methods learn, e.g. how would a simple NN fit it? And I myself learned embarrassingly a lot, many obvious -looking things this way.\n\n\n Everything I ever watched or read without exercises slipped my mind immediately.",
        "Hi , I am in the same position I just created a similar post asking for recommendations https://www.reddit.com/r/learnmachinelearning/s/VtO7kfGnLa \n\nI am someone who learns most by picture, or I would say photographic memory is more. So I want some suggestions on how I should make my learning more interesting and understandable. Can you all suggest me some resources for pattern recognition and neural networks study ?? \n\nThank you",
        "Im also a student myself but based on trial and error, the best way that works best for me is:\n\nDont bother trying to memorize anything. Instead, focus on application. If i ever need to recall the theory, just look it up again. I imagine my approach to learning to be something like stochastic gradient descent. I try to go through things rapidly, just move on as soon as i understand it but dont spend extra effort commiting things to memory. However, i reiterate and revisit the same topics multiple times whenever i have to. \n\nI feel like this approach helps me to cover more ground in less time.",
        "My suggestion is to look deep and access how you learn, I'm sure there  are things you've been exposed to over the years that you've learned to remember and can easy recall, why is that so?\n\nUnderstanding how you learn is the key to retaining information you believe will be important to you.\n\nI've been on this self descovery journey myself. Stay strong 💪🏾",
        "Hey, I think the easiest way is to start building ML models of your own. I started by using model generation through [https://www.plexe.ai/](https://www.plexe.ai/) and iterated on the output and used it in Kaggle competitions",
        "Yes, create machine learning algorithms from scratch. \n\nIt’s relatively simple to do so, at least with what I’ve done (logistic regression, random forests, support vector machines, k-means, neural networks). \n\nIt’s important to understand the theory. If you are coming from a place where the theory is hard, I would do that.\n\nIt’s hard to find a good source that teaches the mathematical theory. I am learning that in my classes at university, but I’m sure a place like Stanford has good ML/statistical learning classes.",
        "I'm the same as you, I have horrible memory loss. I've tackled it by completely encapsulating myself with material as much as I can. \n\nWith books, I'll dog ear pages that are of importance, then go back and read them. \n\nI'll watch videos daily. Sometimes, I'll turn them on in the background while driving. \n\nI'll practice what I learned. Even a simple Jupyter notebook to grasp a specific method will do the trick.\n\nFinally, I use LLMs like chatGPT or Claude to quiz me. If I get something wrong, I'll have it explain it to me in layman's terms. Overall, repetition and practice!",
        "If you work as engineer, don’t waste your time on implementing those from scratch more than understanding general patterns. You can take a look at the theory time to time, but in practice business rate productivity over knowledge. If the wheel is already there you don’t need to build one from scratch more than understanding what it does, what are the constraints and the outputs. Moreover, applied Ml is more engineering than research.",
        "probably just to do it more. Same with any skill.",
        "It means you are moving on too fast and not practising enough.\n\nEveryone learns at different speed and some people need to spend a lot of time practising concepts often.",
        "Libraries abstract away too much imo when you are just starting out.\n\nWhen I started ML, I did it all from scratch with no libraries. I feel like it made it stick better in my head",
        "Take short notes ,whatever you learn new make sure you revise it again before going to sleep ,this works did me all the time",
        "There are various websites which allows you to play and visualise this stuffs, Make sure to explore on your own and use the one's you find the most appealing one!",
        "Yeah you can be building the models but you still need the knowledge can’t rely on  copy pasting without knowing the backbone of the code or even just understanding what all lines do",
        "Agreed! This site gives you an explanation and a one pager about the model's details too, along with the code and the reasoning behind a choice of model. Tbh, I feel the world is going towards AI Agents, sooner or later a lot of knowledge work would probably not require deep knowledge, but would suffice to have base understanding of what a particular thing does and how it works"
    ]
},
{
    "submission_id": "1gmw7yl",
    "title": "Meet Belle!",
    "selftext": "Hey r/learnmachinelearning,\n\nTL:DR; Harmonic AI Project here: [https://youtu.be/JoLOX\\_I3g9M](https://youtu.be/JoLOX_I3g9M)\n\nI wanted to share an early prototype of something I've been developing over the past year - an experimental AI system called Belle that takes a different approach to audio interaction.\n\nUnlike traditional speech recognition or audio processing systems that convert sound to tokens or text, Belle works directly with the continuous patterns and resonance in acoustic waves themselves. The system responds to mathematical properties like coherence and frequency relationships in real-time, creating emergent visual patterns through a dynamic particle field display.\n\nWhat you're seeing in the video is a basic demonstration of Belle's pattern recognition capabilities. While still very much a prototype, I believe it suggests some interesting possibilities for how AI systems might interact with acoustic patterns in ways that don't require converting sound to symbolic representations first.\n\nI'm particularly interested in applications involving non-verbal interaction - where understanding the emotional resonance or pattern structure of sound might be more valuable than processing words.\n\nThis is very much a work in progress, and I look forward to sharing more technical details as the research develops. I'd love to hear the community's thoughts and questions about this approach.",
    "created_utc": "2024-11-08T15:24:58",
    "num_comments": 1,
    "comments": []
},
{
    "submission_id": "1gmvrjf",
    "title": "Best resources to learn about diffusion models in 2024?",
    "selftext": "Hi everyone,\n\nA friend of mine has asked me about up-to-date resources to learn about diffusion models (I am assuming that mostly for image generation), but most of what I used to learn them back in 2020-2021 are severely outdated…\n\nDoes anyone know some good recent resources?\n\nThank you!",
    "created_utc": "2024-11-08T15:03:40",
    "num_comments": 1,
    "comments": [
        "I recently attended some lectures at university on diffusion models. The material is available here (the GH links to a webpage with slides): https://github.com/julioasotodv/ie-C4-466671-diffusion-models\n\nI believe it should be up to date (some cited papers are from like weeks ago). The appendices are IMO the best part, as they explain the DDPM model very thoroughly"
    ]
},
{
    "submission_id": "1gmugbb",
    "title": "Best model architecture for predicting where a sentence belongs in a story",
    "selftext": "So I’d like to build a model which infers which sentence index any arbitrary input sentence belongs in a story if you were to insert the sentence.\n\nWorth mentioning the stories are short - we can use the entire story as input at inference time, which seems necessary since you are guessing where in _that_ story it belongs.\n\nI looked around and didn’t find any papers on this - what model architecture would you use?",
    "created_utc": "2024-11-08T14:05:05",
    "num_comments": 2,
    "comments": [
        "Do you have a labeled dataset for this ? I have an architecure idea that I'm not sure exists yet and I may want to publish some results on experimenting with it",
        "I’m just gonna see what I can do with wikibooks"
    ]
},
{
    "submission_id": "1gmszyy",
    "title": "Need some help ",
    "selftext": "Hey there i hope you'll good , im going to be\n20s old in the next months and i just\ndropped off the university for financial\nreasons my parents aren't that much to\nsupport me,so I'm feeling lost right now i\nwanna invest my time in something that's\ncan earn me some money ,i knew some of\nelectronics repair but im not sure if it's good\ncareer, and i have intereste in Al and\nmachine learning and i heard frome\nsomeone on YouTube it's not for who have\nno coding skills , pls clear me up or you can\nsuggest some finance advice",
    "created_utc": "2024-11-08T13:01:32",
    "num_comments": 2,
    "comments": [
        "Learning about Ai, and ML can take some long ass time, I am talking about minimum of 6 to 8 months if you are putting in around 4-5 hours daily, also I don't think companies would be willing to hire anyone that don't have a college degree, I don't want to sound pessimistic, but it might be good to polish your smartphone repair skills, it can be a lot of money if you get good, and you can start your own store later.\n\nAnyway, whatever you decide to do, do it completely, because I can say this with 110% guarantee that HARDWORK never goes to waste.",
        "Do a labour job at minimum wage."
    ]
},
{
    "submission_id": "1gmstf7",
    "title": "Smart Food Scale using Machine Learning",
    "selftext": "",
    "created_utc": "2024-11-08T12:53:54",
    "num_comments": 0,
    "comments": []
},
{
    "submission_id": "1gmrvyg",
    "title": "120 Dog Breeds, more than 10,000 Images: Deep Learning Tutorial for dogs classification 🐕‍🦺",
    "selftext": " \n\nhttps://preview.redd.it/mf715j0mjqzd1.jpg?width=1280&format=pjpg&auto=webp&s=ba986bf2bf3ed8f0c60cf3c99b6957037df74eaa\n\n📽️ In our latest video tutorial, we will create a dog breed recognition model using the NasLarge pre-trained model 🚀 and a massive dataset featuring over 10,000 images of 120 unique dog breeds 📸.\n\n**What You'll Learn:**\n\n🔹 Data Preparation: We'll begin by downloading a dataset of of more than 20K Dogs images, neatly categorized into 120 classes. You'll learn how to load and preprocess the data using Python, OpenCV, and Numpy, ensuring it's perfectly ready for training.\n\n🔹 CNN Architecture and the NAS model : We will use the Nas Large model , and customize it to our own needs.\n\n🔹 Model Training: Harness the power of Tensorflow and Keras to define and train our custom CNN model based on Nas Large model . We'll configure the loss function, optimizer, and evaluation metrics to achieve optimal performance during training.\n\n🔹 Predicting New Images: Watch as we put our pre-trained model to the test! We'll showcase how to use the model to make predictions on fresh, unseen dinosaur images, and witness the magic of AI in action.\n\n \n\nCheck out our tutorial here : [https://youtu.be/vH1UVKwIhLo&list=UULFTiWJJhaH6BviSWKLJUM9sg](https://youtu.be/vH1UVKwIhLo&list=UULFTiWJJhaH6BviSWKLJUM9sg)\n\nYou can find link for the code in the blog : [https://eranfeit.net/120-dog-breeds-more-than-10000-images-deep-learning-tutorial-for-dogs-classification/](https://eranfeit.net/120-dog-breeds-more-than-10000-images-deep-learning-tutorial-for-dogs-classification/)\n\nYou can find more tutorials, and join my newsletter here : [https://eranfeit.net/](https://eranfeit.net/)\n\n\n\nEnjoy\n\nEran",
    "created_utc": "2024-11-08T12:13:54",
    "num_comments": 1,
    "comments": [
        "Would love to see an equivalent tutorial for PyTorch."
    ]
},
{
    "submission_id": "1gmr9td",
    "title": "Which method or model suits best for information extraction from unstructured text based PDFs?",
    "selftext": "Hey friends,\n\nI am kinda new to the topic of ml and am also not the expert regarding programming. But I've been given a task in my side job in uni,  where I have to build a database. For that, I need to analyse about 15000 PDF documents which are security means of certain products. My task is to analyze the \"root cause\", \"risks\" and \"action to be taken\" from that unstructured text in those PDF-documents.\n\nMy current pipeline is to use certain python-libs to convert the PDFs to txt and than to use a llm to analyze those documents. What is your guyses opinion to this approach? \n\nAnd with using an LLM, which model would suit the best for this task? Right now I'm leaning between GPT4o and Llama 3.1. But I don't really have an idea what to base my decision on. So if you can give me any advice regarding that, I'd be very glad.\n\nThanks ahead :)",
    "created_utc": "2024-11-08T11:47:26",
    "num_comments": 3,
    "comments": [
        "We spent a lot of time on this problem. It’s more challenging that it looks especially for multi page PDFs and dense tables. \n\nWe decided to ship our solution as an API. Would love for you to try and give us feedback: https://tile.run",
        "Your solution looks like i's implemented for structured PDFs based on tables. That's not the use case for me. The PDFs I'm looking into are mostly unstructerd and text based...  \nThanks tho",
        "Oh we won’t need the PDF to be structured. Works great on unstructured PDFs as well. As long as you need JSON as the output it will work well. \n\nIf you need markdown out of a PDF, we wouldn’t be the right tool"
    ]
},
{
    "submission_id": "1gmqxgm",
    "title": "Using Pyinstaller to make an executable for YOLOv8 and YOLOv11 model",
    "selftext": "I am working on a project to detect dice and their rolls. I used the YOLOv8n-seg architecture to train a model to detect dice type (d4, d6 ...) and then use the mask and bounding boxes predicted to create smaller images which are then fed to another model trained on the YOLOv11n-cls architecture to detect the rolls. I have the models to a point where I am happy with their results and want to package it into an exe that users can run just by clicking it.\n\nI was able to do this with pyinstaller but had issues with it running on other peoples computers. I reran the script after reading more documentation and learning about the ability to make it into one file without the internal folder. This version works on my pc, a windows vm on that pc and a windows vm on my mac. When I try to run it on any of their computers it seems to crash when trying to run the dice type model and just restart the exe with no error output. It will do this infinitely until cancelled. The only package needed to run my python script is ultraltyics and it's dependencies. Any help for how to debug, fix or an alternative would be much appreciated. I am new to using pyinstaller so if there is something that can help with understanding the problem better let me know.",
    "created_utc": "2024-11-08T11:32:28",
    "num_comments": 0,
    "comments": []
},
{
    "submission_id": "1gmqiju",
    "title": "Suggestions for a new MacBook ",
    "selftext": "I have recently been interested in buying a new MacBook. Recently with the new addition of M4 chips I am thinking of buying a new MacBook. I currently own a Lenovo Legion 7i, here are the specs : \ni9-14900HX\nRTX 4070\n32GB DDR5 RAM \n1TB + 2TB (Both SSD, internal) \n\nThe only problem with this laptop is that I can’t use it to full potential or even half potential on battery. Its battery does not last long if I do programming . It usually lasts about 3 hours if I lower the brightness, volume and run it on iGPU. \n\nWith this I was tempted to buy a new MacBook Pro 16inch with the specifications :\nM4 Pro \n36 GB RAM \n2 TB SSD storage \n\nI am a first year undergraduate who is learning AI and Machine learning. Besides that I love gaming and I am learning game development. Soon I will start working on AI and start learning “machine learning” .\n\nI want some suggestions that is this decision rational enough for me or I need to make some changes, like for example in changing the specs of MacBook Pro or maybe waiting for one more year for newer architecture. All opinions are appreciated, feel free to comment. ",
    "created_utc": "2024-11-08T11:14:23",
    "num_comments": 0,
    "comments": []
},
{
    "submission_id": "1gmpda5",
    "title": "New to ML: Working on image segmentation: Looking for a sanity check...",
    "selftext": "Hey, all. First off, please excuse any incorrect jargon. I'm new to ML. Because I tend to write *way* too much, I'll summarize my issue:\n\n**I am:**\n\n1. Working on a comic book reading app.\n2. Want to implement \"smart\" navigation where, rather than page-to-page, the user navigates pane-to-pane.\n3. This pane-to-pane navigation requires that I know the bounding coordinates of each pane (when user imports comic book, I'd process each page, then store a JSON file locally containing all the coordinates for each page, so I'd only have to hit my endpoint during import).\n\n**I have:**\n\n1. Created a starter set of images for training consisting of 50 comic book pages.\n2. Hand-drawn the masks for image segmentation training. (Filenames correspond: an original comic book image \"001.jpg\" has a corresponding mask \"001.png\")\n3. Created both CSV and JSONL files that indicate where the data is (one column for the originals, and one column for the corresponding masks).\n4. Tried to use **Google Vertex AI ML** to train an image segmentation model.\n5. Uploaded my JSONL to the appropriate Google storage bucket (which the ML training interface accepts).\n6. Tried to run the training job, and gotten errors...\n\n**Error output following submission of JSONL file to start the job (linebreaks added for clarity):**\n\n    Operation ID:  \n    projects/############/locations/us-central1/operations/7590777211557249024  \n    \n    Error Messages:  \n    Error: Could not parse the line, json is invalid or the format does not match the input schema: Cannot find field: maskGcsUri in message google.cloud.aiplatform.master.schema.ImageSegmentationIoFormat. for: gs://MY-SECRET-PROJECT-URI.appspot.com/dataset/vertex\\_ai\\_dataset.jsonl line 1\n      \n    Error: Could not parse the line, json is invalid or the format does not match the input schema: Cannot find field: maskGcsUri in message google.cloud.aiplatform.master.schema.ImageSegmentationIoFormat. for: gs://MY-SECRET-PROJECT-URI.appspot.com/dataset/vertex\\_ai\\_dataset.jsonl line 5  \n\n**The JSONL file looks like (linebreaks added for clarity):**\n\n    {\"imageGcsUri\": \"gs://MY-SECRET-PROJECT-URI.com/dataset/pages/001.jpg\", \"segmentationMask\": \"gs://MY-SECRET-PROJECT-URI.appspot.com/dataset/masks/001.png\", \"dataItemResourceLabels\": {\"aiplatform.googleapis.com/ml_use\": \"TRAIN\"}}\n        \n    {\"imageGcsUri\": \"gs://MY-SECRET-PROJECT-URI.com/dataset/pages/002.jpg\", \"segmentationMask\": \"gs://MY-SECRET-PROJECT-URI.appspot.com/dataset/masks/002.png\", \"dataItemResourceLabels\": {\"aiplatform.googleapis.com/ml_use\": \"TRAIN\"}}\n        \n    ETC...\n\n**Solutions?** If anybody can help steer me in the right direction, I'd be, y'know, all kinds of grateful.\n\n**Note:** Training the model with Vertex isn't a requirement. However, *deploying* the model **is** a requirement. So if there's a way to train my model elsewhere, then host it via Google Cloud, that'd be fine. I talked to an ML nerd last night who told me Vertex's image ML is a bit cumbersome.",
    "created_utc": "2024-11-08T10:25:52",
    "num_comments": 0,
    "comments": []
},
{
    "submission_id": "1gmpaqj",
    "title": "Need help in my college project in using predictive modeling ",
    "selftext": "Hello everyone I need some help in machine learning project under my college professor to pass her citrea can any help me in this .",
    "created_utc": "2024-11-08T10:22:45",
    "num_comments": 0,
    "comments": []
},
{
    "submission_id": "1gmonqc",
    "title": "Need inspiration for graduation project ",
    "selftext": "Hey guys,\nI’m taking GP next semester and we were asked to give a bunch of ideas for the supervisors.\nThe problem is that I can’t find any inspiration and I wanna work on something impactful and trying to avoid any basic ideas.\nThe requirements for it is that it needs to have an interactive interface either web/mobile application and a database. My original idea was rejected due to it not  checking all the boxes.\nI would really appreciate if any of y’all can help me with my problem.",
    "created_utc": "2024-11-08T09:55:40",
    "num_comments": 0,
    "comments": []
},
{
    "submission_id": "1gmolxc",
    "title": "Query on how I should choose a research domain",
    "selftext": "Till now as a ug student, I've worked with  DL models as well as LLMs for hackathons and stuff. But other than that and knowing stuff behind how few algorithms work, I have no idea which domain I should pursue for research this coming summer.\n\nAll my projects till now are mostly based on DL on image datasets, which I just experimented a lot with the hyperparameters. Please suggest how I should select a domain and dive a bit before applying anywhere so that I can actually send my application with \n\n\" I'm intrested in this topic in this domain due to my intrest and project in this domain, hence I would like to work with you\"\n\nDo suggest if you guys have some idea!",
    "created_utc": "2024-11-08T09:53:34",
    "num_comments": 2,
    "comments": [
        "RemindMe! 8 hours",
        "I will be messaging you in 8 hours on [**2024-11-09 10:45:17 UTC**](http://www.wolframalpha.com/input/?i=2024-11-09%2010:45:17%20UTC%20To%20Local%20Time) to remind you of [**this link**](https://www.reddit.com/r/learnmachinelearning/comments/1gmolxc/query_on_how_i_should_choose_a_research_domain/lw6ve7q/?context=3)\n\n[**CLICK THIS LINK**](https://www.reddit.com/message/compose/?to=RemindMeBot&subject=Reminder&message=%5Bhttps%3A%2F%2Fwww.reddit.com%2Fr%2Flearnmachinelearning%2Fcomments%2F1gmolxc%2Fquery_on_how_i_should_choose_a_research_domain%2Flw6ve7q%2F%5D%0A%0ARemindMe%21%202024-11-09%2010%3A45%3A17%20UTC) to send a PM to also be reminded and to reduce spam.\n\n^(Parent commenter can ) [^(delete this message to hide from others.)](https://www.reddit.com/message/compose/?to=RemindMeBot&subject=Delete%20Comment&message=Delete%21%201gmolxc)\n\n*****\n\n|[^(Info)](https://www.reddit.com/r/RemindMeBot/comments/e1bko7/remindmebot_info_v21/)|[^(Custom)](https://www.reddit.com/message/compose/?to=RemindMeBot&subject=Reminder&message=%5BLink%20or%20message%20inside%20square%20brackets%5D%0A%0ARemindMe%21%20Time%20period%20here)|[^(Your Reminders)](https://www.reddit.com/message/compose/?to=RemindMeBot&subject=List%20Of%20Reminders&message=MyReminders%21)|[^(Feedback)](https://www.reddit.com/message/compose/?to=Watchful1&subject=RemindMeBot%20Feedback)|\n|-|-|-|-|"
    ]
},
{
    "submission_id": "1gmoig3",
    "title": "How to find features among drawings?",
    "selftext": "Hello everyone,\n\nI am a little new to machine learning, and only know very basic architectures like the usual ANNs or other basic algorithms like k-Means.\n\nLet's say I have drawings from people, and I want to find common patterns or \"motives\" among them. How could I achieve this, if there even is any feasible way?\n\nI asked ChatGPT about it, and got a few recommendations:\n\n- using a pretrained model like ResNet or something more adapted for stylized stuff, and simply seeing what it finds\n\n- using a CNN, but cutting off the last (classification) layer to get access to the features it \"found\"\n\n- using an Autoencoder to see what (latent?) features it learned\n\nAll of these would conclude with dimensionality reduction if needed, and clustering via k-Means, according to ChatGPT!\n\nNow, I dont know much about such advanced architectures and wanted to ask, if those methods are feasible, or would be the simplest option!\n\nI would gladly appreciate any help or advice on how to approach this, what things to look into, or honest comments if this is not really feasible!\n\nAlso, just ask if any more information is needed!\n\nThank you!\n\nIMPORTANT EDIT: They are (very) abstract drawings, so no guarantee that there are perfect drawings of a house or a tree on them, haha. So, it really is about finding recurring, abstract patterns, themes, ...",
    "created_utc": "2024-11-08T09:49:50",
    "num_comments": 1,
    "comments": [
        "CNNs are like cheat codes for image processing. They excel at extracting spatial features from images and learning patterns directly from these structures. For your use case, exploring CNN architectures could be incredibly helpful. Start with popular ones like VGG16, EfficientNet, or YOLO, as each has unique strengths depending on your goal.\n\nFor your task of identifying recurring abstract patterns, using a pretrained model like ResNet and removing the final classification layer to access feature embeddings is a solid approach. Couple that with dimensionality reduction techniques (PCA or t-SNE) and clustering algorithms like k-Means to group similar patterns.\n\nAlternatively, Autoencoders can be a powerful way to learn latent features, especially with abstract inputs, as they compress data into meaningful representations.\n\nGoodluck!"
    ]
},
{
    "submission_id": "1gmoc4m",
    "title": "21 year old high school dropout here.",
    "selftext": "Should I start studying AI/ML? I’m thinking of going to university now I’m able to but I definitely lack a lot of the foundational knowledge/skills required for this type of subject. I’m willing to put all my time and effort into study but I’m just unsure if I’ll succeed. Is it too late for me? Or should I just start? (If so where would be the best place to begin?)",
    "created_utc": "2024-11-08T09:42:27",
    "num_comments": 18,
    "comments": [
        "Step 1) Go get your GED\n\nIf that's too much for you, you're simply not going to make it here.",
        "Get a GED go to community college and transfer to a top school. There are no entry level ML jobs. Those jobs are all from those PhD positions. Not all but really you need to research. \nI’m 22 now in CC. Math is important, you’re gonna learn hard math but math is a skill that is learnable.",
        "To late? I started my master at age 26. You are not late at all my friend. It's doable as long as you are dedicated to the cause. Study hard and take it seriously and it will be very easy and fullfilling!\n\nIf you want a starting point, look up linear algebra and calculus material and there you can see the pre-requisites for these subjects. It's not \"hard\" but it will take dedication to study since you didn't finish highschool \"assuming you didn't get the full math courses you should've in highschool because you stopped\". Then again, your bridging programme will also help you with this!",
        "I am a primary school dropout. I started with 29. I did go to a relatively competitive uni and finished it.   \nBut you need discipline and be genuinely curious and like it, otherwise it will just be too hard to stay on task when things get frustrating.\n\nI ended up TA'ing too and I saw so many people drop out, not because they weren't smart, smarter than me most. Simply because they were not really interested so they couldn't put in the effort consistently.\n\nMaybe try and see if you can pass an online course or finish a text book, then decide if you want to go full in?",
        "How about the tradie work?",
        "Also look up SMART goals and make sure to use it to plan your studies for the GED. Dunno how much you had left to finish in high school or how well you were doing, but it really is a LOT of studying to get to do AI/ML professionally. On the scale of 5-10 years. So if you don’t complete your GED in 2-3 years of effort, it’s probably best to finish it but try something else",
        "Is that an American thing?",
        "Thanks mate. :))",
        "I’ve been told YouTube can be a good resource for linear algebra and calculus. Would you agree?",
        "No, the idea that finishing your high school is a necessary condition for working in AI/ML is not an exclusively American thing.",
        "It is another name of High school in usa , stands for GENERAL EDUCATIONAL DEVELOPMENT",
        "Go to community college or something. Don't just self-learn, you also need certification.",
        "Hell yeah, when I was following calculus specifically I used YouTube to explain concepts to me that was very vague in the book or the teacher really didn't explain well. All the knowledge you'll ever need  is on youtube to get started.",
        "The universities in New Zealand don’t require you to have any high school diploma to enter, nor do we have a GED equivalent. If you don’t have any formal education most offer free foundation/bridging courses that will then lead to your under grad study’s. So I guess in this case “finishing high school” is an American thing. My original question was less about how to I get the degree and more about what skills I can start building before I sign up for one of these foundation courses next semester.",
        "Thanks",
        "Yeah, but in another reply to someone he said his university offers a bridging period that will fill in the knowledge you don't have",
        "Eh well.. so when I started uni as a computer engineering student, I had just completed high school with *literal* straight A's, and a commendation in mathematics.\n\nIn my uni freshman class of roughly 150 students, I was in the *lower* midrange in terms of 'good at math.' And if there is a place where you'll *need* math, it's AI/ML. At my first ML course, my head was literally swimming. And that was after two years of non-stop math classes that *also* made my head swim.\n\nTo give you a picture, the first class of one of the various kinds of math we took in semester 1, the teacher walks in. *'Welcome. This is uni. This is discrete mathematics. This is my name, e-mail address, syllabus here. Now in the first 30 minutes, let's recap all the mathematics you have learned from grade school to high school graduation.'* It took him 25 minutes. The rest of the semester happened at the same tempo.\n\nLucky for me I *loved* maths, so I pulled through. But you know... you need to worship maths to survive in this field. It's not about your high school diploma, it's 'will you have anything to build on.'",
        "Well learn maths, at least highschool level and make sure you are really good at it. I'd say differentials, vectors/matrices and general probability/statistics are very important.\n\nAlso learn how to code in Python, start with basics variables, if conditions, loops and lists/dictionaries, oop etc.\nThen move on to hands on ml projects for beginners."
    ]
},
{
    "submission_id": "1gmnr41",
    "title": "Friday's Oxen AI Water Cooler, 10:00 am PST",
    "selftext": "Project to be discussed:   \n \"Ice Cream\" an open source  AI Hardware alternative to the Rabbit R1   \n  \nA bunch of us are into wearables. I have 5.   \n  \nHW and SW mavens Akshay and Bently will define their build path and answer questions. \n\nOr join the space on X directly at: [https://x.com/oxen\\_ai/status/1853465669481975972…](https://x.com/oxen_ai/status/1853465669481975972)  \nFriday Nov 8. 10:00 AM California time, 1:00 PM Boston, 6:00 PM London, 10:00 PM Dubai",
    "created_utc": "2024-11-08T09:18:30",
    "num_comments": 0,
    "comments": []
},
{
    "submission_id": "1gmnq75",
    "title": "Looking for a ML Model to parse/extract skills from Job Descriptions",
    "selftext": "I am working on a project, which requires me to parse a given job description, and extract relevant skills it demands from an User. For example, if Web Development is in the Job Description, it gives me, React/Node/CSS/Web Developement keywords. If Accounts Manager is in the JD, it gives me Excel, Tally, as skills.\n\nI am not from an ML Background, so I do not want to invest time to create it myself and fine tune it, instead wanna use a good pre built/trained one. Please suggest me good ML Models I can use in my project. TIA",
    "created_utc": "2024-11-08T09:17:29",
    "num_comments": 9,
    "comments": [
        "Maybe you could get away with some regex? If not I would probably use api calls to some LLM bc I’m lazy",
        "Most likely you don't need a heavy LLM model\n\nAn LLM might actually be a decent choice here, but don't just pick up a model and use it. Maybe pick something lightweight fine time it and then, process the data accordingly. I haven't worked much with LLMS but I know you can use them for keyword identification.\n\nThe end of the day you need to understand what your problem definition is and what you are looking at is keyword identification. \n\nStart with word cloud, then move to\nrake using rake nltk, and if you want a language model only for extraction of keyword use keybert.\n\nEdit:\n\nUse keybert or rake-nltk.",
        "If this is for building a cover letter you can definetly prompt engineer an LLM to achieve this desired result.",
        "We have an API that will help you do this in a few minutes: [https://tile.run](https://tile.run)\n\nJust provide the file or text, a schema and you are done",
        "Regex wouldn't be an ideal choice imo, I also thought to using API calls to an LLM, but it will cost me every time , I do not want it.",
        "Well better start reading about NLP then",
        "U can initially try checking with Groq Api they wont cost …",
        ">it will cost me\n\nI've seen posts like these... Why not download a free model like llama3? Is it really that bad? Or do you mean cost not in money but computation?"
    ]
},
{
    "submission_id": "1gmmbiu",
    "title": "Pattern Matching != Reasoning: We analyzed 2 distinct paths to make LLMs actually think [Technical Deep Dive]",
    "selftext": "I'm a lead ML and Cryptography Researcher. Our team just did a study on why current LLMs are basically sophisticated pattern matchers rather than reasoning engines. We published our full findings but here's some key insights.\n\n**TL;DR:**\n\n* Current LLMs don't actually reason, they pattern match really well\n* We identified two promising paths forward: training-time and inference-time enhancements\n* PEFT + Chain of Thought prompting together show surprising results\n* All research/code will be open-source\n\nLet me know your thoughts on the full article.[ https://blog.bagel.net/p/train-fast-but-think-slow](https://blog.bagel.net/p/train-fast-but-think-slow)",
    "created_utc": "2024-11-08T08:18:51",
    "num_comments": 17,
    "comments": [
        "A) you forgot the article link\nB) isn't this fairly well known?",
        "Where's the article?",
        "Looks exciting,can you share the code to understand",
        "yep. I think we can do the jump to reasoning but there are some hurdles to get there. Probably wont be long tho.",
        "Link to article/ code?",
        "I don’t understand. What is reasoning? If I see a picture of cat, I say “cat” because I see a picture of it, which is reasoning, is logical and is sensible. But isn’t it also pattern matching?",
        "I don't think anyone has rigorously defined \"reasoning\", so it's hard to come to consensus on it. The most influential research in my mind is the existence of ARC-AGI which at least is a benchmark that is hard to cheat and hard to beat.",
        "Lol, we went from \"LLMs can think\" to \"isn't this fairly known\".",
        "This is the article: [https://blog.bagel.net/p/train-fast-but-think-slow](https://blog.bagel.net/p/train-fast-but-think-slow)",
        "[https://blog.bagel.net/p/train-fast-but-think-slow](https://blog.bagel.net/p/train-fast-but-think-slow)",
        "Code will be open source when it is ready",
        "[https://blog.bagel.net/p/train-fast-but-think-slow](https://blog.bagel.net/p/train-fast-but-think-slow)",
        "Code will be released as open source when it is ready",
        "There's three types of reasoning, common sense, arithmetic, and symbolic. You may be referring to symbolic reasoning.",
        "There's a bit more analysis in the article: [https://blog.bagel.net/p/train-fast-but-think-slow](https://blog.bagel.net/p/train-fast-but-think-slow)",
        "It's more of a question of \"who did you ask?\".. someone that's studied the field for a bit or someone who struggles to use an iPhone",
        "It's more about the type of reasoning they can do that continues to get more advanced: [https://blog.bagel.net/p/train-fast-but-think-slow](https://blog.bagel.net/p/train-fast-but-think-slow)"
    ]
},
{
    "submission_id": "1gmllmq",
    "title": "Is there a way to determine accuracy of multiple models?",
    "selftext": "Hello! I am pretty new to machine learning. I fine-tuned LayoutLMv3 over the summer. And now I am looking at other models like LLaVA and llama 3.2 for text extraction from images/pdfs. I want to find which one would yield best results and most accurate for text extraction from images. Is there a way to use the same code to test the accuracy of the text being displays for each model without having to tailor each one to that specific model? For example if the text extraction is 50% right or 90% or something?",
    "created_utc": "2024-11-08T07:48:26",
    "num_comments": 0,
    "comments": []
},
{
    "submission_id": "1gmll47",
    "title": "Why does my ssim_loss, img_loss, and psnr_loss spike massively when learning rate gets low? The image generated image quality gets completely obliterated",
    "selftext": "",
    "created_utc": "2024-11-08T07:47:50",
    "num_comments": 1,
    "comments": [
        "I'm trying to train a DQN to play Pong, by including a world model representation as the input. The observation (raw pixel image) is encoded to a latent space, and then decoded back to an image (i.e. encode the image into latent space which represents the image).\n\nI did have some very good results with CosineAnnealingWithWarmRestarts but it would spike and cause the image to get deep fried, and now I'm only using ReduceLROnPlateau, but apparently it has the same issue. I'm using RAdam.\n\nAll the image losses are summed and used to train the decoder, and the adv\\_loss\\_real and \\_fake are summed and used to train the discriminator. So there's nothing super strange about my implementation that would cause this.\n\nAll are equally weighted. Gradient norm is clipped at 1.0. Qualitatively it gets quite good at first, so AFAIK it must by a hyperparameter issue or some kind of training issue.\n\nlog(loss) is what is displayed btw\n\n**Solution: Adding generator loss seemed to caused the issue.** You can actually see it on the graph, when the discriminator gets the advantage, the losses spike. It seems the gradient from the generator network explodes but it's just a guess."
    ]
},
{
    "submission_id": "1gmlh03",
    "title": "Advice on customer returns forecasting",
    "selftext": "Hey guys, so I work in a marketplace and I am trying to forecast costs related to customer returns and lost packages (this has the largest cost of all reasons btw). There is some correlation with peak seasons like christmas and black fridays, but I cant seem to be getting good results when forecasting 2024. Note that this year has had a significantly more downward trend.  \nI am a junior in the forecasting team, so all advice and tips are welcomed.  \nHow should I approach this? I have included the sales volume in the model, because we have a good forecast for this.\n\nAre there any variables that I should try to look into? Something related to supply chains?\n\nAttached is an image of the STL decomposition of my series.\n\nhttps://preview.redd.it/nheye8la7pzd1.png?width=1200&format=png&auto=webp&s=8151f39e04b31e0c7027e4b2708d5391f36f602e\n\n",
    "created_utc": "2024-11-08T07:42:51",
    "num_comments": 0,
    "comments": []
},
{
    "submission_id": "1gmlf6x",
    "title": "LLM training implementation",
    "selftext": "Hello everyone.\n\nCan someone help me implementing the training loop?\n\nHere's my code:  \n[https://pastebin.com/q6Un0m1b](https://pastebin.com/q6Un0m1b)",
    "created_utc": "2024-11-08T07:40:39",
    "num_comments": 1,
    "comments": [
        "Sorry for the short message, for some reason all of my previous posts with more detailed information got automatically removed.\n\nI know how most of the stuff works in theory but never trained a pytorch transformer model.\n\nWhen trying to write a training script before, it just spammed the eos\\_token all of the time and it still had to be trained for hundreds of epochs. Even using ignore\\_indecies=tokenizer.eos\\_token\\_id for the Loss function didn't help.\n\nThanks in advance!"
    ]
},
{
    "submission_id": "1gml28w",
    "title": "Any Benchmark results website for pytorch nvidia amd",
    "selftext": "Hello all are there any websites that show benchmark results for amd and nvidia gpu for pytorch performance. I'm confused between 4070 and 7800xt as one has more vram and another has cuda... I need windows for microsoft analytics software but can try running it in windows vm if 7800xt is leagues ahead or atleast 20% better then 4070 due to high vram... Ml is my objective with pytorch",
    "created_utc": "2024-11-08T07:25:00",
    "num_comments": 1,
    "comments": [
        "Use Nvidia as much as possible"
    ]
},
{
    "submission_id": "1gmkvse",
    "title": "How to Read Deep Learning Code?",
    "selftext": "",
    "created_utc": "2024-11-08T07:17:16",
    "num_comments": 0,
    "comments": []
},
{
    "submission_id": "1gmkazy",
    "title": "Best workstation to do clinical ML/DL research ",
    "selftext": "Hello, \n\nI am looking to purchase a workstation to do clinical ML/DL research.  I'm a clinician so I don't come from a computer science background but currently doing ML research using python/juptyer note book/ R. \n\nI am currently doing ML research using a slurm HPC but the place I will be moving to is new to ML and therefore its better to have my own desktop. Right now i am working on a data base with approx 40000 patients.\n\nI am looking into doing DL research eg Modified BERT model etc.\n\nThe new department have said they are willing to pay for 2 GPUs and a computer. a quote of $25000 was suggested. \n\nBecause I lack the computing technical expertise, I prefer having something set up rather than building it from scratch but would be willing to buy parts separately if the department can help find someone to set it up. Obviously, I need good cpu, ram etc. . I am looking for something that will work for the next 2-3 years, as part of research involves applying for grants which help me employ people with expertise and hopefully I can expand on what I need as time evolves, \n\nSomeone proposed the GPU NVIDIA A100 80gb but I am not sure what other parts I will need to purchase if I get this\n\nI found the Lamda vector [Customize Your Lambda Vector | Lambda](https://shop.lambdalabs.com/gpu-workstations/vector/customize?_gl=1*11mfuuu*_gcl_au*MTc3ODUzMzgzOS4xNzMxMDc0MDk4*_ga*MTg5ODExNTg2Mi4xNzMxMDc0MDk4*_ga_43EZT1FM6Q*MTczMTA3NDA5Ny4xLjEuMTczMTA3NjAyNS4yNC4wLjA.)\n\nBut reviews are mixed was wondering if anyone else has any other suggestions for something similar that is reasonably priced \n\nThank you!\n\n\n\n. \n\n\n\n\n\n",
    "created_utc": "2024-11-08T06:52:14",
    "num_comments": 0,
    "comments": []
},
{
    "submission_id": "1gmhh7q",
    "title": "How can I learn to fine tune LLMs in two days?",
    "selftext": "Hi, I am a Senior Software Engineer, and I have worked briefly in ML engineering and Data engineering space as well. I have a project starting next week, and before that I want to learn to fine tune LLMs, can anyone help me how can I get started in this?",
    "created_utc": "2024-11-08T04:35:04",
    "num_comments": 20,
    "comments": [
        "these days fine tuning has became much easier thanks to projects like llmstudio, msty and many more. using these you can fine-tune any llm, open-source ones that runs on your machine or cloud ones. But to optimse, find those optimal hyperparameters, is more of an alchemy... all the best..",
        "[https://www.youtube.com/watch?v=Gv9\\_4yMHFhI&list=PLblh5JKOoLUICTaGLRoHQDuF\\_7q2GfuJF](https://www.youtube.com/watch?v=Gv9_4yMHFhI&list=PLblh5JKOoLUICTaGLRoHQDuF_7q2GfuJF)\n\nStatQuest has a great crash-course on machine learning. \n\nI am currently studying ML and I used this playlist to understand the concepts.\n\nAfter this, research PyTorch so you don't need to program it all from scratch.",
        "Just curious, what's your goal with fine tuning? From other comments it's clear the process to do it is easier, but aside from some kind of loss optimization, how are you planning to decide if it's working for you?",
        "Hugging face tutorials.",
        "Search in r/LocalLLaMA for tutorials. Despite the name, the sub has broadened to cover all open source LLMs. They are very beginner-friendly. It's mainly a community for enthusiasts and hobbyists rather than students.",
        "check this out https://github.com/unslothai/unsloth all you wanna do is a simple thing tune you can do most of it right there",
        "Use open ai API",
        "check this out https://github.com/unslothai/unsloth all you wanna do is a simple thing tune you can do most of it right there",
        "To add to this, there's also unsloth library to easily fine-tune llama and other models. If you access the repo you'll find a notebook with everything you need.",
        "There are notebooks from unsloth.ai too, which claim to finetune llms using only colabs memory (and yea it's pretty easy)",
        "Thanks for sharing this!! This will be helpful! :)",
        "Great videos— but everytime he makes that BAM and Triple Bam sounds—it annoys the shit outta me",
        "I’m trying to grasp an overview of things for my upcoming contract work.",
        "Interesting",
        "Thanks! Unsloth seems to be really popular!",
        "Can you explain how it works? I have tried to use their colab notebook but they had multiple options to download the fine-tuned model, I didn't really know how since it's my first time, should I do the regular download of the gguf type download, and when I download it gets saved to the colab server not to my local pc, and even if I did download on my pc how would I do inference? I was hoping to fine tune a llama since they seem to be popular",
        "Wish there was a mix between StatQuest and 3Blue1brown.",
        "I downloaded the notebook to my Linux PC with RTX 3080 24 GB and had no problems running everything locally.",
        "woof—true that.",
        "Check out @aibutsimple on instagram!!!! i love their content and i think that they have exactly what you’re looking for:))) (im not sponsored or anything btw lel)",
        "Check out @aibutsimple on instagram!!!! i love their content and i think that they have exactly what you’re looking for:))) (im not sponsored or anything btw lel)"
    ]
},
{
    "submission_id": "1gmgokc",
    "title": "Do you use scraping tools? Which one?",
    "selftext": "Or rather how do you obtain your data?",
    "created_utc": "2024-11-08T03:48:00",
    "num_comments": 16,
    "comments": [
        "> import requests\n\n😎👍",
        "Depends on what data you’re talking about. 🤷🏾",
        "You have python libraries like scrapy which allow you to scrape web data - but sites will have mostly some kind of anti scraping tech to hinder that. You can look up on the different solutions or different tools that help you.",
        "Yes, Python + Selenium (+ Chromedriver) + Vivaldi",
        "requests if I can get away with it.\n\nIf not then selenium with chromedriver works 95% of the time once you set some chrome options (the only issues can be IP based reputation and if the host has set under attack mode in Cloudflare in my experience).",
        "If you want a scraper with GUI, I used “Web Scraper” extension on chrome. Takes a little bit getting used to but for most cases it’s fine for beginners.\n\nOtherwise, Beautiful Soup, or Scrapy or Selenium",
        "Selenium",
        "I use Bright Data's [scraper APIs](https://get.brightdata.com/bd-products-web-scraper?sid=c001) for my projects. They have plenty of APIs for sites like Amazon, Facebook, LinkedIn, etc. Works perfectly for me. Extremely reliable service and [GDPR compliant](https://get.brightdata.com/bd-ethical?sid=c001) as well.",
        "Yes in the clinic .......",
        "Scrapy is light weight and scalable",
        "I usually use scrapy and it gets the job done with ease.",
        "Could u share your experience? What data do u consume?",
        "Web, and IOT sensor data etc.",
        "How do u obtain your web data?",
        "Get the data by sending a GET or POST request. Then clean the data using regex, or beautifulsoup etc. Pandas can directly convert html table into dataframe, so you can use that too.",
        "Raw requests, or selenium/alternatives"
    ]
},
{
    "submission_id": "1gmfzbs",
    "title": "Looking for benchmarks on ML model times and energy consumption",
    "selftext": "\nHi everyone!\n\nI'm working on a project that touches on ML for predictive maintenance, although it's not the main focus. I need data on training and inference times for generic ML models used in predictive maintenance, ideally with information on different hardware resources (CPU, GPU, TPU, etc.).\n\nAdditionally, if there's data available on energy consumption for these tasks, that would be helpful too, though it's not my main priority.\n\nDoes anyone know where I could find this kind of data, or any relevant benchmarks? I'd really appreciate any pointers to datasets, research papers, or resources where I could find this information.\n\nThanks so much in advance!",
    "created_utc": "2024-11-08T03:01:33",
    "num_comments": 1,
    "comments": [
        "That kind of data doesn't exist. Check fp32 performance and average power draw"
    ]
},
{
    "submission_id": "1gmfvp9",
    "title": "7 Computer Vision Projects for All Levels",
    "selftext": "",
    "created_utc": "2024-11-08T02:54:45",
    "num_comments": 0,
    "comments": []
},
{
    "submission_id": "1gmf2ru",
    "title": "Computer for Data Science ",
    "selftext": "Hello, I'm going to build a computer for machine learning and deep learning, and I'm interested in learning about computer configurations from other Data Scientists. If it’s not difficult for you, then could you tell us about your config. Thanks in advance!",
    "created_utc": "2024-11-08T01:57:30",
    "num_comments": 1,
    "comments": [
        "Most laptops will do."
    ]
},
{
    "submission_id": "1gmen8u",
    "title": "Time series classification",
    "selftext": "TL/DR: what is the best way to output for DL classification model for timeseries? Single output with many values per label, or many „binary” outputs, each for one option?\n\nHello, I am new to DL, but I have prepared low level/embedded/CUDA acceleration solutions for ML/DSP.\n\nI have a timeseries of indicator, and I would like to prepare machine learning or DL model that will classify if I should open long or short trade position, based on this indicator (it’s timeseries, sampled by 1 min, but I’d rather use it with 30min interval, and calculate mean or median, range of values is between 0 and 1)\n\nIs it better to have one output model and check if it outputs -1 for short, 0 for cash and 1 for long? Or maybe I should have 3 outputs, one for each option mentioned before, and check which output was set to 1?\n\nI tried this first option with Conv1d and LSTM layers and got nothing but overfitting…",
    "created_utc": "2024-11-08T01:23:26",
    "num_comments": 4,
    "comments": [
        "Usually for classification task you use n column that corresponds to n classes. Values in this columns are normalized to be between 1 and 0, because binary crossentropy loss uses sigmoid to estimate correctness of the output. You won't be able to apply that loss in case of 3 number which in my opinion will make the task much harder",
        "I think you should read up a bit on ML/DL before diving into time series, especially if it's financial market data. Regarding your question, the standard way is to one-hot-encode your outcomes so it forms a 3-dimensional vector as your output. Typically you would then stick a softmax activation on this so it forms a probability distribution.",
        "I read oreillys „Hands on machine learning with scikit, keras and tensorflow” (probably I’ll do that again) and watched bunch of online course videos, do you suggest something else? Very happy to pick that",
        "Maybe Elements of Statistical Learning? Lots of people learn ML these days by hands-on implementing things which is great, but don't really seem to understand the fundamental statistical principles of what we are trying to do here. \n\nI only mentioned that as it's a very basic question on general classification (which is totally fine!), but there are some very difficult challenges to deal with for time series and especially finance - least of which is nonstationarity and changes in functional relationships.",
        "Perfect, I had statistics at college, but my major was electronic engineering, so I did not learn much.\n\nThank you!"
    ]
},
{
    "submission_id": "1gme6yj",
    "title": "📉 Linear Regression Made Easy: Predict Like a Pro in 5 Minutes! 🚀",
    "selftext": "Don’t miss out—check out our videos!\n1) Naive Bayes - https://youtu.be/fNRswUlLx4g?si=xlz4Qvb8gFzZbT-a\n2) Decision Trees - https://youtu.be/oGXbD36YM0s?si=aydB9iM3YLUH4FaG\n3) SVMs - https://youtu.be/3yEkquDCRHA?si=pXQAfmwvVOH_cRQF\n4) KNN - https://youtu.be/bzwU8B41EV4?si=wJIbLYQPqzxzVDo_\n5) Logistic Regg - https://youtu.be/bzwU8B41EV4?si=wJIbLYQPqzxzVDo_\n\n🎉 Welcome to SyntaxGrid, where data meets excitement! Today, we’re taking a thrilling ride into the world of Linear Regression! 🚀✨ This incredible machine learning algorithm might sound simple, but it packs a punch when it comes to making predictions! 🤖💡\n\nIn this video, you’ll discover:\n\n📉 What is Linear Regression? – Learn how this technique helps us predict continuous values based on input data!\n💡 Simple & Powerful – Master the basics of Linear Regression and start making predictions like a pro!\n🔍 How It Works – See how it fits a straight line to your data points and predicts future values.\n🏡 Real-World Applications – From housing prices to stock market trends, discover where Linear Regression is used!\n🔥 Easy-to-Follow Example – Watch step-by-step as we build a model and predict real-world data!",
    "created_utc": "2024-11-08T00:48:58",
    "num_comments": 0,
    "comments": []
},
{
    "submission_id": "1gme04k",
    "title": "Choosing a Backend Framework for ML Prediction and Matching",
    "selftext": "Hey, if you’re going with some ML for prediction and matching—nothing very complicated—and implementing it in a web app, what sort of backend framework should I use, and what workflow should I implement? Usually, I’ve worked with Express.js.",
    "created_utc": "2024-11-08T00:33:59",
    "num_comments": 0,
    "comments": []
},
{
    "submission_id": "1gmdexf",
    "title": "How do you acquire a good skillset to be a proper ML engineer and land a good job in it? ",
    "selftext": "I(20M) have been trying to build some good skills to dive deeper into this field, i've to write some papers and also create some projects i've created a couple of them like deep fake video detection and a couple of them using yolo as well, but i'm quite confused at this moment and what do companies actually want to see within an student they might hire for a job/intern role? would love if someone could drop some tips or maybe some resources as well for me to work more on those topics",
    "created_utc": "2024-11-07T23:50:22",
    "num_comments": 1,
    "comments": [
        "Honestly, you dont need to be \"really good\" at anything to land an entry level job.  \n  \nI reckon the greatest skillset for landing any job is to make connections.  \nGo up to the people in authority, sell them your skills and basically convince them that you can make shit work."
    ]
},
{
    "submission_id": "1gmd73t",
    "title": "Need help running local LLM inference with AMD GPU on Linux (Navi10, gfx1010, RX 5600 XT, Ubuntu 22.04.5 LTS)",
    "selftext": "Hi, so recently I came over alot of guides and succesfully setup Stable Diffusion to run on my Navi10 card with ROCm + pyTorch + ComfyUI (webUI)\n\nI want to be able to also demo some LLMs locally, but ran into some problems. My setup currently: \namdgpu-dkms 6.8 + ROCm 6.2 + torch1.13.1+rocm5.2 + ComfyUI\n+ llama (latest)+ggml_hipblas+amdgpu_targets=gfx1030\n(I have problems with latest torch so had to downgrade torch1.13.1+rocm5.2 so SD can work, set environment variable to gfx1030)\n\nI also want to retain somewhat of my previous setup so that I can switch to SD at ease\n\nMy problem is: apparently, latest llama failed on my setup with the same way latest pyTorch failed before (screen froze, with artifacts, it didnt crash but hang and i eventually had to press shutdown button), I remember trying this on a llama3 7b model (did not installed any additional webUI such as Ollama yet)\n\nMy question is: since I saw ppl guide to first uninstall amdgpu-dkms when setup llama.cpp, I want to know if it has any effects on this? And if uninstall it now would prevent me to run SD? Any advice to make it work on my machine is greatly appreciated.\n\n",
    "created_utc": "2024-11-07T23:33:48",
    "num_comments": 1,
    "comments": [
        "."
    ]
},
{
    "submission_id": "1gmbkvo",
    "title": "Convert Any PyTorch ML Model to TensorFlow, JAX, or NumPy with Ivy! 🚀 ",
    "selftext": "Hey everyone! Just wanted to share something exciting for those of you working across multiple ML frameworks.\n\nIvy is a Python package that allows you to seamlessly convert ML models and code between frameworks like PyTorch, TensorFlow, JAX, and NumPy. With Ivy, you can take a model you’ve built in PyTorch and easily bring it over to Tyour framework of choice, be it TensorFlow or JAX without needing to rewrite everything. Great for experimenting, collaborating, or deploying across different setups!\n\nOn top of that, we’ve just partnered with [Kornia](https://kornia.readthedocs.io/en/latest/get-started/multi-framework-support.html), a popular differentiable computer vision library built on PyTorch, so now Kornia can also be used in TensorFlow, JAX, and NumPy. You can check it out in the latest Kornia release (v0.7.4) with the new methods:\n\n* `kornia.to_tensorflow()`\n* `kornia.to_jax()`\n* `kornia.to_numpy()`\n\nIt’s all powered by Ivy’s transpiler to make switching frameworks seamless. Give it a try and let us know what you think!\n\n* **Install Ivy**: `pip install ivy`\n* **More info**: [Ivy on GitHub](https://github.com/ivy-llc/ivy)\n* **Ivy Demos**: [Demos](https://www.docs.ivy.dev/demos/examples_and_demos.html)\n* **Ivy Discord:** [Discord](https://discord.com/invite/vKqazsCK2Y)\n\nHappy experimenting!",
    "created_utc": "2024-11-07T21:42:20",
    "num_comments": 4,
    "comments": [
        "This is cool. As a diehard TensorFlow fan, it could be nice to have a feature for TF -> Torch for those annoying people who refuse to use TF. How does it handle custom layers or models though?",
        "Haha, isn’t the PyTorch -> TensorFlow direction going to help with that a bit? 😄 We’ve noticed a big shift towards PyTorch in the ecosystem, but we know there’s still a solid TensorFlow fanbase out there (and plenty of people who use both). Our goal is to give everyone access to models across frameworks, regardless of preferences, so those “stubborn PyTorch users” aren’t left out of the TensorFlow world, and vice versa!\n\nOn custom layers and models: Ivy’s IR works at the functional API level (so things like `torch.mean`, `torch.sin`, etc.), and since high-level classes/layers are essentially wrappers around these functions, we’re able to convert any custom layer/model pretty seamlessly. This means we can take almost any arbitrary PyTorch model or custom class and run it in supported frameworks like TensorFlow, JAX, or NumPy without extra tweaks. Long-term, we’re definitely working towards enabling easy conversions across all framework permutations—stay tuned!",
        ">for those annoying people who refuse to use TF\n\nYou say that as if it's not 99% of the population",
        ":("
    ]
},
{
    "submission_id": "1gmash8",
    "title": "Invitation to present/teach at \"The AI Hour\" ",
    "selftext": "Team Vizuara is starting a lecture series called \"The AI Hour\". This is Season 1, and we will have 12 lectures this season.\n\n \n\nhttps://preview.redd.it/lsrnsv8ozlzd1.png?width=1200&format=png&auto=webp&s=7db89a800acbdfb6485ce0334b8784bdcde94c9f\n\nYou can be anyone: school student, college student, professor, industry professional, PhD, post-doc, etc. \n\n \n\nWe invite you to teach anything related to AI for 1 hour as part of this lecture series. You can teach any of the following. \n\n \n\n\\- Your AI research\n\n\\- Technical topics or concepts in AI\n\n\\- Any interesting research paper you have come across\n\n\\- Your AI-related project\n\n\\- Future of AI \n\n \n\nYou can even teach simple concepts like how neural networks work.\n\n \n\nYour lecture will be delivered live to an audience via Zoom, and we will post your talk to Vizuara's YouTube channel for added visibility. Our typical audience consists of AI enthusiasts, professors from India and the US, PhD students, industry professionals, etc.\n\n \n\n\"The AI Hour\" will give you, your work, and your ideas tremendous visibility. If you wish to gain visibility and an audience, this is the ideal platform for you.\n\n \n\nWe have 12 lectures this season. So we will be shortlisting only 12 applicants.\n\n \n\nDo you have to be an expert at everything related to AI to give a talk at \"The AI Hour\"? No. \n\n\n\nThe only criterion will be storytelling ability: \"How well can you deliver the talk to a broad range of audiences?\". \n\n \n\nOur first lecture will be in November (the date is to be decided) and Season 1 of \"The AI Hour\" will go on till early 2025.\n\n \n\nIf you are interested in applying for \"The AI Hour\" Season 1, here is the application link. You don't have to submit your lecture content right now, but just the broad topic. Filling out the application won't take you more than 5 minutes: [https://docs.google.com/forms/d/e/1FAIpQLSfcAr\\_p4IOq-cS63VhC4qWUH3zziJLlNRGo1nI36tC7B9qCsQ/viewform](https://docs.google.com/forms/d/e/1FAIpQLSfcAr_p4IOq-cS63VhC4qWUH3zziJLlNRGo1nI36tC7B9qCsQ/viewform)\n\n \n\nDeadline to apply: November 12th, 11:59 pm IST",
    "created_utc": "2024-11-07T20:54:46",
    "num_comments": 0,
    "comments": []
},
{
    "submission_id": "1gm8301",
    "title": "How can i get a code dataset quickly?",
    "selftext": "need to gather a dataset of 1000 snippets of code for 4 different languages each. Does anyone have any tips on how i could get that quickly? 1 tried githubs API but i can't get it to do what i want. Same with code forces API.\nMaybe there's something like a data dump or something? I can't use a kaggle dataset i need to get it myself and clean it and stuff. Thanks for your time",
    "created_utc": "2024-11-07T18:27:42",
    "num_comments": 1,
    "comments": [
        "I mean, you could webscrape from public github repos, if the API does not work"
    ]
},
{
    "submission_id": "1gm79r6",
    "title": "Paper suggestions Neural networks",
    "selftext": "Hello, \nI’m an aspiring NN researcher, I’m currently pursuing a MS and want to go forward with a PhD m. \nI found that I’m mostly interested in NNs mostly. I am looking to get myself familiar with popular papers and new technology to stay up to date. \nWhat are some good papers to start with? (beginner) Maybe 3-4 recommendations? I have basic understanding of neural networks and their algorithms and how to implement them.\n\nThank you! ",
    "created_utc": "2024-11-07T17:46:26",
    "num_comments": 1,
    "comments": [
        "https://github.com/terryum/awesome-deep-learning-papers"
    ]
},
{
    "submission_id": "1gm5s7o",
    "title": "Semantic Segmentation for Flood Recognition using PyTorch",
    "selftext": "Semantic Segmentation for Flood Recognition using PyTorch\n\n[https://debuggercafe.com/semantic-segmentation-for-flood-recognition/](https://debuggercafe.com/semantic-segmentation-for-flood-recognition/)\n\nFollowing the previous article, we have another project combining deep learning and environment. Millions of people all over the world get displaced due to floods. It’s true that by using deep learning + computer vision, we cannot always predict when the next flood will hit. But we can train a semantic segmentation algorithm on images of flood-hit areas. Such a model can help in the analysis and decision-making for future situations. To do our tiny bit, we will train a **semantic segmentation model for flood recognition** using PyTorch in this article.\n\nhttps://preview.redd.it/d9lhrhbvokzd1.png?width=1000&format=png&auto=webp&s=d93d27df52fb65250edfbaac8865a562e9018da1\n\n",
    "created_utc": "2024-11-07T16:32:30",
    "num_comments": 3,
    "comments": [
        "What's your thought on SAM2? Now that SAM2.1 is released, I believe it can perform well on unseen data due to the zero-shot nature of the model. But again, the dataset is so tiny to fine tune it- we might need to do this so carefully (less likely)",
        "That's a great question. The smallest SAM2.1 mode is around 37M parameters. Maybe I can try zero-shot segmentation through pointing and then fine-tuning for semantic segmentation to check how it performs.",
        "Exactly"
    ]
},
{
    "submission_id": "1gm4kg2",
    "title": "What do you think about this approach to function-calling (Text to Action)?",
    "selftext": "For a specific application, we would create embeddings for sample prompts for functions descriptions. Then:\n\nSearch vector database-> Get most appropriate action to perform -> Get input parameters using an LLM -> Perform the action\n\nThe goal is to make it easy to automate tasks from natural language queries. So, unlike other systems that fully rely on LLMs for every part, here the LLM is mostly for interpreting commands, while actual action execution is handled by the codebase itself.\n\nAre there any improvements you’d suggest, or things I should consider? Are there any specific features you think would make a system like this even more useful?\n\n[https://github.com/sri0606/text\\_to\\_action](https://github.com/sri0606/text_to_action)",
    "created_utc": "2024-11-07T15:35:30",
    "num_comments": 0,
    "comments": []
},
{
    "submission_id": "1gm3m6j",
    "title": "Why is my logistical regression a straight line? ",
    "selftext": "Completely new to JASP. It says it’s suppose to be S shaped and I really don’t know what I’m doing wrong. The dependant variable is binary, so I really don’t know. Any help would be appreciated.",
    "created_utc": "2024-11-07T14:53:00",
    "num_comments": 12,
    "comments": [
        "Never used JASP so can’t help you there. I’m not going to lie, I’ve never quite seen the straight line you’re getting here. But it seems like “life satisfaction” is a really poor predictor of “belief id”. \n\nLogistic regression assumes the odds of your dependent variable (binary) are correlated with your independent variable (continuous). E.g. if life satisfaction goes up, so do the odds; you get the S-shaped curve when people who have a high life satisfaction tend to have belief=No and people who have a low life satisfaction have belief=Yes (or vice versa). \n\nLooking at the plot, it seems like there is no real relationship between these two variables. People seem to hold either belief no matter their life satisfaction. My guess is that the estimate line is hovering around 0.25 because 25% of people have belief=No, and the value of life satisfaction does not hold enough information to describe how that proportion should change.",
        "It seems, from the data distribution, that  X variable is irrelevant to the y variable \n\n[if you look at logistic regression ](https://en.wikipedia.org/wiki/Logistic_regression) you will notice that the variance given by the data influence the curve of the regression\n\nsince number of people who are satisficed with their lives equal number of people who aren't at both when prob =0 or 1 that means that your independent variables will give equal probability to both classes so your logistic regression will be a straight line ( insignificant)",
        "post data",
        "Did you check your model assumptions and potential data correlation before modelling ? Can't just jump into it with two variables and hope for the best (most of the time)",
        "You'll need to give some more information to equip others on here to help you.\n\nHave you run descriptive statistics / made charts for each of your variables of interest before modelling? This is absolutely step number 1, as it allows you to check whether the variable values make sense.\n\nWhat exactly is your binary 'y' variable measuring?\n\nHave you performed any cleaning or recoding of the variables?\n\nWhat others have said appears true – that life satisfaction is a poor predictor of your outcome variable – but the above steps need to be addressed before you can conclude that.",
        "I’ve ran descriptive statistics, but I don’t have the knowledge to tell if all my pairing variables make sense. However I’m noticing no matter what variable I pair with belief, it’s always showing me a straight line, and I was giving 100 to pick from. This is annoying as belief is meant to be my dependant variable everything I am writing is based off. For reference it’s for an essay where I am currently writing on link between belief in life after death and one’s life satisfaction",
        "To answer your question your estimate is really small, 0.014. You are looking at a small range in the x-Avis (0-10). Look at 0-300 instead.\n\nThe p-value is 0.399 which means there is no significant correlation between life satisfaction and belief. If belief is your target (y) variable, try putting all 100 other variables in the model at the same time. Remove all variables where the p-value i greater than 0.05 (or what you decide is significant with so many variables, 0.05/100 could be good). Then only keep variables that have significant p-values and run the model again. If you have no significant variables this might be your problem and you could conclude that none of the variables correlate to your output. This is a result in itself.\n\n\nEdit: typos",
        "The beta on your independent isn't significantly different from zero.",
        "i feel you",
        "It’s a data set with variables my uni course already provided me. I just then had to make tables and graphs with the variables relevant to my essay. It’s my first time using this kind of software so could you expand what you mean please?",
        "For JIRA, can you get any description or summary of the model to inspect how it looks? Are there any error messages or logs? If you can’t find one variable that fits better than a straight line, maybe you’re not using the software correctly. \n\nBy “descriptive statistics”, we mean things like plots or correlations. Transform your dependent variable from True/False to 1/0. \n\nNow if an independent variable is categorical, like sex, we can use bar charts to see if the proportion of belief in the afterlife significantly changes when you’re male or female. Or just take the mean and see for yourself. If a variable is continuous, like age, use a scatterplot and see if people more often believe in the afterlife (fall on the y=1 line) when they’re older. You can also take the correlation to get a sense for this. \n\nLater, try combining multiple variables that have impacts on the dependent variable into the regression once you get a better-looking output for a simple model with one X variable. I doubt there isn’t a single variable out of 100 that doesn’t help explain your dependent variable, especially if this is a university assignment. But if that’s truly the case, that can be a spectacular write-up, too.",
        "I haven't got any experience with JASP, but it looks like the technology is working correctly and the issue is with the data. It just doesn't look like there's a correlation between the two variables that your model can detect, this could be because your data is messy, you're using the wrong model, or simply that there isn't a relationship between the variables. I'd maybe explore the distribution of your variables and make sure you're using the correct model (assuming the data is given to you with a purposeful correlation)."
    ]
},
{
    "submission_id": "1gm232o",
    "title": "Why are model_q4.onnx and model_q4f16.onnx not 4 times smaller than model.onnx?",
    "selftext": "I see on https://huggingface.co/HuggingFaceTB/SmolLM2-135M-Instruct/tree/main/onnx:\n\n| File Name          | Size   |\n|--------------------|--------|\n| model.onnx         | 654 MB |\n| model_fp16.onnx    | 327 MB |\n| model_q4.onnx      | 200 MB |\n| model_q4f16.onnx   | 134 MB |\n\n\nI understand that:\n\n- `model.onnx` is the fp32 model,\n- `model_fp16.onnx` is the model whose weights are quantized to `fp16`\n\nI don't understand the size of `model_q4.onnx` and `model_q4f16.onnx`\n\n1. Why is `model_q4.onnx` 200 MB instead of 654 MB / 4 = 163.5 MB? I thought `model_q4.onnx` meant that the weights are quantized to 4 bits.\n2. Why is `model_q4f16.onnx` 134 MB instead of 654 MB / 4 = 163.5 MB? I thought `model_q4f16.onnx` meant that the weights are quantized to 4 bits and activations are fp16, since https://llm.mlc.ai/docs/compilation/configure_quantization.html states:\n\n   >  `qAfB(_id)`, where `A` represents the number of bits for storing weights and `B` represents the number of bits for storing activations. \n\n  and [Why do activations need more bits (16bit) than weights (8bit) in tensor flow's neural network quantization framework?](https://stackoverflow.com/a/72397979/395857) indicates that activations don't count toward the model size (understandably).",
    "created_utc": "2024-11-07T13:46:01",
    "num_comments": 0,
    "comments": []
},
{
    "submission_id": "1gm1b2k",
    "title": "Study group to learn ML together",
    "selftext": "Hi! I'm a CS Engineer. I've been a Mobile Dev for 10 years and I'd like to learn ML/AI. \n\nThere are a lot of resources online, but I'd love to learn/study with someone else too. To keep it more entertaining and keep ourselves accountable. \n\nIf anyone is interested let me know! :)\n\nContext:\nI'm not looking to switch careers to ML, but I'd like to have a better understanding of the models. Mostly to see what the use cases are, to know what could be implemented in mobile/web apps. ",
    "created_utc": "2024-11-07T13:13:28",
    "num_comments": 48,
    "comments": [
        "Hello, I would like to know if there is a place for someone who is just learning programming and AI. Currently, I am working on implementing the K-Nearest Neighbor (KNN) algorithm for CIFAR-10 data using PyTorch. PyTorch is only being used for data manipulation.",
        "Im down!",
        "If you would like some AI stuffs for mobile, I suggest checking out MIT Han Lab (https://www.youtube.com/@MITHANLab)",
        "👀 +1",
        "+1",
        "Interested",
        "Count me in",
        "Im in",
        "I’m interested! Any idea what sort of learning pathway you are imagining?",
        "count me in",
        "I am in",
        "Me too",
        "Count me jn",
        "Count me in, I'm doing undergrad research",
        "I’m in as well !!",
        "I’m in ☝🏽",
        "+1 am in",
        "I'm down too! Just bought a book Build a Large Language Model (From Scratch). Currently building projects on agentic search. Turns out building things is only way for me to stick with learning consistently.",
        "I am in 🙋‍♀️",
        "I'm in!",
        "I am in as well",
        "Conduct a meeting daily to share the insights about machine learning daily",
        "Count me\nIn",
        "In also!",
        "I'm in a group in which we started reading and discussing the reinforcement learning book from sutton and barto, we did the part1 of the book (first 8 chapters) then we stopped, now I'm waiting for another user to arrive at the same point to resume, if you want to read the part 1 on your own you can join us",
        "I've started a private discord channel for this exactly. It's not a community it's more of a focus group. I'm running our first hackathon (2 week duration) this week.\n\nWe have a nice array of people, from experienced devs to data scientists and computer scientists. There's about 15 of us, but I expect that a few will drop off.\n\nI'm encouraging high engagement and activity, and I'm driving the content. Although I'm also learning.\n\nDM your discord, I'll add you :)",
        "Hey, I think the easiest way is to start building ML models of your own. I started by using model generation through [https://www.plexe.ai/](https://www.plexe.ai/) and iterated on the output and used it in Kaggle competitions",
        "+1",
        "I'd love to team up. How do you wanna do that?",
        "Maybe ,I drop a whatsapp group",
        "I would be excited to join this study group where people can share with their experiences and recommendations how to learn ml/ai, etc. However, I am just a freshman at uni, so I can assume that I am not as proficient as you are in mathematics or programming. I had some kind of experience with machine learning, and working with simple regression models, knn models or random forest classifier but it is really simple with the libraries that devs created in Python.",
        "Why pytorch for that? Use pandas",
        "me too! let's learn together, shall we? ;)",
        "Personally, considering my background and goals, I'd like to take this course:\n\nhttps://coursera.org/specializations/machine-learning-introduction\n\nBut in the discord group I created, there are several people suggesting other pathways, depending on goals and current knowledge :)",
        "I've created a Discord server since many people were interested. We're 120 already. Feel free to join! :)\n\nhttps://discord.gg/pERTrdtNcW",
        "Because the course it is currently developing uses Pytchorn. I was thinking of taking up learning pandas after I get a little familiar with it all. Is it better now to take up learning pandas right away ?",
        "What we doing, discord?",
        "Sweet! I am actually doing this specialization right now. I just finished course 1 yesterday and have really enjoyed it. \n\nI’d be super down to keep going on this pathway with a group. I was thinking I’d move to the deep learning specialization after.",
        "Im am doing this specialization rigth now. Maybe not the better one, but easy to follow, and gives you the desire of more. Ask chatgpt for a coustom path for your goals, maybe you will be surprised",
        "Dm'ed you, could you please check?",
        "Pandas will take you 3hrs in total to learn, nothing to worry.",
        "yep. have you created a channel already? send the link please",
        "Thanks for the answer, so I'll start learning it and change the code to use Pandas for now.",
        "In my opinion, pandas is basically an easy and a bit limited version of numpy. .",
        "I just created it! Feel free to join! \nhttps://discord.gg/pERTrdtNcW",
        "A telegram group may be more easy for sharing files",
        "Agreed"
    ]
},
{
    "submission_id": "1gm06zj",
    "title": "GPU and Computing Technology Comparison 2024 – day 7\n",
    "selftext": "",
    "created_utc": "2024-11-07T12:26:19",
    "num_comments": 0,
    "comments": []
},
{
    "submission_id": "1glziyx",
    "title": "9x faster model serving without changing hardware?",
    "selftext": "hey. i saw this blog post about onnx-runtime in python/rust:  \n[https://martynassubonis.substack.com/p/optimize-for-speed-and-savings-high](https://martynassubonis.substack.com/p/optimize-for-speed-and-savings-high)\n\ndoes anyone have experience with this at work? does it work/how hard is it to implement/maintain?",
    "created_utc": "2024-11-07T11:58:49",
    "num_comments": 0,
    "comments": []
},
{
    "submission_id": "1glzh60",
    "title": "The field of adversarial machine learning feels a lot like churning \"peer reviews\" into research papers.",
    "selftext": "**Here are some of my observations I would like to share:**\n\nI've always had some problems with the premise of adversarial machine learning.\n\nBefore going further, I do think it is nice to find weak spots of ML systems. Although ML systems are known to be brittle to begin with (Are commercial airlines adversarial ML systems since NN can't fly them?).\n\nHowever, I've also found that a lot of the work in this area are basing on some wild assumptions that are unimplementable in practice. For example, all white box models are cool but useless. Even black box models feel kind of useless. The useful types of adversarial machine learning seems to need to be able get through human security guards (these ML models may or may not exist in reality.)\n\n**Yet despite all the unrealism and non-applicability, the papers in this area just keeps on coming. What's sustaining this area of research?**\n\nAt this point I am wondering whether the field of adversarial machine learning becoming about churning \"peer reviews\" into research paper.\n\nNormally, given any newly introduced model or approach, you can write a peer review in a nice pre-formatted comment section. You, the reviewer, attack the premise of the model/approach.\n\nBut it seems that this is process turning into full-fledged paper writing. Any weakness or things failed to address by a paper (original paper) is turned into another paper (adversarial paper).\n\nAre researchers studying actual problems are inventing problems that aren't there?",
    "created_utc": "2024-11-07T11:56:40",
    "num_comments": 1,
    "comments": [
        "As far as I know, variations of jacobian loss functions are used to help with adversarial loss. In the case of llms, they probably have enough controls beforehand to handle adversarial examples, and no one is clearly doing an active learning version of an llm. (No one wants a repeat of the tay bot)"
    ]
},
{
    "submission_id": "1glxqbk",
    "title": "[P] What would best work with CNN for a hoax url classification model? LSTM or GRU?",
    "selftext": "\nHello! I was thinking about partaking in this project where I would build a fake url detection model per say and as I was reading through the literature the idea of adding an extra element to the model besides it being mainly a CNN based model I could also include LSTM in order to see if that would grant me better results or not. But then I did more research and found out about GRUs and now i’m really conflicted on which to choose to be used with my CNN model.  I know the difference between LSTM and GRU but I think I just need some advice or experts’ opinion about this matter. Also unfortunately there really isn’t as much of papers about using CNN-GRU compared to CNN-LSTM so it’s making me question things more. If someone can help out that would be great! Thank you!",
    "created_utc": "2024-11-07T10:43:19",
    "num_comments": 0,
    "comments": []
},
{
    "submission_id": "1glxc8c",
    "title": "I'm a former Senior Software Engineer at Tesla, had non-technical jobs before I got into software engineering, and now AI/ML instructor at a tech school - AMA",
    "selftext": "UPDATE:  Thanks for participating in the AMA. I'm going to wrap it up (I will gradually answer a few remaining questions that have been posted but that I've not yet answered), but no new questions this time round please :)  I've received a lot of messages about the work I do and demand for more career guidance in the field. LMK what else you'd like to see, I will host a live AMA on YouTube soon.\n\n\\- To be informed about this (and everything I'm currently working on) in case you're interested, you can go here:  [https://www.become-irreplaceable.dev/ai-ml-program](https://www.become-irreplaceable.dev/ai-ml-program)\n\n\\- and for videos / live streams I'll be doing here: [https://www.youtube.com/c/codesmithschool](https://www.youtube.com/c/codesmithschool)\n\nwhere I'll be posting content and teaching on topics such as:\n\n* 💼 understanding the job market\n* 🔬 how to break into an ML career\n* ↔️ how to transition into ML from another field\n* 📋 ML projects to bolster their resumes/CV\n* 🙋‍♂️ ML interview tips\n* 🛠️ leveraging the latest tools\n* 🧮 calculus, linear algebra, stats & probability, and ML fundamentals\n* 🗺️ an ML study guide and roadmap\n\nThanks!\n\n\\--\n\nOriginal post:  I get lots of messages on LinkedIn etc.  Have always seen people doing AMAs on reddit, so thought I'd try one, I hope my 2 cents could help someone. IMO sharing at scale is much better than replying in private DMs on LinkedIn. Let's see how it goes :) I will try to answer as many as time permits. I'm in Europe so bear with me with time difference.\n\nAMA! Cheers",
    "created_utc": "2024-11-07T10:27:15",
    "num_comments": 119,
    "comments": [
        "What was the stack at Tesla?",
        "What would you recommend for non-technical people trying to land internships/jobs right now?",
        "What's your background and what did you feel contributed the most to your transition？going from non technical to technical is tough.",
        "Favorite programming book, but for beginners?",
        "Do you think it's possible for an \"average\" person to break into ML/AI? It seems like such a competitive industry. I would love to be part of it one day and I'm willing to work very hard to get there but it seems the field is filled with exceptional people who are doing the same so I'm wondering whether you think this is even feasible.",
        "A part-time Masters in Artificial Intelligence- is it a good idea? It is from a public university so free of cost and I would be doing that beside my job as a product manager in a digital solutions provider.",
        "What did the transition from software engineering to teaching AI/ML entail, and what prompted that change?",
        "Realistically, how’s the WLB there?",
        "What was the most unique aspect of Teslas work processes",
        "Trying to think of the best way to put this. \n\nHow important is it for people to have a coherent skillset to be competitive applying with big tech? I studied comp e and have been all over the place with software and even ee so far in my early career and feel like my generalist background and lack of “branding” is hurting me. I’m often unsure how to sell myself as a specific type of SWE. For example I have been looked over recently for embedded roles (I’ve done embedded projects and been adjacent to even more, same with android app development and a few other things). Should I abandon trying to be a generalist type guy and try and dress myself up for specific roles? \n\nWorking on a masters right now but even that is pretty unfocused. I just like projects and being able to do a little of everything and am having a hard time trying to stamp myself. Interested in your thoughts. Thanks!",
        "What was your educational background and what roles did you have before landing this role at Tesla?",
        "I am also an ex-software engineer trying to switch to ML(currently doing my masters in ML). What do you think about ML systems as a field, and what do I need to do to build expertise in it? I am asking because I feel that Tesla must have a lot of work to do around low-latency ML models.",
        "What a nice AMA!",
        "No questions as I’m pretty sure reading this will be enough for me. \n\nThanks for taking your time to respond to all these comments.",
        "1: What are some best practices for designing scalable and maintainable software for hardware integration projects?\n\n2: How would you approach testing and debugging in hardware-software integrated systems?\n\n3: Do you have experience with machine learning models deployed on edge devices? What pitfalls should I watch out for?\n\n4: What do you see as the next big thing in AI, and how can an engineer prepare to contribute to it?\"\n\n5: As an AI/ML instructor, what are the most common misconceptions students have about combining AI with hardware, and how do you address them?\n\nThanks in advance,\n\nBest",
        "CyrusYari, i just want to give a shout-out to you.  You provide very thoughtful and thorough responses.  At the end of the day, thats why peopleare here - to learn.  This is one of the best AMAs I've read.  Nice job.",
        "If someone wants to break into AI/ML today, what technical skills should they prioritize learning first?",
        "Hi, thank you so much for the AMA!\n\nI just graduated in mechanical engineering (3 years long Bachelor degree) and now I am in a small company as web developer (they taught me both be e fe stack, mainly Java) and I would love to work for Tesla one day.\nWhat do you suggest I learn? Am I doing good in order to be there one day? Should I do a master in CS?",
        "Just wanted to say thank you for the wonderful conversation you've shared here. I'm 35 years old and a non-technical person (filmmaker / editor) that just transitioned into software. I've never felt more drive than when learning and solving the problems with code, and it's honestly the most rewarding thing I've ever done.   \n  \nA little over one year ago, I took my very first Javascript course on Codecademy, and since then I've built and launched my own SaaS company, Recall (www.recallapp.com), which has some basic ML sprinkled in. I'm proud of what I'm building, but I often still compare myself to ivy league CS grads, or any other individuals with credentials / experience that I don't have. When I hear stories like yours, I'm able to remind myself that it's okay, there is no set path for success, and I feel encouraged to press on.   \n  \nLike many have said to me and what you have echoed, it's the mindset of an engineer that matters most.",
        "What’s your view on AI potentially disrupting tech education? Will AI tools eventually teach AI?",
        "For a fresh graduate, how realistic is it to find entry-level machine learning roles, and if these roles are limited, what would you recommend focusing on—both in terms of projects and skills—to stand out and increase the chances of landing a machine learning position?",
        "What is the best thing you achieved ?",
        "Might be a weird question but, do you ever feel imposter syndrome ? And if yes, have you implemented any techniques (maybe some quotes you tell yourself) that help you with it ? I have been struggling with it a lot lately and getting mixed emotions of feeling like I can do everything but some other times feeling like I know nothing.",
        "1) The internet has millions of \"AI tools\" - could you please tell the stack tools that you think you could have used at your time when you were in Tesla (improving personal productivity and achieving more in work with marginal more effort) \n\n\n\n2) I was an AI doomer, now I'm trying to keep out a balanced view so, what is a single (or a group of) technical skill that you think would be a non negotiable for upcoming 20 odd years (eg learning python, or having a base knowledge of LLM's) \n\n\n\n3) People often mark that 20's are for unprecedented work (forgetting about WLB etc) and 30's are to bear the fruit of that work - do you think it still holds true? \n\n\n\n4) Could you please be kind enough to map your \ntransition out so a person (like me who is currently pursuing a finance pedagogy) could stay afloat?\n\n\n\n5) Can you please name 5 influential people who have affected you the most <in a positive sense> (barring, Elon musk and naval ravikant)\n\n\nThank you so much for answering 🫶",
        "What is the most effective strategy to transition from software engineering to a research engineering position without going back to school? My degree is in mathematics and I work at a FAANG-adjacent tech company. I know that I have the knowledge and skills, but how do I sell/advertise myself for these roles effectively?",
        "Hey OP, thanks for sharing your insights!!!",
        "Good post!",
        "If I want to go from a business analytics requirements and documentation role to a AI strategist or ML developer role, what are your favorite resources (courses, books, blogs, etc) to learn the space and keep up to date with the rapid changes?",
        "i've been coding on/off since 18 (now 27) but dropped out of high school. i want to get into AI/ML but my math is very basic. what areas of math should i focus on first, and realistically how long would it take to get to a functional level if i study 1-2 hours per day? looking for the most efficient path forward",
        "So I have an idea that I thought may significantly improve self driving performance with AI.\n\nThe idea is like a neural network with 3 \"phases\". Instead of trying to teach a single network how to just drive, I thought of 3 ways one could categorize the act of driving with these networks. I figured a general neural network may be inferior, and we should work to isolate different types of \"thought\" which go into driving, similar to how the brain uses different lobes.\n\nThe first modal would be focused on executive function, how does one get from point A to point B efficiently?\n\nThe second would be focused on physics and safety, how likely is this course of action to result in a collision or compromising situation? It's the defensive aspect of training, it is constantly measuring risk.\n\nFinally would be the legal modal, which would be trained to understand traffic laws. This one could be interchangeable based on where you're driving to offer a tailored experience.\n\nWhen the executive modal would plan a move, the safety and legal modals would either allow or deny it.\n\nIf the modals detect that one is at imminent risk of collision with the safety modal, then the executive modal would seek the safest way forward, and override any laws. Sometimes one needs to break a traffic law to prevent injury. \n\nThe legal modal could basically veto the executive modal, and the safety modal could veto both.\n\nCurious on what an expert thinks of this idea!",
        "Thanks for sharing your insights.",
        "I'm trying to switch from software developer to Data Scientist. According to you, how much percent of jobs comprise the use of Deep Learning and how much classical Machine Learning Algorithms ?",
        "Whats your take on a Masters in System Engineering versus a Masters in Applied Computing?\n\nEsp around Big Data/AI/ML",
        "Hey, I live in India and come from a fully non-tech background and studying machine in learning for last 6 months. Want to make a long term career. I am confident that I can acquire the competitive skills required.\n\nBut still many a times I get into self doubt as I know that I will be competing with tech people like you who are switching to AI/ML.\n \nSo is there a way to compete with you guys for someone like me without a tech degree and experience ?",
        "When you say \"AI/ML\" do you mean focused on LLMs? Or in general?",
        "I'm in my freshman year of college majoring in AI&ML, personally my college doesn't teach essential skills to get a proper job.\nSo could you please guide me on how to get a job on AI or ML?!?\nThanks for reading this far!!",
        "How do you keep up with all the news and advancement witbout burning out?  \n\nFeels like there's a new key paper every week and each one is dense. I'm not sure how to keep up.",
        "How was your journey of switching from non tech to tech role?",
        "I am a UNI student, interested to specialize in AI, Do you have an advice, or do you know of usefull free sources in deep learning?\nWhat things should I aim to learn.\nTrying to puzzle together youtube videos is not the greatest. I right now aim to being able to implement the networks myself before using libraries, so I never touched tensorflow so far, I don't know if it is a problem.",
        "what projects do we need to pursue in order to land a job ?",
        "Does Elon actually do anything?",
        "How much did you work with traditional data and analytics teams there?\n\nWere they on top of their game and able to provide  centralised data warehouses with collated and comprehensive data for you, or were they silo’d and struggling with too many data sources. \n\nAsking as I have managed many of these teams and found that the funding going into data science, in many companies, is not being mirrored into the centralised data and analytics teams, creating issues whereby data scientists have much more wrangling than they should, and it becomes far harder to productionize the insights.",
        "What would you say to others who want to make a similar leap from non-technical to technical roles in tech?",
        "AI ML is vast so which part do you have experience or expertise in?",
        "Do you think current educational programs are keeping up with the industry’s demands for AI/ML skills?",
        "Do you think MLE in the tech industry is open to people from unconventional backgrounds? What’s been your experience?",
        "How do you teach pretty complex topics like neural networks or deep learning in an accessible way?",
        "Robotaxi. Yay? Nay?",
        "Howz the work life balance at Tesla?",
        "How do you get your first job on the field",
        "I was laid off in January.\n\nI've been programming since the 90s, and working in the games and simulation industry as a full stack generalist for almost 10 years now. My resume suggests I should be a senior, at least mid, or possibly even staff level. I've worked doing senior work and been labelled staff before... but I can't land a job. I've been doing around an interview a week since January, but it feels like every position I interview for has some kind of 'gotcha' with a piece of tech, or a facet of development that I've not got enough experience in. I'd consider doing online courses or trying to shore up skills, but it's mostly gaps in EXPERIENCE I'm being denied for.\n\nAnd worse, salaries and job situations seem to be getting worse by the day, I've gone from interviewing for full-time salaried positions with benefits to short-term hourly contracts paying barely a living wage with no benefits.\n\nHow do I salvage my career?",
        "How do I get hired at Tesla or a similar company? I have a good resume, but it feels like a slog throwing application at the AI resume reviewer most companies probably use. Should I talk to a taken acquisition through social media?",
        "Hey I’m a current junior at University and I’m interested in AI/ML, have a few ML projects on my resume (nothing crazy a movie recommendation system and a suspect description predictior) and I’m currently in a Data engineering/ML internship (it’s a startup and they make me do a little of everything). I see a lot of full time roles ask for atleast a Masters degree and I don’t want to go back to school. Can I be an ML engineer without one and how helpful is a masters degree for ML specifically",
        "Can you provide a roadmap for learning AI/ML and resources and books?",
        "Planning to pursue a masters in computer science with concentration in AI, as I'm completing my bachelors next year(may). I have never touched software development or did any major thing in frontend or backend development, but always learnt data science, Machine learning and deep learning. Is it okay to do so?? Or do i need to learn all other stuff(dev) in order to break into industry??",
        "What are your suggestions for experienced in IT and seeking AI certifications? What are the best certifications for AI careers?",
        "Have been in the industry for 1.5 years as a data scientist, and I want to switch to a more ML researched focussed role. Any pointers on how to achieve that? I don't know if I should get a master's degree, since I'm 1.5 years out of uni, finding good LORs might be hard for me.",
        "brother i am currenty at tier 4 college in india and doing btcse aiml , can you guide me or roadnmap to crack a high paying job",
        "What did you like about working in Sales/Marketing?\nWhat did you not like about it?\nAnd what are some things that you see that AI can add value to the marketing industry?",
        "im trying to build a career in A.I any advices on how to land a nice role",
        "May I know how was your transition from learning at codesmith and Eloquent JS be like? You have actively contributed to open sources? What have you done there? What are the additional skills you have learned and things you do to become a really good developer?  Thank you 🙏🏽",
        "What trends do you see as the most important in AI and ML over the next 5-10 years?",
        "[deleted]",
        "giving me sales guru type of vibe",
        "I'll say as much as I can share, basically stuff that's also on my resume:  the first team I was in was focussed on \"0-to-1\" projects, for internal software, so like a startup. Here we mostly had free rein to do as we wanted as long as the problem was resolved. My brilliant manager used to say \"the owner of the house doesn't care about what type of paint brush is used to paint the house, rather they care about the finished paint\".  This meant we could use AI/ML only if it meant it actually resolved the task at hand. For the most part it was React/Vue on frontend, Node express backend, MySQL db, w docker, REST APIs, sometimes GraphQL given my experience doing that. For any AI/ML added to the stack used some python, tensorflow, chartjs, aws, etc.\n\nWe worked on everything from internal maps / data interfaces that showed vast amounts of factory equipment / databases, to software that tracked the GPS of moving machinery in factory and finding the most efficient routes. Or say software that automated some of the processes on the car assembly line (e.g. rather than using excel, factory workers on the line could now use this software to mark say a defect car door before it reached the next phase and discard the door and replace it, savings a ton of time, and a ton of cost, bringing down the error %), lots of cool stuff, loved it.\n\nLater I got moved to a big team who did things much more like you'd expect at FAANG type companies, everything from standups to documentation etc, because they worked on already developed software that had been deployed for some time, rather than tinkering with new projects. Safe to say I didn't have as much fun in this team, I'm more of a startup guy, but I really admired the technical expertise of people in this team in a different way, they were masters of proper enterprise-level software.\n\nedit: spelling",
        "Start contributing to open-source software online immediately. This is your proof of work, to show off your abilities in order to get a job. The world is now in a more \"show me, don't tell me\" state, hence less emphasis on credentials and more on actual real world ability.\n\nMany developer tools (e.g. Postman API) accept contributors for their open-source software. There are tons of tools out there looking for contributors.\n\nSome people swear by grinding leetcode but I never did. I prefer the approach above, proof of work and open-source contributions.\n\nThe other half of the job hunt comes down to resume/interviewing skills. I'll just give a couple of small examples from the resume, but there's many more: For the bullet points under each role, try to fill at least 70% of the line (I know it sounds dumb), too much white space is not psychologically good for the hiring managers (whether we like that psychology fact or not). Also I'd show your technical expertise (proof of work) by showing you understand why you made some technical decision in the bullet points under each job. \"show, don't tell\".\n\nThen I'd spend an hour per day active on X/LinkedIn/blogging etc. It's a leverage multiplier as Naval Ravikant says. With one post, hundreds can see your thoughts/work (or more!). Very interesting things happen when you share your work and connect with interesting people. The internet is the greatest gift we have today.\n\n  \nGodspeed.",
        "In one of the comprehensive replies I've given in this AMA, I mentioned programming fundamentals as the first thing to focus on in this journey. That was the key. I went to Codesmith coding school a few years ago (where I'm now an instructor in the new AI/ML programme!) and I found their teaching to be superb and a perfect match for my learning style (hard learning), which is why I've gone back and teamed up with them now.\n\nThe second factor that contributed most to this transition is having a sales/business background. I can talk and market for days on end like my life depends on it (both spoken and written, so resume crafting and interviewing). Once in the job you will see how most technical folks are not good at speaking or writing in the workplace, and the job involves a great deal of speaking (meetings) and writing (emails/comms/teams).\n\nWith AI now entering the workplace, I tell mentees that soft skills are now critical, and will be even more important than math skills eventually (as Peter Thiel has stated too). Sales is possibly the most important skill in life, and I tell young mentees to get a job in sales for 6 months if they can. The technical stuff is going nowhere, and the soft skills will aid your technical career tremendously. They go hand in hand.  \"Learn to build, learn to sell, if you can do both you will be unstoppable.\" - Naval Ravikant.\n\nSo to conclude:  programming fundamentals + soft skills, are the two things that contributed the most to my transition.  (oh and lots of grinding of course, lots of activating the freedom app and going into deep work mode when I was transitioning and learning the technical stuff).\n\n(as for my background, it's very unconventional, always followed my curiosity at the time!: [https://www.linkedin.com/in/cyrusyari/](https://www.linkedin.com/in/cyrusyari/) )\n\nedit: spelling.",
        "I have to go for the OG  'Eloquent Javascript' - I learned programming via JavaScript first.  I did Codesmith's CSX online (it's free and superb: [https://csx.codesmith.io/](https://csx.codesmith.io/) \\- also attended their free workshops) and then whenever I got stuck I'd read the Eloquent Javascript book (also free: [https://eloquentjavascript.net/](https://eloquentjavascript.net/) ) , and I'd supplement that with MDN web docs!  That was the magic trio.\n\n\\--\n\nedit \\~20 hrs after creating the original post:  have had a few messages asking about the AI/ML programme I'm now instructing at, and how to get admitted. I didn't want to plug anything in the original post when I created it, but in case anyone interested (it's not for everyone so please read the FAQ):  [https://www.become-irreplaceable.dev/ai-ml-program](https://www.become-irreplaceable.dev/ai-ml-program)  \\- tell the admissions team you came via the Cyrus AMA and if you get admitted I'll hop on a call with you 1-to-1 :) cheers",
        "Absolutely, yes. 100% feasible. And there is no such thing as \"average\" person, as trite or motivational as that sounds. Everyone has their own strengths to lean into.\n\nGrowing up I went to one of the worst public schools in London and didn't have a single book in my home growing up, no one went to college etc etc. So I naturally always used to think that way and put anyone remotely \"successful\" on a pedestal.\n\nBut over the last 10 yrs what I've witnessed not only in the professional world but also silicon valley, startups, and how this entire machine is oiled, I don't put a single person on a pedestal. Sure, I have great respect for people, but I never see them as above or below me.\n\nThe thing I work on most with mentees (pro-bono work I do for the less privileged kids) is work on their mindset.\n\nI also recommend reading 'Fooled by Randomness' by Nassim Taleb.\n\n\\--\n\nTo get into the industry there are a few areas one needs to learn (I've posted in one of the other replies in this AMA), and that is simply achieved with hard work, discipline and dedication. If you can dedicate 2 hours of deep work (non-negotiable) every morning first thing as soon as you wake up before your job or school, then you will see the power of compounding and how rapidly you will pick things up and it all connects.\n\nYou do your best with the inputs you can control, and you keep doing that. You ignore the variables you can't control. Eventually you will get what you desire.",
        ">Do you think it's possible for an \"average\" person to break into ML/AI? \n\nAs someone that doesn't sell educational products to people, the answer is no, 100% no. You need a few qualities or features about yourself that let you solve difficult problems, like some level of skill and experience with math, experience working with data, a strong desire to learn, and a habit of putting in hard work.\n\nPut another way, if everyone could do AI/ML, then we wouldn't be paying specialists in this area to apply these methods, everyone would just be doing it. Also, the difference between the people who get jobs, and those who don't, often has to do with inherent properties of that person that allow them to accumulate knowledge and skills at a faster rate than others.",
        "If it covers the things I mentioned in my comprehensive reply elsewhere in this AMA on topics to study then sure.  However nothing is free, as the cost is our valuable time (opportunity cost) and you have to weigh that up.  What I find w trad institutions is that their teaching is outdated, and this is a space rapidly advancing, hence online learning or online schools are best (not trad schools that now teach online).",
        "i would say simply just following my curiosity.\n\n# \"Do what feels like play to you, but looks like work to others.\" - Naval Ravikant.\n\nit entailed lots of reading/studying/tinkering/building and explaining things to others day to day, in a fun conversational way. when the opportunity came up I thought it's a dream role based on what currently feels like play to me.  i'm also not built for corporate world! went into Tesla with the intention of staying there for 1-2 years, learn from super smart people, and get the resume stamp for the doors it opens. Goal was always to leave and go back to doing what feels like play.\n\nedit: fixed quote",
        "I always say to people: \"Tesla functions like a startup and has the most technically gifted people I've seen (only SpaceX can match). It's a high performance environment, and it needs to be if in just a few years they've turned the car industry on its head. That wasn't achieved with WLB. If you want WLB there's always Google, but they're scrambling and panicking now due to years of comfort IMO.  In life we must choose our regrets.\"",
        "I'd say it's what I mentioned in another response - that despite it being a \\~100,000 person organisation, it still functions like a startup in a phenomenal way you wouldn't expect. Only Tesla has done this in the world of tech (SpaceX and Nvidia too but their workforces are a fraction of the size of Tesla's).",
        "Probably the 100 work week - not many places these days function like a modern concentration camp",
        "The cult of personality around Elon.\n\nThere's lots of examples of this, but the most stunning one I've heard is that people refer to positions in the hierarchy as the \"e\" number, like \"my skip level is an E-2\". That means elon, minus two people. Everyone in the company might not be obsessed with Elon, but there's enough of a personality cult that you stuff like \"do a good job, Elon might be there\" is a significant motivator.",
        "You really need a few things to get into big tech:\n\n* A resume that gets past filters\n* Enough LC skill that you can pass the technical rounds\n* System design/experience so you can tlak about scale and architecting problems\n* The ability to answer behavioral questions and talk about your experience.\n\nThere's no specific skills assessment, besides maybe familiarity with web technologies. if you're in embedded, that's not something big tech really does a lot of, and you'll get tripped up on systems design, but you should definitely still apply and that's something you can study to get better at.",
        "Think he already said this. Went to Codesmith tech school before Tesla and before that was in sales",
        "You need to understand how predictions algorithms interact with product. Like what are the consequences to where your algorithm is on the ROC curve, and is that a tradeoff that actually works.\n\nThe other thing, is knowing how to show an algorithm works, via some dataset and experiment you've developed, and making that experiment relevant to what you are trying to do.\n\nIf you can do those two things, you can take whatever skills you have devleoping algorithms, and apply them with confidence to a product problem.",
        "Cheers!",
        "Thanks! Appreciate you for being here\n\nwe're all gonna make it",
        "Design for the level of scale you need,.\n\nTests\n\nMake sure your modeling assumptions are as correct as possible\n\nLangGraph\n\nNo idea, not OP. Any AI applied problem requires really good product thinking. LIke you can learn AI/ML/Statistics, but if you don't actually understand the product, you can't apply it.",
        "Appreciate you! Thanks for the kind words",
        "1. programming fundamentals (Python preferred) - due to its simplicity + extensive libraries, i.e. NumPy, pandas, and TensorFlow/PyTorch.  Start by mastering basic programming concepts (loops, functions, OOP) and get comfortable with Python syntax, then explore libraries commonly used in data science.\n2. linear algebra & calculus - many ML algorithms are based on linear algebra (e.g., matrix operations in neural networks) & calculus (e.g., gradients in optimisation).  focus on the fundamentals—matrix operations, derivatives, and gradients. Khan Academy and 3Blue1Brown on YouTube provide great visual explanations.\n3. statistics & probability - understanding distributions, statistical significance, & probability is crucial for analysing data and interpreting ML models.  study basic concepts like Bayes’ theorem, probability distributions, hypothesis testing, and p-values. online courses and books on statistics for data science are great resources.\n4. machine learning algorithms & their intuition - knowing how different algorithms work will help you choose the right ones for different problems and optimise them.  start with the basics like linear regression, decision trees, & clustering. As you progress, dive into more advanced models like random forests, gradient boosting, and neural networks. Focus on understanding the intuition and trade-offs behind each algorithm.\n5. data manipulation & analysis - real world data is messy, so strong data manipulation skills are essential for preparing data for ML models. familiarise yourself with pandas & NumPy for data manipulation, and understand how to handle missing values, outliers, and feature scaling. practising w Kaggle datasets is highly recommended.\n6. model evaluation & experimentation - knowing how to evaluate models properly (e.g., accuracy, precision, recall, F1 score, ROC/AUC) is critical for building reliable AI solutions.  Learn about train/test splits, cross-validation, and evaluation metrics. This is especially important for imbalanced datasets or projects where accuracy isn’t the only metric that matters.\n7. deep learning basics (optional for beginners) - although not essential for starting out, deep learning is a valuable skill for more advanced projects involving computer vision, NLP, or large datasets. once you’re comfortable w traditional ML algorithms, start w neural networks and frameworks like TensorFlow or PyTorch. consider following courses that explain deep learning fundamentals, like Andrew Ng’s deep learning courses on Coursera.\n8. version control (Git) & working w cloud platforms - ML models are often deployed in collaborative & production environments. Git helps w collaboration, while cloud platforms (e.g., AWS, GCP) are used for scaling and deploying models. practice using Git for version control, and try small projects using cloud platforms (many offer free tiers).\n\nStay hands-on, experiment with projects and tinker. If you have a job, find 2 hours of non-negotiable deep work on the above upon waking instantly is best (have a look at Pat Walls online and what he suggests for mornings before work), and then going to the day job once you've dedicated 2 hours to your own learning. If your schedule permits this, in 6 months you will be very impressed with yourself!\n\nGodspeed.\n\nedit: spelling",
        "Why he not replied?",
        "Indeed, very well put!  Just check out indie-hackers online like Pieter Levels, and Marc Louvion, among others.  It is no longer the age of credentialism.  And the tools that help us build are getting even better and better by the day.  Glorious time to be alive.",
        "The disruption will be massive, and I think it is due that disruption. These tools are already phenomenal at teaching the meat & bones of subjects, and we're only getting started. People are waking up to the issues of legacy education (universities) and the student debt minefield. I'm SUPER excited for what Andrej Karpathy is building (his new startup Eureka Labs - focussed on education - [https://eurekalabs.ai](https://eurekalabs.ai) ).\n\nmy 2 cents: The tools will eventually teach AI itself, tho possibly with some limitations if there is any self interest, but then we get into the discussion of Ethics which is huge and not to be downplayed.",
        "Coming to England as a refugee aged 5, and having the tremendous luck of being alive during the internet age :)",
        "yes, I always say I have chronic imposter syndrome when it comes to technical skills!\n\nhere is an awesome inspiring post by Dan Abramov (creator of React) on just how much he \\*doesn't know\\*: [https://overreacted.io/things-i-dont-know-as-of-2018/](https://overreacted.io/things-i-dont-know-as-of-2018/)",
        "That already got asked at the top somewhere",
        ">This meant we could use AI/ML only if it meant it actually resolved the task at hand. \n\nSeems like you were doing automation to improve workplace productivity.\n\nWere you actually able to use AI/ML during that time?\n\nSorry, I couldn't extract from you reply above as to where AI/ML fits. I'm still new to this.",
        "I really resonate with this. Once you were in a big team, what did you do to find a way to get that feeling of your previous job again??",
        "The ML program you're teaching now, what does it offer that free videos on Youtube don't?",
        "I have to learn to do this more without beating myself up for not doing the right thing or the most optimal thing: \"always followed my curiosity at the time\"\n\nedit: typo",
        "Would you recommend the bootcamp for someone who does not have a CS degree?",
        "You didn't have a SINGLE book?? Damn man\n\nYou mentioned Taleb, but - and I think someone here asked it already - what programming books do you swear by, and any on ML (or AI) in particular that you're recommending at the moment?",
        "Do you think companies touch candidates without the qualifications ?",
        "Sounds terrible, no wonder you left",
        "was your compensation appropriate for that level of dedication?",
        "Having not actually been in industry that long, why can you say this with such confidence?",
        "lol, only SpaceX? Gimme a break dude.",
        "not quite! you missed a few steps between each step that you mentioned. my resume is super unconventional (always followed my curiosity at the time) - i think the best place to get an idea of what I mean would be my linkedin (you're welcome to check it out if you wish - [https://www.linkedin.com/in/cyrusyari/](https://www.linkedin.com/in/cyrusyari/) )\n\nedit: added link",
        "Wow - amazing answer, thank you!",
        "I know it’s been six days but it’s worth a shot. I learnt this math all in uni but for the life of me can’t remember them all well (except linear algebra for some reason). So how much does “knowing” these concepts entail? That is - is it important to be able to solve things, or just conceptually? ",
        "So everything?",
        "I agree there's so many ethical dilemmas with AI, and we don't talk about the more nuanced ethical questions nearly enough.\n\nOn that, have you encountered any challenging ethical dilemmas when applying AI in real-world scenarios?",
        "No. They're very simple if you ask the right people.",
        "woooow congrats!",
        "yes we were able to use it during that time, so for products like predictive maintenance, dynamic route optimisation (moving machinery), and a few others. \n\nthe interesting thing with 0-to-1 products is that they are \"asymmetric bets\" so many fail (you learn and move on) and some work. I think there are more lessons to be had from failures.  \n  \nJust like how Amazon would say work on 100 products, and 95 would fail, 3 would do well, and 2 become enormous that we hear of (e.g. Kindle, AWS). \"a portfolio of bets\"",
        "yes! by tinkering on projects outside of work. That feeling of taking something (anything) from 0-to-1 gives me a buzz and I love it.",
        "I'd phrase it differently. I'd think of it like this:  without the coding school I attended a few years ago, I would've still been non-technical.  Sure, there are thousands of tutorials on YouTube, but I personally could never learn from tutorials.  Some people can learn from tutorials, which is why in one of my responses in this AMA I suggested a few courses like Andrew Ng Coursera and some youtube channels.   \n  \nIt depends on your learning style, but for me (and some) our brains aren't wired to learn from tutorials. Rather a dedicated coding school like the one I went to is a place you go in with training wheels, you grind for a few months, and then as you're coming out of it they slowly remove the training wheels and give you a push to go on your own. They push you towards hard learning and it becomes a habit, and if one stops the hard learning on their own even after the programme then it's not enough to make it in this industry. But it becomes a habit in a way where you get a thrill from it and enjoy it and now actually seek it (very trite I know).\n\nSome dedicated programmes also have people (experts) you go back and forth with on things like interview practice, resume optimisation, insights on how different companies carry out interviews, and a plethora of other things you don't find online. on youtube videos someone will speak at you, but they won't sit with you one-on-one to go back and forth on your specific issues (for example they haven't seen your resume, or the ML projects you've been tinkering on to give feedback on) and so much more.",
        "Yes, I never had a CS degree, and most people who attend don't either.\n\nthat's one of the reasons I attended - I value multidisciplinary thinking, and people in the programme had come from different walks of life (and dedicated themselves to completing CSX and passing the test to get in, so they had met the bar of entry). When you combine the proof of work (ability to pass and get in) with the skills/knowledge of non-technical fields, suddenly very interesting things happen - Thinking very broadly and understanding many different models from many different disciplines, inspired by Charlie Munger.\n\nI was studying alongside everyone from teachers, photographers, musicians, ex sales, to ivy league physics grads.\n\nI often refer to technical skills as just simply digital plumbing (and some people online get offended!). With enough training (studying and building, tinkering) you will eventually have it figured out. It just requires locking in and putting the work in. That's why in one of the other replies in this AMA I said there is no such thing as \"average person\".  We all have some strength(s) to lean into.  Study the technical, then combine it with your strengths and great things happen.\n\ne.g. ex sales + learn technical -> the best engineer when it comes to comms within a team\n\nor ex photographer + learn technical -> an eye for aesthetics and good design/colours, leading to slick UX/UI and frontend work.\n\ncould go on all day, but you get the point.\n\nedit: spelling",
        "if you get qualifications from trad universities, then it should be focussed on the things that don't date, e.g. linear algebra & calculus, statistics & probability, etc.  Which then points one to a math degree.  I'm usually suspicious of universities offering degrees in \"Artificial intelligence\" - it's usually outdated and impractical courses.\n\nSee my reply elsewhere in this AMA where I gave a comprehensive outline of things to study.  I think from a more trad university you can study:  linear algebra & calculus, statistics & probability. And then everything else on the comprehensive list I gave you can self-study online (autodidact) or attend a modern online AI/ML school (not a trad university, rather some dedicated place like codesmith). Whether you go the autodidact route or online AI/ML school depends on your learning style (another topic I've addressed in this AMA too).\n\nedit: spelling",
        "😂 my response was very comprehensive. it's not an overnight thing ofc. take it step by step! you don't need to know everything above off the bat to hit the ground running.",
        "This is a fair point. Tutorial hell is real - you lapse into a false sense of learning",
        "I have seen a very few people willing to provide as comprehensive information as this. \nThank you for that. It really means a lot to many of us.",
        "I would prioritize data manipulation and stuff. Learning the data and how to get basic results comes before matrix multiplication or calculus. ",
        "I mean you're not getting a job from just being able to do that."
    ]
},
{
    "submission_id": "1glwnof",
    "title": "Is data augmentation with a BERT-like masking a good idea?",
    "selftext": "This is for pre-training a MLM.\n\nI was wondering for each text sample, rather than using a single random mask would it be a good idea to duplicate the samples, applying a unique attention mask to each one?",
    "created_utc": "2024-11-07T09:59:05",
    "num_comments": 0,
    "comments": []
},
{
    "submission_id": "1glwbca",
    "title": "Getting an intuition for building models ",
    "selftext": "Hey guys this is quite a broad question about workflows, hope people don't get mad seeing this in the sub hah.\n\nI'm experimenting with ML, mostly using image datasets to predict parameters, I'm interested in the physics applications.   \n  \nHow do I go about thinking of an architecture? More importantly how do I iterate and improve upon it? For example say I think dropout layers could help, do I add dropout layers and retrain the whole model again (not very fast progress)? Say the result is good, if I then add batch normalization and see that it doesn't work as great, should I take this as a sign that batch norm isn't going to help my case or that perhaps if I remove dropout and include batch norm I'll get a better result?\n\nI spent a day getting terrible results and then frustrated I asked gpt which instantly gave an architecture with way way better results :(. I tried to use my past models which were successful to inform but they didn't translate at all here.\n\nCan you see where I'm going with this? Thanks in advance!",
    "created_utc": "2024-11-07T09:44:55",
    "num_comments": 2,
    "comments": [
        "There is generally no hard and fast rule as to what to pick and not, it is good to know what each of the 'technique' do in each case, and this intuition can be gained if you know the mathematical aspects or just check the general impact of the technique.  \nAs you mentioned couple of those like dropout (which is useful for preventing model overfitting), if you have a little knowledge of 'under the hood',it can greatly improve your intuition.  \nCompare the model that GPT output and see what were the changes that were made and deep dive into the new 'changes' done to the model.",
        "No worries, this is a super common question. \n\nBefore jumping to fancy architectures, really understand your data and what you're trying to predict. Are you dealing with images with lots of fine detail (like medical scans) or more general features? This can guide your initial choices.\n\nStart with a basic architecture. For image stuff, a few convolutional layers (Conv2D), max pooling, and dense layers at the end often work surprisingly well. Keras or PyTorch make this easy to set up.Don't be afraid to \"borrow\" ideas. Look at architectures known to work well for similar image tasks (e.g., ResNet, VGG). You can find pre-trained versions and fine-tune them on your data, which can be a huge time-saver.\n\nOn the iteration part: Exactly what you said – don't throw the kitchen sink at it! Add dropout, retrain, evaluate. Then try batch norm (or whatever else you want to test) *separately*. This helps you isolate what's actually working."
    ]
},
{
    "submission_id": "1glwa54",
    "title": "Looking for Resources on Basics",
    "selftext": "What’s a good place to start learning about the basics of AI?\n\n\nRn I’m confused about NLP, ML, LLM, ANN, DL and so on. I was under the impression that NLP and LLM are two types of Machine Learning systems that focuses on two different things but work together to help fill each other’s deficits \n\nAnd boy am I wrong\n\nI still am struggling to understand exactly what ML, LLM and NLP is.\n\nAre they all different algorithms with different levels of specific tasks?",
    "created_utc": "2024-11-07T09:43:32",
    "num_comments": 2,
    "comments": [
        "Search bar",
        "YouTube"
    ]
},
{
    "submission_id": "1glw1hg",
    "title": "I'm live, if anyone wanna join discussing project, having doubt, consulting. Ping me, I'll send you meet link",
    "selftext": "[https://youtube.com/live/RCati\\_aC0Ss](https://youtube.com/live/RCati_aC0Ss)",
    "created_utc": "2024-11-07T09:33:32",
    "num_comments": 0,
    "comments": []
},
{
    "submission_id": "1glvrfi",
    "title": "Pytorch Pre-trained models",
    "selftext": "Can pytorch Pre-trained models be used for commercial purposes ",
    "created_utc": "2024-11-07T09:22:01",
    "num_comments": 0,
    "comments": []
},
{
    "submission_id": "1glv9r6",
    "title": "Mysterious issues started after training resumption/tweaking implemented",
    "selftext": "",
    "created_utc": "2024-11-07T09:01:25",
    "num_comments": 0,
    "comments": []
},
{
    "submission_id": "1glumcg",
    "title": "The Fastest Way to Start Your AI Project–Quickstart ModelKits ",
    "selftext": "",
    "created_utc": "2024-11-07T08:34:23",
    "num_comments": 0,
    "comments": []
},
{
    "submission_id": "1gltaed",
    "title": "Which framework and algorithms to use for detecting discrete events in time series data? ",
    "selftext": "Hi, I have some timeseries data which have certain patterns that I'd like to discover and turn into events. I have moderate success with purely numerical routines but it seems like an ideal task for inference.\n\nEach time series appears differently but there will be a similar shape that can be labelled. There's also season shift in it all so need to handle that as well.\n\nMost time-series engines seem to be focused on forecasting - which is not applicable here, and I can't find one with an example like this that I can crib off.\n\nFor context: I've 20yrs experience in software engineering, expert level in a variety of languages, a reasonable grasp of things like pandas etc, a working knowledge of stats, but only a passing knowledge of ML, I'd like to use this problem to be solved as a learning opportunity.\n\nThe way I look at it (literally as well as figuratively) ... It's the shape of the time series and not just the numbers.\n\nAnd I have some data series where I can manually go through and label points for training.\n\nReally grateful if any pointers to get me going... Cheers",
    "created_utc": "2024-11-07T07:38:51",
    "num_comments": 1,
    "comments": [
        "See sktime"
    ]
},
{
    "submission_id": "1glsu3i",
    "title": "ML and LLM system design: 500 case studies to learn from (Airtable database)",
    "selftext": "Hey everyone! Wanted to share the link to the database of 500 ML use cases from 100+ companies that detail ML and LLM system design. The list also includes over 80 use cases on LLMs and generative AI. You can filter by industry or ML use case.\n\nIf anyone here is designing an ML system, I hope you'll find it useful!\n\nLink to the database: [https://www.evidentlyai.com/ml-system-design](https://www.evidentlyai.com/ml-system-design)\n\nDisclaimer: I'm on the team behind [Evidently](https://github.com/evidentlyai/evidently), an open-source ML and LLM observability framework. We put together this database.",
    "created_utc": "2024-11-07T07:19:00",
    "num_comments": 0,
    "comments": []
},
{
    "submission_id": "1glryo5",
    "title": "Minor or research?",
    "selftext": "I'm currently a freshmen in college rn majoring in AI, I got a lot of transfered AP credits so I was wondering what would be the best way to fill up my classes: get a minor in math/stats or reach out to professors and do research? Which do you guys think would be a better use of my time?",
    "created_utc": "2024-11-07T06:40:44",
    "num_comments": 2,
    "comments": [
        "You can major in AI?",
        "Yeah I go to Purdue and is a new major there"
    ]
},
{
    "submission_id": "1glqwh2",
    "title": "Learn positional encoding, the method LLMs use to keep track of the order of words in a sentence, in this friendly video!",
    "selftext": "",
    "created_utc": "2024-11-07T05:51:45",
    "num_comments": 0,
    "comments": []
},
{
    "submission_id": "1glpd6y",
    "title": "NLP vs CV",
    "selftext": "Hey guys I was always confused whether to choose NLP or Computer vision as my main domain in DL. I hope you guys would give out valuable suggestions. Also I am trying to prepare for an internship, I would highly appreciate if anyone would give any valuable advice on how to get hired in the domain of ml. Any certifications required to boost my resume? Also if anyone would give out any list of projects to improve resume and understanding of the concepts",
    "created_utc": "2024-11-07T04:33:28",
    "num_comments": 3,
    "comments": [
        "Bro take nlp , rn llms and chatbots more important",
        "MNIST number classification",
        "iris classification project"
    ]
},
{
    "submission_id": "1glp4wi",
    "title": "Data Scientist Job Choice",
    "selftext": "I am a Data Scientist working more on classical ML for the past years. Although I have nothing bad to say about my current startup job, the state of their datasets have still more ways to go before a robust ML model can be built around them.\n\nI have a job offer for a job that focuses more on time series forecasting. Their models are mature enough and I will mostly do maintenance work while researching for improvements.\n\nThe question is, is it better if I stay with my current which gives me the freedom to do end to end work (mostly classification tasks) but with limited data, or accept the offer and work for a more established data science team with larger datasets?",
    "created_utc": "2024-11-07T04:20:31",
    "num_comments": 1,
    "comments": [
        "Any thoughts please 😔"
    ]
},
{
    "submission_id": "1glnpzo",
    "title": "Publishing research as an undergrad",
    "selftext": "I am currently doing my second year at Uni and I've been coding and studying ML since high-school. I started reading a couple of papers in ML in my first year of university and been trying to find gaps I can fill in sampling methods when it comes to synthetic data generation and I believe I have found one method(it's not ground breaking or anything but I believe it's something worth publishing based on what I've seen on similar papers.) Any advice on how I should go about this? ",
    "created_utc": "2024-11-07T02:50:27",
    "num_comments": 14,
    "comments": [
        "Go to the CS dept, approach your professors. They can advise you in writing your paper, they also have the connections you need for publishing.",
        "damn bro .. you are soo much better than most ...",
        "I am an undergrad, too. I communicate with my professors, but PIs can be stealers, etc. You must be careful. Choose the cutest and most helpful professor. Note: Your first papers can be rejected due to bad English usage.",
        "I would be cautious of running it with just any professor first, make sure you approach someone you trust as they might steal your idea. It depends on the uni but it isn’t uncommon.",
        "Congats already for being in such a position! Though someone has to critically evaluate your ideas/implementations/and eventually the paper. Ask around and approach only trustable professors who are active in the same research topic! There's a lot of competition in the field and good people seem to be a minority.. at least based on my experience for the last 7 years",
        "Also approach a prof who has worked in ML, who is not greedy, easy to go and should not have anger issues.",
        "Thank you.",
        "Not really...",
        "English is not a problem anymore! Most journals/conferences allow authors to use LLMs for editing for English",
        "I think I can trust my professors.",
        "Much appreciated.",
        "i started ML in second yr of my uni ..",
        "Idk mine rejected and said requires resubmission",
        "Rejected for being wriiten in bad English? Or are there other reasons too? If it's only bad English, use chatgpt to edit sentences for structure and grammar. Make sure not to leak important info and generate extra non-sense content"
    ]
},
{
    "submission_id": "1glmmu7",
    "title": "How prepared a dataset for classification? Could you recommend the best practices?",
    "selftext": "Hello! I am collecting messages to recognize insults. Could you give some advice on preparing the dataset for training? My main problem is recognizing words by replacing letters with special characters such as **@..#!?%**. For example, if I add a message with the word **ex@mple,** I want my model to recognize **ex#mple** too.",
    "created_utc": "2024-11-07T01:28:50",
    "num_comments": 0,
    "comments": []
},
{
    "submission_id": "1glld1w",
    "title": "Career Guidance",
    "selftext": "I have been working for Microsoft for 2 years. I've been training models with transcriptions. After I got laid off, I can't find a suitable job outside.\n\nI have intermediate level knowledge in Python, Excel. I gained knowledge in SQL, I even learned to design chatbot using several platforms like Amazon Lex, Voice flow, etc, but without the coding part.\n\nWhat can I do next? What skill or tool I should learn to increase the chance of getting hired in ML roles. Your input will be valuable. Thanks",
    "created_utc": "2024-11-06T23:50:37",
    "num_comments": 3,
    "comments": [
        "AI engineer jobs are in demand these days",
        "Thanks. What more knowledge should I acquire to become an AI engineer.",
        "https://youtu.be/zghjRYt8bcM?feature=shared see this"
    ]
},
{
    "submission_id": "1glkzx3",
    "title": "NSA Director: US Investigating China Telcom Cyberattacks | The Cipher Brief",
    "selftext": "El señor trump ",
    "created_utc": "2024-11-06T23:22:38",
    "num_comments": 0,
    "comments": []
},
{
    "submission_id": "1glkkve",
    "title": "FAANG ML system design interview guide",
    "selftext": "Full guide, notes, and practice ML interview problem resources here ➡️: [https://www.trybackprop.com/blog/ml\\_system\\_design\\_interview](https://www.trybackprop.com/blog/ml_system_design_interview)\n\n  \nIn this post, I will cover the basic structure of the machine learning system design interview at FAANG, how to answer it properly, and study resources.\n\n* [Who encounters ML System Design interviews?](https://www.trybackprop.com/blog/ml_system_design_interview#who-encounters-ml-system-design-interviews)\n* [When is the ML system design interview?](https://www.trybackprop.com/blog/ml_system_design_interview#when-is-the-ml-system-design-interview)\n* [What questions are asked in an ML system design interview?](https://www.trybackprop.com/blog/ml_system_design_interview#what-questions-are-asked-in-an-ml-system-design-interview)\n* [How are candidates evaluated?](https://www.trybackprop.com/blog/ml_system_design_interview#how-are-candidates-evaluated)\n\nThe general ML areas in which a candidate's solution are evaluated. Depending on what level you're interviewing as – entry-level, senior, or staff+ – you'll need to answer differently.\n\n* [Problem exploration](https://www.trybackprop.com/blog/ml_system_design_interview#problem-exploration)\n   * business understanding\n   * technical approach\n   * risk assessment\n* [Train/Eval Data Strategy](https://www.trybackprop.com/blog/ml_system_design_interview#traineval-data-strategy)\n   * data collection & labeling\n   * quality control\n   * cold start\n* [Feature Engineering](https://www.trybackprop.com/blog/ml_system_design_interview#feature-engineering)\n   * feature ideation and structure\n   * task specific relevance\n* [Model Architecture & Training](https://www.trybackprop.com/blog/ml_system_design_interview#model-architecture--training)\n   * model selection and justification\n   * technical depth (not just API calls, but deeper understanding)\n* [Model Evaluation Strategy](https://www.trybackprop.com/blog/ml_system_design_interview#model-evaluation-strategy)\n   * offline evaluation\n   * online experimentation\n   * feedback loops\n\nAnd finally, [this section of the post](https://www.trybackprop.com/blog/ml_system_design_interview#study-material-and-interview-practice-problems) contains useful [study material and interview practice problems](https://www.trybackprop.com/blog/ml_system_design_interview#study-material-and-interview-practice-problems). Hope you find this guide to ML system design interview preparation helpful. Remember, interviewing is like any other skill – it can be learned.\n\n\n\n",
    "created_utc": "2024-11-06T22:51:46",
    "num_comments": 19,
    "comments": [
        "This is great! Thank you!",
        "Great content. I have a question.\n\nIs learning general system design good to have before moving on to ML system design? Can you give am importance score out of 10?\n\nIf yes, where can we get started with general SD and the must know topics.\n\nThanks",
        "Great resources",
        "[removed]",
        "This is so helpful, thanks!",
        "Great post OP - thank you for the resources!",
        "Awesome stuff, cheers dude!",
        "Nice!",
        "thanks for the resources!",
        "Thank you for reading! Please provide any feedback if you have any!",
        "Both general and ML system design are important because you'll need to pass both in the interview loop. I actually wrote about the general system design process here: [https://www.trybackprop.com/blog/system\\_design\\_interview](https://www.trybackprop.com/blog/system_design_interview) \n\n  \nHope you find it helpful!",
        "To answer your other question regarding the importance score, both are equally important. It might seem like a lot to study, but that's why I mention in the linked article that you should try to ace the coding phone screen first and then schedule with the interviewer a month out for the onsite/second round of interviews that includes both regular and ML system design interviews. That way you can use that month to focus on those two while keeping your coding/algorithms/Leetcode skills warm.",
        "which resource/advice did you find most helpful? Thank you for reading!",
        "Please leave feedback on any helpful areas of the guide, thanks!",
        "Glad to hear it, let me know if you'd like me to write about any topics in particular. Thanks!",
        "Thank you for reading, please provide feedback if you have any!",
        "Thanks, always open to more feedback on the content!",
        "Cool, thx man",
        "I am just a newbie so knowing just the content was enough"
    ]
},
{
    "submission_id": "1glkfhu",
    "title": "I think I'm Satisfied With This Tortoise TTS Alternative",
    "selftext": "For those of you who don't know what [Tortoise TTS](https://github.com/neonbjb/tortoise-tts) is, it's a Text-To-Speech program that people say is good for voice cloning. I tried it, and it was very slow and only worked with GPU.\n\nSo, I tried [Parler TTS](https://github.com/huggingface/parler-tts), which is another Text-To-Speech model that works okay. [It was faster than Tortoise at least and I got it working very quickly.](https://www.niftylittleme.com/articles/using-parler-tts-with-google-colab-in-python) I'm new to machine learning, and would like to know your thoughts on Parler TTS.",
    "created_utc": "2024-11-06T22:40:54",
    "num_comments": 0,
    "comments": []
},
{
    "submission_id": "1glhsxy",
    "title": "Fine-Tuning LLMs interview questions compiled",
    "selftext": "I've compiled a list of Generative AI Interview questions asked in top MNCs and startups from different resources available. This 1st part comprises all the questions and answers for the topic Fine-Tuning LLMs. https://youtu.be/zkzns74iLqY?si=GWv27wMA0L4dZyJ_",
    "created_utc": "2024-11-06T19:59:33",
    "num_comments": 0,
    "comments": []
},
{
    "submission_id": "1glfzph",
    "title": "[d]Is it possible to create a custom vocal separation model?",
    "selftext": "I tried to use Ultimate Vocal Remover, but the UI is inconvenient for me to use, so I am using python-audio-separator.\n\nI think the models used in audio-separator and Ultimate Vocal Remover are similar.\n\nI downloaded a lot of models and separated audio.\n\nIt seemed that sound effects or music often used on YouTube or TV could not be separated.\n\nSo I want to create a model with the data I have.\n\nI don't have much data.\n\nI want to know if fine tuning is possible with a small amount of data.\n\n\n\nMy system is 5600x rtx3060oc 12gb and the RAM capacity is 32gb.",
    "created_utc": "2024-11-06T18:20:49",
    "num_comments": 1,
    "comments": [
        "No"
    ]
},
{
    "submission_id": "1glcl73",
    "title": "4070S slower than M2 Pro",
    "selftext": "Hi guys,\n\nI was training a Fashion MNIST model in pytorch on an M2 Pro with MPS acceleration. The model uses two conv. layers, with 16 filters. Out of curiosity, I tried running the same code on a 4070S desktop with CUDA,  expecting it to crush the MacBook, but the training time was actually slower.\n\nAny ideas on what is happening?\n\n\\----  \nUPDATES: So, even though the task manager was showing a lot of GPU activity, I checked with Afterburner and saw that both the core and memory clocks were idle most of the time. Turns out, the training wasn’t really pushing the GPU, it was most probably expending time transferring memory, as it was pointed out. It wasn’t until I bumped up the filters to 230 that the clocks started boosting more frequently. What’s funny is that even with so many filters, the times actually improved a bit. With 500 filters, the 4070S totally smokes the M2 Pro — it takes around 66s on the M2, while the 4070S only takes 11s.\n\n[4070S](https://preview.redd.it/8a6f12ye9dzd1.jpg?width=968&format=pjpg&auto=webp&s=ffa4e4363fd278076b3dd7c1bbf1305b2bd35045)\n\n[M2 Pro](https://preview.redd.it/ofldek4badzd1.jpg?width=1598&format=pjpg&auto=webp&s=42bd040854888db9ba370a9fbf7225fcf7902534)",
    "created_utc": "2024-11-06T15:36:33",
    "num_comments": 12,
    "comments": [
        "That is a tiny dataset and a tiny neural network. You are spending more time moving the data on and off the GPU than you are training. \n\n\nTry it with a larger model like YOLO.",
        "Commenting to come back to this post once there's more replies",
        "Hello, I might be wrong but I think that CUDA might be better when training on larger datasets and for longer training times. I am only speaking through what I think and I do not have any proof about it so take this with a grain of salt.",
        "Up. Genuine question... Waiting for experts",
        "How many gigs of ram does your m2 have?",
        "Where was the dataset stored for the 3070 run?\n\nThe Mac has unified memory, meaning the GPU can directly access the same memory as the CPU. For a PC with a PCIE GPU, following most boilerplate fashion MNIST training examples, you'll be moving a fraction of the data onto the GPU once per iteration, and moving data is MASSIVELY slower than computing with it, especially for a small model. Even when the data size is small, individual transfers have a wall clock time overhead.\n\nFor a more fair comparison, move the Fashion MNIST dataset into the 3070's memory once before training. It should only use up \\~188.16 megabytes.",
        "goodluck finding an apple m2 pro vm at the same rates 👍",
        "Or push everything onto the gpu if it's a tiny dataset, should absolutely crush any apple gpu. Cuda is way more optimized with tons of kernel fusions under the hood. Loading data is usually the biggest challenge with nvidia gpus. Memory isn't fast enough to keep up with the throughput of these newer Blackwell cards. There's a reason why nvidia wanted to acquire ARM. Altho now they have their ARM DGX systems (grace hopper for eg) which has some insane memory all on die like macbooks. Too bad even selling both my kidneys won't get me one.",
        "Yes, you are right. I updated the post.",
        "Yes, I increase the model, and got much better results. I updated the post with the details",
        "16gb",
        "I am currently using dataloader with batch size 32. I will load the entire dataset and see how it goes. But on top of that, I increased the number of filters, and the 4070S ended up outperforming the M2. I updated the post with more details on the result."
    ]
},
{
    "submission_id": "1glc8k2",
    "title": "MLops",
    "selftext": "Plz suggest some good MLops resources for beginners preferably video resources. Is mlops actually needed for job in ml as data scientist or research scientist in any ml job ? ",
    "created_utc": "2024-11-06T15:20:31",
    "num_comments": 3,
    "comments": [
        "yes",
        "1) Datatalksclub's MLOps Zoomcamp (Video lectures available)\n\n2) Made with ML by Goku Mohandas (probably the best but no video lectures present)\n\n3) Designing Machine Learning Systems by Chip Hyuen (Book - definitely recommended - Supplement this book with any of the above options)",
        "chip book is good"
    ]
},
{
    "submission_id": "1glbdkv",
    "title": "Helping student understand their machine learning project - stock price prediction | Episode 1 #live ",
    "selftext": "",
    "created_utc": "2024-11-06T14:42:12",
    "num_comments": 2,
    "comments": [
        "Is this your video or did you just find it on the internet?\n\nIs the Api key displayed at 2:50 (and other timestamps) still in use? And shouldn't it be a secret?",
        "Yes it's my video.\n\nAPI key was deactivated. Thanks for pointing out though, will make it a secret from now on."
    ]
},
{
    "submission_id": "1gla3k7",
    "title": "Masters vs Internal Switch",
    "selftext": "Hi, I’m currently working at a FAANG for the past 2.5 years in a SWE role (backend) and interested in moving to ML focused roles (have some experience from undergrad). Would a Masters (research based, US) be the right move or would it be better to self learn some stuff and try shifting internally, especially considering the current job market in the US?\n\n[View Poll](https://www.reddit.com/poll/1gla3k7)",
    "created_utc": "2024-11-06T13:47:09",
    "num_comments": 6,
    "comments": [
        "Depends on what your final goal is.\n\nIf it's ML research, you want to get into a PhD program. MS won't cut it - but if PhD admissions are too competitive and you're getting rejected from all the good schools then yeah maybe doing an MS and building a publication record will strengthen your admissions case.\n\nIf it's just working in ML development in industry the MS doesn't add any extra value compared to field experience. Keep getting your FAANG salary with that internal transfer, you make bank AND get relevant experience, there's no better way.\n\nThose are two different jobs. You need to pick one and go all in, don't half-ass it.",
        "I don't want to lose my income for 2+ years",
        "i dont think its the best subreddit to ask from ... most of the people here are beginners and students .. so for us leaving FAANG is ... a dumb move..\n\nidk why someone would leave faang and then go through the sh\\*tstorm of college / placements",
        "I've seen both types of coworkers transition to ML. If you have the time and the financial resources to do it, I would suggest pursuing an ML role after obtaining a masters because it's a year or two of graduate studies and efficient knowledge acquisition. Another pro of obtaining the research based masters is you'll open yourself up to more research lab opportunities. If your life circumstances don't make it as easy to pursue a masters, then shifting internally also works. I've seen many FAANG engineers do this very successfully. I myself shifted internally, and it's worked out really well for me. I actually wrote a blog post about the various FAANG engineers who did this: [https://www.trybackprop.com/blog/2024\\_06\\_09\\_you\\_dont\\_need\\_a\\_phd](https://www.trybackprop.com/blog/2024_06_09_you_dont_need_a_phd)",
        "Honestly I’m not too keen on a PhD, which is why the dilemma of whether this would be worth it came up. \n\nWhen it comes to ML roles at FAANG, I guess the max I can switch to is ML - SWE (deployment and the such) I’d like to work in CV roles ideally but I understand that without a PhD this would be difficult to get into at FAANGs? Unless I move to startups.",
        "I don't work at FAANG but I know people who do. I'm pretty sure you don't need a PhD for ML SWE roles there. I think there's plenty of non-research advanced development you can get into especially at FAANG scales. These aren't some cookie cutter problems where you just download a solution in a can and then most of the work is just tuning and operationalizing it, like 99% of work in ML applications in startups tends to be. At FAANG levels and scale, you might need to understand and apply the latest and most advanced technologies at scales that they've never been applied at, and these would involve solving technical problems that haven't been solved before.\n\nThe difference between that kind of work and the more foundational ML work that really needs a PhD, is that you're not coming up with brand new foundational ideas and pushing academia forward in conference publications. If you're not interested in the academia papers-and-citations game then don't get a PhD."
    ]
},
{
    "submission_id": "1gl9mxg",
    "title": "Transfer learning vs fine tuning ",
    "selftext": "",
    "created_utc": "2024-11-06T13:27:27",
    "num_comments": 0,
    "comments": []
},
{
    "submission_id": "1gl8nts",
    "title": "Python-Introduction To Data Science And Machine Learning A-Z | Free Udemy course for limited enrolls",
    "selftext": "",
    "created_utc": "2024-11-06T12:46:30",
    "num_comments": 0,
    "comments": []
},
{
    "submission_id": "1gl7br3",
    "title": "Basic Probability Distributions Explained",
    "selftext": "",
    "created_utc": "2024-11-06T11:50:56",
    "num_comments": 0,
    "comments": []
},
{
    "submission_id": "1gl6gzw",
    "title": "Advice on Continuously Training a Page Number Detection Model with New Data in Production",
    "selftext": "We working on a solution to isolate page numbers in a continuous stream of scanned book pages. We currently have a model trained on about 20,000 images, and it's performing well on this initial dataset. However, given the wide variety of formatting styles and page layouts in the real world, we know the model won't be able to generalize to every case we encounter in production.\n\nHere’s the approach we’re considering:\n\n* Once the model is deployed, it will detect and generate bounding boxes around page numbers in each new image.\n* We’ll validate each prediction in production by manually checking the bounding boxes and providing the correct ones where the model fails.\n* We plan to track the model’s performance over a \"window\" of 300-500 images by calculating the accuracy of its detections (i.e., how many page numbers were isolated correctly).\n* If the failure rate exceeds a certain threshold within this window, we’d trigger a retraining session using the newly collected data (including corrected bounding boxes).\n\nSome specifics:\n\n* Validation will be done subjectively, aiming for bounding boxes that tightly cover the page number without cutting off any parts of the number itself.\n* We’re able to validate every image in the incoming stream manually, so we’ll have reliable ground truth data for each prediction the model makes.\n\nMy question is: **What would be an effective strategy to incorporate this new data into the model?** We are assuming that the model has enough capacity to learn a solution for every image.",
    "created_utc": "2024-11-06T11:15:07",
    "num_comments": 0,
    "comments": []
},
{
    "submission_id": "1gl5mun",
    "title": "Where can I train a Neural Network and export the learned parameters (ie weights and biases)?",
    "selftext": "I need to train a large neural network then export the parameters into R. Are there any cloud services that allow me to do this because my laptop isn't powerful enough to do this locally. I dont mind paying for a service. I appreciate any suggestions.",
    "created_utc": "2024-11-06T10:40:19",
    "num_comments": 3,
    "comments": [
        "Google Colab is free and gives you free GPUs too. No clue how to export into R, though. Perhaps R can read models exported by PyTorch? Maybe there are R bindings to PyTorch?",
        "Yes, you can use Google Colab to train the model then just store the weights on a Google cloud storage bucket, then read them into R",
        "Perfect. Looking into this now thank you."
    ]
},
{
    "submission_id": "1gl5ctz",
    "title": "Brief Introduction to Contrastive Learning",
    "selftext": "",
    "created_utc": "2024-11-06T10:28:44",
    "num_comments": 0,
    "comments": []
},
{
    "submission_id": "1gl52ds",
    "title": "Seeking Advice for open contribution.",
    "selftext": "I'm a second-year Computer Science student currently enrolled in Machine Learning. I am eager to begin my journey of open-source contributions, with a specific focus on contributing to ML, DL, and NLP projects .\n\nMy goal is to participate in Google Summer of Code (GSoC) 2025, and I am seeking guidance or advice on how to effectively get started in open-source contributions.\n\nAny suggestions regarding project selection, communities to engage with, or general tips for GSoC would be greatly appreciated.\n\nThankyou !",
    "created_utc": "2024-11-06T10:16:42",
    "num_comments": 8,
    "comments": [
        "Have you stumbled upon any bugs or inefficiencies in the libraries you're using? If so, you can open an issue or a PR. Would you like some library to have a particular feature (perhaps a particular ML algorithm)? You can implement it and submit a PR.\n\nOr you could just start your own open-source project. Perhaps you've got some efficient code that fits a new model - you can publish it if you want.",
        "Come check this: https://github.com/prayas7102/NodejsSecurify/issues",
        "We need an ML-savvy person to help us understand how to properly integrate our model with an ONNX runtime. Check out our open issue here: [https://github.com/omeryusufyagci/fast-music-remover/issues/48](https://github.com/omeryusufyagci/fast-music-remover/issues/48)\n\nWe're building a lightweight audio enhancer for internet media, completely free and open source.",
        "Thankyou...As I'm not much experienced,I think it will be difficult for me to start my own open source project, but yeah I will try for it..",
        "Thankyou",
        "Thankyou..I will look into it and I hope I can help . As I'm a newbie to open source Is it okay to ask questions .",
        "questions are absolutely welcome. We also opened a discord very recently, it should provide a less stressful venue for questions :)) you can join via: [https://discord.com/invite/xje3PQTEYp](https://discord.com/invite/xje3PQTEYp)",
        "Okay 👍"
    ]
},
{
    "submission_id": "1gl4ai3",
    "title": "Kaggle Competitions vs. Open-Source Projects for ML Experience?",
    "selftext": "Hello everyone!\n\nAfter months of sharpening my analysis and deep learning skills, I finally feel confident enough to tackle projects in a community setting. I’ve seen many of my seniors participate in open-source initiatives like GSOC and LFX, but I’m leaning towards Kaggle for machine learning.\n\nQuestions:\n\n1. Which is better for gaining real-world experience: Kaggle competitions or open-source projects (like GSOC)?\n\n2. Which one would give me an extra edge in the industry?\n\n\nFor those working in the industry, what would you recommend? I haven’t explored either yet, so I’d love to hear your insights!\n\nThanks in advance for your guidance.",
    "created_utc": "2024-11-06T09:44:19",
    "num_comments": 7,
    "comments": [
        "I’ve found that projects that I’m interested in and actually want to do greatly improves my chances of staying motivated, learning new stuff and following through. Whatever it may be. I have long running projects that I still fell motivated to improve after years. If you do it only to farm experience you will probably learn less in the long run.",
        "adding to Ops query, where should one begin to get into opensource ML??",
        "I read this as Kegel Competitions and got a chuckle",
        "What are examples of OSP for ml?",
        "Open source projects and your own projects would give you an edge in industry. In fact, interviewers don't like it when the candidate only has experience from Kaggle unless the candidate is extremely strong in theory.",
        "To become a Kaggle competition expert, I recommend using BrainyCalc ([https://brainycalc.com/](https://brainycalc.com/)). It provides in-depth insights into the successful strategies of top competitors, focusing on advanced feature engineering, model ensembling, and validation techniques. These insights can help you learn the winning approaches and apply them to your own projects, improving your chances of securing a top rank.",
        "That's totally true, working on projects that you know are applicable in the real world gives you a boost of dopamine."
    ]
},
{
    "submission_id": "1gl39ii",
    "title": "Should I get Masters Degree if I need to work as ML engineer?",
    "selftext": "I’m a software engineer working mostly in Python, and I really want to switch to a machine learning engineer role because there’s not much to learn in my current job. I’m stuck trying to decide whether I should go for a master’s in ML or learn on my own. Many people say that a master’s is necessary to work as an ML engineer, but I don’t have a lot of money to spend on a degree. I’m really confused about the best path forward. Any advice?",
    "created_utc": "2024-11-06T09:01:31",
    "num_comments": 70,
    "comments": [
        "A masters is the baseline at this point, the market is just too competitive. It's not going to improve anytime soon either.",
        "• Learn basics like data handling, cleaning, analysis and visualizations and get good at it first. \n• Learn about how machine decides (it’s pretty much all maths) and know what deciding factor (ML model / Math ) to use on what certain problems.\n• You should try to learn the math / or get an idea of how the equations handles the data. By this you will know how to tune the parameters in the equation (ML Model) which is called hyper parameter tuning.\n• By now, You will know if you want to continue or not.\n• If yes, by now you will know how to go on with it.",
        "If you are young and just getting started with your career I would say yes its totally a good investment. If you are in your forties or older its just a waste of time and money.",
        "I don’t care what people say or think. I don’t care about downvotes coming from engineers/ computer scientists/ ds-ml grads too. For me, if you want to be really good in ml, first you need to learn nonlinear optimisation and convex analysis. I see that many data science/ml msc programs doesn’t offer it or offer it as elective. Go get a masters in applied mathematics/operations research.",
        "From what I was seeing, many positions \"just\" required a Bachelor's degree with work experience in ML. So I do not think it is strictly necessary. However, I guess it is easier to get work experience in ML if you have a Master's degree.",
        "Companies will always trust people with degrees than without degrees \nGet the master",
        "I have the same qs as you, I currently have a DS role\nand have had no issues with it but the main issue I see rn is marketability of work if I need to switch. There are a lot of ppl who can get the 'skills' reqd. for the job. To truly stand out I think  a masters/PhD could help but more importantly a research paper in top rated conf. is what I have seen sets a profile truly apart",
        "if you're working you should see if your company will cover your masters program for you and you can work on it part time.",
        "Go beyond. Some well-paid ML jobs at Google require a PhD.",
        "Hey I'm doing graduation from IIT (well known institute all over the world) but will I need to do masters in ML too if my dream is to do job in this field rn??",
        "You don’t NEED a masters. But if it’s available to you (in terms of time and money) its probably the easiest route.\n\nDiplomas carry weight. Degree’s are awesome for career pivots. Youll get a decent education, can get foots in the door with internships and are supported by the career service.\n\nPlus you have tons of permissions to tinker and dabble to get good on novel technologies. And bc. Of your prior experience you’ll have less of a entrylevel (you need experience to get experience) problem bc. You have experience.",
        "Masters is recommended but there might be many who first became software engineer then ML engineer. Also these are the ones who are good ML engineers because in the end an ML engineer is an engineer. \n\nI personally have built my career on foundations of ML but I am more than sure that a software engineer can over time transition to ML engineer. \n\nEither way it will be fine. In general a master's degree helps.",
        "Getting a masters would help streamline the conversion process from software engineer to ML engineer/researcher, but it's not necessary. Even if you get a masters, you still need to line up a job, and that takes time and in this market, a lot of persistence. I actually wrote a blog post about this journey and it was well received: [https://www.trybackprop.com/blog/2024\\_06\\_09\\_you\\_dont\\_need\\_a\\_phd](https://www.trybackprop.com/blog/2024_06_09_you_dont_need_a_phd)",
        "Only do a master's if you're confident you can get high-quality research experience. Other than that a master's may be just a waste of time.",
        "what ur opinion on doing online masters? is it worth the same like doing it onsite?",
        "No",
        "I will say that for my hiring process, masters degrees tend not to count for much, especially relative to actual experience working on actual problems with actual teams. \n\nIt's extremely difficult to assign any particular value to a given masters degree in a hiring process, and many folks with fine degrees completely flail in technical rounds.\n\nMasters degrees just don't provide a lot of signal to a hiring team.",
        "I was lucky to get into data science early (about 13 years ago) with an undergraduate degree in Math.\n\nI now hire teams of data engineers, data scientists, MLEs etc. Unfortunately it's gotten so competitive that you need a lot of experience to stand out.\n\nA Masters from a good program will help. If it's not known, just try to find some related MLE work in your existing company and build that experience.",
        "are you a student?",
        "It's funny. 6 years ago when I would get resume rejections, I assumed it was my lack of work experience particularly at large companies or types of algorithms that was the root cause.\n\nFast forward to today with over 10 yrs of experience, including multiple large companies as well as a Faang, I still get rejected. Its a rough/tough market out there for sure\n\n\nAnd that's with a Masters degree",
        "I think it's less about the degree, and more about specific projects and skills you acquired. \n\nMany of the people I know got masters degrees during economic downturns when they couldn't find another job.\n\nIf they did something substantial during that time (say, published a notable paper) -- it can be incredibly awesome to any employer looking for that specific skill.   Otherwise it's mostly resume-gap-filler.",
        "people put masters in job descriptions just to scare people off who aren’t confident in their skills. i’m a machine learning engineer with a bachelors at a big company. people spend 2 years getting a masters in things that aren’t even applicable in the job.",
        "Source?",
        "Can you guide me with How should i move forward with Masters?",
        "I have taken Ml as a subject for one sem and it was pretty intresting . Most of the jobs demand for masters. How should i move forward?",
        "Is it a good idea even if you're in your mid-20s, have a 4 year of work-ex (non-ML related, more into analytics and product)? Asking for myself as I'm considering a similar thing. Please advice",
        "I've been working for 1.4 yrs now . Problem is that every university ask for entrance exam and this thing will take lots of time to get prepared. I'm just confused what to do ?",
        "Why do you think that? I am finishing my undergrad in İndustrial engineering and i haven't really felt my knowledge of non linear optimization made a significant difference. Also convex analysis isnt an elective i can take in my university (to my knowledge) why do you think it's important. Sorry for the potentially dumb questions",
        "what if i’m taking classes in Bsc Mathematics, would it be okay to do Msc in ML??",
        "Noted",
        "What you’re forgetting is that you will be competing against hundreds of others with masters, phds and publications. Do not be fooled by the requirements listed in the job posting.",
        "How to get work experience in ML?",
        "I can see that we both are on same boat rn",
        "Well it does not have any of such programs ",
        "That was great help",
        "That's was great help",
        "In my opinion on site program offer structured learning, immediate access to instructors, and better networking opportunities. ",
        "What are the qualities or skills do they look for hiring if u don't mind answering ",
        "Currently yes, but this is my second masters that I'm doing for fun and because it's free for me through my employer. My first masters is in DS.",
        "Nope, not right now.",
        "I never understand why people lie on the internet.",
        "Market trends",
        "Look at most ML job postings and their requirements.",
        "Yeah, go apply to a school. Finish the courses. Apply for graduation. You're welcome.",
        "Check out r/OMSCS and stop using entrance exams as an excuse. If you think getting into grad school is the hard part you are in for a world of hurt.",
        "Nonlinear optimisation is a grad level course. I don’t know what you saw, but it is directly core of machine learning. In machine learning, you basically try to minimize a loss function. This is a nonlinear optimization problem. For example in neural networks, optimizer is a parameter, such as sgd, Adam, Adagrad etc, and a learning rate(which is step size). In my nonlinear optimization course, we were coding our own optimizers to solve nonlinear problems.",
        "I am currently searching for a job in this area as well. I did my Master's in ML and thus have no problems getting invites to job interviews for junior positions (in Germany that is). I do not know how difficult it is to get interviews if you learn it by yourself and only have certificates, kaggle, github portfolio to show.",
        "The biggest signal is professional (or similar to professional) experience working on similar problems with similar tools.",
        "doing masters for fun ??????? damn .....",
        "What is the current masters degree you are persuing?",
        "I started my Masters 2 years ago so I'll have it in a year! Fastest way to get something is to start doing it",
        "Applying for colleges takes time, as first we need to crack their entrance exam",
        "Yes, of course. I took a couple optimization courses, some of which we did learn how to solve optimization problem with non linear lost functions. However it seems like the course where we learned this stuff in depth, only contained linear programming problems. We did saw non linear optimization but it was another course we only learned how to properly formulate it and used gurobi to solve. Thank you for your comment, maybe I'll have the opportunity to learn more about it in grad school.",
        "Where’d you go?",
        "So how's the job hunting going on?",
        "where did you do the masters from?",
        "CS, just for something to do on the side. If I ever want to pivot, it'll be nice to have",
        "Which country are you doing your master's in? Isn't it usually completed in two years?",
        "Then do that",
        "These posts are getting so annoying. You know what else takes time? Learning ML. If you’re looking for a quick path to making big money, this isn’t it.",
        "What are you looking for ? \n\nA quick and easy no-effort no-cost option that guarantees the best job ever with crazy high assured income ?\n\nIf you want to differentiate yourself from others, then you have to *differentiate* yourself by doing more and better at harder things. \n\nOtherwise you’re just like everybody else.\n\nSome people get lucky, and there’s no guarantee for anyone no matter how hard they work, but if you want to maximize your chances you gotta do the work, and as the previous commenter said, it’s competitive out there and only getting worse. \n\nGet to work.",
        "I hate to bring it to you but gradschool is way harder than its entrance exam. So if an entrance exam is enough to make you second guess then perhaps rethink your goals.",
        "We? \"I\"",
        "“Should I get a Master’s degree?”\n\n“Yes”\n\n“Wait okay how do I do that”\n\n“Apply to college and graduate”\n\n“… idk if you’ve considered this but that takes time and effort! Not to mention an entrance exam! I gotta get into the school! There’s a wrinkle in your advice!”\n\n👆this is what you sound like and if you don’t get it, it does not sound good",
        "Good to hear! Just curious, which country are you in, that companies are funding masters program? Getting paid to get a degree sounds super cool.",
        "United States I've been doing it for a year I guess",
        "I’m not sure which country he is pursuing his master's degree in, but in Brazil, master's degrees (as well as undergraduate and PhD programs) are free at public universities. However, admission is quite competitive. It is common for students to receive government scholarships for their master's studies, although the scholarships typically provide only the minimum wage."
    ]
},
{
    "submission_id": "1gl34nn",
    "title": "Predicting personal jewellery preference",
    "selftext": "Hi everyone,\n\nI'm currently working on a passion project where I’m trying to predict whether someone will like a jewellery image based on a set of images they've previously rated as \"like\" or \"dislike.\"\n\nTo do this, I'm using transfer learning with pre-trained models like ResNet50/18 or DenseNet, applying them to a small, user-specific dataset. Typically, each dataset contains around 1,000 disliked images and only 70–100 liked images per person. These images are product photos of various types of jewellery (earrings, necklaces, rings, etc.) in a range of styles and colours.\n\nI'm facing challenges with achieving high accuracy and precision. The model tends to either overfit or swing between predicting too many dislikes or too many likes every few epochs. To address the class imbalance, I'm applying heavy augmentations to the \"liked\" class and adjusting the loss function with class weights.   \n  \nUsing SMOTE to increase the liked samples, yielded worse results compared to the augmentation approach and made the model even more prone to overfitting.\n\nIf anyone has suggestions for improving my approach or ways to evaluate it further, I would appreciate the input. I'm relatively new to the ML space, so any advice is welcome!\n\nLet me know if more details would be helpful.",
    "created_utc": "2024-11-06T08:56:07",
    "num_comments": 0,
    "comments": []
},
{
    "submission_id": "1gl1cdt",
    "title": "introduction to quantum measurements",
    "selftext": "we've created a free course to help people learning machine learning also learn quantum fundamentals. our new video is on measurement: [https://www.youtube.com/watch?v=i8U5LA4m2sw](https://www.youtube.com/watch?v=i8U5LA4m2sw)\n\nyou can access the full free course with real application chapters coming soon: [https://www.ingenii.io/qml-fundamentals/](https://www.ingenii.io/qml-fundamentals/)",
    "created_utc": "2024-11-06T07:39:27",
    "num_comments": 0,
    "comments": []
},
{
    "submission_id": "1gkzwii",
    "title": "Benchmarking GGUF Models",
    "selftext": "This might be a noob question since I'm new to this, but what is the easiest way to benchmark GGUF models and test the speed in tokens per second? I want to benchmark on MMMU, MMLU, MathVista, HumanEval, RealWorldQA, and HumanEval. ",
    "created_utc": "2024-11-06T06:35:11",
    "num_comments": 0,
    "comments": []
},
{
    "submission_id": "1gkz9u8",
    "title": "Finishing my PhD in Biochemistry, aiming for a BioComputational postdoc—what training would you recommend to learn Machine Learning?",
    "selftext": "Dear community.\n\nI need help on what decision to make. In January I am going to finish my PhD thesis in Biochemistry. In the last year I have approached the world of data science through structural bioinformatics (I just implemented some Linear Regressions, and a K-Means clustering to a reduced dataset generated by my experiments). I found it really interesting and for this reason I want to keep growing and learn Machine Learning.\n\nI would like to find an applied BioComputational Postdoc where I can learn, but at the same time I do not consider myself the best candidate for this kind of positions as I have not dedicated all the time of my thesis to Computational Biology. For this reason I would like to learn during this winter or winter/spring to be more attractive for a good postdoc. What kind of training would you recommend? Bootcamp? University professional certification? MSc? Online certifications from Coursera or related? \n\nI understand that, in general, an MSc would be the best option, but in my particular case it might not be the best option (because of the time). I am thinking perhaps in something online to be ready to move abroad for the postdoc. Any Idea? Suggestions? I'm working and living in Europe. Thanks.",
    "created_utc": "2024-11-06T06:06:00",
    "num_comments": 0,
    "comments": []
},
{
    "submission_id": "1gkx83q",
    "title": "ml projects as beginner",
    "selftext": "hello. from the beginning of my journey with ml/ds/ai i was trying to create something cool on my own. i started in 2021, since then the field skyrocketed, however i have the feeling that creating something on your own is still challenging - access to compute, data etc is limited. what's your take on that, how would you progress without big team and resources?",
    "created_utc": "2024-11-06T04:21:02",
    "num_comments": 2,
    "comments": [
        "Create your simple dl library and train for MNIST",
        "Actually there are plenty of resources out there for anyone to build something cool in dl/ai. There are many websites that provide free computation at least for an intermediate project."
    ]
},
{
    "submission_id": "1gkvyxb",
    "title": "[D] NLP/ML Research Topics (2024)",
    "selftext": "Hi everyone! I am starting my bachelors thesis on NLP/ML next spring. I have taken all ML and NLP courses at my university and I have made good number of projects all by myself along with one conference paper.\n\nI am exploring interesting ideas for doing research on. I am aware that bachelors thesis is supposed to be simple but I am not afraid to tackle on more complex problems. I love coding, algorithms and implementations from scratch.\n\nIf you have any recommendations and any general query, please write down below! All recommendations are welcome. Thank you!",
    "created_utc": "2024-11-06T03:04:16",
    "num_comments": 0,
    "comments": []
},
{
    "submission_id": "1gkvubm",
    "title": "Perplexity AI PRO - 1 YEAR PLAN OFFER ",
    "selftext": "As the title: We offer Perplexity AI PRO voucher codes for one year plan.   \n\nTo Order: https://cheapgpts.store/Perplexity\n\nPayments accepted:  \n\n- PayPal. (100% Buyer protected)  \n- Revolut.",
    "created_utc": "2024-11-06T02:56:15",
    "num_comments": 0,
    "comments": []
},
{
    "submission_id": "1gkohn1",
    "title": "Looking for advice on training a model to ‘think’ around a specific philosophy or idea set",
    "selftext": "Hey all! I’m new to working with large language models (LLMs) but really excited to start a project that builds a kind of “functional mind” around a specific philosophy. My aim is to create a small library of text around a certain set of ideas so the model can understand this philosophy in-depth and be able to generate practical applications, variations, and insights from it.\n\nThe ideal setup would let me train the model to recognize the core principles of this philosophy, but then go further and relate it to other topics or areas of knowledge. I’d also like to eventually connect it with an image generation model to visualize the concepts it generates. Think of it like building a model that can be both a thinker and a creator around these ideas.\n\nSince I’m a beginner, I’d love any advice on:\n\n\t•\tThe simplest, most approachable tools or models to start with for this type of project. Hugging Face? Other platforms?\n\t•\tHow much time I should expect to spend learning the basics to actually make this happen.\n\t•\tTips for preparing text data: what’s the best way to get a philosophy “into the model’s head”?\n\nIf you’ve done anything similar or have some insight on the best way to approach this, I’d really appreciate your advice and honest opinions. \n\nI’m guessing since the resource cost of the operations on a local level would be pretty intense, and since I don’t have the cards or memory for it, so a cloud based solution would probably be ideal. \n\nThe best way to go about doing this kind of thing is something I’m not fluently versed in and researching without opening a discussion has just led down rabbit holes to confusion and indecision.\n\nAll insights welcome.\nThanks a ton!",
    "created_utc": "2024-11-05T18:48:41",
    "num_comments": 3,
    "comments": [
        "[removed]",
        "rag",
        "Thanks, I’m stoked to dig into it. 🙏"
    ]
},
{
    "submission_id": "1gko1w6",
    "title": "Regression model ",
    "selftext": "Hi all,\n\nAs a beginner of machine learning field, I read some books about the regression model. Just a few questions: \n\n1. How often is a baseline model created before developing a machine learning model for a use case? \n\n2. Usually, RSME is an important evaluation metric to evaluate the linear regression model. What is the guideline for us to judge if it is a good machine learning by looking at RSME (validation) and RSME (testing)?\n\nThank you in advance\n\nST",
    "created_utc": "2024-11-05T18:24:01",
    "num_comments": 1,
    "comments": [
        "1) once, 2) Nothing"
    ]
},
{
    "submission_id": "1gklol0",
    "title": "Model Fit and Scaling",
    "selftext": "Hi Everyone,\n\nThis feels like a sort of silly question, but I want to clarify something for myself:\n\nLet’s say I perform my train-test split, fit and transform my training data on the necessary scalers/encoders, and fit and tuned the model hyperparameters on the training data. Now, let’s say I want to use the whole dataset to train the model. Would I reuse the scalers/encoders during the initial tuning stages to transform the entire dataset, rather than fit a whole new series of scalers and encoders? I feel like that should be the case, since the training data was majority (~80-90%) and changing the scaler and encoders could cause a difference in performance when fitting the whole dataset. \n\nSorry if that doesn’t make sense entirely, but I’m just trying to wrap my head around if we want to fit new scalers/encoders when applying the tuned model to the whole dataset, rather than using the scalers and encoders during training",
    "created_utc": "2024-11-05T16:20:00",
    "num_comments": 1,
    "comments": [
        "Just change the percentage in train test split to 1. And re run the code after saving model output from previous iteration."
    ]
},
{
    "submission_id": "1gklffp",
    "title": "Alternatives to PyTorch in Python?",
    "selftext": "Those that are using Python, which libraries are you using?\n\nWhich libraries have you considered?\n\nI'm trying to search [PyPI.org](http://PyPI.org), for \"machine learning\", and there are 10,000+ hits. I want to narrow it down to 100",
    "created_utc": "2024-11-05T16:07:30",
    "num_comments": 21,
    "comments": [
        "The direct alternatives that exist are JAX and Tensorflow, but there is really no reason to not Pytorch these days. If you're having trouble with Pytorch, your issue is probably framework-agnostic.",
        "If you want an alternative to pytorch, the only real one is jax and its ecosystem. Tensorflow is getting deprecated soon, and pytorch just dominates everything. Beware, jax is not a 1 to 1 conversion to pytorch. \nAnd these are deep learning frameworks, very different than ml frameworks in general. So if you search for ml in pipy you are targeting a different application that is not exactly the same as pytorch, tf and jax.",
        "JAX and PyTorch are the big 2 now",
        "PyTorch is the wave. \n\nI like PyTorch Lightning as well, it’s a nice wrapper that trades control for simplicity of code. Makes it easy to set up logging and eliminates bugs from forgetting stuff like model.eval() or resetting your optimizer. Again, if you really need control over those types of things, you’re probably an advanced user who deeply understands PyTorch already. \n\nIf you’re on an Apple device, I’ve been really curious about MLX lately. But I haven’t had the time to set up a codebase equivalent to what I have in PyTorch and benchmark them.",
        "Take a gander at this: \n\nhttps://aicodewizards.com/2023/05/21/comparison-of-deep-learning-frameworks/",
        "Don't find alternatives to PyTorch, just use Pytorch",
        "Tinygrad if you are into minimalism",
        "Maybe better: check out GitHub for machine learning projects. It can really show you what's popular and how people are using different libraries in the real world. You can filter by stars and see how recently they've been updated to spot the ones that are well-maintained",
        "i can see tinygrad taking off in the future ",
        "Jax, and maybe Tinygrad but it's still beta.",
        "> but there is really no reason to not Pytorch these days.\n\n\n\nWhat if the model is to run locally on Android?\nSo far I thought the export from Tensorflow to rtlite / tensorflowlite is easier than the one from Pytorch.",
        "Do you have any idea about the reason for the deprecation of TensorFlow?",
        "I'm just starting, so I want to experiment with different libraries.\n\nI don't really have a specific goal, I just want to learn stuff by trying different things.",
        "[deleted]",
        "pytorch to onnx",
        "Its not deprecated perse, but no one in research uses it, universities dont teach in it anymore, keras moved to support more backends... Its an open secret that it is dying.\n\nAs of why? Timing, pytorch was more confortable at the start of deep learning, much more pythonic, while tensorflow was a pain in the ass to use. When Tensorflow introduced the eager mode, it was too late, as most private R&D had moved to pytorch already. And employees learn the framework that gives money. So it ended up like this.",
        "Marry one of them. The knowledge is incredibly transferrable. The concepts behind all of them is the same, pytorch is very OOP heavy and jax is more functional",
        "Start with a meta framework, like Keras, PyTorch-Lightning, or fastai.\n\n\nThen after a month drop down to PyTorch.\n\n\nThere are other not so popular alternatives like Flax, and ancient ones like chainer, theano, caffe.",
        "If you just want to experiment and learn, then I'd push for Jax. It's definitely a different way of thinking about things that you may come to appreciate. It also has some fun ideas like vmap and pmap that are worth learning about.\n\nJax on its own is more of a numpy replacement so you'd probably want to learn either flax or equinox on top of that.",
        "Your question is a bit confusing can you rephrase it",
        "I guess the keyword for what you're describing is knowledge distillation or transfer learning.",
        "I agree with all of what you said. I tried to certify myself early on and I could never install the version that they required on my mac."
    ]
},
{
    "submission_id": "1gkka55",
    "title": "PGP-AIML Program by The McCombs School of Business at The University of Texas at Austin",
    "selftext": "I have an undergrad degree and 10 years of IT experience. Thinking about a Masters in AI/ML program in future. Any feedback if this course for 7 month? Course fee is around $4000. Another option is to apply for MSAI from UT Austin for $10000. TIA",
    "created_utc": "2024-11-05T15:14:39",
    "num_comments": 4,
    "comments": [
        "There’s will be a huge difference on your profile if it says master’s from UT Austin compared to PGP. Also the learnings from master’s program will be in different league compared to PGP. \n\nBut if you don’t want to invest long time for maater’s, the PGP is good to introduce you to the world of AIML. That’s it.",
        "I appreciate your feedback. Really echoing my thought as well. Planning to take this one and in parallel start applying for Masters.",
        "Also if you want to get introduced to AIML, there are lot of cheaper options like udemy. They can be cheaper alternatives to PGP.",
        "Do you think PGP will have any impact to get accepted for the Masters Program?"
    ]
},
{
    "submission_id": "1gkj75v",
    "title": "Is this derivative correct? The dj/dd = 2 part, like I just don't get why increasing d by 0.001 would increase j by 0.002?",
    "selftext": "https://preview.redd.it/1zlnuq9fs5zd1.png?width=2556&format=png&auto=webp&s=ec58f4edb5e405569097ce2fe4c362a85fc5f3f6\n\nit's on the back propogation videos",
    "created_utc": "2024-11-05T14:25:47",
    "num_comments": 3,
    "comments": [
        "You can also calculate it by hand. If you increase d by 0.001 you increase J by roughly 0.002 which is dj/dd*0.001 (your partial derivative)\n\nBut this only works if your value is very small. In this case you will have an error of 0.0000005. This is because you can treat the function J as linear for very small values",
        "just look at it as rise over run dj = 2 if dd = 1, so j doubles with an increase in d",
        "It's from andrew ng's course in ML specialisation (I'm a complete ML beginner but I have studied calculus before so I'm confused about exactly what I'm missing here"
    ]
},
{
    "submission_id": "1gkhq7v",
    "title": "[P] Predicting Offensive Content in Alphanumeric Strings",
    "selftext": "I'm working on a task to predict if a given string is offensive or not. For example, let's say I’m given a 7-character alphanumeric string like \"AB@D@55\" which stands for A BAD.... can't complete it as the post might get removed by moderators. How can I determine if such words or patterns are offensive?\n\nI’m looking for guidance on:\n\n* **Data Collection**: What types of data should I be gathering to recognize offensive patterns?\n* **Feature Engineering**: What features should I focus on to improve accuracy in detecting offensive content?\n* **Model Selection**: Which models would work best for this type of prediction?\n* Any other guidance ...\n\nSince I’m new to data science, I’m not fully sure how to structure this process, but I’m ready to dive into advanced concepts like NLP and large language models (LLMs). Any advice or resources to get started would be greatly appreciated!",
    "created_utc": "2024-11-05T13:22:39",
    "num_comments": 1,
    "comments": [
        ">What types of data should I be gathering to recognize offensive patterns?\n\nYou need annotated data. Meaning given a sentence or word, there's a label attached saying this sentence is offensive or not.\n\n>What features should I focus on to improve accuracy in detecting offensive content?\n\nGiven the fact that this is text dataset, you could only use NLP feature extraction method like tfidf, word2vec or language model embedding.\n\nAlso, accuracy is a bad performance metric, especially for classs imbalance dataset, please use AUC ROC instead.\n\n>Which models would work best for this type of prediction?\n\nIt depends on your features and your expected model performance. Tree based models might work in this case but since this is an NLP task, perhaps RNN or LSTM or transformers might perform better."
    ]
},
{
    "submission_id": "1gkgh1v",
    "title": "How does a C in real analysis affect an otherwise good transcript for PhD applications?",
    "selftext": "Looking like I'm going to get a C in real analysis this semester. How would it affect application if all my other grades are A/A-, and 2 B+ in freshman year?\n\nAm applying to phd/masters for robotics/vision/ml for fall 2025 admission with 1 2nd author workshop (and possibly conference) paper. I know that ml theory does have some real analysis, which is why I am slightly worried.\n\nI could also drop the course this semester and retake it next semester. Dropping would have no effect on GPA.",
    "created_utc": "2024-11-05T12:29:18",
    "num_comments": 18,
    "comments": [
        "Realistically I would just withdraw and retake. A withdraw can be explained away and likely wont even be questioned.",
        "you're probably find, no worries ",
        "I was accepted into a world-class phd program for ML with definitely suboptimal grades… I didn’t think it was even worth for me to try to get in. As long as your research is strong and you find a good advisor who believes in you, you’ll be fine :)",
        "Based on my experience serving on the graduate admission committees, it would be challenging to get into a decent PhD program with such abysmal undergraduate grades. If you can't master trivial things, then you are not likely to succeed at a PhD level. Of course, there are exceptions, but they are rare. I personally have not witnessed them.",
        "It looks like the final grades for this class won't even be out before the majority of my application deadlines. In this case, would it be fine to leave it on? Would there be any issues with the school finding out about this later (if they request updated transcripts and/or after admission)",
        "Why are you calling the courses op took trivial ? You don't know at which level he took them.",
        "[deleted]",
        "why would you consider a single C abysmal? the rest of the grades are overwhelmingly A/A-.\n\nthe reason for this C was because 1) professor was not the greatest and 2) wanted to focus on research. i'm very confident I could get an A or at least a B if I retook it with a different professor, but I'm just wondering if it's worth the effort/tuition",
        "They likely wont withdraw your offer if thats what you are worried about",
        "I'm sure you know better than top experts in my field who write evaluations of my research for promotion cases and the National Science Foundation that funds my research program.",
        "A-, B+, C are bad grades. I don't think you can be selected for any PhD programs with low grades",
        "Because you'd be competing against students with nearly perfect undergraduate records (many of them won't even have a single B). Even a single C grade would typically make your application below average and lower your likelihood of acceptance. I'm not saying it's impossible, just not likely. If you can replace this grade with a better one, you should do so.\n\nI'm sorry to see you took offence with my answer. I naively thought you wanted an honest opinion from someone who reviewed hundreds of graduate applications.",
        "That makes sense. Do you know if schools ask for updated transcripts after the initial application, but before decisions are sent out?",
        "[deleted]",
        "not sure what i said that made you think i took offence, but thanks for the info",
        "imagine your whole future getting jeopardized by having just a single bad day/test lol... The wonders of modern academia.",
        "I dont remember",
        "God I hate how you're somehow pretentious and anti-intellectual at the same time. Truly a cursed combo.",
        "I hope at least these fantasies make you feel better. Cheers!",
        "[deleted]",
        "In general I'm fine with anti-gatekeeping, but at this point it's swinging too far in that direction. You need to be good at math to do research in a math-heavy field like ML. Being bad at calculus will, and should limit your opportunities in the field. The solution is putting in more work and being good at it instead."
    ]
},
{
    "submission_id": "1gkfnib",
    "title": "Roast my resume, have been looking for ML/DS Roles with 4+ years of Exp.\n",
    "selftext": "",
    "created_utc": "2024-11-05T11:54:46",
    "num_comments": 3,
    "comments": [
        "Can mods ban resume posts? These are getting annoying",
        "I think my biggest gripe with this resume is while I get what you did, I can't tell why you did it. I feel like with 4 years of experience you need to be showing that not only can you do the technical side of the job but also have a solid understanding of the business side as well which isn't shown here. Like great you used LangChain and RAG which is super hot right now but why was that the right solution to use? What impact did that have on the business / users? What metrics did you use to track whether or not the project was successful? You probably don't have the space to answer all these questions for every bullet point but it would be good to show you understand this side of the work throughout your CV.",
        "Thank You So much !! i should expand each bullet points with above suggestions."
    ]
},
{
    "submission_id": "1gkfi5j",
    "title": "Built an On-Device Small Language Model (SLM) Leaderboard",
    "selftext": "Hey all! We recently put together an on-device SLM leaderboard as a resource for us local language models lovers. It’s designed to make it easier to compare models' quantized versions based on both quality (using ifEval) and performance metrics (like response time, output speed, prefill speed, and power consumption).\n\nYou can check it out here: [nexa.ai/leaderboard](https://nexa.ai/leaderboard)  \n\\* Code is open-sourced, so feel free to reproduce or contribute!\n\nRight now, the leaderboard includes these models:\n\n* **Llama3.2**\n* **Qwen2.5**\n* **Phi-3.5**\n* **Phi-3**\n* **Gemma2**\n* **Llama3.1**\n\nFor example, if you're trying to figure out the best model that’ll run on **an iPhone 15 with 6GB of RAM**, you can set the filters and see that **gemma-2-9b-instruct Q3\\_K\\_S** is a solid option. But if faster response time is a priority, **Llama3.2-3B Q5\\_K\\_M** cuts the total response time in half, which could improve user experience.\n\n\\*Total Response Time is the time taken to receive 100 tokens.\n\nhttps://preview.redd.it/iguo3os905zd1.png?width=2830&format=png&auto=webp&s=8508261f825f1b26aed3e4d5c2ceb34564df3a14\n\nIf you have ideas for more metrics, models to add, or any other features, let us know! We’re hoping this can be a useful tool for anyone experimenting with local LLMs, so any feedback or suggestions would be awesome. Cheers <3",
    "created_utc": "2024-11-05T11:48:17",
    "num_comments": 0,
    "comments": []
},
{
    "submission_id": "1gkdv8i",
    "title": "Is this a Google Colab issue or is there something wrong with my code ",
    "selftext": "def replace\\_multiple\\_periods(text): return re.sub(r\".{2,}\", \".\", text)  # Replace 2 or more periods with a single one\n\n    paragraph_parallel['English'] = paragraph_parallel['English'].apply(replace_multiple_periods)\n    paragraph_parallel.head()\n    \n    '''\n    def split_paragraphs_to_sentences(df):\n        new_rows = []\n        for _, row in df.iterrows():\n            english_sentences = row['English'].split('.')\n            kannada_sentences = row['Kannada'].split('.') if 'Kannada' in row else []  # Initialize as empty list if column not present\n            malayalam_sentences = row['Malayalam'].split('.') if 'Malayalam' in row else []\n            tamil_sentences = row['Tamil'].split('.') if 'Tamil' in row else []\n            telugu_sentences = row['Telugu'].split('.') if 'Telugu' in row else []\n    \n            # Get the maximum number of sentences across all languages\n            max_sentences = max(len(english_sentences), len(kannada_sentences), len(malayalam_sentences),\n                               len(tamil_sentences), len(telugu_sentences))\n    \n            # Pad shorter lists with None to align sentences\n            english_sentences += [None] * (max_sentences - len(english_sentences))\n            kannada_sentences += [None] * (max_sentences - len(kannada_sentences))\n            malayalam_sentences += [None] * (max_sentences - len(malayalam_sentences))\n            tamil_sentences += [None] * (max_sentences - len(tamil_sentences))\n            telugu_sentences += [None] * (max_sentences - len(telugu_sentences))\n    \n            for i in range(max_sentences):\n                new_row = {\n                    'English': english_sentences[i].strip() if english_sentences[i] else None,\n                    'Kannada': kannada_sentences[i].strip() if kannada_sentences[i] else None,\n                    'Malayalam': malayalam_sentences[i].strip() if malayalam_sentences[i] else None,\n                    'Tamil': tamil_sentences[i].strip() if tamil_sentences[i] else None,\n                    'Telugu': telugu_sentences[i].strip() if telugu_sentences[i] else None,\n                }\n                new_rows.append(new_row)\n    \n        return pd.DataFrame(new_rows)\n    '''\n    \n    '''sentences_parallel = split_paragraphs_to_sentences(paragraph_parallel)\n    sentences_parallel.head()'''\n    \n    def preprocess_english_text(texts):\n        # Ensure the function returns a string if the input is a list of strings\n        return ' '.join([''.join(c for c in text.lower() if c.isalnum() or c.isspace()) for text in texts])\n    \n    # slicing the dataset into first 25 rows\n    paragraph_parallel = paragraph_parallel.head(15)\n    \n    def preprocess_indic_text(text, lang):\n        factory = IndicNormalizerFactory()\n        normalizer = factory.get_normalizer(lang)\n        normalized_text = normalizer.normalize(text)\n    \n        #Introduce spaces between words after normalization for trivial_tokenize to work as expected\n        normalized_text = \"\".join([char if char.isalnum() else \" \" for char in normalized_text]).strip() # Add space to separate tokens after normalization\n    \n        # Tokenize the text after normalization:\n        tokens = trivial_tokenize(normalized_text, lang)  #Now the tokenizer will work as expected.\n    \n        return \" \".join(tokens)\n    \n    def split_sentences(text):\n        sentences = re.split(r\"(?<!\\w\\.\\w.)(?<![A-Z][a-z]\\.)(?<=\\.|\\?)\\s\", text)\n        return [s.strip() for s in sentences if s.strip()]\n    \n    # Core Alignment Function (optimized with batching and tensor operations)\n    def align_sentences_in_paragraphs(params):\n        para1_list, para2_list, lang1, lang2, threshold = params\n    \n        if not para1_list or not para2_list:\n            return []\n    \n    \n        para1_processed = [preprocess_english_text([p]) if lang1 == 'en' else preprocess_indic_text(p, lang1) for p in para1_list if p] #Added empty string check\n        para2_processed = [preprocess_english_text([p]) if lang2 == 'en' else preprocess_indic_text(p, lang2) for p in para2_list if p] #Added empty string check\n    \n        all_sentences1 = [sentence for para in para1_processed for sentence in split_sentences(para)]\n        all_sentences2 = [sentence for para in para2_processed for sentence in split_sentences(para)]\n    \n        #Crucial: Remove empty strings *before* checking for short paragraphs\n        all_sentences1 = [sent for sent in all_sentences1 if sent]\n        all_sentences2 = [sent for sent in all_sentences2 if sent]\n    \n        if not all_sentences1 or not all_sentences2: #Checking for empty sentences before short paragraph check.\n            return []\n    \n    \n    \n    \n        if len(all_sentences1) < 2 or len(all_sentences2) < 2:\n            try:\n                encoded1 = model.encode(para1_list, convert_to_tensor=True, device=device)\n                encoded2 = model.encode(para2_list, convert_to_tensor=True, device=device)\n    \n                if all([isinstance(enc, torch.Tensor) for enc in [encoded1, encoded2]]):\n                    similarity = util.cos_sim(encoded1, encoded2)\n                    if similarity >= threshold:\n                        return [(para1_list, para2_list)]  # Return list of tuples\n                return []\n    \n            except Exception as e:\n                print(f\"Error during short paragraph comparison: {e}\")\n                return []\n    \n    \n        embeddings1 = model.encode(all_sentences1, device=device, convert_to_tensor=True)\n        embeddings2 = model.encode(all_sentences2, device=device, convert_to_tensor=True)\n    \n    \n        similarity_matrix = util.cos_sim(embeddings1, embeddings2)\n    \n    \n        aligned_pairs = []\n        used_indices1 = set()\n        used_indices2 = set()\n    \n        for i in range(len(all_sentences1)):\n            best_j = torch.argmax(similarity_matrix[i]).item()  # Use torch.argmax\n            if similarity_matrix[i][best_j] >= threshold:\n                if i not in used_indices1 and best_j not in used_indices2:\n                    aligned_pairs.append((all_sentences1[i], all_sentences2[best_j]))\n                    used_indices1.add(i)\n                    used_indices2.add(best_j)\n    \n        return aligned_pairs\n    \n    def create_sentence_aligned_dataset(paragraph_data, langs, threshold=0.7, num_processes=multiprocessing.cpu_count()):\n        \"\"\"Handles all language pairs with correct batching.\"\"\"\n    \n        multiprocessing.set_start_method('spawn', force=True)\n    \n        num_langs = len(langs)\n        sentence_aligned_data = {}\n    \n        for i in range(num_langs):\n            for j in range(num_langs):\n                if i != j:\n                    lang1, lang2 = langs[i], langs[j]\n                    sentence_aligned_data[(lang1, lang2)] = []\n    \n        all_args = []\n        for i in range(num_langs):\n            for j in range(num_langs):\n                if i != j:\n                    lang1, lang2 = langs[i], langs[j]\n                    para1_batch = [para_group.get(lang1, \"\") for para_group in paragraph_data]  # Extract paragraphs for lang1 in a single batch.\n                    para2_batch = [para_group.get(lang2, \"\") for para_group in paragraph_data]  # Extract paragraphs for lang2\n    \n                    all_args.append((para1_batch, para2_batch, lang1, lang2, threshold)) #Corrected arguments\n    \n        total_comparisons = len(all_args)  # Correctly calculate the number of comparisons.\n    \n    \n    \n        with multiprocessing.Pool(processes=num_processes) as pool, tqdm(total=total_comparisons, desc=\"Processing Language Pairs\") as pbar:\n          results = []\n          for result in pool.imap_unordered(align_sentences_in_paragraphs, all_args):\n              results.append(result)\n              pbar.update()\n    \n        #Corrected: Assign the entire result from each process call\n        k = 0\n        for i in range(num_langs):\n            for j in range(num_langs):\n                if i != j:\n                    lang1, lang2 = langs[i], langs[j]\n    \n                    sentence_aligned_data[(lang1, lang2)] = results[k] #Directly assign the result.\n                    k+=1\n    \n    \n    \n        return sentence_aligned_data\n    \n    \n    langs = ['en', 'kn', 'ml', 'ta', 'te']\n    paragraph_data = []\n    for _, row in paragraph_parallel.iterrows():\n        paragraph_data.append(dict(zip(langs, row.values.tolist())))\n    \n    sentence_aligned_data = create_sentence_aligned_dataset(paragraph_data, langs)\n    \n    for lang_pair, pairs in sentence_aligned_data.items():\n        print(f\"Language Pair: {lang_pair}\")\n        for pair in pairs:\n            print(pair)\n\n* A bit of context , I am trying to create embeddings for sentences , after a long while i am somewhat close to the results so I try switching the runtime setting from cpu to gpu.\n* But once i do that the same program not gets stuck at the line showing in the bottom \"line xx > next() > wait() \" for more than 3 mins(here)--->for result in pool.imap\\_unordered(align\\_sentences\\_in\\_paragraphs, all\\_args): results.append(result) pbar.update()with multiprocessing.Pool(processes=num\\_processes) as pool, tqdm(total=total\\_comparisons, desc=\"Processing Language Pairs\") as pbar:     results = \\[\\] \n* This is weird because the same on cpu took me 30-40 sec for one iteration. Also note that the same issue occured before where it was stuck for an hour.\n* I have initalized the gpu at the beginning too`model = SentenceTransformer('l3cube-pune/indic-sentence-similarity-sbert').to(device)`\n   * Please help me with this , I can provide more information in the comments if needed.",
    "created_utc": "2024-11-05T10:39:59",
    "num_comments": 0,
    "comments": []
},
{
    "submission_id": "1gkcukr",
    "title": "Network anomaly detection using clustering?",
    "selftext": "This is a project that I wanted to work on but didn't understand how to get started with it basically what it does is classifies the unusual activity in the network into different clusters.\n\nI know some basic ML, any perspective would be helpful?\n",
    "created_utc": "2024-11-05T09:58:06",
    "num_comments": 0,
    "comments": []
},
{
    "submission_id": "1gkav9y",
    "title": "Exploring Pretrained Embeddings for RNNs: Static vs. Contextual Approaches",
    "selftext": "I’m tinkering on an NLP project with RNNs and am debating whether to use traditional pretrained embeddings like GloVe or more advanced, contextual embeddings. I know BERT-based vectors and other transformer-based approaches are popular, but I’m not sure if the added complexity is worth it for my project. Has anyone tested both static and contextual embeddings in RNN setups? Any insights on which approach yielded better results or required specific tuning?",
    "created_utc": "2024-11-05T08:35:26",
    "num_comments": 18,
    "comments": [
        "If you’re open to newer options, have you considered sentence embeddings like Sentence-BERT? They seem to be better at capturing context over whole sentences, which might suit an RNN well",
        "You could try both and report back which one works well.",
        "Instead of word2vec or GloVe, have you thought about using embeddings from T5? It’s great for sentence-level context, and I’ve seen it work better than static embeddings on downstream tasks.",
        "I’ve seen that static embeddings sometimes miss out on context that’s crucial for understanding nuances.",
        "I actually moved away from GloVe/word2vec entirely and found DistilBERT to be a solid compromise on speed and accuracy. Plus, it’s much smaller if you’re concerned about resources.",
        "Heres an example of the difference:\n\nTake the word \"key\". Your static embedding might have one token for this word to mean an object that opens a lock, but this will fail to represent other meanings like \"important\", which might be key to understanding some phrases.",
        "I used to rely on GloVe for simplicity, but switching to ELMo embeddings gave my RNN a noticeable accuracy boost since they add context for each word dynamically. Might be worth a try",
        "For an RNN setup, embeddings from models like XLNet can also add context well, especially for tasks that require bidirectional understanding. It’s a bit of a hybrid between traditional RNNs and transformers.",
        "In my experience, GloVe and word2vec were okay, but I saw a significant boost in performance with embeddings from RoBERTa.",
        "Any reason you’re sticking to RNNs instead of experimenting with transformers directly?",
        "Just a thought: pre-trained embeddings are good, but training your own embeddings on your specific dataset can sometimes yield better results than GloVe or BERT.",
        "Awesome, will take a look, thank you!",
        "thanks for your input! what would you advise?",
        "DistilBERT could be just what I need for a lightweight model. I’ll check this out too, cheers!",
        "🫡🫡",
        "XLNet sounds interesting - I haven’t considered that one - I’ll look into how it compares. Thank you!",
        "I actually have a specific interest in capturing the sequential dynamics in the data, and RNNs, especially LSTMs, do a good job of modelling dependencies over time. While transformers capture long-range dependencies well, RNNs often have a more straightforward inductive bias for tasks that rely heavily on sequential order. LMK your thoughts tho! curious"
    ]
},
{
    "submission_id": "1gk9vfh",
    "title": "Wondering where my career in AI is taking me",
    "selftext": "Had a look at my CV recently and noticed I have been in corporate for 3 years now. Mostly B2B. I have been developing AI/ML solutions using LLMs for the past 2 years. Wondering if there are any opportunities available for domains other than Retail, Banking , IT, etc. (I am tired of catering to the old ones who think LLMs and Agentic AI is the solution to everything)? I wake up every single day, wondering if this is my true calling or not? Sounds cheesy, but I wonder if there's a field where I can learn, express, but also make real change in the world. I need to get out of this endless cycle and do something that suits my skillset, yet being able to wake up every single day knowing what I do is making a difference.",
    "created_utc": "2024-11-05T07:53:37",
    "num_comments": 4,
    "comments": [
        "[https://80000hours.org/](https://80000hours.org/)",
        "Police or military probably. Both need data scientists and I would probably feel pretty meaningful.  \nI fairly recently branched into anti fraud solutions from your run of the mill banking work and it's been extremely fun and engaging. Still just doing coding but somehow hunting bad guys makes it feel like a game and it's super satisfying.",
        "Sounds interesting. Any links I could check out to get an idea of the fields you have mentioned?",
        "It'll depend on your country. My country's equivalent to the CIA, FBI, NSA etc. often post job ads to their websites and LinkedIn. The military has huge amounts of job ads as well.\nAs for antifraud, if you're already into banking I'd give your closest customers/partners a call. Always easier to pivot with existing relations."
    ]
},
{
    "submission_id": "1gk9n29",
    "title": "Recommend some good resources to learn Generative AI and ML from basics",
    "selftext": "\n\nSo I got some bunch of ideas which got good potential which can be built mainly using Generative AI and ML, I am good at Python, so please suggest me some good resources to learn the required tech skills to learn them and apply \nThere have been lots of resources, but I want to know which is something better to start with? ",
    "created_utc": "2024-11-05T07:43:41",
    "num_comments": 4,
    "comments": [
        "If you just want to apply large language models to a task you're much better off using existing APIs or taking open source models like Llama or Mistral and fine-tuning them.\n\nBut if you want to learn how they're built for educational purposes, this playlist by Andrej Karpathy is excellent:  \n[https://karpathy.ai/zero-to-hero.html](https://karpathy.ai/zero-to-hero.html)",
        "Best course: machine Leanings by Andrew NG\nOne of the best book: Introduction to statistical learning",
        "i would recommend more to follow youtube there are so many wonderful tutorials related to genAI. if you are also into learning through books search for genAI books by oreily...",
        "* Machine Learning Specialization - Andrew ng course\n* Machine Learning for all Supervised Machine Learning regression and classification\n* IBM Machine Learning with Python\n* IBM Machine Learning introduction for everyone\n* Machine Learning A-Z - Udemy\n* Complete Machine Learning Bootcamp - Udemy are some of the [best machine learning courses for beginners](https://codingvidya.com/best-machine-learning-courses/)"
    ]
},
{
    "submission_id": "1gk9he3",
    "title": "ML Project Inquiry",
    "selftext": "Hello guys, I am considering to do a Graduation project that uses Machine learning and Image processing to Identify and write reports about X-rays , where do you think I shuold start? do you think stanfords university's Andrew Ng course is a good starting point? And what sources should I look for for the image processing side of things? I think python would be the optimal tool to do this project with, right?",
    "created_utc": "2024-11-05T07:36:55",
    "num_comments": 0,
    "comments": []
},
{
    "submission_id": "1gk8qov",
    "title": "BERT Token classification question",
    "selftext": "Hi all, I'm finetuning BERT for emotion classification on token level, see code below. Specifically, I want to predict intensity values for a list of specific emotions for each token, e.g. `\"This party sucks!\" -> This (sad: 0.0, angry: 0.0, happy: 0.0, ...), party (sad: 0.5, angry: 0.6, happy: 0.0, ...), sucks (sad: 0.5, angry: 0.6, happy: 0.0, ...)`. My question is, can the model accurately predict the emotion on token level when it hasn't \"read\" the whole sentence yet? E.g. if the model predicts the emotion for \"party\" it hasn't read \"sucks\" yet and doesn't know if it's positive or negative, right?\n\n    class BertForTokenEmotionIntensity(BertPreTrainedModel):\n        def __init__(self, config, num_emotions=12):\n            super().__init__(config)\n            self.num_emotions = num_emotions\n            self.bert = BertModel(config)\n            self.dropout = nn.Dropout(config.hidden_dropout_prob)\n            self.classifier = nn.Linear(config.hidden_size, self.num_emotions)  # Output layer for 12 emotions\n            self.init_weights()\n        \n        def forward(self, input_ids, attention_mask=None, labels=None):\n            # Get BERT's outputs\n            outputs = self.bert(input_ids=input_ids, attention_mask=attention_mask)\n            sequence_output = outputs.last_hidden_state  # Shape: (batch_size, seq_len, hidden_size)\n            \n            # Apply dropout and classification layer\n            sequence_output = self.dropout(sequence_output)\n            logits = self.classifier(sequence_output)  # Shape: (batch_size, seq_len, num_emotions)\n            loss = None\n            \n            if labels is not None:\n                # Calculate mean squared error loss for intensity prediction\n                loss_fn = MSELoss()\n                active_loss = attention_mask.view(-1) == 1  # Mask out non-target tokens\n                active_logits = logits.view(-1, self.num_emotions)[active_loss]\n                active_labels = labels.view(-1, self.num_emotions)[active_loss]\n                loss = loss_fn(active_logits, active_labels)\n    \n            return {\"loss\": loss, \"logits\": logits}\n    \n    model.train()\n    for epoch in range(100):\n        for batch in dataloader:\n            input_ids = batch['input_ids']\n            attention_mask = batch['attention_mask']\n            labels = batch['labels']\n    \n            outputs = model(input_ids=input_ids, attention_mask=attention_mask, labels=labels)\n            loss = outputs['loss']\n    \n            optimizer.zero_grad()\n            loss.backward()\n            optimizer.step()",
    "created_utc": "2024-11-05T07:04:40",
    "num_comments": 4,
    "comments": [
        ">if the model predicts the emotion for \"party\" it hasn't read \"sucks\"\n\nBERT doesn't use causal masking. Each token sees all tokens (which is why [CLS] goes first - it sees everything)",
        "Your question about predicting emotions at the token level with BERT is a great one. Typically, BERT shines because it processes text in both directions, picking up context more effectively. \n\nYou’re right to be concerned about it not \"reading\" all tokens completely. When BERT makes predictions, it takes into account the entire input, so the meaning of something like \"party\" is definitely affected by words like \"sucks\" in the same sentence. \n\nIf you're going for token-level classifications but want to keep BERT's strengths, make sure to train it on a variety of examples. That way, it can really grasp those subtle context shifts. Hopefully, this clears things up for you! Good luck with your project!",
        "Thanks for the explanation - that makes sense with the CLS token!! So if I understand correctly, during the finetuning step, the input is processed in the following order: tokenization, embedding, attention (the step where each token \"knows\" the other tokens\"), and then the forward function is called (the step where we make the prediction and get the output logits). Does that make sense?",
        "I'm sorry but is your response AI-generated?"
    ]
},
{
    "submission_id": "1gk6n9g",
    "title": "GAN to extend data in time",
    "selftext": "Looking for guidance here.\n \nWill it be possible to use GAN(still relevant) as a way to 'predict' from sensor data of a year?\n\nAny python lib to help with it?\n\nSorry for english and if this is not te proper subreddit for this.",
    "created_utc": "2024-11-05T05:28:07",
    "num_comments": 0,
    "comments": []
},
{
    "submission_id": "1gk6ln8",
    "title": "Best ways to improve data preprocessing in ML+ your top 10 Python libs for ML ?",
    "selftext": "I'm working on a machine learning project and wanna make sure my data preprocessing is on point. I’m already doing the basics (like normalization, handling missing values, etc…), but is there any new or advanced techniques, algorithms..etc I should know about ? Also, can you give me your top 10 python libs that will improve my project?",
    "created_utc": "2024-11-05T05:25:51",
    "num_comments": 4,
    "comments": [
        "sklearn with custom functions",
        "You don’t need a ton of libraries. You need a small handful (at most) that can efficiently run whatever preprocessing logic you decide on. A combination of Spark and Numpy, for example can be used to implement whatever logic you have at scale for preprocessing.\n\nIf you want to research new and better ways to handle missing values, outliers, etc. I recommend looking into (1) unsupervised learning/clustering and (2) testing assumptions/heuristics/hypotheses. This will hopefully guide you to finding the “logic” to implement in preprocessing. \n\nOn a tangent, here are my top libraries for DE/ML in no particular order: Spark, Pandas (for EDA and early prototyping only), PyTorch, PyTorch Geometric, Numpy, Transformers, Scipy, Scikit-Learn (for early prototyping only).",
        "Why are pandas and sklearn only for prototyping?",
        "They’re not efficient on large datasets/in production. Sklearn is also too abstracted for engineering purposes."
    ]
},
{
    "submission_id": "1gk6asi",
    "title": "Macbook for ML class, research and coding",
    "selftext": "What laptop do you recommend for work - teaching ML classes, coding e.g. mobile apps, research? I would like at least 16GB RAM. On the one hand, due to the fact that I have to take the laptop to university, portablity and weight are tempting, on the other hand, cooling is in the pro. In addition, I do not know whether to look around for the new M4, or something older like the M2/M3 will suffice. What more in the case of a pro or take a PRO processor?",
    "created_utc": "2024-11-05T05:10:10",
    "num_comments": 16,
    "comments": [
        "You can spend an extra $1k+ and get a really burly laptop that will still be underpowered compared to a desktop with a heavy Nvidia card.  Or spend a lot less and get a passable system and have a ton of money left over for GPU rental. The best laptop you can buy will still be weaker than a desktop, and for really significant networks, you’d need a cluster anyway. So I’d say: pick a vendor and build the infrastructure to deploy multi-GPU training systems on demand. I think you’ll do better work and save money that way",
        "get the baseline M2 Air - now comes with 16GB ram at the same price. i'm using M1 Air rn (8GB even) and it's still more than enough for coding (as long as you don't train heavy models)",
        "I work as a research engineer, I use a macbook air m1.",
        "I am in process of building DL models, I use a MI Pro. I think the latest Pro one would last you years. So I would invest in that one and forget about it.",
        "Definitely go Pro. I mainly use Mac’s for ML and even with the M chips they can get quite toasty after a few minutes of running an all core code. So you definitely need the cooling",
        "MacBook Air is enough, though if you have the money go for the pro. It won’t matter much since you won’t run anything locally. It mostly comes down to preference.",
        "I might add that I have a desktop computer with GPU at home (I'm considering upgrading with new one in the future aIso), and there are also some at university. However, I need something that I can work on both at university and at home (and ‘outsource’ the heavier work to a desktop computer).",
        "How??? I am a researcher in ML/AI as well and my M3 Max MacBook and maxed out M2 Ultra constantly crash being out of resources, let alone the MacBook’s fans screaming like a jet engine!",
        "I have a big beefy M3 Pro 16 and I almost never do any actual ML on it. It's nice having the horsepower for other apps photoshop, resolve etc. but def don't buy it for workloads. It's also not real fun to lug around all day. So if you need to be on the go constantly id second the air. If you will be at a desk all day and use it for mixed productivity the larger pro is quite nice. For ML just consider it a terminal interface.",
        "Aye get a >=16gb MBP, best you can afford and install some Linux distro on your computer at home (dual boot or whatever) ssh into the your main machine to run heavy stuff \n\nI literally run this set up on a M1 MBP 8gb, I keep looking for reasons to upgrade but I honestly have none",
        "I have a DGX at work with 8 A100s I can use whenever. Also have access to a GCP cluster with 32 A100s but it's GCP so my company gets billed for it hence I try not to use it unless I really need that level of compute, quite easy to run large scale jobs there using slurm tho. I'm not really running models on my laptop. A t4 on kaggle is typically fine for any ML learner like OP here. Universities with good ML programs also usually have clusters so no need to run locally for research.",
        "Ok so you are not really using an M1 Air. I would argue that you cannot really learn ML uploading on servers. Also they are not readily available to students for practice. I think the OP should buy something like a 32GB MBP to start. Using the servers is gonna be a later step.",
        "Yes though, I literally am. You need at best a kaggle T4 to learn ML. They are always available. You can learn everything from diffusion to LLMs on a cloud T4 that's dirt cheap (or literally free on kaggle and collab). \n\n- code on local \n- debug on local \n- launch small scale experiment on some server\n- log results \n- learn\n\nYou don't have to train on terabytes of data to learn. Right before covid so a few years back I took Aaron Courvilles representation learning course. Do you know how students were expected to complete his assignments that ranged from VAEs, diffusion to Bert style LMs? Collab. \n\nIf you want to do research for a conference paper I would still argue a macbook air + lambda or similar cloud providers is more economical. The only time you should invest in a proper workstation is well if you build a proper workstation with a proper gpu because you earn money through it and run experiments on it all the time. For a beginner I can't find a reason to justify investing 1000$s of dollars in compute. When all you really need is a place to run vscode or pycharm and run things online.",
        "Yeah but you cannot code and debug locally unless your machine is powerful enough and has enough RAM/VRAM to load the models. \n\nEven something like a 32GB M1 Pro is great for a beginner, as it has good RAM/VRAM and can run forever since it has cooling. Anything less than this and you cant run anything locally.",
        "As I said before : \n\n1. 99.99% of things can be done locally on a potato. \n\n2. The 0.01% of the time you need compute you can ssh into an instance and debug your code IF you need 32gb ram to debug to begin with that is. \n\nI don't know what you are talking about but MOST models require less than 16gb ram to debug. Even for vision models with high res images. As a beginner there is no real requirement for someone to use ViT XL3 when they could just use a ViT S. I can't think of a reason why a beginner would need to load something that requires 32gb ram. And if they do as I said, going into vscode or whatever IDE and clicking on the ssh button takes less than a second and has infinitely more compute for dirt cheap as long as you don't use an A100 or higher. Nobody needs to run full blown llama locally on their machine to learn as a beginner.\n\nEdit : i shoukd also add that I have worked for around 5 years now as a researcher with an m1 air or worse. I have a personal m1 pro base model (16gb ram I think, could be 8gb idk) which I used during my masters, did research for 2 tier 1 CV publications and 1 tier 2 and also wrote my own path tracer with next event estimation and photon mapping when reading up on physics based rendering. All with a base m1 pro or a base m1 air. \n\nI have a desktop too I use for gaming that has a 4090 and THE ONLY time I used it was when I was doing some quantization work because I couldn't install the correct tensorrt version on gcp.\n\n\nImo it's about being resource efficient. Sure having a 32gb Mac pro makes it easier but I would never ask someone to buy one and spend 1000s of $$ to learn something they don't really need all this for.",
        "It's not about running LLAMA, but as long as you want to run any mathematical ML your VRAM usage skyrockets. Tbh thats the reason why I suggest people buy Macs instead of even 4090's, as the 4090 desktop I have in the lab runs out of VRAM in three quarters of the watered down models I try to run. \n\nI think we are talking about different stuff though, because what my students (uni. prof. in maths and AI) need to run definitely can't run in potatoes. Our constant struggle I that we cannot assign an A100 to each student obviously. We have these clusters in my lab to offload the research projects of the lab when they are in a level they can run, and use the full dimensionality, but these are strictly unavailable for class projects. \n\nThere is also the supercomputer but the students have to upload their script, and wait for time to be allocated which depending on the time of the year can take from a few hours to a day.\n\nThats why I am saying, anyone that wants to do it seriously should invest on something with enough power to run basic stuff locally."
    ]
},
{
    "submission_id": "1gk5ml9",
    "title": "Music generation state of the art",
    "selftext": "Is there a music/sounddesign model that allows fine-tuning with own data or sound-to-sound-generation, similar what you can do with Stable Diffusion for images?",
    "created_utc": "2024-11-05T04:35:01",
    "num_comments": 0,
    "comments": []
},
{
    "submission_id": "1gk5b1d",
    "title": "what bachelor degree is the most suitable for machine learning research? math, cybersecurity and computer engineering at a lower level university are my options",
    "selftext": "question in title. im currently majoring in maths, but im thinking about transferring to cybersecurity   \n[cybersecurity curriculum](https://bbf.itu.edu.tr/egitim/lisans/siber-g%C3%BCvenlik-m%C3%BChendisli%C4%9Fi) [math curriculum](https://bogazici.edu.tr/en_US/Content/Academic/Undergraduate_Catalogue/Faculty_of_Arts_and_Sciences/Department_of_Mathematics)",
    "created_utc": "2024-11-05T04:16:49",
    "num_comments": 3,
    "comments": [
        "CE and cybersecurity aren't even related to ML. You may want to do some research on the subject to see if ML is even what you want to study as it seems like you don't understand what it is based on 2 of your options. Math is even a questionable choice, but it's close enough. Statistics is the better choice between those two.",
        "CS, Math, Physics",
        "Most of the MLEs on my team has done CS usually with a graduate degree in CS. Not to say math or physics wouldn’t work but it’s mainly CS."
    ]
},
{
    "submission_id": "1gk59b7",
    "title": "scikit-learn's ML MOOC is pure gold",
    "selftext": "I am not associated in any way with scikit-learn or any of the devs, I'm just an ML student at uni\n\nI recently found scikit-learn has a full free MOOC (massive open online course), and you can host it through [binder from their repo](https://github.com/INRIA/scikit-learn-mooc). [Here is a link to the hosted webpage](https://inria.github.io/scikit-learn-mooc/). There are quizes, practice notebooks, solutions. All is for free and open-sourced. \n\nIt covers the following modules:\n\n* Machine Learning Concepts\n* The predictive modeling pipeline\n* Selecting the best model\n* Hyperparameter tuning\n* Linear models\n* Decision tree models\n* Ensemble of models\n* Evaluating model performance\n\nI just finished it and am so satisfied, so I decided to share here \\^\\^ \n\nOn average, a module took me 3-4 hours of sitting in front of my laptop, and doing every quiz and all notebook exercises. I am not really a beginner, but I wish I had seen this earlier in my learning journey as it is amazing - the explanations, the content, the exercises.",
    "created_utc": "2024-11-05T04:13:59",
    "num_comments": 19,
    "comments": [
        "Also plugging in the YouTube channel probabl. The guy who makes those videos is afaik one of the co founders of scikit learn and he delves into pretty niche topics but it's interesting",
        "I recently started doing some data science and machine learning and was looking for something like this for ml really appreciate this mate",
        "Thanks mate",
        "Is there something similar for DL?",
        "I’m such a fan of the sklearn docs. I literally don’t think I used any other resources except the course materials for the machine learning classes I’ve taken using sklearn. For someone approaching my MDS from a physics background, being able to focus on the problem solving rather than the coding was a huge help.",
        "goldmine",
        "Awesome! Thank you for sharing",
        "Gold indeed, big thanks!",
        "This seems to be a  good one to get some hands on practice early on! Thanks for sharing bud..",
        "thx!",
        "Thanks for sharing, really useful",
        "Hi folks, Vincent here, while I appreciate the compliment I would like to point out that I am \\*not\\* a co-founder. Merely a fan of the project ;)",
        "probabl have also recently launched a [scikit-learn cert program](https://certification.probabl.ai/) where the proceeds partially go towards funding sklearn development",
        "channel link?",
        "Check out fastai course for torch, I still have to go through it myself",
        "Ah my bad. I'll keep it in mind the next time. But thanks a lot for the videos :D",
        "does this have a learning roadmap with materials ? I tried the above link but i can see it has only exam? Where is the studying pathway ? Where is the materials",
        "https://youtube.com/@probabl_ai?si=rNARohNCD8ybGrZr",
        "you can use the MOOC for that"
    ]
},
{
    "submission_id": "1gk4ykv",
    "title": "Need Advice regarding machine learning  ",
    "selftext": " how hard is machine learning to pursue as a career if i am only interested but not that much passionate for it ? do machine learning engineers have to work with the reasearch papers or what would entry machine learnung engneers and senior ml engineers do ???? reply n give suggestions",
    "created_utc": "2024-11-05T03:56:35",
    "num_comments": 3,
    "comments": [
        "As far as I know u do  the research stuff generally when ur pursuing the academia (phd) except that there isnt much research thing in normal ml engineers u gotta be good with algorithms and stuff",
        "Phd in Machine learning for creating or inventing neww stuffs in field of ai ?",
        "Kinda, heard if ur interested in research u go for the phd"
    ]
},
{
    "submission_id": "1gk4j14",
    "title": "how to deal with false negatives and true negatives in confusion matrix where my model doesnt generate those values?",
    "selftext": "I'm using change point detections algorithms where, given a timeseries, it predicts the timestamp where it thinks there is a change point. \n\nfor example prediction = \\[1, 200, 365, 1000\\]\n\nso if i have a groundtruth to check the correctness of my model with groundtruth = \\[1, 300, 365, 900\\]\n\nWe can compute some cell in our confusion matrix. like\n\ntrue positives = \\[1, 365\\]\n\nfalse positives = \\[200, 1000\\]\n\nbut.. what about false negatives and true negatives? This is a binary classification problem, where 1 is equal to a change point and 0 should be the rest where there is no change point. But my model only returns the prediction of 1. it doesnt generate any 0.",
    "created_utc": "2024-11-05T03:28:10",
    "num_comments": 4,
    "comments": [
        "It does actually produce 0's; most of it's outputs are zeros. The right way to interpret this is that your input is a list of timestamp indices [1,2,3,4,5,...] and your output is a binary sequence [0,0,1,0,1,1...] that indicates whether each corresponding timestamp is a change point. You just don't see zeros in your output because you've implicitly structured your model to return sparse vectors.",
        "MHH nice idea. I managed to avoid my question because, by definition of recall and precision, it's just precision = TP/prediciton\\_array where prediction\\_array is my prediction that is made by TP and FP\n\nwhere recall = TP/groundtruth\\_array where groundtruth=TP+FN\n\nso i managed to avoid knowing the FN\n\n  \nBut your question made me thinking. right now i just computed the f1 score. But should i compute the f1 macro score by considering also the 0 and making an average?\n\nA colleague of mine worked on randomforest on the same dataset, but he trained the RF by giving example with label 0 and 1. So computing a macro f1 scores makes sense because in this way you see if the model learned good both for 1 and 0. But in my case where the 0 is implicit, it's just the \"absence of 1\", should i calculate the macro f1 score as well or just the normal f1 score?",
        "This is a binary classification problem so i think only the normal F1 score is appropriate. Basically everything works exactly as it usually does, you should just use all of the labels - zeros and ones - in your calculations. As you noted you can sometimes simplify calculations to use only the ones.",
        "Instead what about my colleague with rf? The thing that he used macro f1 was erong and he should have used the normal f1 score?"
    ]
},
{
    "submission_id": "1gk3u43",
    "title": "Besides memory, What are the benefits of using these Data Center GPUs like H100 instead of a Quadro 6000 ADA when using as a single GPU for this whooping price difference ?\n",
    "selftext": "I am new to this AI area.\n\nLet me break down the key technical specifications between the H100 and Quadro RTX 6000 Ada:\n\nNVIDIA H100 (SXM5):\n\n* CUDA Cores: 18,432\n* 4th Gen Tensor Cores: 576\n* Memory: 80GB HBM3\n* Memory Bandwidth: 3.35 TB/s\n* Clock Speed: \\~1.7 GHz base\n* FP32 Performance: \\~60 TFLOPS\n* FP8 Performance: \\~2,000 TFLOPS (with sparsity)\n* 80 GB VRAM\n\nQuadro RTX 6000 Ada:\n\n* CUDA Cores: 14,592\n* 3rd Gen Tensor Cores: 228\n* Memory: 48GB GDDR6\n* Memory Bandwidth: 960 GB/s\n* Clock Speed: 2.3 GHz base\n* FP32 Performance: \\~91 TFLOPS\n* FP8 Performance: \\~364 TFLOPS\n* 48 GB VRAM\n\nWell, yes.\n\nH100 has more specs than 6000 Ada. But the price difference is very huge.\n\n6000 Ada is $7,500 whereas H100 price is $45000. I searched the internet and found the advantages like \"good for multi GPU setups, has Transformer Engine...\".\n\nBut if I use as single GPU, what is the reason of the huge price difference?\n\nThat is, the price difference is 6.2 times.\n\nSo, if I bought a H100 and a 6000 Ada and running a model (LLM or Img Gen ) which uses just 40 GB of RAM on both, so, do I get atleast 5x performance in H100?",
    "created_utc": "2024-11-05T02:41:12",
    "num_comments": 8,
    "comments": [
        "No, but a H100 shouldn’t really be used for solely inference. A L20 would be recommended, and the price difference is around the 6000 Ada. Even so, the two GPUs (along with the H100) cater to different markets. One being the workstation market, the other the data center/server market.\n\nYou see OpenAI or big companies using multiple H100s because they have fast interconnects that allow massive scaling across nodes, to which the L20s can’t.",
        "H100s scales like crazy. If you buy a single one there won't be much improvement over a 6000 ada. But use an nvlink and connect ten H100s and it is unmatched.",
        "ooh.\n\nIf I use 2 x H100, then do they show as single GPU that is they support frameworks those don't support multi GPUs ?",
        "ooh. The price difference is very huge, though when compared with 6000 ada",
        "No, they will not show as a single GPU. However, distributing the workload (different layers) to different GPUs and running them in parallel via software can speed up the calculations. There is lower performance loss because of the fast interconnects (e.g. NVLink switches) compared to simply the PCIe bandwidth.",
        "So, virtual we able to simulate  it as a single GPU to the Framework?",
        "No. Everything needs to be coded to be able to distribute the workload across GPUs. Read these: \n\n[https://huggingface.co/docs/text-generation-inference/en/conceptual/tensor\\_parallelism](https://huggingface.co/docs/text-generation-inference/en/conceptual/tensor_parallelism)\n\n[https://huggingface.co/docs/transformers/v4.13.0/en/parallelism](https://huggingface.co/docs/transformers/v4.13.0/en/parallelism)",
        "yeah got it.\n\nAnd L20 not found in Nvidia website. Only L40 is there ?"
    ]
},
{
    "submission_id": "1gk3tdw",
    "title": "Need suggestion and guidance ",
    "selftext": "hello , i am currently a prefinal cs grad , i have started learning ml and want to go deep in it . Started with andrew ng course on coursera , currently completed with course 1 (linear regression and logistic regression) and hence i want to build somethings upon those topics and include whatever techniques learnt during the course before starting course 2 of neural networks .  \nSo the help/suggestion i need is what to and how to start with projects to learn . I wasnt a DS student and have no prior experience in this field . Also will the courses by andrew be enough to learn about the topics ?",
    "created_utc": "2024-11-05T02:39:41",
    "num_comments": 1,
    "comments": [
        "keep learning until specialization get completed. You cannot do anything useful with current knowledge you got during course 1. it’s too basic imo"
    ]
},
{
    "submission_id": "1gk3l5w",
    "title": "Best video series on probability and statistics",
    "selftext": "I’ve been trying to refresh the maths I studied during my engineering undergrad since it’s been a while, and I’ve just been through the 3b1b linear algebra course and khan academy multivariable calculus course (also given by Grant from 3b1b lol) which I really enjoyed. \n\nI was wondering if there was an equivalent high quality video series for probability and statistics. I would want it to go to a similar level of roughly undergrad level maths and I’m doing this to prepare myself for some ML + physics-based modelling work so it would be great if the series also covered some stochastic modelling and markov processes type stuff alongside all the basics of course. \n\nI would take a text book and dive in but unfortunately I don’t have the time and the quick but thorough refresh a video series can provide is great, but if you do have any non video recommendations which you think would really work please do let me know! \n\nThank you!! ",
    "created_utc": "2024-11-05T02:23:15",
    "num_comments": 21,
    "comments": [
        "MIT's statistics and probability on edX",
        "There is this great resource I used during my graduation for the statistics: https://www.jbstatistics.com/",
        "If you’ve already seen the content, I’d highly recommend Steve Brunton’s lecture series on statistics/probability. 2 separate playlists and he is an amazing teacher. Pair that with Jbstatistics and you’re good to go. Brunton recently released the playlists, so others may not be recommending it. I’ve been through both in the past month they are amazing.",
        "Do you have access to the videos that you studied during undergrad period?",
        "Will check it out, thanks!",
        "I second this resource. I keep coming back to this for interview preparation as I am yet to find a resource who explains p-values, CI and power as clearly as JB does.",
        "Wow this is great and very nicely organised, thanks!!",
        "Will definitely check out both Steve and Jbstatistics, thanks!",
        "No unfortunately not",
        "It starts in May 2025 and if you want to learn rn then i would recommend taking MIT's statistics and probability course that is on yt in 2011 and it has been taught by the same instructor who will be guiding you through the course that's gonna start in 2025",
        "Can't seem to find it, can you please share the link? It'd be huge for me.",
        "Here you go:\nhttps://youtube.com/playlist?list=PLUl4u3cNGP61MdtwGTqZA0MreSaDybji8&si=KaCtYL46-55IZ93E",
        "Thank you!! I got this as a result but was confused since the name was not probability & statistics....",
        "Ah sorry, this course is focused on probability which sets you up for 18.650 which is solely focused on statistics.\n\nDisclaimer - the course on edX may need some pre-requisites to go through that course smoothly coz I saw a comment on reddit saying you would need understanding of calculus up until integration (inclusive) to completely understand everything in the course. So it could be same for this course which is on yt as well but then again I don't know. I will be learning probability at uni's level in May so yeah I don't know.",
        "I've enrolled for the course on edX, wanted to fast track my learning a bit since there was a machine learning course & data analysis course on edX by mitX also happening from January.... So, can't wait till may. I have all the prerequisites, I've done single variable calculus + multivariate calculus+ linear algebra up till now. So, I'm confident I can do both of these courses!!",
        "Haha I am just stepping into this world. Although I have got my resources like I will be doing statistics and probability from edX that I just reccomended you. Linear algebra from LAFF course which is offered by UT Austin on edXand differential calculus from Steve's brunton's channel, i would like to know where did you these things from if you don't mind me telling",
        "I'd say for Linear Algebra especially, Gilbert Strang is the best in the world... you can search for MIT 18.06SC ....\n\nI've personally taken MIT 18.01SC (Single Variable Calculus), MIT 18.02 (Multi-variate Calculus) & MIT 18.06SC (Linear Algebra by Strang) ... + do checkout \"Essense of Linear Algebra by 3blue1brown on yt\" - it's a goldmine.\n\nI'm not aware about the resources you've planned to take, so can't comment on how they fare comparatively.",
        "Personally I checked out a video of 18.06 by strang and I found it bit overwhelming so decided to drop it",
        "Actually in MIT 18.02 only, they introduce the concept of vectors and plane etc. so when you get to MIT 18.06SC by Strang, you already have a basic idea about those things so whatever Strang teaches, you'll be able to grasp easily.\n\nPersonally since I had already done that, I found Strang lectures quite intuitive & definitely the best I've seen in the market of free resources. Maybe you should consider taking 18.02 if you have time before diving into Strang lectures (obviously considering your basic limits, derivatives, integrals, linear/quadratic approximation, MVT etc. topics are all cleared up - basically single variable calculus)",
        "Thanks for your response will do that",
        "All the best!!"
    ]
},
{
    "submission_id": "1gk3d8l",
    "title": "VQGAN unstable training",
    "selftext": "I'm training an image tokenizer with vqgan setting and adam optimizers on OpenImages in fp32.\n\nIn the early stage, everything looks ok but after training for 5-7 epochs, the gradient norm got quite large which make the discriminator and generator hard to balance to learn new features. How to solve this? I 've got r1\\_gradient\\_penalty with weight of 10 but it's not enough. \n\nhttps://preview.redd.it/gj2rcsp642zd1.png?width=1924&format=png&auto=webp&s=0b3a524d97fcde7d63b173d06c11797d15cc5d12\n\nBtw, the reconstructed images look fine (the first column within the red box is the reconstruction result, the 2nd column is the input images. I took the rest examples from traming  transformers' colab.)\n\nhttps://preview.redd.it/6liurmir32zd1.png?width=727&format=png&auto=webp&s=d181f19cbc0562e813d3d0f1cf8a0410e0080d54\n\n",
    "created_utc": "2024-11-05T02:06:48",
    "num_comments": 0,
    "comments": []
},
{
    "submission_id": "1gk1lsy",
    "title": "How to access YOLOv8n performance metrics across all classes?",
    "selftext": "I want to be able to do something like this:\n\n    epochs = [50, 100, 150]\n    batch_sizes = [32, 64]\n    best_model = None\n    best_mAP = 0\n    \n    \n    for epoch in epochs:\n        for batch_size in batch_sizes:\n            print(f\"Training for {epoch} epochs with batch size {batch_size}...\")\n    \n    \n            # Train the model\n            results = model.train(\n                data='dataset.yaml',\n                epochs=epoch,\n                batch_size=batch_size,\n                imgsz=640, \n                device='0'\n            )\n    \n    \n            if model.results[3] > best_mAP:\n              best_model = model\n              best_mAP = model.results[3]\n\nBut to access metrics I have to do this and get class wise.:\n\n    print(\"Precision\\t  Recall\\tmAP50\\t\\tmAP95\")\n    results.class_result(0)\n\nI would get mAP@50 like this:\n\n    results.class_result(0)[3] \n\nIs there a way to get overall metrics across all classes? \n\nShould save model according to best mAP@50 for each class?\n\nOR\n\nshould I calculate average mAP@50 for across all classes ( avg(results.class\\_result(i)\\[3\\]) ) then select the model with better average mAP@50?\n\nPlease provide some insight.\n\n",
    "created_utc": "2024-11-04T23:48:57",
    "num_comments": 0,
    "comments": []
},
{
    "submission_id": "1gk0yym",
    "title": "🚀 AI Unlocked: A Practical Guide to Mastering Large Language Models (LLMs) 🧠🔓",
    "selftext": "Hey Redditors! 👋\n\nI recently put together a blog series on Medium called **“AI Unlocked: Building & Mastering Large Language Models, Step-by-Step”** that’s all about breaking down complex AI concepts into bite-sized, relatable pieces. Whether you’re just starting out in the world of AI or looking to deepen your understanding, this series is designed to make learning **fun** and **practical**. 🧑‍🏫✨\n\n**What’s Inside?**\n\nHere’s a quick overview of what each chapter covers:\n\n• **Understanding LLMs** 🏗️: Dive into the foundational concepts like attention mechanisms and transformers. Think of it like understanding the structural blueprint of a skyscraper 🏙️!\n\n• **Prompt Engineering** 🤔: Learn how to guide AI responses effectively, just like training a pet with the right cues 🐶.\n\n• **Retrieval-Augmented Generation (RAG)** 🔄: Explore how to turn your AI into a real-time information expert 🌐 by combining memory with live data retrieval for dynamic responses.\n\n• **Fine-Tuning for Precision** 🎯: Specialize your AI for complex, nuanced tasks by giving it a focused “apprenticeship” 🎓.\n\nEach chapter includes **real-world examples**, **hands-on projects**, and **clear analogies** to make learning accessible and enjoyable. 🛠️\n\nIf you’re curious about how AI can be applied practically, or just want to build a solid understanding of LLMs, I’d love for you to check it out. **Under 100 minutes** total, and you’ll come out with a strong grasp of AI fundamentals!\n\n🔗 Here’s the link to the series: [https://medium.com/@yusufsevinir/building-llms-from-poc-to-production-an-overview-ea7ceb9aa8d8](https://medium.com/@yusufsevinir/building-llms-from-poc-to-production-an-overview-ea7ceb9aa8d8)\n\nWould love to hear your thoughts and answer any questions you might have. Let’s make AI learning accessible and fun for everyone! 💪🚀",
    "created_utc": "2024-11-04T23:00:47",
    "num_comments": 3,
    "comments": [
        "Please include evaluation and comparision metrics.",
        "u/MadridistaMe thanks for the feedback , in detail you meant? I had a chapter for this , be specific about it so I can enrich the related chapter",
        "arxiv.org/abs/2307.03109"
    ]
},
{
    "submission_id": "1gk0vyv",
    "title": "Suggestion for books for learning GenAI and suggestion on concepts required to grasp GENAI effectively and the respective books for these concepts as a beginner  ? ",
    "selftext": "I'm looking for a book for learning GEN AI as a beginner \n\nAnd what all other concepts and books are required for learning GENAI efficiently \n\nDo we need to learn Natural Language processing along with Machine learning and Deep Learning too ? \n\nWith so many books on various concepts present I dont know which concepts I actually need to know for GEN AI\n\n  \nKindly help \n\n  \n",
    "created_utc": "2024-11-04T22:54:50",
    "num_comments": 0,
    "comments": []
},
{
    "submission_id": "1gk04ac",
    "title": "How to run GGUF LLMs using python? Explained ",
    "selftext": "GGUF is an optimised file format to store ML models (including LLMs) leading to faster and efficient LLMs usage with reducing memory usage as well. This post explains the code on how to use GGUF LLMs (only text based) using python with the help of Ollama and LangChain : https://youtu.be/VSbUOwxx3s0",
    "created_utc": "2024-11-04T22:01:21",
    "num_comments": 2,
    "comments": [
        "Can you make a video on running multi modal LLMs on CPU with quantization",
        "Will do it soon. Thanks"
    ]
},
{
    "submission_id": "1gjymba",
    "title": "\"Mathematical Foundations for Machine Learning: A simple physical intuition for determinants\"",
    "selftext": "https://preview.redd.it/7gqcsaijg0zd1.png?width=1280&format=png&auto=webp&s=e85e4d6cd95058d9d3d1f32293c8d6164e6e8ef3\n\n\n\nFor many of us, linear algebra may have felt abstract or even tedious, especially when it came to determinants. But determinants offer far more than just a mathematical calculation—they provide a way to understand transformations intuitively.\n\n\n\nIn my latest lecture on Vizuara’s YouTube channel titled \"Foundations for Machine Learning | Determinants and Linear Transformations,\" we approach determinants from a new angle. Rather than seeing them as mere numbers, we delve into how they represent the stretching, squishing, or even collapsing of areas in 2D space.\n\n\n\nThis lecture breaks down:\n\n1) How determinants correspond to scaling areas in linear transformations\n\n2) How transformations affect space, whether by expanding, contracting, or reducing areas to lines or points\n\n3) The meaning behind positive, negative, and zero determinants, giving these values a concrete interpretation\n\n\n\nThis lecture is part of a larger 45-hour course I have developed over the past 4 months. This course, \"Foundations for Machine Learning\", spans around 65 lectures, each designed to build an intuitive understanding of linear algebra and its role in machine learning. The focus is on simplifying complex concepts through geometric and logical intuition, providing a clear path for those interested in machine learning without requiring a deep mathematical background.\n\n\n\nExplore this new lecture on determinants and see how this essential concept can transform the way we understand space and transformations in machine learning: [https://www.youtube.com/watch?v=X5AKfID0aXQ&feature=youtu.be](https://www.youtube.com/watch?v=X5AKfID0aXQ&feature=youtu.be)",
    "created_utc": "2024-11-04T20:30:20",
    "num_comments": 0,
    "comments": []
},
{
    "submission_id": "1gjx8bv",
    "title": "Do I have to upload a poster and a video recording for my accepted paper in NeurIPS 2024 to be published?",
    "selftext": "I registered the virtual pass of NeurIPS 2024. Do I have to upload a poster and a video recording for my accepted paper in NeurIPS 2024 to be published? The emails or the instructions do not make any clarification about this. ",
    "created_utc": "2024-11-04T19:13:16",
    "num_comments": 0,
    "comments": []
},
{
    "submission_id": "1gjtkq4",
    "title": "Best Machine Learning Books for Beginners to advanced",
    "selftext": "",
    "created_utc": "2024-11-04T16:12:40",
    "num_comments": 3,
    "comments": [
        "Hands on machine learning with sklearn",
        "not a good list",
        "1, 3, 12, and 13 are good."
    ]
},
{
    "submission_id": "1gjsvoi",
    "title": "Bad Results with MNIST GAN",
    "selftext": "My goal is to create a GAN where the generator uses transposed convolutional layers and the discriminator uses convolutional layers. I need to make sure both have 5 layers and I am not allowed to change the first layer for either the generator or the discriminator.\n\nFor some reason, my code runs, but the training is not promising whatsoever, just producing random white dots even at 70 epochs. I am told that I should see convergence (some legible numbers being output) within 50 epochs or so.\n\nHere is my code for reference:\n\n    import torch\n    import torch.nn as nn\n    import torch.optim as optim\n    from torchvision import datasets, transforms\n    from  import DataLoader\n    import matplotlib.pyplot as plt\n    from tqdm import tqdm\n    \n    # Check if CUDA is available and set the device\n    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n    \n    # Define the Generator class \n    Generator(nn.Module):\n        def __init__(self, latent_dim):\n            super(Generator, self).__init__()\n            self.main = nn.Sequential(\n                nn.ConvTranspose2d(latent_dim, 512, kernel_size=4, stride=1, padding=0),\n                nn.BatchNorm2d(512),\n                nn.ReLU(True),\n                nn.ConvTranspose2d(512, 256, kernel_size=4, stride=2, padding=1),\n                nn.BatchNorm2d(256),\n                nn.ReLU(True),\n                nn.ConvTranspose2d(256, 128, kernel_size=4, stride=2, padding=1),\n                nn.BatchNorm2d(128),\n                nn.ReLU(True),\n                nn.ConvTranspose2d(128, 64, kernel_size=4, stride=2, padding=2),\n                nn.BatchNorm2d(64),\n                nn.ReLU(True),\n                nn.ConvTranspose2d(64, 1, kernel_size=3, stride=1, padding=1),\n                nn.Sigmoid()\n            )\n    \n        def forward(self, x):\n            return self.main(x)\n    \n    # Define the Discriminator class \n    Discriminator(nn.Module):\n        def __init__(self):\n            super(Discriminator, self).__init__()\n            self.model = nn.Sequential(\n                nn.Conv2d(1, 16, kernel_size=4, stride=2, padding=0),\n                nn.LeakyReLU(0.2),\n                nn.Conv2d(16, 32, kernel_size=4, stride=2, padding=1),\n                nn.LeakyReLU(0.2),\n                nn.Conv2d(32, 64, kernel_size=4, stride=2, padding=1),\n                nn.LeakyReLU(0.2),\n                nn.Conv2d(64, 128, kernel_size=3, stride=2, padding=0),\n                nn.LeakyReLU(0.2),\n                nn.Conv2d(128, 1, kernel_size=1, stride=1, padding=0)\n            )\n    \n        def forward(self, x):\n            return self.model(x).view(-1, 1)  # Reshape output to [batch_size, 1]\n    \n    # Hyperparameters\n    latent_dim = 64\n    batch_size = 128\n    num_epochs = 100\n    learning_rate = 0.002\n    \n    # Data loading\n    transform = transforms.Compose([\n        transforms.ToTensor(),\n        transforms.Normalize((0.5,), (0.5,))  # Normalize the dataset\n    ])\n    \n    mnist_data = datasets.MNIST(root='./data', train=True, download=True, transform=transform)\n    dataloader = DataLoader(mnist_data, shuffle=True, batch_size=batch_size)\n    \n    # Initialize models, loss function, and optimizers\n    generator = Generator(latent_dim).to(device)\n    discriminator = Discriminator().to(device)\n    criterion = nn.BCEWithLogitsLoss()\n    optimizer_g = optim.Adam(generator.parameters(), lr=learning_rate, betas=(0.5, 0.999))\n    optimizer_d = optim.Adam(discriminator.parameters(), lr=learning_rate, betas=(0.5, 0.999))\n    \n    # Function to generate and display images\n    def display_generated_images(epoch):\n        z = torch.randn(16, latent_dim, 1, 1, device=device)\n        generated_images = generator(z).detach().cpu().numpy()\n    \n        # Plot generated images\n        fig, axes = plt.subplots(4, 4, figsize=(8, 8))\n        for i in range(16):\n            ax = axes[i//4, i%4]\n            ax.imshow(generated_images[i][0], cmap='gray')\n            ax.axis('off')\n        plt.tight_layout()\n        plt.show()\n    \n    # Training the GAN\n    for epoch in range(num_epochs):\n        for i, (real_images, _) in enumerate(dataloader):\n            batch_size = real_images.size(0)\n    \n            # Move real images to device and create labels\n            real_images = real_images.to(device)  # Move real images to device\n            real_labels = torch.ones(batch_size, 1, device=device)  # Move labels to device\n            fake_labels = torch.zeros(batch_size, 1, device=device)\n    \n            # Train Discriminator\n            optimizer_d.zero_grad()\n            outputs = discriminator(real_images)\n            d_loss_real = criterion(outputs, real_labels)\n            d_loss_real.backward(retain_graph=True)\n    \n            z = torch.randn(batch_size, latent_dim, 1, 1, device=device)  # Latent vector on device\n            fake_images = generator(z)\n            outputs = discriminator(fake_images.detach())\n            d_loss_fake = criterion(outputs, fake_labels)\n            d_loss_fake.backward()\n            optimizer_d.step()\n    \n            # Train Generator\n            optimizer_g.zero_grad()\n            outputs = discriminator(fake_images)\n            g_loss = criterion(outputs, real_labels)\n            g_loss.backward()\n            optimizer_g.step()\n    \n        # Print loss values and visualize every epoch\n        print(f'Epoch [{epoch + 1}/{num_epochs}], d_loss: {d_loss_real.item() + d_loss_fake.item()}, g_loss: {g_loss.item()}')\n    \n        # Display generated images every epoch for testing\n        display_generated_images(epoch + 1)\n    \n    # Generate images at the final epoch\n    display_generated_images(num_epochs)torch.utils.data\n\nI would greatly appreciate any insight to help improve the training.",
    "created_utc": "2024-11-04T15:41:03",
    "num_comments": 12,
    "comments": [
        "What about your loss plot (gen vs classifier)?",
        "I don’t have time to check it exactly, but your training routine seems off to me. For example, you shouldn’t need retain_graph, with it in there, you are at least using the real image gradient twice, possibly mixing it with the fake gradient in the process.\n\nTo start out, I would recommend to use three separate, clean, standard training loops. Train the discriminator on real images, then reset the gradients, train it on fake images, reset both gradients again, and then train the generator with a new set of generated data, and reset a final time.\n\nOnce this works, you can move on to optimisation with reusing the generated images etc.",
        "The generator has sigmoid activation at the end which ranges the output between 0 and 1 whereas the original data is normalised between -1 and 1. Change the activation to tanh.",
        "Thanks, I'll get rid of the retain\\_graph. I was actually supposed to base this code off of an existing project, which uses a single training loop rather than 3, but I don't think it's enforced for me to do it that exact way, so I'll try out what you've mentioned here.",
        "You still need to put all three training steps into a single for loop.\n\nBut separating them more cleanly should rule out anything that has do so with usage or retention of the wrong gradients as the cause of your issue.",
        "Thanks a lot for your reply! I rewrote the code from start with a clearer training loop. Epochs happen much faster now (around 30 seconds or less compared to 60 seconds or more previously), but no good results even after 200 epochs or so.  \nIs this a problem with my Generator / Discriminator, or is it my training loop again?  \nI'll post the code in 2 comments since otherwise it is too long for Reddit.\n\n    # Parameters\n    epochs = 1000\n    lr = 0.00001\n    loss_func = nn.BCEWithLogitsLoss()\n    bs = 128\n    latent_dim = 64\n    \n    # Check for GPU\n    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n    \n    # DataLoader\n    dataloader = DataLoader(MNIST('.', download=True, transform=transforms.ToTensor()), shuffle=True, batch_size=bs)\n    \n    # Generator\n    class \n    Generator\n    (\n    nn\n    .\n    Module\n    ):\n        def __init__(self):\n            super().__init__()\n            self.gen = nn.Sequential(\n                nn.ConvTranspose2d(latent_dim, 512, kernel_size=4, stride=1, padding=0),\n                nn.BatchNorm2d(512),\n                nn.ReLU(True),\n                nn.ConvTranspose2d(512, 256, kernel_size=4, stride=2, padding=1),\n                nn.BatchNorm2d(256),\n                nn.ReLU(True),\n                nn.ConvTranspose2d(256, 128, kernel_size=4, stride=2, padding=1),\n                nn.BatchNorm2d(128),\n                nn.ReLU(True),\n                nn.ConvTranspose2d(128, 64, kernel_size=4, stride=2, padding=1),\n                nn.BatchNorm2d(64),\n                nn.ReLU(True),\n                nn.ConvTranspose2d(64, 1, kernel_size=4, stride=2, padding=3),\n                nn.Sigmoid()\n            )\n    \n        def forward(self, noise):\n            return self.gen(noise.view(-1, latent_dim, 1, 1))\n    \n    # Discriminator\n    class \n    Discriminator\n    (\n    nn\n    .\n    Module\n    ):\n        def __init__(self):\n            super().__init__()\n            self.disc = nn.Sequential(\n                nn.Conv2d(1, 16, kernel_size=4, stride=2, padding=1),\n                nn.LeakyReLU(0.2, inplace=True),\n                nn.Conv2d(16, 32, kernel_size=4, stride=2, padding=1),\n                nn.BatchNorm2d(32),\n                nn.LeakyReLU(0.2, inplace=True),\n                nn.Conv2d(32, 64, kernel_size=3, stride=2, padding=1),\n                nn.BatchNorm2d(64),\n                nn.LeakyReLU(0.2, inplace=True),\n                nn.Conv2d(64, 128, kernel_size=3, stride=2, padding=1),\n                nn.BatchNorm2d(128),\n                nn.LeakyReLU(0.2, inplace=True),\n                nn.Conv2d(128, 1, kernel_size=2, stride=1, padding=0)\n            )\n    \n        def forward(self, img):\n            return self.disc(img).view(-1)  # Flatten to a single value per batch",
        "    # Instantiate models and optimizers\n    gen = Generator().to(device)\n    gen_opt = torch.optim.Adam(gen.parameters(), lr=lr)\n    disc = Discriminator().to(device)\n    disc_opt = torch.optim.Adam(disc.parameters(), lr=lr)\n    \n    # Function to generate noise\n    def gen_noise(number):\n        return torch.randn(number, latent_dim, device=device)\n    \n    # Visualization function\n    def visualize_images(images):\n        data = images.detach().cpu()\n        grid = make_grid(data[:16], nrow=4).permute(1, 2, 0)\n        plt.imshow(grid, cmap=\"gray\")\n        plt.axis('off')\n        plt.show()\n    \n    # Loss calculations\n    def calc_gen_loss():\n        noise = gen_noise(bs)\n        fake = gen(noise)\n        pred = disc(fake)\n        return -loss_func(pred, torch.ones_like(pred))\n    \n    def calc_disc_loss(real):\n        noise = gen_noise(bs)\n        fake = gen(noise).detach()\n        disc_fake = disc(fake)\n        disc_real = disc(real)\n        fake_loss = loss_func(disc_fake, torch.zeros_like(disc_fake))\n        real_loss = loss_func(disc_real, torch.ones_like(disc_real))\n        return (fake_loss + real_loss) / 2\n    \n    # Training loop\n    for epoch in tqdm(range(epochs)):\n        for real, _ in dataloader:\n            real = real.to(device)\n    \n            # Update discriminator\n            disc_opt.zero_grad()\n            disc_loss = calc_disc_loss(real)\n            disc_loss.backward()\n            disc_opt.step()\n    \n            # Update generator\n            gen_opt.zero_grad()\n            gen_loss = calc_gen_loss()\n            gen_loss.backward()\n            gen_opt.step()\n    \n        # Output generated images every 5 epochs\n        if (epoch + 1) % 5 == 0:\n            print(f'Epoch [{epoch + 1}/{epochs}]')\n            noise = gen_noise(16)\n            fake_images = gen(noise)\n            visualize_images(fake_images)",
        "You should remove the minus sign in the „return -loss_func(…)“ line in calc_gen_loss.\n\nYou need this inversion if you use Wasserstein loss, but not if you’re making a „standard“ GAN with a binary classifier and labels like you are. Here, it causes the network to maximise the distance between the generator output and real label, which is the opposite of what you want.\n\nApart from that everything looks correct now.\n\nIt’s interesting that it runs faster as well, I wasn’t expecting that at all. retain_graph is pretty heavy on performance, but a 2x Speedup is huge either way!",
        "I really appreciate the analysis!\n\nI removed the minus sign as you suggested, but unfortunately, although epochs are quick, the results are similar to before, where I only get fuzzy images with randomly interspersed white pixels. For example, here is the output at 65 epochs.\n\n[**https://ibb.co/5KX9wFL**](https://ibb.co/5KX9wFL)",
        "Hmm, at this point, the basic training system should be correct.  \nHow do the discriminator and generator loss evolve over time?",
        "I notice that the Discriminator overpowers the Generator. I output their loss and while the Discriminator quickly reach 0 loss, the Generator loss increases every few epochs and is constantly rising instead of reducing.",
        "Then increase the generator learning rate relative to the discriminator learning rate until it's balanced, or train the generator several times for each discriminator training step."
    ]
},
{
    "submission_id": "1gjr4ml",
    "title": "Is tensor flow an appropriate python library to classify HPE movements? ",
    "selftext": "Lately I’ve been developing a project that uses tensorflow and cv2 to get people’s movement on a sport and classify what they’re doing.\n\nThe problem is that i’ve been struggling to make tensorflow identify which movement is done by the athlete on a video, either because the angle’s different from the video, or the models created doesn’t seem’s to even try to classify.\n\nI’m new to the area and living through this kind of problem makes me think if tensorflow is really the most appropriate python lib to identify HPE movement extracted from cv2.\n\nAlso, does the players angle, by being different from the videos separated in their specific folder, affects the model performance?\n\nThank you",
    "created_utc": "2024-11-04T14:23:38",
    "num_comments": 1,
    "comments": [
        "it isnt wrong."
    ]
},
{
    "submission_id": "1gjq0rw",
    "title": "Mangio RVC ",
    "selftext": "I’ve been using Mangio RVC, not a desktop version but a browser via space on HuggingFace so it’s only using CPU and can’t be used for training but only for inference. I’m on MacOS with M2 processor. I have 2 questions. \n\n1. I’m not interested in training my own vocal models but want to use already existing ones. Will using a Windows computer with decent GPU speed the process of swapping the vocals using Mangio RVC ofc a desktop version so it can use GPU? Right now I need to wait around 15-20minutes for it to process a 5min long .WAV file while using Mangio RVC via a space on huggingface. If I was able to get that 5min audio file to be processed in 10seconds with a GPU and for me it’d be well worth the investment considering my workflow (I’m a music producer). If that’s the case what GPU would I need? Already own a Windows computer with Intel i7-11700k 16GM RAM using integrated graphics. \n\n2. Can I use MacOS’ GPU as well for the same purpose or does it have to be a Windows or Linux machine with a graphic card? \n\nI just want know if GPU is being used only for training the models or also can be used to speed things up for inference itself as well or does inference use only CPU for this task. ",
    "created_utc": "2024-11-04T13:37:22",
    "num_comments": 0,
    "comments": []
},
{
    "submission_id": "1gjq0o1",
    "title": "how to get started",
    "selftext": "i want to learn how to code my own machine learning model. how would this work? I'm thinking about making something simple related to investing and trading decisions based on something. maybe like tracking what hedge funds, investment companies, famous people are doing, but idk exactly what and maybe (attempting to) turn this into a business or a model anyone can use.\n\ni have some basic knowledge of coding (java, scratch, HTML learned but I don't remember everything), but not much in python.\n\nalso, I am thinking about getting a m3 MacBook air with 24 gb of ram and 256 gb of storage. would this be enough for me to run a VM or something to host the model? so I don't have to pay extra for apples storage. would it still run fast and well? as of now I cant get a MacBook pro because its too expensive for me.",
    "created_utc": "2024-11-04T13:37:14",
    "num_comments": 4,
    "comments": [
        "I'd start with research. Look at examples of people doing the kinds of things you'd be interested in working on. For example, for stock tracking, you may want to look into coursework or projects on time series forecasting, learning concepts like stationarity etc. Maybe reading research papers could be a good option if you are at a level where you could understand some of the fundamental ones in the field you're looking at. \n\n  \nAs for hardware. Very rarely is training done on a personal computer, especially a laptop. Nearly call companies use clusters to handle training loads, but as an individual who's mostly messing around from a learning perspective, you'd probably want to train on an Nvidia GPU (which would be in a gaming desktop, for example). I have heard that some of the Macbooks now have good ml chips on them, but I'm not sure to what degree that's intended for training vs inference of Apple's own models.",
        "There is a pretty good introductory book available as a pdf online ”hands-on machine learning with scikit-learn, keras, and tensorflow”. Combined with Kaggle notebooks you can come a long way.\nAlso the m3 Mac is plenty powerful for hobby projects, but you need to utilise the conda environment that enables gpu usage for Mac’s",
        "thank you so much! after getting some background info about a specific field, what would be the next steps in making a machine learning model?",
        "Reimplement a common model in a ML framework, likely PyTorch. A common one is the MNIST classification task (which is classifying handwritten numbers). If you've never programmed an ML model, that's probably the place to start."
    ]
},
{
    "submission_id": "1gjpjze",
    "title": "If llms are trained on internet text, why don’t they make grammatical errors and misspellings?",
    "selftext": "",
    "created_utc": "2024-11-04T13:18:02",
    "num_comments": 19,
    "comments": [
        "largely internet has grammatical text .. dont just think of reddit , think of all the articles ,web pages etc..",
        "They do.",
        "(They do sometimes) they mitigate by using high quality sources/texts and with humans reviewing model answers after and during training (RLHF Reinforcement Learning from Human Feedback)\n\nAlso… frontier model are so big they just “know words” and language very well, they are language model after all",
        "They do...",
        "It's very unlikely that too many people will make the exact same grammatical error or misspelling. But you have too many people writing in the exact same correct manner.",
        "I have seen at least ONE - but that's it.",
        "They do if you use the same style of writing in the prompt. I tried one of the earlier Microsoft models talking like a lower educated American high school kid and got a racist tirade back which was a bit of a shock.",
        "Take the word “strawberry”. In internet corpuses, you’ll be able to find every possible misspelling - “stawberry”, “strawbry”, etc. Put them in a statistical distribution. What do you think a language model that produces the most probable word in a sequence is going to return?",
        "They paid attention to the grammar police posts.",
        "Chatgpt makes typos all the time",
        "[deleted]",
        "They do, but also, grammatical rules are pretty easy to encode",
        "This and train data could be preprocessed to remove simpler errors before building the tokenizer.",
        "Also, many models have a late fine-tuning step on whatever they want to be good at (like, say, academic papers) -- and those tend to be more grammatically correct.\n\nOr in the big censored models, the RLHF lobotomizing also makes them favor grammatically correct answers, because those are the examples given during that training phase.\n\nOf course many LLMs are capable of speaking many dialects -- you can ask them to speak like a 3rd grader; or a ghetto thug; or a retarded person; or yoda.    They just default to whatever their fine-tuning at the end preferred.",
        "But the available output tokens include single letters, so the architecture does not prevent misspellings at all.",
        "Lol, that's not how machine learning works",
        "[deleted]",
        "Interesting. I guess it depends on whether you know what you are talking about or not. Most generative AI PRODUCTS do not dump their output directly to users, it has a post production validation and correction. It is easy to encode into the post correction process. But hey you got to make a snarky comment!",
        "But bpe shouldn’t prevent the model from learning typos, since the typo token sequences arent discarded during training, or am i missing something?",
        "Once again this is not how this works. What if the user asks the model to include grammatical mistakes in every response ? The model gives the expected response and the second layer modifies it to make it correct, thus defeating the entire purpose ? \n\nThe billions of parameters of the models are adjusted in the training process in order to get expected output formats. AI does not work with IF and ELSE statements.",
        "It's an abstraction of the underlying data. So as long as the majority of the data has the word spelled correctly, the model will aggregate towards the mean. This allows some room error in the training data and also some tolerance for low quality. It's the sheer quantity of data that makes this possible. \n\nAnother example of this at work is something like the bodybuilding community. Its full of mostly bro science and 'trust me bro', yet the model doesn't generate these responses because they are statements in isolation. When there is a sufficient representation of a pattern across a large amount of data, it will be captured by the model. This means that if a lot of people have the same experience and report this in forums etc, there will likely be sufficient data to establish an abstract representation of that and the model will state it to be true, even if the science is lacking (it's tuned to understand this context and account for it in the response) \n\nIm honestly not sure how it reconciles discrepancy between something like an old wife's tale that has clearly defined patterns in the data, and new emerging science which is to the contrary"
    ]
},
{
    "submission_id": "1gjmyy0",
    "title": "Pre-Trained Models for Animals",
    "selftext": "Hello,\n\nI've developed a model for cats by downloading 8,000 cat images and their annotation files.  Got my [weights.pt](http://weights.pt) file.  Worked well.  It can successfully detects cats.  But did I do it the hard way?  Are there good pre-trained models out there for animals in general?  Where would I look?  I am especially interested in inference of raccoons, cats, deer, rats, skunks, groundhogs or gophers, rabbits, birds of any type, squirrels..\n\nMartin",
    "created_utc": "2024-11-04T11:32:25",
    "num_comments": 6,
    "comments": [
        "https://huggingface.co Is my go-to for pre trained models",
        "INaturalist dataset can be a starting point. Also do not share your pt pth files they execute code. If you want to share use safetensors",
        "Do you have any idea if it is open source?",
        "Yes you can install it with torchvision",
        "Cool. Thank you.",
        "You are welcome"
    ]
},
{
    "submission_id": "1gjlzyt",
    "title": "Lightweight Model Serving",
    "selftext": "The article below explores how one can achieve up to 9 times higher performance in model serving without investing in new hardware. It uses ONNX Runtime and Rust to show significant improvements in performance and deployment efficiency:\n\n[https://martynassubonis.substack.com/p/optimize-for-speed-and-savings-high](https://martynassubonis.substack.com/p/optimize-for-speed-and-savings-high)",
    "created_utc": "2024-11-04T10:52:54",
    "num_comments": 0,
    "comments": []
},
{
    "submission_id": "1gjlk6k",
    "title": "Test metrics became worse after resampling data",
    "selftext": "I am training and comparing multiple classification models on a dataset. The target variable is imbalanced, so I decided to do an oversampling of the training set. However, after we trained out model, the test accuracy came out horrible (<50%) and the accuracy was actl better if we dont resample the data. (the precision and recall is pretty bad too)\n\nCan someone enlighten me on why is that so? I thought that resampling is important to avoid misclassification of minority classes. I understand that it cld be due to introducing of noise during the oversampling and all, but i wasnt expecting the results to be this bad",
    "created_utc": "2024-11-04T10:34:55",
    "num_comments": 1,
    "comments": [
        "https://stats.stackexchange.com/questions/247871/what-is-the-root-cause-of-the-class-imbalance-problem"
    ]
},
{
    "submission_id": "1gjljer",
    "title": "Recreated (Coded) entire transformer architecture and training pipelines from scratch",
    "selftext": "",
    "created_utc": "2024-11-04T10:34:04",
    "num_comments": 0,
    "comments": []
},
{
    "submission_id": "1gjkeu6",
    "title": "Please Help me! I need help ASAP ",
    "selftext": "    from indicnlp.normalize.indic_normalize import IndicNormalizerFactory\n    from indicnlp.tokenize.indic_tokenize import trivial_tokenize\n    \n    # Set the start method to 'spawn' for compatibility with CUDA\n    multiprocessing.set_start_method('spawn', force=True)\n    \n    # Load the model on GPU\n    device = 'cuda' if torch.cuda.is_available() else 'cpu'\n    model = SentenceTransformer('l3cube-pune/indic-sentence-similarity-sbert').to(device)\n    \n    # 2. Preprocessing functions (same as before)\n    def preprocess_english_text(text):\n        cleaned_text = ''.join(c for c in text.lower() if c.isalnum() or c.isspace())\n        return cleaned_text\n    \n    def preprocess_indic_text(text, lang):\n        factory = IndicNormalizerFactory()\n        normalizer = factory.get_normalizer(lang)\n        normalized_text = normalizer.normalize(text)\n        cleaned_text = ''.join(c for c in normalized_text if c.isalnum() or c.isspace())\n        tokens = trivial_tokenize(cleaned_text, lang)\n        return \" \".join(tokens)\n    \n    def split_sentences(text):\n        sentences = re.split(r\"(?<!\\w\\.\\w.)(?<![A-Z][a-z]\\.)(?<=\\.|\\?)\\s\", text)\n        return [s.strip() for s in sentences if s.strip()] \n    \n    # 3. Core Alignment Function (adapted for GPU use)\n    def align_sentences_in_paragraphs(params):\n        para1, para2, lang1, lang2, threshold = params\n    \n        if not para1 or not para2:\n            return []\n    \n        para1 = preprocess_english_text(para1) if lang1 == 'en' else preprocess_indic_text(para1, lang1)\n        para2 = preprocess_english_text(para2) if lang2 == 'en' else preprocess_indic_text(para2, lang2)\n    \n        sentences1 = split_sentences(para1)\n        sentences2 = split_sentences(para2)\n    \n        if not sentences1 or not sentences2:\n            return []\n    \n        if len(sentences1) < 2 or len(sentences2) < 2:\n            return [(para1, para2)] if util.cos_sim(model.encode(para1, device=device), model.encode(para2, device=device)) >= threshold else []\n    \n        embeddings1 = model.encode(sentences1, device=device)\n        embeddings2 = model.encode(sentences2, device=device)\n    \n        similarity_matrix = util.cos_sim(embeddings1, embeddings2)\n    \n        aligned_pairs = []\n        used_indices1 = set()\n        used_indices2 = set()\n    \n        for i in range(len(sentences1)):\n            best_j = np.argmax(similarity_matrix[i].cpu().numpy())\n            if similarity_matrix[i][best_j] >= threshold:\n                if i not in used_indices1 and best_j not in used_indices2:\n                    aligned_pairs.append((sentences1[i], sentences2[best_j]))\n                    used_indices1.add(i)\n                    used_indices2.add(best_j)\n    \n        return aligned_pairs\n    \n    # 4. Function to create the dataset\n    def create_sentence_aligned_dataset(paragraph_data, langs, threshold=0.8, num_processes=multiprocessing.cpu_count()):\n        num_langs = len(langs)\n        sentence_aligned_data = {}\n    \n        for i in range(num_langs):\n            for j in range(num_langs):\n                if i != j:\n                    lang1, lang2 = langs[i], langs[j]\n                    sentence_aligned_data[(lang1, lang2)] = []\n    \n        all_args = []\n        for para_group in paragraph_data:\n            for i in range(num_langs):\n                for j in range(num_langs):\n                    if i != j:\n                        lang1, lang2 = langs[i], langs[j]\n                        try:\n                            para1 = para_group.get(lang1, \"\")\n                            para2 = para_group.get(lang2, \"\")\n                            all_args.append((para1, para2, lang1, lang2, threshold))\n                        except Exception as e:\n                            print(f\"Error processing paragraph group: {e}\")\n    \n        with multiprocessing.Pool(processes=num_processes) as pool:\n            results = pool.map(align_sentences_in_paragraphs, all_args)\n    \n        k = 0\n        for i in range(num_langs):\n            for j in range(num_langs):\n                if i != j:\n                    lang1, lang2 = langs[i], langs[j]\n                    num_groups = len(paragraph_data)\n    \n                    sentence_aligned_data[(lang1, lang2)] = [\n                        result\n                        for result in results[k:k+num_groups]\n                    ]\n    \n                    k += num_groups\n    \n        return sentence_aligned_data\n    \n    # Example usage:\n    langs = ['en', 'kn', 'ml', 'ta', 'te']\n    paragraph_data = []\n    for _, row in paragraph_parallel.iterrows():\n        paragraph_data.append(dict(zip(langs, row.values.tolist())))\n    \n    sentence_aligned_data = create_sentence_aligned_dataset(paragraph_data, langs)\n    \n    for lang_pair, pairs in sentence_aligned_data.items():\n        print(f\"Language Pair: {lang_pair}\")\n        for pair in pairs:\n            print(pair)\n\n* I have been trying to run this for the past day and I can't seem to figure it out .\n* Some context ,  it's my first time dealing with transformer models. I know it's pretty dumb to even try to use one without any background knowledge on it but our university has weird demands, more on that later .\n* I didn't set the gpu for the first few runs where I ran this for 30 mins , 45 mins and 1 Hr without any output before I stopped . \n* I then set the Gpu to T4 in colab (Which I exhausted the limit of now) and ran it for 3Hr 50Min before my internet disrupted for 2 mins and yk the rest. \n* Now ,  I don't have GPU on colab and I can't seem to figure out if there's something wrong with the code or it just takes that long .\n* Additional Info : It shows (line 120)-> pool->map()->wait()->wait()->wait() in the bottom of colab . Which when i searched the internet is supposed to be normal for embeddings , can anyone confirm this.\n\nThe task I am trying to do is create sentence similarity embeddings using a fine tuned SBert model. Now these are the following options left\n\n* Somehow try to access colab GPU again OR\n* Ask for permission to use the local computers on my university campus . Heard they have decent GPU capcity , atleast 32 GBs. \n* OR if anyone can suggest changes to code due to which it might've been hindering the process.\n\n  \nPlease help me with this , I'd be very grateful since I have the deadline coming up soon. Doing this on my notebook laptop is definitely not feasible (afaik) . Thank you for taking your time to read this , if you need anymore info , I can reply in the comments",
    "created_utc": "2024-11-04T09:48:52",
    "num_comments": 5,
    "comments": [
        "You could debug it by using a smaller dataset initially to ensure that ur code does indeed work. Large datasets can be very taxing on resources and increase runtimes. With a smaller dataset, it could speed up the process initially so you can validate ur code works. Whether you use university computers or purchase more colab credits, you may just be wasting ur time and money when you can run a subset of data first to test for bugs/errors. Once you are confident in your code, then you can scale up.",
        "That's a really viable approach. Thank you very much! \n\nCurrently I am dealing with 2800 ish rows where every cell of let's say the csv table is a paragraph made of 3-4 sentences. So should i go for like 100 rows for starters? or even less",
        "100 is a good starting point. You can raise or lower the rows from there based on the performance, but 100 is definitely more manageable for the initial run than the full 2800.",
        "Yes, I will go with that. Thank you for taking time to help me out with this kind sir",
        "Of course! Good luck with your project 🙂"
    ]
},
{
    "submission_id": "1gjjp64",
    "title": "Why is ReLu(x)^2 such a bad activation funciton?",
    "selftext": "Given its similarity to the normal ReLU and its variants in that it outputs zero for negative numbers, I expected it to have a similar performance, maybe slightly worse but it seems like it's really bad. I tried to train a network with ReLU(x)^2 activation function on MNIST and it doesn't seem to learn anything, just gets stuck around 10% accuracy. What property does it have that makes it so bad?",
    "created_utc": "2024-11-04T09:20:31",
    "num_comments": 5,
    "comments": [
        "There are paper stating it's pretty good, actually.\nMaybe the other features of your architecture and training recipes are overoptimized for ReLU instead of ReLU².\nCheck normalizations, for example.\nIf the activations are big, ReLU² is going to f...mess up your model, because its y=x² is supralinear, going to Inf for x approaching infinity; while ReLU, GeLU, SiLU are linear in that regime and y=x.\nIf you stay in [-2,2] ReLU² might be quite cool actually",
        "Exploding gradients would be my first guess",
        "The lower relu get, the smaller relu^2 gets. Since relu^2 is an exponent, it'll quickly vanish compared to just relu",
        "You might have a value like 10 and it squares it into a 100, and then the next layer into a 10000, and so on, so you get giant values and giant gradients. That's why norm layers helped",
        "You're right, I applied layer normalization and it's now learning with good accuracy... makes sense"
    ]
},
{
    "submission_id": "1gjjnjc",
    "title": "How Outsourcing Data Annotation Services Can Supercharge Your AI Model",
    "selftext": "Hi, everyone \n\nI recently read a blog discussing how outsourcing data annotation can significantly enhance the performance of AI models. It covers benefits like: \n\n* Access to specialized expertise \n\n* Cost-effectiveness \n\n* Faster turnaround times \n\nIf you're working on AI projects, this could be a game changer! Check it out here: [How Outsourcing Data Annotation Services Can Supercharge Your AI Model](https://www.damcogroup.com/blogs/how-outsourcing-data-annotation-services-can-supercharge-your-ai-model) \n\nWhat are your thoughts on outsourcing data-related tasks? Would love to hear your experiences. ",
    "created_utc": "2024-11-04T09:18:40",
    "num_comments": 0,
    "comments": []
},
{
    "submission_id": "1gjj6ka",
    "title": "Help unable to find accurate ASL datasets",
    "selftext": "Hello I’m an engineering student working on a project based on machine learning using CNN for processing ASL or American Sign Language  recognition any help where I can find the accurate ones , the ones on kaggle are all modified like some letters like P what do I do",
    "created_utc": "2024-11-04T08:59:50",
    "num_comments": 0,
    "comments": []
},
{
    "submission_id": "1gjiubs",
    "title": "Learning from deep learning to TNN",
    "selftext": "Hi, I am a mathematics major with a background in statistics. I also know a bit of basic machine learning models like linear regression, decision trees, and RFM. Now, I want to start studying about deep learning, with the final goal of learning TNN for machine translation because I am really fascinated by NLP stuff and computational linguistics. So, how do I get started? Can you please recommend beginner-friendly resources (because it gets a little bit overwhelming)?\n\nFor context, I only learned about machine learning in uni and from a local online course. I have not really explored free online courses yet as I get overwhelmed. Thanks a bunch!",
    "created_utc": "2024-11-04T08:46:14",
    "num_comments": 1,
    "comments": [
        "I think coursera courses are good start.\nThen you can refer some books\nNNandDL by michael nielsen\nDL by Ian goodfellow \nNlp with transformers \nHands on ml\nOrelly foundation books are good"
    ]
},
{
    "submission_id": "1gjiotk",
    "title": "🚀 Channel Growth Update: Supervised Classification Recap & Regression Kickoff!🚨",
    "selftext": "🚨Links:\nhttps://youtube.com/playlist?list=PLCMYmx4y_qBL8fZkiP67PKLRBTh-sRJ4S&si=eENkDMGGYSNJ0Qh2\n\nNaive Bayes : https://youtu.be/fNRswUlLx4g?si=mN1_d5rASimbE34s\nDecision Trees : https://youtu.be/oGXbD36YM0s?si=zcB8WJjNYh1c7TSd\nSupport Vector Machines : https://youtu.be/3yEkquDCRHA?si=yq5In0rfUAQMtXJk\nK-Nearest Neighbors : https://youtu.be/bzwU8B41EV4?si=NxyMgUj4euArtfJL\nLogistic Regression : https://youtu.be/bzwU8B41EV4?si=NxyMgUj4euArtfJL\n\nIn this video, we’re celebrating the amazing growth of our channel! 🎉 With 25+ subscribers and 500+ views, thank you for being part of this journey!\n\nHere’s what we’ll cover:\n\n📈 Channel Growth: A quick look at our milestones!\n🔄 Recap of Supervised Classification: We’ll revisit key concepts, including:\nNaive Bayes\nDecision Trees\nSupport Vector Machines\nK-Nearest Neighbors\nLogistic Regression\n🚀 Kickoff Regression Analysis: We’ll introduce regression, discussing its significance in data science and what you can expect in future videos.\nJoin me as we reflect on our progress and step into the fascinating world of regression. Don’t forget to share your thoughts in the comments below! \n\nLet’s keep learning together!",
    "created_utc": "2024-11-04T08:40:07",
    "num_comments": 0,
    "comments": []
},
{
    "submission_id": "1gjhzjm",
    "title": "About PINN's and normalization.",
    "selftext": "I'm using physics informed neural networks to solve a differential equation in which the solution is a probability distribution, should I normalize my output? and if so what are some best practices around that? \n\nSome notebooks I found are about heat equation, an the solution to the heat and diffusion equation are treated as distributions sometimes, but I haven't seen anyone using output normalization. But at the same time it seems like you should.\n\nPS: Just to add more context, I'm using tanh as activation function, and I'm trying to solve the Edwards equation, that describes polymer chain conformation under an external field.",
    "created_utc": "2024-11-04T08:11:11",
    "num_comments": 4,
    "comments": [
        "I think this can be a complicated issue, and normalizing the output probably isn't the best solution. A better solution involves using a model that preserves the properties of your input that you care about. In this case you want a model for which, if the input vector has coordinates that are positive and which sum to 1, then the same should be true of the output. \n\nIf what you're doing is modeling the vector field of the DE using your model, then one option is to include additional terms in the model such that the vector field is always orthogonal to the gradient of the sum of the vector components (so that the sum does not change) and also orthogonal to some approximation for the gradient of the sign of each coordinate.\n\nThat's a brute force approach just off the top of my head and it may not be a good idea, but hopefully it gives you a sense of what better options than normalization look like.",
        "I'm trying to solve the Edwards equation, most of the times it finds either a negative solution, or something that doesn't add up to 1. My sampling is normalized and everything about the PDE implementation seems to be correct, that's why I started looking for something related to the neural network itself. I'm using tanh as activation function since most papers on PINN's seem to use that. But I don't know what to anymore.",
        "I see, I think normalizing the solution is a good place to start then. You can just have the final output of the neural network be a softmax layer; that'll give you a probability distribution.\n\nFYI I don't know what the edwards equation is and i couldnt find it via google search, so if you ask for help from folks you should include a lot more detail about what, exactly, you're trying to do.",
        "Ok, Thanks, I was wondering if putting a softmax in the output layer was a good idea. The edwards equation is a PDE that gives the probability of finding the monomer of a polymer based on the potential that affects the polymer, roughly speaking."
    ]
},
{
    "submission_id": "1gjh2bx",
    "title": "Best ML / NLP Certifications",
    "selftext": "Hey there, I have just started a part time positiv at my local University in the field als NLP and knowledge graphs. \n\nAs I will be having some time to learn new skills in the next months I am looking for some certifications that I could do. \nWhat are the best beginner / intermediate certifications I could do? I‘m fine paying them myself as long as they are helpful and respected within the field. \n\nThanks in advance :) ",
    "created_utc": "2024-11-04T07:33:40",
    "num_comments": 2,
    "comments": [
        "mit",
        "From what I know there are none that are \"recognized\""
    ]
},
{
    "submission_id": "1gjgf8m",
    "title": "Text classification: N-shot prompt classification VS training a linear classifier on top of an embedder",
    "selftext": "I have the task of making a email classifier.\n\nI have 200 emails for each category, and 5 categories (thus 1000 examples).  \n  \nWhich approach will yield the best results, in general?  \n  \n\\- Classifying emails with n-shot prompt classification, possibly with LoRA finetuning  \n\\- Using a pre-trained embedder, e.g. OpenAI's \\`text embedding 3 large\\`, and training a linear classifier on top of the embedder\n\n",
    "created_utc": "2024-11-04T07:06:17",
    "num_comments": 1,
    "comments": [
        "There is no \"in general\" in ML. You should pick the classifier which works best for your task which implies that you should try a variety of techniques.   \n  \nStart by setting up a baseline with a simple model and solution and try to improve on it. Once you have a functioning baseline you can move on to trying more sophisticated techniques."
    ]
},
{
    "submission_id": "1gjfl9f",
    "title": "Help on music generation project",
    "selftext": "Hi everyone,\n\nI want to create a model for unconditional generation of music coming from a very specific niche genre (~10 hours of data). Should I go for finetuning an existing diffusion/flow matching model like Flux on mel spectrograms or would you recommend sth else?\n\nThank you!",
    "created_utc": "2024-11-04T06:30:57",
    "num_comments": 0,
    "comments": []
},
{
    "submission_id": "1gjf4xx",
    "title": "Advice - GANs Capstone Project ",
    "selftext": "I'm a CS undergrad in my 4th year and am doing a capstone project on GANs. I was hoping someone with experience could tell me if my project goal is viable and maybe point me in the right direction to start.\n\nThe projects goal is basically to implement a GAN that can create floorplans for architects. As an undergrad the requirement for novelty in the project is pretty small, I was hoping I could basically take a GAN model that has been made by other research papers and then attempt to improve it by augmenting the dataset(the dataset I'm looking at right now is called RPLAN)\n\nI have no experience with Machine Learning but was hoping to use this project as an opportunity to learn about them. I need a basic prototype by December and then a full version by May. Admittedly I've been very intimidated by what I've seen so far. \n\nSome of the other papers I've seen use the pix2pix model for this. Could I take an implementation of one of these models and change the loss function and retrain it on my data?  Would this have some big hardware requirements or can I use a cloud service? \n\nThanks for any advice, sorry if I've left out any details you'd need",
    "created_utc": "2024-11-04T06:11:00",
    "num_comments": 4,
    "comments": [
        "\n1. drop GANS in favor of diffusion models and finetune a LORA model on the RPLAN Dataset any existing sota model would do sd 3 3.5 , flux \n\n2. additionally use an LLM which improvises the prompt that a user would give to generate a house plan \n\n3. Try making a Gradio App for the demo",
        "you can use any cloud service, a few that i know of Jarvislabs.ai or primeintellect.ai both are good",
        "thank you so much, I'll look into diffusion models. Do you recommend this because diffusion models are easier for beginners or would it be better for this type of generation ?",
        "1. Correct, they are easier to get started  , but all Generative Models struggle with text   \n2. They have well supported and very fast ecosystem , take a look at [https://huggingface.co/docs/diffusers/](https://huggingface.co/docs/diffusers/) for getting started   \n3. For more of a general intuition behind the models , take a look at [https://www.youtube.com/watch?v=ZBKpAp\\_6TGI&t=802s&ab\\_channel=UmarJamil](https://www.youtube.com/watch?v=ZBKpAp_6TGI&t=802s&ab_channel=UmarJamil) , codes the entire model from scratch , you would start getting into things."
    ]
},
{
    "submission_id": "1gjcrfk",
    "title": "What category is this? What's this profession even call?",
    "selftext": "",
    "created_utc": "2024-11-04T04:12:45",
    "num_comments": 21,
    "comments": [
        "It is literally in the title and then explained in the description perfectly.",
        "Big data meets ML. You'll have the chance to get bald pulling your hair while debugging Dask code.",
        "The category is in the job title: “Distributed Systems”. If you listen to enough practitioner focused podcasts, you’ll hear this skill set mentioned quite often.",
        "ml ops?",
        "The fact this is for ML honestly isn’t that important to the description I think. But tons of Software Engineers who work on Distributed Systems and Infra at big tech companies or companies like Snowflake/Databricks would be a good fit.",
        "This is what I do.\n\nIt is a combination of MLOps, Data Eng, and AI Eng.",
        "SRE MLOps",
        "I think it is a niche skillset and it makes sense to separate it from distributed computing in general there is a lot more specialized focus on optimization as ml models ingest much larger datasets and models use a lot more compute so performance and scalability are more important even though in thw general case this can be more or less the same level of importance.",
        "sysadmin",
        "The role is getting more and more matured, however there is a lack of good nomenclature. We call it ML Engineer itself.",
        "idk ?",
        "😂 😂 😂 Nvm",
        "Hi could you tell me some good podcasts to listen to ? Thank you in advance",
        "[deleted]",
        "Interesting",
        "Expected salary and required years of experience?",
        "😭",
        "Latent Space, The Gradient, Machine Learning Street Talk, Practical AI…",
        "It does!",
        "Are you trolling? That is the entire role. What else would MLOps engineers do apart from building, optimizing, and maintaining systems for use with ML models?",
        "Salary is like 150 to 200+\n\nThey typically ask for 5+ years of experience, but not necessarily all AI related",
        "Thank you"
    ]
},
{
    "submission_id": "1gjcczr",
    "title": "Seeking Linear Regression Project Ideas with Real-Time Data Updates",
    "selftext": "",
    "created_utc": "2024-11-04T03:49:51",
    "num_comments": 1,
    "comments": [
        "See if you find something related: https://drive.google.com/drive/folders/1NpXcBCAF8_JB0JqoaV5ARe_gu8rnD2Vg"
    ]
},
{
    "submission_id": "1gjbuiz",
    "title": "Feature selection with OLS vs other models",
    "selftext": "OLS is quick, so it is easy to see which set of features is potentially optimal (minimal f-statistic, AIC, BIC?). Will this set of features be good for other models, XGBoost, Lasso, etc? Or should feature selection be done for each model independently?",
    "created_utc": "2024-11-04T03:17:35",
    "num_comments": 0,
    "comments": []
},
{
    "submission_id": "1gjbk9r",
    "title": "Analytics Vidhya Agentic AI",
    "selftext": "Has anyone taken the Agentic AI course by Analytics Vidhya? I've been working on building RAG pipelines and fine-tuning LLMs at my current job, but the course curriculum caught my attention. It covers building AI agents using tools like LangGraph, AutoGen, and CrewAI, which seems pretty interesting.\n\nBefore I commit (the course costs 40k INR), I'd love to hear your thoughts—do you think it's worth it?\n",
    "created_utc": "2024-11-04T02:58:51",
    "num_comments": 2,
    "comments": [
        "I haven’t had a chance to look at this particular thing, but going by the content they have posted in the past, my guess is that it would be low quality. Better take some courses from deep learning.ai",
        "Thanks, will look into a few agentic ai courses by deeplearning .ai"
    ]
},
{
    "submission_id": "1gjasl2",
    "title": "Best AI/ML Masters Program for a Physician?",
    "selftext": "I am an MBBS (Bachelor's Degree in Medicine and Surgery) graduate from India. I want to learn AIML and build my career at the intersection of AI and Medicine. \nQ1. Courses to pursue (Both India And Abroad)?\nQ2. Roles to pursue?\nQ3. Companies that employ such people?\n\nNeed roadmap and guidance.",
    "created_utc": "2024-11-04T02:02:30",
    "num_comments": 8,
    "comments": [
        "Always check the curriculum and go for the course which offers the most statistics (like probability theory, mathematical statistics, regression analysis, multivariate statistics, bayesian methods, time series analysis, stochastic processes etc. etc. Don’t be misled by fancy titles like NLP etc. – these ones you easily pick up from MOOC platforms, but the hard mathematical/statistical theory you would hardly pick up from the internet (even technically it is possible).",
        "Uni of Freiburg has a bioinformatics group that is pretty good. You can check the content and the courses on their website.",
        "[deleted]",
        "Get in touch with Tavpritesh from the IIIT Delhi. Fantastic scientist, amazing human being. He is doing what you aspire to do. He was my professor once. If you wanna learn, why not learn from the best. Don't get me worng. I am not saying you wanna be a professor. He is much more than just a professor though. I don't wanna make it cheesy but he might just nudge you in the best direction you need. All the best mate. 👍",
        "I have been working on a list of [AI courses, books and tutorials](https://github.com/duncantmiller/ai-developer-resources) that might be helpful. Most of them are free so they could be a good way for you to test the waters and see what is interesting before jumping into a full program.",
        "My opinion: Find a good Masters program in AI/ML. Get in. Thats the way forward. It'll open doors you never knew even existed.",
        "Master of Science in Applied Computing (MScAC) at the University of Toronto, Canada. It's definitely the best ML masters program in Canada, and likely one of the best in the world.\n\nThere's an AI in Healthcare concentration that does pretty much exactly what you're describing. The kinds of companies that hire from there are varied but think of companies like Roche, and Hospital research groups.",
        "This article on the AI server company Gigabyte's website should give you some direction. It's no replacement for academic courses but it outlines the main fields of study in AI and medicine, so you can find a field that speaks to you and do more digging in that direction: https://www.gigabyte.com/Article/how-to-benefit-from-ai-in-the-healthcare-medical-industry?lan=en",
        "Java and Python intermediate level \n\nMathematics high school level"
    ]
},
{
    "submission_id": "1gjao1g",
    "title": "Does anybody knows a repository or python package that calculates ROUGE scores in multilingual setting?",
    "selftext": "Especially targeting low resource languages from Indian subcontinent",
    "created_utc": "2024-11-04T01:53:23",
    "num_comments": 0,
    "comments": []
},
{
    "submission_id": "1gjam7s",
    "title": "2025 Artificial Intelligence Internship & New Grad Positions (not made by me)",
    "selftext": "",
    "created_utc": "2024-11-04T01:49:23",
    "num_comments": 0,
    "comments": []
},
{
    "submission_id": "1gja1l8",
    "title": "How do smaller weights help neural networks generalize better?",
    "selftext": "I'm having a hard time making the connection between smaller weights and a neural network that performs better on test data. More specifically how exactly do smaller weights help a Neural Network ignore \"outlier\" observations in the feature/label space? Any explanation breakdown is appreciated. ",
    "created_utc": "2024-11-04T01:03:34",
    "num_comments": 1,
    "comments": [
        "In simple terms, large weights lead to higher variance (small changes in inputs changes outputs by a lot). With smaller weights, this problem is realized less and you do not end up changing your outputs a lot by making small changes in inputs. This gets a better generalization."
    ]
},
{
    "submission_id": "1gj9jgm",
    "title": "Last Month In AI | AIGuys Newsletter",
    "selftext": "🔍 **Inside this Issue:**\n\n* 🤖 ***Latest Breakthroughs:*** This month it’s all about **Scaling RAGs for Production, The Prompt Report, and LLM's black box nature.**\n* 🌐 ***AI Monthly News:*** Discover how these stories revolutionize industries and impact everyday life: **AI Scientists winning the Noble Prize in Chemistry and Physics, OpenAI challenges Google Search and Big Tech makes big money.**\n* 📚 ***Editor’s Special:*** This covers the interesting talks, lectures, and articles we came across recently.\n\n>**Check our Publication:** [**https://medium.com/aiguys**](https://medium.com/aiguys)\n\n# Latest Breakthroughs\n\nThis article covers different issues with creating a production-grade RAG system, understanding the deterministic nature of processes, and delving deep into the advanced RAG components. We will cover everything from reranker to repacking, from query classification to query expansion and many more such techniques that form the backbone of a modern RAG system.\n\n[**Why Scaling RAGs For Production Is So Hard?**](https://medium.com/aiguys/why-scaling-rags-for-production-is-so-hard-a2f540785e97?sk=4df6d89d82428917c7c0a350c4e7a3d1)\n\nDon’t worry I’m not going to give you a list of the top 50 prompts to try, anyways that just doesn’t work at scale. We are here going to talk about different prompting techniques.\n\n**The Six Major Prompting Categories**\n\nWithin the 58 categories, there are 6 top-level categories.\n\n1. Zero-Shot\n2. Few-Shot\n3. Thought Generation\n4. Decomposition\n5. Ensembling\n6. Self-Criticism\n\n[**The Prompt Report: Prompt Engineering Techniques**](https://medium.com/aiguys/the-prompt-report-prompt-engineering-techniques-254464b0b32b?sk=7d86e4ddfda990628c809c68983859fc)\n\nA brand new paper from Google and Apple, where they looked into the internal LLMs to understand the nature of hallucinations. They showed that internal representations can also be used for predicting the types of errors the model is likely to make, facilitating the development of tailored mitigation strategies.\n\nThey also reveal a discrepancy between LLMs’ internal encoding and external behavior: they may encode the correct answer, yet consistently generate an incorrect one. Taken together, these insights deepen our understanding of LLM errors from the model’s internal perspective, which can guide future research on enhancing error analysis and mitigation.\n\n[**LLMs Know More Than They Show**](https://medium.com/aiguys/llms-know-more-than-they-show-3923c772a6bf?sk=697f8848f249c293c798aa1fc49e1adb)\n\nApple says: ***“we found no evidence of formal reasoning in language models …. Their behavior is better explained by sophisticated pattern matching — so fragile, in fact, that changing names can alter results by \\~10%!”***\n\n[**Apple Says LLMs Are Really Not That Smart**](https://medium.com/aiguys/apple-says-llms-are-really-not-that-smart-6912db2a227d?sk=1fb037dbe11db02987d61ddd4ef5a0ff)\n\n# AI Monthly News\n\n# Computer Scientists Wins Noble In Both Physics and Chemistry.\n\nThis year’s two Nobel Laureates in Physics have used tools from physics to develop methods that are the foundation of today’s powerful machine learning. John Hopfield created an associative memory that can store and reconstruct images and other types of patterns in data. Geoffrey Hinton invented a method that can autonomously find properties in data, and so perform tasks such as identifying specific elements in pictures.\n\n**Press release:** [**Click here**](https://www.nobelprize.org/prizes/physics/2024/press-release/)\n\nThe Nobel Prize in Chemistry 2024 is about pro­teins, life’s ingenious chemical tools. David Baker has succeeded with the almost impossible feat of building entirely new kinds of proteins. Demis Hassabis and John Jumper have developed an AI model to solve a 50-year-old problem: predicting proteins’ complex structures. These discoveries hold enormous potential.\n\n**Press release:** [**Click here**](https://www.nobelprize.org/prizes/chemistry/2024/press-release/)\n\n# OpenAI Challenges Google’s Search Monopoly\n\nOpenAI has introduced a search capability within ChatGPT, enabling real-time web browsing to provide up-to-date information. This feature positions ChatGPT as a direct competitor to traditional search engines like Google.\n\n**News Article:** [**Click here**](https://www.wired.com/story/chatgpt-ai-search-update-openai/)\n\n# Big Tech Makes Big Money\n\n**Elon Musk’s xAI Seeks $40 Billion Valuation**: Elon Musk’s AI startup, xAI, is in talks to raise funding at a valuation of $40 billion, up from $24 billion five months prior. The company is developing an AI chatbot named Grok, available on Musk’s social media platform X.\n\n**News Article:** [**Click here**](https://nypost.com/2024/10/30/business/elon-musks-xai-in-talks-to-raise-funding-at-40b-valuation/)\n\n**Both Microsoft’s and Google’s AI-driven investment leads to a profit surge:**\n\nMicrosoft’s substantial investments in AI have resulted in a 16% increase in quarterly sales, reaching $65.6 billion. The Azure cloud computing division saw a 33% revenue rise, highlighting the impact of AI on business processes.\n\nGoogle’s parent company, reported a 34% increase in profit, earning $26.3 billion in the July-September quarter. This growth is attributed to AI investments and a 15% revenue surge to $88.27 billion\n\n**News Article:** [**Click here**](https://www.thetimes.com/business-money/technology/article/microsofts-big-bet-on-ai-reaps-reward-3cvkxpkmc?region=global)\n\n**News Article:** [**Click here**](https://apnews.com/article/google-alphabet-earnings-artificial-intelligence-antitrust-30a75937bfbd9a4dfcee91cd4594cd59)\n\n# Editor’s Special\n\n* The Elegant Math Behind Machine Learning [**Click here**](https://www.youtube.com/watch?v=URtF_UHYBSo)\n* AI RISING: Risk vs Reward — The Hinton Lectures™: [**Click here**](https://www.youtube.com/watch?v=RXjLGn14Jo4)\n* A fireside chat with Sam Altman OpenAI CEO at Harvard University: [**Click here**](https://www.youtube.com/watch?v=FVRHTWWEIz4)\n* NVIDIA’s New Ray Tracing Tech Should Be Impossible!: [**Click here**](https://www.youtube.com/watch?v=UwL-4LOhxx8)\n\n",
    "created_utc": "2024-11-04T00:23:11",
    "num_comments": 0,
    "comments": []
},
{
    "submission_id": "1gj7zm5",
    "title": "Should I buy intel's i9/i7 or Ryzen or M3 pro powering CPU's Laptops for machine learning and Web Dev?",
    "selftext": "I am an undergraduate student in Computer Engineering, I am learning AI/ML, Cloud Computing and I do web development too. I do open alot of tabs in chromes and I currently use one external monitor but I am planning to get other too and I want a laptop with great cpu say 14th gen.  \nBut the major dilemma I am getting is to choose between i9 14th gen or i7 14th gen or m3 pro 11 core CPU, I just found out alot of issues like crashing that are booming regarding i9 chipset is it already fixed by updates or i should not consider that. And what about Ryzen series is it better or should i go for mac or intel.  \nI appreciate your answer, Thank you!",
    "created_utc": "2024-11-03T22:24:40",
    "num_comments": 7,
    "comments": [
        "Machine learning will never be feasable in laptops, you can develop it yes, but running an llm or tuning is lost cause. For developing an okay ish cpu is enough, for everything else cloud.",
        "The new m4 pro should last you for a while.",
        "M3 pro",
        "Get a Macbook Air. Web dev you can comfortably do locally and any intense ML projects, you can use free cloud based sources to train your models. Doing everything locally is just silly.",
        "web dev will work in any laptops you choose.\n\nTo study ml, even i7 or equivalent is enough.\nBut for training big ones of dl programs like llm, any of your choices would stand a chance. \nSo go for anything you wish for, learn ML, then it's time to figure out what to do next.\n\nAlso go for 16GB ram and 8GB vram configs, if for ml",
        "buy a second hand laptop any relatively new generation for cheap get extra ram, and SSD, use linux and after a year or so if you still need computing power build your system",
        "windows is garbage. go with mac"
    ]
},
{
    "submission_id": "1gj7kv9",
    "title": "How to do this ML project as a beginner?",
    "selftext": "Im in my final year of college and my group has decided to make an ML project on ‘pneumonia detection from Xray’(our guide said its mandatory to make a ML project) .But no one in the group has any idea on where to start and what to do?\n\nCan someone pls tell how to start learning and how to implement this. We have to show the module divisions within two days. It’d be great if someone could help!",
    "created_utc": "2024-11-03T21:56:18",
    "num_comments": 16,
    "comments": [
        "I think the best would be if u search for some papers and  try to rebuild it.\nAlso check on kaggle, maybe there are some similar datasets and notebooks",
        "When choosing a project, make sure you choose the spec alongside the datasets you have access to. Do you have xray photos labeled y/n with pneumonia? If yes, then a deep cnn architecture with batch norms",
        "Chat gpt taught me how to build an ml model using pretty cheap cloud computing in about a week, for a specific project.  I'm not fluent in tensorflow, but I have built the end to end process which is pretty cool",
        "did your professor not give you resources for this?",
        "Yeah, *”A Deep Learning Method for Pneumonia Detection Based on Fuzzy Non-Maximum Suppression.”* is the paper my team has chosen. I have to read and understand it.",
        "Yes, its marked with 1 and 0s. Can u pls tell me what do i learn first inorder to make this",
        "Damn. You must be a genuis! I cant imagine learning these stuff within a week.",
        "It's really great! So can you share the prompts you used for chatgpt.",
        "Well, no. He has no idea about this. Our faculty quality isnt good. He asked us to manage it on our own",
        "Look up basics of image classification with deep CNN architectures with residual connections.",
        "I recommend trying. Gpt is excellent at helping bridge the gaps and producing code. It's not without flaw but totally awesome still",
        "it's not about 'prompts' just ask it questions. create a bot with a background:\n\n\"This is the project we will be working on.  Assume i'm a novice to ML, python but experienced with NT8 and some C#.\n\nWorkflow Summary:\n\nTick Data Collection: Extract tick data (with fields like instrument, result, date) in NinjaTrader 8 (NT8) and save to a CSV file locally.\n\nData Visualization: Convert CSV data into visual time series images (at intervals like 1m, 5m) for pattern analysis.\n\nTraining and Comparison: Use visualized data to train a machine learning model (CNN) to recognize patterns, compare new data to trained models, and generate trading signals based on pattern matching.\n\nUse multiprocessing with Pool and imap\\_unordered for CPU pooling. This technique creates a pool of worker processes (using all available CPU cores) and distributes tasks across them. With imap\\_unordered, each process returns results as they complete, rather than sequentially, improving throughput. Add a tqdm progress bar to monitor task completion dynamically, ensuring a balance of high CPU efficiency and progress tracking.\"\n\nIt does really well at shorthand, sometimes i just paste errors to be addressed.  If you use too much memory, it will get circular and senile, which is when you ask for a summary and open a new chat\n\nI have an rtx 3080 which didnt cut it, so i used [tensordock.com](http://tensordock.com) to offload CPU to 20 cores and 3x gpu which makes all the difference here",
        "lol thats tough.\n\ni cant even tell you what people would use nowadays but i would just use some basic feature engineering and a covnet.\n\ngood luck.",
        "Ok, will do that!",
        "Do you only use chatgpt for learning ml?",
        "Yes"
    ]
},
{
    "submission_id": "1gj65cz",
    "title": "NVIDIA cuGraph : upto 500x faster alternate for NetworkX, Graph Analytics in python",
    "selftext": "Extending the cuGraph RAPIDS library for GPU, NVIDIA has recently launched the cuGraph backend for NetworkX (nx-cugraph), enabling GPUs for NetworkX with zero code change and achieving acceleration ***up to 500x for NetworkX CPU implementation***. Talking about some salient features of the cuGraph backend for NetworkX:\n\n* **GPU Acceleration**: From up to 50x to 500x faster graph analytics using NVIDIA GPUs vs. NetworkX on CPU, depending on the algorithm.\n* **Zero code change**: NetworkX code does not need to change, simply enable the cuGraph backend for NetworkX to run with GPU acceleration.\n* **Scalability**:  GPU acceleration allows NetworkX to scale to graphs much larger than 100k nodes and 1M edges without the performance degradation associated with NetworkX on CPU.\n* **Rich Algorithm Library**: Includes community detection, shortest path, and centrality algorithms (about 60 graph algorithms supported)\n\nYou can try the cuGraph backend for NetworkX on Google Colab as well. Checkout this beginner-friendly notebook for more details and some examples:\n\nGoogle Colab Notebook: [https://nvda.ws/networkx-cugraph-c](https://nvda.ws/networkx-cugraph-c)\n\nNVIDIA Official Blog: [https://nvda.ws/4e3sKRx](https://nvda.ws/4e3sKRx)\n\nYouTube demo: [https://www.youtube.com/watch?v=FBxAIoH49Xc](https://www.youtube.com/watch?v=FBxAIoH49Xc)",
    "created_utc": "2024-11-03T20:25:15",
    "num_comments": 10,
    "comments": [
        "in my experience networkx is slow as fuck, at least for my use cases, when data grows graphtool scales way better.",
        "Can the work done here be potentially used for Pytorch geometric or Deep Graph Library, or is this not related at all?",
        "There’s also rustworkx which promises massive speed ups because of their rust backend",
        "Yepp, Networkx is a 🐌",
        "It does support. The cuGraph libraries also include extensions for PyG and DGL\n\n- Official examples for cuGraph-PyG: https://github.com/rapidsai/cugraph-gnn/tree/branch-24.12/python/cugraph-pyg/cugraph_pyg/examples\n- Official examples for cuGraph-PyG published by PyG themselves: https://github.com/pyg-team/pytorch_geometric/blob/master/examples/multi_gpu/papers100m_gcn_cugraph.py\n- official examples for DGL: https://github.com/rapidsai/cugraph-gnn/tree/branch-24.12/python/cugraph-dgl/examples",
        "Ohh, let me check that as well. Any idea on average speed up expected? For say community detection?",
        "considering how much polars(rust) faster is than pandas. i guess a lot.  \nbut it can be worth to try neo4j",
        "Benchmarks are weak because they talk about thread and memory safety more than performance everywhere. Here are some very limited benchmarks: [https://www.rustworkx.org/dev/benchmarks.html#](https://www.rustworkx.org/dev/benchmarks.html#)",
        "Ok, let me explore this",
        "Oh"
    ]
},
{
    "submission_id": "1gj2fru",
    "title": "[Step-by-step guide] Here’s how you can use large language models to perform financial research ",
    "selftext": "I am a software engineer. I've been using LLMs to help me with backtesting and financial research for the past year or so. Today, when the market opened, I asked myself the following question:\n\n>If I was a day trader, because SPY opened green, would it make sense to buy SPY at open and sell at close?\n\nI used an AI model to answer that question.\n\n# Methodology: How can the AI know what happened in the stock market?\n\nAs subscribers to this sub, you know that AI models are powerful tools, but they do not have access to real-time (or historical) stock data. So how could it answer this question?\n\nIt's actually quite simple. AI models are exceptionally good at generating syntactically-valid structured data.\n\nInstead of asking the AI questions about the stock market, I hydrated stock market data into an analytical database and then used the AI to query the database.\n\nThe steps are as follows:\n\n* Save a bunch of stock market data into BigQuery.\n* Create an LLM prompt with my BigQuery schema, instructions, constraints, and TONS of examples to query my database.\n* Add the AI to my web app.\n\nI then asked the model to answer questions such as:\n\n* In the past 6 months, if QQQ opens up 1% or more, what is the probability that it will close higher?\n* In the past 12 months, if QQQ opens up 1% or more, what is the probability that it will close higher?\n* In the past 24 months, if QQQ opens up 1% or more, what is the probability that it will close higher?\n* Same questions for SPY.\n\nThe model answered these questions one after one. You can read the full conversation I had [**with the model here.**](https://nexustrade.io/share/672545360e3e695328f0e8c1) From this, I learned that SPY and QQQ have drastically different gap-up behaviors. SPY is better to buy overall if the market opens up 0.5%+, and QQQ is only 50% likely to close higher if it opens up 1% (and is even worse if it opens up lower).\n\nHere's a snippet of the conversation.\n\n[A summary of my conversation with the AI](https://preview.redd.it/evfk3am1bsyd1.png?width=1846&format=png&auto=webp&s=ee21d13432e58411628c0505a71120a3c02f10a3)\n\nI think this is an exciting time for finance! Of course, I didn't **need** the AI to answer these questions; I could've written the queries myself and summarized the results by hand.\n\nBut the AI made it effortless. It took minutes to derive **real insights** directly from data, and in a way that's easy to read and understand. That's incredible.\n\nWhat do you think about this use case of AI? Have you used LLMs for financial research? Would you ever?\n\nIf you want to ask my model other finance questions, please do! [**It's free to try**](https://nexustrade.io/chat)**.**",
    "created_utc": "2024-11-03T17:03:49",
    "num_comments": 14,
    "comments": [
        "Very cool, thanks for sharing. I built something similar to answer questions about a different type of financial data. \n\nI'm wondering if you faced issues with the size of the system prompt given all the instructions and table schema. \n\nFor mine it got up to almost 2k tokens on gpt-4o-mini which is still fine but if I were to add more tables it seems like it won't scale.\n\nAlso any reason for BigQuery over other options like PostgreSQL, Mongo etc? I'm considering if I should try BigQuery instead (using PostgreSQL now).",
        "Cool, which LLM did you use?",
        "This is definitely very cool, extremely interesting, and has the potential to be an incredibly useful tool.  What are your plans for it?  Have you continued to ask it more questions in the hopes of finding other useful patterns?  Any plans to let others use it?  Sell its use?  Or are you just planning to use it for your own research and/or potential trading advantage?",
        "Thank for reading!\n\nNot with this specific problem, but in other cases where I did function calling, I encountered issues, and had to separate the one prompt into multiple prompts, and do prompt chaining. Fortunately, with modern LLMs, the context window is pretty big, and I believe it’s only going to increase as time goes on.\n\nI used to use MongoDB for this, as it’s the main database I use for my application. But, the aggregation pipeline is nightmarish. It’s slow and really hard to understand and debug. LLMs also struggled to generate correct pipelines.\n\nInstead of duplicating the data to Postgres, I went with the lowest lift option and decided to use BigQuery. It’s lightning fast, but the cost is starting to add up. If you wanted to experiment, I’d suggest you try timescaleDb first.\n\nBut, if cost isn’t a concern, BigQuery is great!",
        "The model is configurable in the UI! I usually use Claude or GPT-4o-mini",
        "Thank you so much! You can ask it all types of questions. I need to create a comprehensive doc of all the types of questions that can be asked.\n\nOthers can use it now! I linked it in the post.",
        "Ah I see, yes that's true. 4o-mini is at 128k context now iirc. Relational does seem better for the LLM to query in my exp as well.\n\nIf you don't mind, another thing I'm wondering is if you had to add anything extra for the LLM to handle complex queries? E.g stuff with joins across tables, CTEs, or some sort of custom logic.\n\nI'm having issues getting the LLM to handle those correctly and I wonder if integrating some extra analysis step with pandas or a generated rule based plan would help.",
        "[removed]",
        "Do you have an endpoint to your table so the model can query it, or is all the knowledge provided in the examples?",
        "To handle these complex cases, I like to have a bunch of examples in the system prompt. That helps a lot. But you’re right though; it can struggle, especially the smaller models.\n\nI want to build some sort of agentic framework to improve the accuracy of the model. But it’s not easy and requires substantial investment.",
        "This is awesome! Do you know how expensive the models are? I’m having trouble finding pricing information.",
        "I have a system prompt, which has all of the information in the table! Additionally, I have tons of examples. I believe the combination of both is essential",
        "I have not used Grok yet. \n\n\"In terms of pricing, the xAI API differs from OpenAI's models. According to the [API portal](https://openai.com/api/pricing/), Grok-2 and Grok-2 mini are priced higher than OpenAI's GPT-4o and GPT-4o mini. The Grok-beta can handle 131,072 tokens for both input and output, while OpenAI's GPT-4o supports 1 million tokens. Grok-beta costs $5 for 131,072 input tokens, compared to GPT-4o's $2.50 for 1 million. For output, Grok-beta charges $15 for its limited token count, while GPT-4o costs $10 for 1 million tokens. Additionally, OpenAI provides Batch API pricing that is at least 50% cheaper than standard API requests.\"\n\n[https://www.infoq.com/news/2024/11/xai-grok-api/](https://www.infoq.com/news/2024/11/xai-grok-api/)\n\nI am mostly using free tier Google Gemini 1.5 Pro/Flash.  See\n\n[https://ai.google.dev/pricing#1\\_5flash](https://ai.google.dev/pricing#1_5flash)\n\nAnd:  \n[https://cloud.google.com/free?utm\\_source=google&utm\\_medium=cpc&utm\\_campaign=na-US-all-en-dr-bkws-all-all-trial-b-dr-1707554&utm\\_content=text-ad-none-any-DEV\\_c-CRE\\_665665924741-ADGP\\_Hybrid+%7C+BKWS+-+MIX+%7C+Txt-Google+Cloud-Google+Cloud+Free-KWID\\_43700077212109634-kwd-310728588663&utm\\_term=KW\\_gcp%20free%20tier-ST\\_gcp+free+tier&gad\\_source=1&gclid=CjwKCAiAxKy5BhBbEiwAYiW--2JQoJFEkLERlL67FVrUL54jpXdpcIidcwMdypWTOyTvAnyZfnccrRoC39UQAvD\\_BwE&gclsrc=aw.ds](https://cloud.google.com/free?utm_source=google&utm_medium=cpc&utm_campaign=na-US-all-en-dr-bkws-all-all-trial-b-dr-1707554&utm_content=text-ad-none-any-DEV_c-CRE_665665924741-ADGP_Hybrid+%7C+BKWS+-+MIX+%7C+Txt-Google+Cloud-Google+Cloud+Free-KWID_43700077212109634-kwd-310728588663&utm_term=KW_gcp%20free%20tier-ST_gcp+free+tier&gad_source=1&gclid=CjwKCAiAxKy5BhBbEiwAYiW--2JQoJFEkLERlL67FVrUL54jpXdpcIidcwMdypWTOyTvAnyZfnccrRoC39UQAvD_BwE&gclsrc=aw.ds)",
        "Sounds like a good use of LLMs, a bit like the graphRAG concept 👍 thanks for the inspiration!",
        "Thank you!"
    ]
},
{
    "submission_id": "1gj25y2",
    "title": "Supervised Sentence augmentation for very unknown language",
    "selftext": "\n\nHi everyone, I am building a self-attention transformer for a small language spoken by around 100 people at this moment.\n\nI have collected around 10k sentences and their translation so far, but want to augment the dataset with grammatically and contextually correct sentences before using automated stuff like back translation (I have a lot of texts in only the original language that needs translation).\n\nThe idea I am following is sitting together with natives (I don’t fully understand the language) and developing sentence combinations that would work.\n\nThe goal is to reinforce grammar for simple sentences, reinforce context and use vocabulary that is not present in the dataset (Names, species, ingredients…)\nI also have a list of 80 irregular verbs and all of their forms, so I was aiming for at least one „sentence set” for each form of the verb (past/present/future) since I like to be structured about it. That would give me 240 „sets”\n\nFor example\nFew / Many / All / Almost all / Some\n+\nFish name out of a list of 50 fish names in singular or plural form\n+\nis / are\n+\nfitting adjective (small, huge, big, rare, delicious, colorful, easy to catch,…)\n.\n\nOr of course other examples:\nI / you / he / she / they / my parents / my grandma / our grandma / …\n+\nproper form of „live”\n+\nnear / above / close to / next to / in / at / behind / opposite of /…\n+ the\nchurch / supermarket / beach / townhall / bridge / bank / port / …\n\nDoing this I would get a ton of sentences. I would shuffle them and use a small percentage of each sentence structure. I would do this for questions, direct speech, past/present/future sentences, etc. Trying to make it complex BUT 100% correct.\n\nIs there any best practice to this? I don’t want to rely on automated augmentation since the language is unknown and undocumented (at least to existing datasets).\n\nAnything to totally avoid?\nI was looking to add around 10000 augmented sentences. My original dataset is very complex with many long sentences. The translator is doing pretty well so far, but the accuracy isn’t as reliable as I hoped, especially on simple sentences like „My aunt lives next to the mayor”.",
    "created_utc": "2024-11-03T16:50:35",
    "num_comments": 3,
    "comments": [
        "I think the best way to do this, or even to identify whether or not it's a good idea, depends on your project goals. Like, why are you building this model? What will it be used for?",
        "Allowing for general translation tasks such as writing christmas cards, general conversation, etc. and allowing for developing further simple learning tasks for people (to learn the language)\n\nAs  well as the backtranslation of the foreign sentences to english. Lets say the language becomes extinct, but we find an old book years later. It should be possible to translate it that way.",
        "Ok I think I get it.\n\nThe fundamental rationale behind data augmentation is that you have a priori knowledge about some symmetry of your data. For example if you rotate an image of a puppy then it is still an image of a puppy; nothing has changed. \n\nThis is sort of what you're doing with your augmentation plan: you know that the validity of certain sentence structures is symmetric with respect to certain word choices. This is a fine plan but I think it might not accomplish your goals. It is sort of like having a dataset with images of puppies, kittens, and ducklings, but only doing the rotation augmentation with the puppy images and not with the other two. \n\nThe problem is essentially data imbalance. The symmetries that you can identify in the language are almost certainly a very small subset of all of the symmetries that exist in the data. Doing this data augmentation will improve your model's ability to translate sentences of that particular structure, but it won't necessarily improve performance on sentences with different structures. It might even impair the model's abilities on other sentences, if the data imbalance is substantial enough.\n\nThis is an experimental question and there's no harm in trying it and seeing how it goes. Getting more data might be the only solution that can work really well, though.\n\nAnother option is to do *full* data augmentation. Rather than using artificially constructed sentences, you can permute word choices in every single sentence in your data set. This will impair the model's ability to learn facts about the world (because you'll get nonsensical but grammatical sentences like \"dogs can fly and fulminate sleeping inside rocks\"), but if you're trying to do translation then maybe you don't care about that anyway. This will require a lot of labor though because you'll want to do traditional NLP and use software to parse the sentences and permute word choices."
    ]
},
{
    "submission_id": "1gj03y7",
    "title": "Exploring AI Model Integration, Need Advice ",
    "selftext": "Hey\n\nI've run a few AI models from Hugging Face while working with Unity Sentis, including speech recognition and pathfinding models. However, I've mostly followed tutorials, and as a C++ game developer, my understanding of AI concepts is quite basic.\n\nI know the implementation depends on the framework being used, but could someone explain how these models operate? Specifically, I'm curious about:\n\n- How models run on a machine, including how they take input, evaluate it, and produce output.\n- The role of tensors and their involvement in the process.\n- How to implement a pre-trained model in an application.\n- What the development pipeline looks like for integrating AI models.\n\nAdditionally, I want to run some models in Unreal Engine at runtime using NNE.\n\nI would appreciate any resources or explanations that could help deepen my understanding. Thank you!",
    "created_utc": "2024-11-03T15:12:09",
    "num_comments": 2,
    "comments": [
        "> how they take input, evaluate it, and produce output.\n\nThey are like any computer program; they can take input by whatever means you want, and output it by whatever means you want. Evaluation is complicated, it consists of translating inputs into a useful data structure and doing complicated math on them.\n\n>The role of tensors and their involvement in the process.\n\nTensors are just multidimensional arrays. This is the data structure that almost all of ML uses; inputs are usually translate into this first. These arrays can usually be interpretted as multidimensional arrays of single dimensional arrays, with the single dimensional arrays representing a mathematical concept known as a \"vector\", which in turn represents an arrow pointing in some direction in high dimensional space (\"dimension\" here being a math concept different from the idea of the dimension of an array).\n\n> How to implement a pre-trained model in an application.\n\nIt's the same as calling any library, really. You'll just have to follow examples to see how it works. Coding assistants like Github copilot or Cursor can also help.\n\n> What the development pipeline looks like for integrating AI models.\n\nSame as calling any library. If you want to train your own model then things become more complicated.\n\nLike, ultimately, if you're just trying to use a pretrained model in some project then things work exactly like any other software development project: read the docs, look at examples, muddle through until things make sense.",
        "Thank you for taking your time for a great answer"
    ]
},
{
    "submission_id": "1gizlvb",
    "title": "how to deal with outliers ",
    "selftext": "hey , so i am preparing this dataset about student performance in exams , so the target is exam score wich it kinda have some outliers idk if that the right way to describe it , but in all the dataset we can only find those score , 100 , 90 , 80,50,40,30 once or twice so are they considered as outliers , the skew of my histogram is like 1.64 which is kinda high , any adivece about how to deal with them ",
    "created_utc": "2024-11-03T14:49:55",
    "num_comments": 2,
    "comments": [
        "How you deal with outlier is depending on:-\n\n1. Why there's an outlier \n2. Are the outliers obstacles to whatever you are trying to do?",
        "It’s hard to give a fully informed answer without understanding the goal of your analysis or having access to your dataset. But in my experience, dealing with outliers is not too different than dealing with missing data. Ultimately, you end up dropping the outliers or imputing them with some other value. \n\nThe most important thing is to keep your end goal in mind and be able to justify your choice. If you’re just trying to build the best predictive model and those outliers are harming your ability to fit the dataset, you can do basically anything and pick whatever performed best (so long as you properly cross-validate so your decision does not overfit the data you have available). But if you’re doing regression analysis for a research study, it would probably make more sense to keep those outliers (given they aren’t due to erroneous data collection) and consider a robust estimator instead. This is because dropping those rows would be fundamentally altering the distribution of the sample you are using to understand the population."
    ]
},
{
    "submission_id": "1giysav",
    "title": "What are some good resources for learning about sequence modeling architectures",
    "selftext": "What are some good resources for learning about sequence modeling architectures? I've been preparing for exams and interviews and came across this quiz on GitHub: [https://viso.ai/deep-learning/sequential-models/](https://viso.ai/deep-learning/sequential-models/) and another practice site: [https://app.wittybyte.ai/problems/rnn\\_lstm\\_tx](https://app.wittybyte.ai/problems/rnn_lstm_tx). Do you think these are comprehensive, or should I look for more material? Both are free to use right now",
    "created_utc": "2024-11-03T14:12:56",
    "num_comments": 1,
    "comments": [
        "Jurafsky and martin is a great deep read for the heavy theory and pure math"
    ]
},
{
    "submission_id": "1giyrot",
    "title": "LLMs for compressing literature and stories",
    "selftext": "Hi,\n\ni am just trying to find out good LLMs for compressing stories and literature of some pages till up to 1000 or more pages to lets say 40 scenes.  \nIt seems that its not always getting it right, especially with the last scenes.  \nDoes anyone know the best LLMs for literature compression and can give me ressources to good prompts.  \nIn my prompts I try to also include examples and aldo have reasoning panels to get the llm to reason why this scene is important and include a reasoning\\_parsing\\_confidence from 0 to 1.\n\nAny help is appreciated a lot. Thanks!",
    "created_utc": "2024-11-03T14:12:09",
    "num_comments": 0,
    "comments": []
},
{
    "submission_id": "1gixud5",
    "title": "How can I add an additional loss function to be monitored during neural network training?",
    "selftext": "Id like to add a customized loss function that is only monitored (so it's not used for actual training) after each epoch. Does the keras/ tensorflow API in R allow me to do this? If not where can I make this happen ? I appreciate any suggestions.",
    "created_utc": "2024-11-03T13:31:59",
    "num_comments": 0,
    "comments": []
},
{
    "submission_id": "1giwbfu",
    "title": "k-fold cross validation",
    "selftext": "Hello everyone. I am studying k-fold cross validation but I am having some difficulties in understanding it. In cross validation, in general, it is always recommended to use a layered verisone of this.\n\nAlso the alley is placed that all instances must be in the various test folds exactly once (so if instance X is placed in the test fold at the first iteration, it cannot appear again in the test sets of the next iterations).\n\nMy questions are twofold:\n\n1. Does this constraint really exist, or have I misunderstood it ?\n2. If the imbalance between classes does not allow me to satisfy this constraint, what happens ?",
    "created_utc": "2024-11-03T12:25:01",
    "num_comments": 1,
    "comments": [
        "1. There is always some flexibility in ways people do it. If you are doing standard k-fold cross validation, then the constraint makes sense. Datapoints will not repeat across folds.\n2. You do a [stratified k-fold](https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.StratifiedKFold.html) cross validation, that allows maintaining class percentages across folds. You could choose your model to be trees with xgboost. This will give some more weight to the imbalanced classes in training process"
    ]
},
{
    "submission_id": "1giv80a",
    "title": "How do you decide which llm is the best for coding?",
    "selftext": "For example on twitter everyone says that gemini is really bad but on llmarena its ranked very high.how do you decide? what website do you use to make your decision?\n\n",
    "created_utc": "2024-11-03T11:38:38",
    "num_comments": 7,
    "comments": [
        "Claude sonnet 3.5 is best right now",
        "Test different models and see for yourself? You can switch b/w the models on aggregators like [Poe](https://poe.com/), [Openrouter](https://openrouter.ai/), [Bind AI](https://appsumo.com/products/bind-ai/?p=1&clickId=SJ9WVCRCoxyKTNBzH1yZnU6OUkCWZOSFOVPFww0&irgwc=1&utm_medium=4245229&utm_campaign=Online%20Tracking%20Link&utm_source=IR)",
        "It depends on your requirements, open/closed source choices, local/cloud environments etc.  \n  \nhere's a couple pages that give more detail:   \n[https://github.com/continuedev/what-llm-to-use](https://github.com/continuedev/what-llm-to-use)\n\n[https://huggingface.co/spaces/mike-ravkine/can-ai-code-results](https://huggingface.co/spaces/mike-ravkine/can-ai-code-results)",
        "try them all... right now they are all similar if you know what to ask. Is like enhanced search engine.",
        "I’ve had pro versions of Claude and ChatGPT and I find Claude to be much better for coding problems",
        "Just use Claude",
        "Try source graph's cody it can parse repositories if provided with link."
    ]
},
{
    "submission_id": "1giv6p7",
    "title": "How do you decide which llm is the best for coding?",
    "selftext": "On twitter, everyone says claude sonnet is the best, but on lmarena it shows otherwise. how do you decide? what website do you use to make your decision? ",
    "created_utc": "2024-11-03T11:37:01",
    "num_comments": 7,
    "comments": [
        "Just try them and see whether you’re happy with the quality of code each model outputs\n\n\nEDIT: Lol at people downvoting me. Are people this clueless that they literally need to be told which LLM to use? Just literally try the LLM’s and see if they suit your needs lmao",
        "I find claude to be much more helpful, especially since ChatGPT keeps changing stuff I don’t want. For instance, when working on a translation task and getting a simple error, asking chatgpt for the fix, he will almost always just randomly change my hyperparameters and change my MacRoman txt format to Unicode, introducing new errors. Changing stuff that was deliberate and tested. Even when I explicitly say to not change anything that isn’t directly related to the error.",
        "Just my personal preference, I subscribe to both chatGPT and Claude. I have noticed that if I'm working on a project that requires a fairly long chat with chatGPT, it tends to make more mistakes. I normally get a response from chatGPT and get a second opinion from Claude. I find that this strategy works very well for my projects, but it can be costly with two subscriptions.",
        "There are many online reviews of coding LLMs. I recommend starting with one or more of those reviews, then test them for yourself. If you use a particular IDE, you could also start with those that integrate easily.",
        "Depends on what you're doing but also the code on more complex and math based system aren't good. I usually ask specific questions that help build into the code block, piece by piece, and code it myself most of the time. It will get better though",
        "If you are looking for code completion, Amazon Q and Cursor are worth looking at",
        "To be very honest I don't think it matters that much. The most that I use LLMs for is autocompletion and they usually all are able to do that well."
    ]
},
{
    "submission_id": "1giv4bh",
    "title": "What Neural Networks really are?",
    "selftext": "Hi everyone! As I dive into machine learning, I have realized just how challenging it can be to learn deep learning (DL) these days. This challenge isn't due to a lack of resources, but rather their overwhelming abundance. I think it is essential for everyone to understand the core workings of neural networks.\n\nSo I wrote a series of 4 articles on the working of Neural Networks. It covers the basics of Neural Nets. I always recommend [3Blue1Brown](https://www.youtube.com/watch?v=aircAruvnKk&list=PLZHQObOWTQDNU6R1_67000Dx_ZCJB-3pi&pp=iAQB). But if you are looking for short reading content, this is a good start.\n\nWhat you will find in these articles is:\n\n* The true definition of NN\n* Types\n* Weights and biases\n* Activations\n* Gradient Descent\n* Forward and Backward Propagations\n\nI think it is helpful and even a beginner can get an idea before getting into AI. Here are the links:\n\n1. [What are Neural Networks](https://medium.com/@umerfarooq230/but-what-are-neural-networks-part-1-b4088790f986)\n2. [What is Forward Propagation](https://medium.com/@umerfarooq230/but-what-is-forward-propagation-part-2-5bbed9fba106)\n3. [What is Gradient Descent](https://towardsdev.com/what-is-gradient-descent-intelligence-for-everyone-part-3-8686bdebfe64)\n4. [What is Backward Propagation](https://medium.com/@umerfarooq230/what-is-backpropagation-intelligence-for-everyone-part-4-00e2c9dfc068)\n\nLet me know what you think. Should I continue this series and/or make an ebook for this?\n\n  \nor you can email me at [umerfarooq230@gmail.com](mailto:umerfarooq230@gmail.com)",
    "created_utc": "2024-11-03T11:34:14",
    "num_comments": 4,
    "comments": [
        "I appreciate the effort it took to make this but honestly I think you are contributing to the very issue you raised \"This challenge isn't due to a lack of resources, but rather their overwhelming abundance.\"   \n  \nYou're guide are superficial and not always very accurate.   \n  \nFor example You say it contains the \"True\" definition of a neural network but where is this exactly ? Your definition as\n\n\"A way of structuring a piece of code so that it can think and function on its own. Those functions often would be tiring and tedious for any human if they code it themselves\"\n\nis just not very good.\n\nIf you are doing this series to further your own learning then that is great and you should definitely continue but if the aim is to help others learn machine/deep learning I don't think what you're doing is really helpful. Sorry if that sounds harsh :/",
        "Hey there. Like the other commenter, I appreciate the time and effort you've put into writing these articles. However, I feel compelled to ask: are you confident that you yourself understand what a neural network really is before trying to teach it to others? I'm asking because, beyond the irony of noting the wealth of resources on ML as a challenge, only to contribute to the noise with more content, there are some clear errors in your explanations that suggest a shaky grasp of the subject at best.\n\nIn just the first article, you open by giving a very dubious definition of machine learning as \"code structured such that it can think on its own,\" which I'm afraid is very plainly wrong. You then mention converting an email into numbers, despite there being no prior or further reference to an email in the context, which adds unnecessary confusion. You also say that we \"pass in the output to the input layer of the NN,\" but that’s incorrect; we only use the true output to compute the loss for the last layer’s output. Additionally, you impose an arbitrary restriction of features to the range \\[0, 1\\] which is not required.\n\nThere are further issues in later articles, including the misconception that neurons \"hold values\", the claim that activations are limited to the \\[0, 1\\] range (despite ReLU, whose range is non-negative real numbers, being mentioned), and the misinterpretation of Df as the learning rate in gradient descent when it’s actually the gradient itself, with the learning rate being a separate parameter. You also say that \"the lower the learning rate, the more efficient our network is,\" which is simply incorrect—consider what happens if you set a learning rate of zero. Lastly, you mention shuffling datasets \"so each value has an even probability of occurrence,\" when the real purpose is to reduce biases related to the order of learning.\n\nI know this critique may come across as harsh, but it’s important to address these errors if your material is being shared as educational or introductory content for beginners. If an actual beginner were to try learning from these articles, they would likely walk away with significant misunderstandings. Accurate information is essential, especially if this work is being distributed across various subreddits. I hope you consider revising your articles, and best of luck on your own ML journey.",
        "Seriously though: OP says that \"the challenge\" is due to an overwhelming abundance of learning resources (which is thus not good, since it causes \"the challenge\") and immediately proceeds to contribute to said abundance by writing 4 (!) articles and planning to write even more.\n\nI just don't see the logic here. A logical thing to do is to organize this abundance of learning resources into a library and classify everything, thus providing a single entry point and reducing the initial complexity. Although I'm pretty sure such libraries already exist and we need a library of libraries at this point.",
        "Even worse, consider a negative learning rate. You would be actively making your model worse."
    ]
},
{
    "submission_id": "1gis4hh",
    "title": "Image Grouping In Structured Sets?",
    "selftext": "Hello, I am a somewhat seasoned developer, but I have never touched on machine learning. I'm asking this question to see if someone can point me in the right direction or provide resource links.\n\nI want to make something that groups similar images based on color, style, theme, or functionality. Specifically, I'd like it to be able to \"match\" items that could go together in a cohesive way within a well-defined set. For example, if I had a list of images of clothing items like a shirt, jacket, pants, shoes, and hat, I’d like the program to suggest combinations that create a complete outfit, pairing items that match in style or color.\n\nAlternatively, if I were working with images of items like a sofa, coffee table, rug, and lamp, I’d want it to suggest pieces that fit together to create a coordinated room setup. The point is that within this defined set, certain items “fill slots” to complete a cohesive grouping based on style or theme.\n\nI’m looking to understand if there’s a framework or library suited for matching items in this way, as well as any key machine-learning concepts that would help me get started.",
    "created_utc": "2024-11-03T09:26:07",
    "num_comments": 0,
    "comments": []
},
{
    "submission_id": "1giryku",
    "title": "Building an LLM based on my lecture content in class",
    "selftext": "I am trying to build an LLM that is trained on the lecture PDFs and PowerPoints provided by my professors with no prior experience. Is this even possible? How feasible is it? \n\nThe more I read on LLMs the more confused I get honestly. I went down a rabbit hole after reading about how people were spending $5000 on that new MacBook Pro to run their LLMs. I feel like I have a great idea since it’s hard to get relevant answers to STEM questions when studying without sifting through hundreds of lecture slides.",
    "created_utc": "2024-11-03T09:19:00",
    "num_comments": 9,
    "comments": [
        "You could just do RAG and use similarity search or some other approach like using vespa ai and retrieve n amount of documents to give to gpt and have it answer questions for you about it.",
        "You can also try NotebookLM instead, if it suits your needs",
        "I doubt you'll be able to get enough training data for anything useable",
        "Do you want to fine tune an LLM with your notes, or build a RAG system?",
        "I have one doubt does notebook llm has api to to integrate with other solutions. I tried searching for it but was not able to find.",
        "I had the same question upon reading the problem statement.\n\nIt sounds like OP just wants an easy way to get answers. RAG is the way to do that right now; fine-tuning is by and large for tweaking the style of the LLM’s output, not for baking in new knowledge. If you fine-tune with the goal of teaching new knowledge, it could work, but it could also lead to catastrophic forgetting. Fine-tuning can also be quite computational challenging, depending on the model size and fine-tuning approach. So don’t decide to fine-tune lightly.\n\nIf the questions you need answers to are relatively basic, I doubt OP would have to do anything custom. Just pay the $20/mo for GPT-4o and use that.\n\nIf the questions really do require access to class-specific content, don’t worry about the LLM, just build a RAG system whose job will be to identify pieces of content relevant to your query. You can then just embed the identified content into the prompt going into the LLM. You don’t have to do anything fine-tuning yourself.",
        "Probably a RAG with a hybrid search",
        "Where do I even start in regards to building a RAG system? Do I need to setup something like ollama and then somehow link it to a file directory? Or can I setup a RAG with ChatGPT? I have the premium subscription. Thanks for the reply!",
        "https://www.kaggle.com/code/gpreda/rag-using-llama-2-langchain-and-chromadb"
    ]
},
{
    "submission_id": "1girw2n",
    "title": "Best laptop to work on Robotics and AI",
    "selftext": "I am a second year robotics and ai student. The laptop that I currently have is very slow when I do something computationally intensive. But confused in buying a new laptop. Which can easily process data and build models. I am actually new to it. Please suggest some good laptop with good performance. Most people have advised me to get a decent laptop and then use cloud services, how much does cloud cost? The laptops I've been seeing cost around 900 pounds (1150 USD) and the ones with GPU cost more than 1400 pounds (1800 USD). What do you guys recommend? I want something that works for at least 3 years till I get a job. I don't want to buy another laptop. Most of my friends using a gaming laptop, but I'm not a fan of them. This laptop meets all my requirements for now but it doesn't have GPU, **Operating system**Windows 11 Home\n\n* **Processor**Intel® Core™ Ultra 7 155H\n* **Memory**32 GB RAM\n* **Storage**1 TB SSD\n* **Display**35.6 cm (14\"), 3K (2880 x 1800), OLED, 120 Hz, 0.2 ms Response time\n* **Graphics**Intel® Arc™ Graphics, I'm getting it for 899 pounds  What do you guys recommend? ",
    "created_utc": "2024-11-03T09:16:02",
    "num_comments": 7,
    "comments": [
        "I say all of this while not really understanding what you mean by \"AI\". Are you wanting to train/tune LLM's? Or are you messing with neural nets? What kinds of projects do you forsee? But because I don't know those things, I'll just assume you want to do local stuff with LLM's, as cloud solutions are very expensive.\n\nPersonally, you'll get more milage (speaking towards AI) with more VRAM. Think like the 3080 has 10GB of VRAM while the 3090 has 24GB. So the 3090 can load larger models onto it. (3080 you can tune/train 1B or 3B)\n\nThere's tricks to all of it, but realistically speaking: Get as much VRAM as you can. The rest of it is still important, but you want that VRAM for sure.",
        "get a normal laptop and use free cloud GPUs from google, amazon or kaggle. They will almost always outperform local machine runs",
        "[deleted]",
        "I am curious why you arent a fan of gaming laptops. If its the look I completely understand but I am sure there are lots of gaming laptops that dont look weird. Gaming hardware is also great for ML because they both heavily rely on the gpu. If the laptop is for ML gpu is without a doubt the most important thing. Cloud computing is a good option but you can learn quite a bit if you have to run it on your own hardware, I personally like having my own hardware alot more.",
        "find one with an rtx 3060 or something if you want to do development locally. however you'll need more expensive hardware or use cloud computing if you want to go beyond simple local prototyping",
        "I think you must build a workstation and connect it with a cheap laptop. You can use the Google remote desktop app I use this way. No listen to them they talk nonsense. As a robotic engineer, you will need simulations and DRL so you need visualizations. You cant do that on cloud mostly",
        "I think he cant because he study upon robotics so he must code simulations you cant do that on colab",
        "Its the look. Do you have any recommendations?"
    ]
},
{
    "submission_id": "1girq4w",
    "title": "Medical Artificial Intelligence course at TUM GERMANY",
    "selftext": "",
    "created_utc": "2024-11-03T09:08:52",
    "num_comments": 0,
    "comments": []
},
{
    "submission_id": "1giqmei",
    "title": "BERT and predicting next word task",
    "selftext": "Question, going over how BERT is working on hugging face. One of the unsupervised training technique is predicting next word. All samples i saw go over easy cases but i am not sure it's that easy in real life.\n\nSo we take a sentence \"I am purchasing a piano\", replacing word \"purchasing\" with \\[MASK\\] token and expect BERT to predict this word.\n\nBUT, there is no word \"purchasing\" in BERT's vocabulary so it will actually be tokenized as 2 tokens \"purchas\" and \"ing\". So once we tokenize the sentence we would end up with \"I am \\[MASK\\] \\[MASK\\] a piano\"? Am i correct?\n\nAlso i am wondering when i use it in production, I would not know if missing word exists in BERT's vocabulary So i feed to BERT \"I am \\[MASK\\] a piano\" to BERT. Will BERT only predict \"purchas\" as a predicted token and it will result in incoherent sentence or will it actually predict 2 tokens \"purchas\" and \"ing\"?\n\nPS: word \"purchasing\" is just an example. My word is different. No need to actually check if \"purchasing\" actually exists in the Bert's vocabulary.",
    "created_utc": "2024-11-03T08:21:54",
    "num_comments": 0,
    "comments": []
},
{
    "submission_id": "1giqkr3",
    "title": "🚀 Exploring Real-Time Face Landmark Detection with MediaPipe and OpenCV! 🎥",
    "selftext": "",
    "created_utc": "2024-11-03T08:19:57",
    "num_comments": 0,
    "comments": []
},
{
    "submission_id": "1giperv",
    "title": "Beginner, using text data to estimate labor",
    "selftext": "I have text descriptions of a rework task. I have a million rows of different rework task descriptions, each one has an associated number of labor hours it took to resolve each defect. What methods should I try to read in the new defect descriptions and estimate the amount of labor hours charged to resolve the defect?",
    "created_utc": "2024-11-03T07:29:02",
    "num_comments": 1,
    "comments": [
        "Sounds like an interesting project. Sounds like youre interested in regression on sequences. And with a million rows you have access to state of the art tech. I'd try out a transformer encoder and then use a linear classifier on the output of the encoder (cell) to output a number. Combine that with a good regression loss (e.g. mse loss) and you should be in good shape. \n\nIf you want even more accuracy look into pretrained models like bert for embeddings, or even an entire foundation model + transfer learning."
    ]
},
{
    "submission_id": "1gio7mn",
    "title": "Understanding Multimodal LLMs: The Main Techniques and Latest Models",
    "selftext": "",
    "created_utc": "2024-11-03T06:34:56",
    "num_comments": 5,
    "comments": [
        "Thank you for your amazing content. I learned a lot from your blog and videos and i hope you will continue doing great work",
        "This is a great write-up! Nice job.",
        "Thanks for the kind words and feedback! It's really motivating to hear that these are useful!",
        "Yeah, i know how hard it is to make such content, especially for it to be presented in a clear and understanding way like yours is. \n\nI also really liked your book LLMs from scratch",
        "Thanks for the kind support! And I hope you get lots out of the book (it's been my favorite thing to write so far)"
    ]
},
{
    "submission_id": "1ginxok",
    "title": "Is natural language effectively a vector quantization for semantic space?\n",
    "selftext": "If each token represents a vector, then effectively your language vocabulary is your VQ codebook right? Out of curiosity, has anybody ever tried auto-encoders that use NLP tokens as the codebook?",
    "created_utc": "2024-11-03T06:21:59",
    "num_comments": 1,
    "comments": [
        "I think that's a bit of a stretch. IMO the better way to think about things is that vector quantization is one way of discretizing a continuous signal, but there are others too, and natural language is one of them.\n\nI'm not sure about autoencoders by themselves, but people do diffusion with NLP tokens. Here's a paper about doing latent diffusion for language modeling, which uses both an autoencoder and a diffusion model: https://arxiv.org/abs/2306.02531"
    ]
},
{
    "submission_id": "1gin9kz",
    "title": "Seeking a Roadmap",
    "selftext": "I'm a student from India and require a roadmap to go through ML and DL (with more focus on DL) as soon as possible.\n\nI'm proficient with college-level calculus and high-school level statistics and probability. I also know Python (with Numpy, Matplotlib and Pandas)\n\nA roadmap with resources/links that would help me start working with projects and hackathons as soon as possible would be really helpful. Thank you",
    "created_utc": "2024-11-03T05:50:07",
    "num_comments": 3,
    "comments": [
        "Use a search bar. There's no less than 5000 duplicate posts.",
        "Go through Kaggle Learn. Check out OCW too.",
        "Hey, I think the easiest way is to start building ML models of your own. I started by using model generation through [https://www.plexe.ai/](https://www.plexe.ai/) and iterated on the output and used it in Kaggle competitions"
    ]
},
{
    "submission_id": "1gin8fc",
    "title": "Career Change at 36: Mechanic/Delivery Specialist → Data/Tech - Need Advice",
    "selftext": "I'm 36 and living in the UK, currently working in delivery and installation of appliances, with previous experience as a mechanical fitter and parts sales. I've been teaching myself Python in my spare time and working on two personal projects:\n\n1. An XRP cryptocurrency market analysis tool using APIs and data visualization and Postgres. \n\n2. A diabetes management app (I'm Type 1) that analyzes CGM data to help optimize insulin dosing.\n\nNo formal tech qualifications, but I'm passionate about data analysis and problem-solving. Currently spending 10-20 hours weekly learning and coding.\n\nQuestions:\n- At 36, is this transition realistic? \n- What's more valuable to focus on first: building projects, getting certifications, or look into formal education?\n- Any advice from others who've made similar transitions?\n- What skills should I prioritize learning?\n\nAppreciate any insights or suggestions from those who've been there.",
    "created_utc": "2024-11-03T05:48:39",
    "num_comments": 4,
    "comments": [
        "I think these are tough questions for others to answer who don’t know your situation. Some questions to think about:\n- how much time will you allow yourself in your transition \n- what budget do you have for education? i.e. will you be able to take time off to do a masters degree? \n- what type of job are you looking for? There’s a spectrum of jobs between data analysis, ML-based reporting (think predicting business metrics and presenting to leadership), data science, and ML Engineering \n\nI recommend talking with folks in the field and seeing how they made the transition.",
        "I agree, I will try and give a little context. \n\nI can allow a good amount of time for the transition, and I'm willing to take absolute entry work to get a foot in a technical company. I have no ability to come out of work for full-time education.  I'm interested in data analysis with a strong interest in programming and health data in particular. I will continue to stroll various sub reddit. Thanks for your input.",
        "It’s definitely possible. I’d recommend continuing to learn Python, learn the basics of the stats of ML (I.e. the Andrew Ng Coursera course) and then most importantly build projects to display on your portfolio. If you don’t go the Masters / PhD route that will be your main way of displaying your quality. Then apply for startups who don’t care as much about certifications. Goodluck!\n\nEdit: check out fast.ai for projects to build and show",
        "Thanks for the follow-up. I will continue to study Python and complete the intro to statistics course I've begun on udacity. I will then  focus on my github profile and look at the Andrew Ng course you suggested for concepts to apply to health related datasets. I am a type 1 diabetic who wears a cgm and tracks carbs and insulin data. This data, as well as a deep subject knowledge( currently analysing global guidelines for type 1 diabetes management in a public guthub repo), gives me ideas for at least a few projects. Would you suggest focusing solely on health related analysis and applications and maintaining a focused portfolio or broadening the scope of my projects? Thanks again"
    ]
},
{
    "submission_id": "1gilpxb",
    "title": "NEAT algorithm and MNIST - good choice?",
    "selftext": "How useful is the NEAT algorithm for MNIST if it was binarized (784 binary inputs, 28 by 28 image). All I seem to find in examples is XOR, pole balancing and game play. What I've tried doesn't generate any meaningful networks oddly. Any thoughts on it would be very helpful.",
    "created_utc": "2024-11-03T04:28:00",
    "num_comments": 1,
    "comments": [
        "So NEAT is just an alternative to gradient based learning where you optimize using evolution, rather than gradient descent. So obviously it can be used, but it doesn't make much sense to me."
    ]
},
{
    "submission_id": "1gil17t",
    "title": "Are there statistical techniques to identify the subsets of data where your ML model is particularly more predictive relative to itself?",
    "selftext": "So I'll just use a finance example: Say I want to predict security returns over the next 5 minutes. My X matrix is just a time series of feature values sampled every minute in the day that the market is open (could be stuff like buy volume, sell volume, relative returns of other securities, etc.)\n\nSo I fit an ML model and I get a certain R squared and rmse. Based on my intuition, I know that there are certain situations where the model has more predictive power than average. It could be something arbitrary, like the hour of the day is 11 AM and the relative return of my security compared to some other security exceeds some threshold. Let's say in this situation the model is a surefire bet, in fact if I were to only consider the subset of data fulfilling my criteria, I could predict the y\\_labels almost perfectly each time. And just for theoretical discussion's sake, let's say this fully translates to the test set and live prediction as well, we just fully understand this phenomenon in the real world as long as our conditions are satisfied.\n\nHowever, all of this gets watered down because my model is evaluated over the full data set and it doesn't do so well in most other situations. I don't want to just rely on my intuition to figure out what these subsets of data are, are there statistical techniques to employ that can inform me of subsets of data which are extra predictive? (of course, I'd want each subset to have at least some number of samples to evidence significance)\n\nAnd beyond this, are there techniques that can identify these subsets at the model training phase? This feels a bit parallel to bagging, but the intent is different - I'm not aiming for some generalized reduction in variance in a broader model, I want to know the models that seem to work particularly well on some subsets.",
    "created_utc": "2024-11-03T03:46:09",
    "num_comments": 1,
    "comments": [
        "Look into Epistemic uncertainty"
    ]
},
{
    "submission_id": "1gikbnw",
    "title": "Use ROCm for machine learning projects on a mobile RX 6700S?",
    "selftext": "Hello, I'm currently using an AMD G14 with a RX 6700S GPU and I am interested in running some machine learning projects. I am currently using Windows.\n\nIs there any way for me to use the RX 6700S GPU to run machine learning projects that uses tensorflow and pytorch on Windows? If not, can I do them using WSL?\n\nI am not that familiar with installations yet so if you can give me some detailed answers or instructions I would really appreciate it.\n\n  \nThank you!",
    "created_utc": "2024-11-03T02:58:17",
    "num_comments": 6,
    "comments": [
        "Sell your current laptop and get one with Nvidia GPU",
        "I have seen that tinygrad does have support for amd gpus but personally haven't tried using that",
        "I wish i could bro",
        "Hi, is tinygrad a framework to for ML or I can use iy to run ML code written in pytorch/tensorflow written for CUDA?",
        "It is a Ml framework similar to pytorch \nCheck its GitHub page for compatibility \nhttps://github.com/tinygrad/tinygrad",
        "You can't use pre existing TF/ torch code directly \nYou will have to convert it"
    ]
},
{
    "submission_id": "1gij186",
    "title": "scholarship",
    "selftext": "I register for scholarship [https://sprints.ai/journeys/AIandMLBootCamp-7275368](https://sprints.ai/journeys/AIandMLBootCamp-7275368) \n\nand they will send me an online assessment then HR interview \n\nDoes this kind of online assessment need to study something before doing it or no or it's not a technical or knowledge assessment\n\nand from details  in the link is it worth it\n\nif I got the chance to join it can I do it beside working hour from 9 to 5 ",
    "created_utc": "2024-11-03T01:21:30",
    "num_comments": 3,
    "comments": [
        "If the program were free of charge, then why are they asking for a deposit?\n\nLooks like a scam to me.",
        "The program will be free of charge, but a deposit of 500 EGP will be refunded upon graduation",
        "That sounds dodgy ngl"
    ]
},
{
    "submission_id": "1gihrof",
    "title": " Seeking Recommendations for a Comprehensive Statistics Textbook",
    "selftext": "Hello everyone,\n\nI'm looking to deepen my understanding of statistics and I'm in need of a textbook that covers statistical tests in detail, including their assumptions and the mathematical derivations that explain why these assumptions are important. I want a resource that not only provides examples but also digs into the theory behind the tests to help me grasp the underlying concepts more fully.\n\nIf you have any recommendations for textbooks that fit this description or any other resources that could aid in my learning, I would greatly appreciate it!",
    "created_utc": "2024-11-03T00:42:12",
    "num_comments": 11,
    "comments": [
        "Statistical Inference by Casella & Berger",
        "Did you try reading - All of Statistics by Larry Wasserman?",
        "Any mainline mathematical stats book for upper undergrad or similar level like Mendenhall/ Wackerley “Mathematical Statistics with Applications” or Hogg/ Tanis “Probability and Mathematical Statistics” or Wasserman mentioned elsewhere has the content you’re after. Put any of those in Amazon and look at both those and the other recommendations that pop up. ",
        "I tried studying ISLP it's okay, but having understanding of university level probability and stats is enough I guess.\nI am also looking for more comprehensive material,let me know if you find smnth",
        "Statistics by Freedman",
        "RemindMe! 1 week",
        "Casella is too hard. Wasserman is too terse. \n\nYou want DeGroot’s book. \n\nBear in mind, what you want to learn requires knowledge of calculus at least at the level of Spivak’s book, or you’re wasting your time.",
        "Introduction to statistical learning",
        "Seems the intuitive choice - does what it says on the tin. ",
        "I will be messaging you in 7 days on [**2024-11-10 20:13:48 UTC**](http://www.wolframalpha.com/input/?i=2024-11-10%2020:13:48%20UTC%20To%20Local%20Time) to remind you of [**this link**](https://www.reddit.com/r/learnmachinelearning/comments/1gihrof/seeking_recommendations_for_a_comprehensive/lv8fx7x/?context=3)\n\n[**CLICK THIS LINK**](https://www.reddit.com/message/compose/?to=RemindMeBot&subject=Reminder&message=%5Bhttps%3A%2F%2Fwww.reddit.com%2Fr%2Flearnmachinelearning%2Fcomments%2F1gihrof%2Fseeking_recommendations_for_a_comprehensive%2Flv8fx7x%2F%5D%0A%0ARemindMe%21%202024-11-10%2020%3A13%3A48%20UTC) to send a PM to also be reminded and to reduce spam.\n\n^(Parent commenter can ) [^(delete this message to hide from others.)](https://www.reddit.com/message/compose/?to=RemindMeBot&subject=Delete%20Comment&message=Delete%21%201gihrof)\n\n*****\n\n|[^(Info)](https://www.reddit.com/r/RemindMeBot/comments/e1bko7/remindmebot_info_v21/)|[^(Custom)](https://www.reddit.com/message/compose/?to=RemindMeBot&subject=Reminder&message=%5BLink%20or%20message%20inside%20square%20brackets%5D%0A%0ARemindMe%21%20Time%20period%20here)|[^(Your Reminders)](https://www.reddit.com/message/compose/?to=RemindMeBot&subject=List%20Of%20Reminders&message=MyReminders%21)|[^(Feedback)](https://www.reddit.com/message/compose/?to=Watchful1&subject=RemindMeBot%20Feedback)|\n|-|-|-|-|",
        "Honestly I don’t think it has what OP is asking for in their question, good though it is for what the authors intended."
    ]
},
{
    "submission_id": "1gihjqc",
    "title": "Seeking Guidance on Multi-Level Classification Psychological Assessment Results with Explainable AI\n",
    "selftext": "Hello everyone!\n\nThe project aims to classify responses from a psychological questionnaire into various severity levels for mental health factors (anxiety and depression). I plan to use a Machine Learning model to classify these responses (Normal, Mild, Moderate, and Severe) and apply Explainable AI (XAI) techniques to interpret the classifications and severity levels.\n\n**Model Selection**:\n\n* **Transformer Model** (e.g., BERT or RoBERTa): Considering a Transformer model for classification due to its strengths in processing language and capturing contextual patterns.\n* **Alternative Simpler Models**: Open to exploring simpler models (e.g., logistic regression, SVM) if they offer a good balance between accuracy and computational cost.\n* **Explainable AI Techniques**:\n   * Exploring **SHAP or LIME** as model-agnostic tools for interpretation.\n   * Also looking into **Captum** (for PyTorch) for Transformer-specific explanations to highlight important features contributing to severity levels.\n   * Seeking a balance between accurate interpretability and manageable computational costs.\n* Is a Transformer model the most suitable choice for multi-level classification in this context, or would simpler models suffice for structured questionnaire data?\n* Any cost-effective Explainable AI tools you’d recommend for use with Transformer models? My goal is to keep computational requirements reasonable while ensuring interpretability.",
    "created_utc": "2024-11-03T00:24:36",
    "num_comments": 3,
    "comments": [
        "Do you need classification for this task? Seems to me that you need the labelled data anyway. Just go with SVMs and call it a day. You won't get better classification results than those on such a simple task.",
        "Is this a Likert scale questionnaire?",
        "Yes, this is like a Likert-scale questionnaire"
    ]
},
{
    "submission_id": "1gifs43",
    "title": "Hello there, I'm a 15-year-old high schooler, and I put together a video on binary classification mathematics for my AI Club. I’d love to get your feedback!",
    "selftext": "Hello r/learnmachinelearning!\n\nI’m a 15-year-old aspiring machine learning engineer, and I’ve put together a video on binary classification math for my high school AI Club. I started this club because of my passion for machine learning and deep learning, and we currently have about 12 members. While the number of members is already not the best, the 6-7 members who regularly attend are genuinely interested in the topics as well as the mathematics behind them.\n\nAlthough I haven’t ever touched a calculus or linear algebra class yet, I’ve been teaching these concepts because I’m passionate about them and have spent a lot of time self-studying since my freshman year and the summer. I’d really appreciate your feedback on the video so I can improve my teaching, understanding, and knowledge of machine learning concepts. It would benefit both me and my AI Club.\n\nIn the video, I go over logistic regression, the sigmoid function, negative log likelihood, and gradient descent. I also spend extra time introducing basic foundational things (like derivatives) for the students who haven't taken calculus like me. I may have some mistakes, so any constructive criticism or suggestions you have would be incredibly helpful.\n\nHere’s the video: [https://www.youtube.com/watch?v=hsW1zzQ-IRE&t=7070s](https://www.youtube.com/watch?v=hsW1zzQ-IRE&t=7070s)",
    "created_utc": "2024-11-02T22:14:05",
    "num_comments": 4,
    "comments": [
        "Great job! While obviously it's only the tip of the iceberg, I am very impressed especially considering that you are only a high school student. \n\nI only skimmed it for 30 seconds, but I particularly like the derivation of the loglikehood. While your video does not explicitly make this connection (and you are not expected to), you'd be surprised how many people even in graduate programs do not know the connection between cross-entropy loss and the maximum likelihood estimate of a bernoulli distribution. \n\nGeneral suggestion for presentation and especially considering your audience. It may be helpful to start sections with a simple visualization or geometric interpretation before diving into the details. For binary classification, I think some discussion on decision boundaries and SVMs really illustrate the concept well intuitively. Neural networks are a rather restrictive case of smooth functionals (or at least C^1 to enable gradient-based optimization), and binary classification is of course a much broader concept.",
        "Fellow 15-year-old here! Huge respect for what you’re doing—starting an AI Club and doing machine learning at our age is rare and impressive. It’s awesome to see other people around my age getting into this field. Like you said, it's rare to find peers who are into this stuff, so it's great to connect with someone on a similar path.\n\nI checked out your video, And just one small suggestion: try pacing yourself a bit slower. You’ve got a lot of valuable information to share, and slowing down could help people catch all the details. Also, a better mic might really improve the audio quality, making the whole experience even smoother for your audience. Keep it up, though—you’re doing an awesome job, and I’m excited to see what you and your club come up with next!",
        "I appreciate your advice! I think I really should have shown some more visual representations of things, especially more during gradient descent and some geometric intuition on vectors and matrices. I should have also shown the plot of a Bernoulli distribution and address that more, I believe I didn't talk about it all!  I'm still learning things myself right now, but its nice to learn more detailed topics and I'll try to advance my learning more, and then I'll hopefully be able to bring much more experience in my future videos. Your insight is very meaningful, especially about SVMs and decision boundaries, I was going more in the direction of neural nets and assuming the function is differentiable everywhere.",
        "Thank you for your advice! I'm aware of my microphone problem, that is because I use my phone to record, and I'll problem invest in a microphone because I can also notice that the audio quality is not great. I'll also try to speak more deliberately next time. I just have a cracky voice so it's sometimes hard to let out words, so sometimes I speak very fast or very slow."
    ]
},
{
    "submission_id": "1gicyyp",
    "title": "Buying a new laptop",
    "selftext": "Hi guys. I'm a second year robotics and ai student. I'm looking to buy a laptop as my current one is very slow when I'm doing something computationally intensive. If you guys have any recommendations, my maximum budget is 1400 pounds. ",
    "created_utc": "2024-11-02T19:27:45",
    "num_comments": 4,
    "comments": [
        "Cheap laptop + cloud. If you ever reach half serious tasks no laptop will have enough juice and you will just burn them up.",
        "https://www.backmarket.com/en-us/p/macbook-pro-2021-14-inch-m1-max-10-core-and-32-core-gpu-32gb-ram-ssd-512gb/0833e614-3ca0-4a2b-a0f1-1db5a3f0ac4c?l=12&offerType=0#scroll=false\n\nM1 max, so its good for ai applications. 32 gb ram, enough for a student in ai. \nAnd 512gb should be plenty for the next 3-4 years. \n\nThe m1 is older, but will still be efficient and fast as hell.",
        "32 gb of ram , 12 gb (or more) of GPU ram",
        "hi, thanks for the reply, do you think this one is good enough **Operating system**Windows 11 Home\n\n* **Processor**Intel® Core™ Ultra 7 155H\n* **Memory**32 GB RAM\n* **Storage**1 TB SSD\n* **Display**35.6 cm (14\"), 3K (2880 x 1800), OLED, 120 Hz, 0.2 ms Response time\n* **Graphics**Intel® Arc™ Graphics ?"
    ]
},
{
    "submission_id": "1gi9ss4",
    "title": "Neural network always outputs the same thing",
    "selftext": "I built a neural network for image recognition, with 43 output neurons. As my weight update equasion I used the equasion uptop. My basic problem is that very quickly the network focuses on one output neuron that then always has the highest activation with about 0.9998, while all others have activations of on average 0.15. While this decreases the average error per neuron significantly, with the average error going from 0.5 down to below 0.15 in training, this obviously isnt what I desire. Is that a common issue or did I make some dumb mistake somewhere?",
    "created_utc": "2024-11-02T16:44:10",
    "num_comments": 58,
    "comments": [
        "> While this decreases the average error per neuron significantly…\n\nI mean, neural nets are just machines to reduce error. If you give it the ability to game your system, it will. Try using different forms of loss, or diversifying your dataset so training on one classifier isn’t numerically advantageous.",
        "Here are some good resources\n\nMITS Algorithmic Aspects of Machine Learning \n\nhttps://people.csail.mit.edu/moitra/docs/bookexv2.pdf\n\n\nCambridge University,  intro to machine learning\n\nhttps://alex.smola.org/drafts/thebook.pdf",
        "What activation function are you using",
        "Which book is that 😂",
        "Did you normalize your inputs? It sounds like your doing a clarification project, what does your output layer look like? One typically uses softmax as output function and cross entropy  as loss",
        "Side note, what book is that?",
        "What initialisation did you use for your initial weights?",
        "To clarify, do you mean that you are doing a multi-class model with 43 outputs? E.g. 43 classes in Y/types of images which you one-hot-encoded.\n\nIf so, first issue is that your outputs don't form a probability distribution over classes. Are you using a softmax activation or something similar in the final layer? What loss are you optimizing?",
        "What type of image dataset are you using?",
        "What book is that? Could you tell me?",
        "What book is this?",
        "What I saw here the derivative of the cost function with respect to the parameter w, then ? Focus on math if you are a beginner, ML is certainly %99 applied math",
        "Excuse me, kindly asking what book is this",
        "How are you initializing, how many layers? There are lot of things you need to give context of...",
        "Wouldn't this be improved by adding dropout to your model? If you were to be using tensorflow, it would be very easy to do but all you'd need to do is based on a value you assign from 0 to 1, that's the percentage of variables you randomly drop out from consideration with the model. It allows the model to reduce dependency upon a single neuron as you describe.",
        "Make sure you’re zeroing the gradient each iteration, not each epoch or something",
        "@sleekmeec, why did this help? Can you please give some pointers?",
        "[deleted]",
        "Your right, Ill first try to change the hyperparameters and training structure and go on from there, at least its doing its basic job right",
        "Thanks, Ill have a read. If nothing else works Ill switch to ReLU, thanks for taking the time to help",
        "Sigmoid",
        "A book which apparently has to explain in depth the mathematical notation for summation and at the same time just casually drops in “this is the partial derivative..”.",
        "The one without an editor. 🤣",
        "Neural Network Math; a visual introduction for beginners by Michael Tyler, but not too sure if Id recommend it, wasnt quite clear on a few parts and doesnt have page numbers, but gave a (in my uneducated opinion) quite good basic understanding on how neural networks and backprop etc work",
        "Random uniform between -1 and 1",
        "Dont know what a lot of the lingo you are using but Ill answer what I think your question is: Ive got a Dataset of 150 images per Claas with a total of 43 classes. I define the output as being the neuron with the highest activation. The loss I am optimizing is the mean squared error of the output neurons, eg (actual output - 0)^2 if not the class that was tested for, else (actual output - 1)^2 . \nBut since the overwhelming opinion in the answers was to change to ReLU and implement abSoftmax layer Ill be doing that going forward",
        "Neural Network Math; a visual introduction for beginners by Michael Tyler, but not too sure if Id recommend it, wasnt quite clear on a few parts and doesnt have page numbers, but gave a (in my uneducated opinion) quite good basic understanding on how neural networks and backprop etc work",
        "Neural Network Math; a visual introduction for beginners by Michael Tyler, but not too sure if Id recommend it, wasnt quite clear on a few parts and doesnt have page numbers, but gave a (in my uneducated opinion) quite good basic understanding on how neural networks and backprop etc work",
        "Well that part checkes out already since my network is decreasing the average error per neuron",
        "Neural Network Math; a visual introduction for beginners by Michael Tyler, but not too sure if Id recommend it, wasnt quite clear on a few parts and doesnt have page numbers, but gave a (in my uneducated opinion) quite good basic understanding on how neural networks and backprop etc work",
        "The Dataset is clean and as mentioned the Output node activations all decrease, apart from one or sometimes two. Ill consider adding regularization and adjusting the hyperparameters, thanks",
        "I recommend you learn what an exploding or vanishing gradient is. This occurs with Tanh and sigmoid activations",
        "Try Relu and get back to me",
        "You say this like it's a bad thing - it's completely reasonable to expect the reader to know what partial derivates are, while also foreseeing that there is a ton of auxiliary notation involved which can get confusing very fast (none of which is something you can \"learn\" beforehand - like what a partial derivative is).\n\nI can't begin to tell you how many textbooks/papers I've gone through where I spend disproportionately longer trying to understand the author's notation than trying to understand the underlying math they're talking about. \n\nIn papers, I understand - it's not the author's responsibility. But textbooks? The whole point of them is to convey information to people who are trying to learn and build an overall picture of what's going on.\n\nDoes it make the textbook more verbose and longer? Absolutely, but I'd rather spend an hour flipping through 10 loosely packed pages than spend an hour and a half on 3 densely packed pages written in size 12 font. \n\n\nSorry for the rant, thanks for coming to ted talk.",
        "It really aint that good, doesnt even have page numbers the bloody thing, definetely wouldve needed an editor for all those spelling mistakes and unclear explanations, but its what Ive got",
        "it looks like msword equations lol",
        "Use -0.5 to 0.5 instead.",
        "So there's your problem. The other comments mean well but are likely not the issue. For example relu activations are better due to computational efficiency and avoid vanishing gradients, but that is not the problem here. You are doing a classification model but not training it properly.\n\nPut a softmax on the end so it's a probability distribution., and switch your loss to something more appropriate like cross entropy. You also should check whether the classes are balanced - i assume there are a lot of samples for the 1st class which explains your results.",
        "Oh I see, do you have some recommendations for a pure beginner? I have some basic blocks but I don't feel like they're sufficient enough",
        "Also have you normalized the range of color. Color is on a spectrum of 0-255. Therefore you should divide by 255 so that values are within 0-1 range. Your model will thank you",
        "Is it worth it? I dont use tensorflow or anything similar, I have the whole network, training and testing written by myself and would have to adjust all the derivatives etc",
        "Thats actually done wonders, quickly did that Yesterday evening and let it do some basic training overnight and it looks like that resolved the issue of the network focussing on one output neuron. To further improve the network Im going to implement ReLU, add/replace to a softmax layer and improve the data prepareation.",
        "Can you please give a hint regarding why that would help?",
        "Okay since softmax has been the most prominent advice Ill tackle that first and look into Cross entropie, but first Ill see if the changes Ive made tikl now will have any effect. And the classes are perfectly balanced, it was always a different output neuron that received all the support beforehand depending on the randomization, but thats been resolved by changing the weight inutialization.",
        "Then I think this book might be perfekt, I myself are a bloody beginner, only ai thing Ive done before is CS50ai and now a few beginner books, but this book gives a very good starting point for implementing a neural network by hand, but it does have its confusing parts (and I camt rly evaluate bcouse as I said Im a bloody newbue). But its insanely beginner friendly",
        "Yes already did that",
        "When you have freetime, try refactoring your code so that changing something like activation function should be no more than replacing 1 or 2 functions.",
        "Use something like chat gpt to fill in the code with Relu function instead of sigmoid.\n\nSigmoid function has many downsides.\n\nRELU is standard for image recognition.\n\nJust research the two if you want more info",
        "the relu derivative is basically trivial.",
        "Yes. Also very very easy to implement.",
        "Nice!!! Good job",
        "Try using adam optimizer too.",
        "Will do, tho the main issue was me simply not knowing the derivative of ReLU, the normal activation function I can change in one function. But I definetely have to refracture the part with the partial derivatives of the error with respect to the weights, thanks",
        "This is the way, OP. ReLU is typically used as the activation function for image-related tasks in ML. ",
        "Agreed here. And ReLU has a simpler derivative calculation anyhow!",
        "> the main issue was me simply not knowing the derivative of ReLU\n\nThis is arguably the simplest part about ReLU! The derivative will only ever take on two values: 1 if the activation is nonnegative, else 0. Don’t even need to do any calculus.",
        "This one easy trick lets you skip calculus!",
        "Yup. Either 0 or 1."
    ]
},
{
    "submission_id": "1gi7pj9",
    "title": "Pytorch can't find image directory",
    "selftext": "I'm trying to run the tutorial below (see link) locally.\n\nI have downloaded the cat images data set and put it in the same directory as my python (.py) script.\n\nSo the python script is in c:\\\\bleh\\\\pythonscript.py\n\nand the cat images are in c:\\\\bleh\\\\cat\\_v1\n\nWhen I run it though, I get the following error:\n\nThe system cannot find the path specified: '/cat\\_v1'\n\nThe only changed I made to the code in the tutorial below is:\n\ninstead of: dataset = datasets.ImageFolder('/kaggle/input/cats-breed-dataset/cat\\_v1')\n\nI have: dataset = datasets.ImageFolder('/cat\\_v1')\n\n[Transfer Learning for Computer Vision: A PyTorch Tutorial | Medium](https://medium.com/@meda.abdullah/transfer-learning-for-computer-vision-a-pytorch-tutorial-c5c4e022bcdf)\n\n",
    "created_utc": "2024-11-02T15:05:12",
    "num_comments": 4,
    "comments": [
        "If you require help it is necessary that you post all the code.",
        "Why did you cut the path? If you want to point to the local path add a dot before the first slash like ./cat_v1",
        "Try providing the whole path instead of relative.",
        "It’s the wrong path. Either remove the slash and put the entire path. \n\nI’m not sure how windows works but it could be a permissions issue as well. "
    ]
},
{
    "submission_id": "1gi7cgh",
    "title": "JavaScript in learning ML / AI",
    "selftext": "Hi guys , just a quick question for a complete beginner in ML , is it worth it to learn machine learning using JavaScript and \ncan i do much with it or not ?\n\nThanks in advance",
    "created_utc": "2024-11-02T14:48:19",
    "num_comments": 9,
    "comments": [
        "Sorry, I have to ask: why?  \nI mean, you could. There are libraries.\n\nPython is the most popular choice for anything AI-related though and I would strongly recommend it instead of js.",
        "Like most comments, stick with Python. \n\nBut unlike other comments, Javascript has a strong future in the browser. Look into C++/Rust compiled to WASM. The ONNX framework does exactly this with webgl support for GPU in the browser.\n\nReally it's not Javascript, but compiled languages converted to WASM with Javascript bindings.",
        "Everything is done in python (and some R), why not just learn that? There's a lot of ancillary libraries that you're going to be hard pressed to find in other languages that work as well out of the box. \n\nAlso, js is a garbage language. Downvote all you want, you know it's true.",
        "Hey, I think the easiest way is to start building ML models of your own. I started by using model generation through [https://www.plexe.ai/](https://www.plexe.ai/) and iterated on the output and used it in Kaggle competitions",
        "Learn Python. It’s the de facto default ML language.",
        "If someone is a newbie in python, what version would you recommend learning - 2.7 or the latest 3.xx? \n\nThis would only be about building AI software and not anything else (webapps/infra etc).",
        "The thing is i know JavaScript and i'm currently using it  ,\n\n I've also heard that JavaScript can also be used for ML/AI  even though that i know python is the way to go with ML/AI \n\nAnd i want to get a good grasp on ML/AI first with a language i know that would make it easier for me to get into ML/AI\n\n then learning python won't be a problem i think",
        "Best comment in my opinion. I litterally agree. I started with Python, then R, now Julia with JavaScript and C (both through Nim). Everything combine so well thanks to web technology!",
        "2.7 has been end of life for nearly 5 years now. No one should really be using it. I would say at least start at 3.10, but I often find my self switching between 3.xx versions for different projects anyways due to libraries not being compatible with each other. You’ll probably have to skim over the release notes for each version at some point."
    ]
},
{
    "submission_id": "1gi6hul",
    "title": "🚨Decision Trees Explained in 60 Seconds ⏳",
    "selftext": "Welcome to @SyntaxGrid! 🌟 In this quick video, we explore Decision Trees, a powerful machine learning model that simplifies complex data decisions. Here’s what you’ll learn:\n\nTree Structure 🌳: See how Decision Trees split data into branches based on feature values, guiding you from the root to the leaves.\nEasy Visualization 👀: Discover the intuitive design that makes interpreting data simple.\nVersatile Data Handling ⚖️: Learn how Decision Trees can handle both categorical and numerical data with ease.\nWhether you're a data science beginner or looking to refresh your knowledge, this 60-second explainer is perfect for you. Check out more insightful content on our channel, @SyntaxGrid!\"",
    "created_utc": "2024-11-02T14:09:10",
    "num_comments": 0,
    "comments": []
},
{
    "submission_id": "1gi2mth",
    "title": "Should I use sklearn or should I build a neural net?",
    "selftext": "Hi\n\nI am a CS grad and I am learning ML. I learned the theory and math and all. Now, I am looking at datasets to implement Linear Regression. Should I use sklearn only or should I build neural networks from scratch to implement it? I am told to use sklearn for smaller datasets. But, I can just build a neural network for all usecases right?\n\nThanks in advance!\n\n",
    "created_utc": "2024-11-02T11:14:00",
    "num_comments": 27,
    "comments": [
        "You haven’t learned either the theory or the math if you’re asking this question. Go back to the basics first and spend more time there.",
        "Neural networks have a tendency to underperform vs. simpler models on small datasets. NNs typically need large, complex datasets to be worth it. NNs are also more difficult to tune and optimize than simpler models.\n\nThat being said, if you're learning, do both and compare the results. You should also get some hands-on experience with NNs, since building them feels very different from calling a model from sklearn.",
        "Do you scramble your personal F-15 fighter jet to HALO jump into the grocery store to get bread, milk, and eggs?",
        "\"CS grad, learned the theory and math and all\"\n\n😭😭😭",
        "CS grad and asking this question",
        "Dataset size is not the only consideration here. Far from it. More important is the nature of the task and how your data relates to it.\n\nBottom line though is don’t use linear regression if your data is not linearly related to your target. Plot some scatterplots between your features and the label, and if you see lots of straightish lines, start with linear regression.",
        "Would be a good exercise to use a variety of models and compare the results. \n\nIf you're using relatively small datasets which don't require a lot of computation there isn't really any reason to not try simpler models and then try building a neural network. \n\nNeural networks are a classic case of \"just because you can it doesn't mean you have to\". But if you're getting into ML there's absolutely no harm in trying. You'll quickly learn why you don't always need to use NNs",
        "Good rule of thumb is to move from simplest and smallest to more complex and larger. First basic statistics, then scikit linear regression, decision tree, etc. then scikit random forest, MLP, svm, ...\n\nOnly after you have exhausted the basic models should you look for pytorch for neural networks.\n\nIt's very good practice to use AIC or BIC scores to compare complexity vs performance. Why? Because less complex models tend to evaluate faster, have less unexpected evaluations, consume less resources and are easier to move to production.",
        "try implementing Linear Regression from scratch (numpy is allowed)",
        "Hi guys, I get it I should have worked more. Never imagined one post would be taken this seriously.",
        "Sure, will keep these points in mind",
        "Lmaoooooo exactly what I thought",
        "This comment perfectly captures the vibe of this sub: Elitist pricks with almost no desire to help newcomers or entertain creative ideas. Exactly what I’ve come to expect as the top answer to a post in this sub.",
        "This is the right answer.",
        "You’re right. But the comment you responded to is also right. OP is putting the cart before the horse.",
        "I think that ironically, your comment comes off this way as well. \n\nTrue, OP needs project experience and is trying to do that. However, I think people who have put in the time and effort are sick of the people jumping on the AI/ML bandwagon that can’t even be bothered to do 2 minutes of research prior to asking a stupid question on reddit. \n\nOP would instantly have a better answer to this question from asking an LLM.",
        "Don't know why you are being downvoted but what you said is the truth. People on this sub care more about gatekeeping rather than actually trying to help someone.",
        "I think that’s totally wrong. OP could absolutely have the math down for all we know. What seems obvious to me is that the thing OP needs is experience working on real projects… which is *exactly* what he is attempting to get help with right now. \n\nHe MIGHT lack the math background, but that is simply an assumption the top commenter asserted. Totally snobbish behavior in a sub called r/learnmachinelearning.",
        "If you’re sick of the AI bandwagon then why don’t you just leave the AI bandwagon sub? This is literally the sub for people looking to jump on the bandwagon.",
        "If I had to guess, I’d say this subs most active members are people with one or two years of fulltime experience with just enough confidence to look down on noobies and too little experience to realize that some of the most basic and simple questions (eg “which algo should I use?”) are actually complex and difficult to answer. It’s gatekeeping for sure, but it’s the fucking grad students on Mount Stupid enforcing it.",
        "I'm not on a bandwagon... I'm here because I study this, and it's my career.",
        "I was just using the language you were using. Hilarious THAT is what you chose to respond to lol.",
        "You sound very immature :)",
        "Sounds good. Glad we were all able to rally and help OP",
        "OPs question doesn’t even remotely make sense. It’s kind of like asking if a hammer or screwdriver is the best tool for every task.\n\nOP is claiming to understand to have ‘learned the theory and math’, and then asks if they should use sklearn or a neural network to implement linear regression. Neutral networks have nothing to do with linear regression. Maybe you didn’t know that either.",
        "Sounds good!"
    ]
},
{
    "submission_id": "1gi2ish",
    "title": "Issues Parsing LLM Responses into JSON with Errors in Tool Calls",
    "selftext": "Working with a relatively small language model (Qwen 2.5 7b) + tools, I'm encountering issues when trying to parse its response into JSON format. The problem usually occurs when the response includes code samples, leading to JSON parsing failures.\n\nFor example, I am using the [try\\_parse\\_tool\\_calls function](https://qwen.readthedocs.io/en/latest/framework/function_call.html#vllm), it fails to fetch the tool calls with an errors like: Expecting ',' delimiter: line 2 column 709 (char 709), caused by single quotation marks in the response, which lacks escape characters.\n\nI've tried patching the problem by replacing problematic characters (in the response before parsing), but it's more of a workaround than a solution.\n\nAre there any more robust methods (besides prompt engineering) for ensuring well-formed JSON responses? For example, are there any small open-source model (SLM) or an encoder-decoder that can validate and ensure the correct JSON formatting of the output (request -> LLM -> response -> validate with SLM -> convert to JSON).",
    "created_utc": "2024-11-02T11:08:54",
    "num_comments": 0,
    "comments": []
},
{
    "submission_id": "1gi27ti",
    "title": "[P] Instilling knowledge in LLM",
    "selftext": "",
    "created_utc": "2024-11-02T10:55:21",
    "num_comments": 0,
    "comments": []
},
{
    "submission_id": "1gi0ego",
    "title": "Roast my resume (ML/CV internship search for PhD)",
    "selftext": "Hi everyone,\n\nI’m applying for AI research and machine learning roles, especially in fields like computer vision, multimodal learning, and generative AI. I've attached an anonymized version of my CV and would really appreciate any feedback on how to improve it.\n\n**In addition to general feedback, I’d love some insight into a specific issue:**\n\nI've noticed a low response rate from recruiters, and I’m wondering if there’s something in my CV or presentation that could be affecting my chances. Any advice on potential red flags, areas that might need clarification, or ways to make my CV more impactful for research-oriented roles would be really helpful.\n\n**Specific feedback I’m looking for:**\n\n1. Does my experience and project list effectively showcase my skills and qualifications?\n2. Are there any ways I could make the CV more concise or targeted?\n3. Any ideas on improving my presentation for better recruiter engagement?\n\nThank you so much for taking the time to help!\n\nhttps://preview.redd.it/kr5uyhowsiyd1.jpg?width=2550&format=pjpg&auto=webp&s=a390e6b2571452c2164cd211c8fcadef1f37b6fe\n\nhttps://preview.redd.it/enjfv8owsiyd1.jpg?width=2550&format=pjpg&auto=webp&s=4dd2b5b354162cde7769560aabec2546e6336110\n\n*Note: For privacy reasons, I’ve removed specific institution names, project details, and publication titles.*",
    "created_utc": "2024-11-02T09:33:12",
    "num_comments": 5,
    "comments": [
        "Try updating keyword specific for roles, its good to be very specific",
        "If you want to target research roles put publication section after education",
        "I highly recommend using a resume service if you're looking for good advice. People that do this for money are absolute wizards and can probably build you one better than crowdsourcing it here. [This website](https://premium-resume-services-f44.blogspot.com) is where I got mine done, and it was worth every penny!",
        "Thank you",
        "Thank you!"
    ]
},
{
    "submission_id": "1ghyf7n",
    "title": "How to get into really good companies which do state of the art ML and AI?",
    "selftext": "I just discovered this sub and you might find my question a bit naive, but i would appreciate if you could give me some insights. \nI studied electronics and communication in my undergrad and I do not have a in-depth background in CS. However, I self-learnt the basics of ML and have focused on applied ML/NLP research in my during my undergrad outside of my university. I am doing my MS in CS now, where I have taken graduate level courses on ML, NLP, RL and Data Science. I feel that ML is an ever growing field with tons of sub-field and I cannot possibly know everything. \nPeople who have already cracked the code of working on frontier AI companies, please share your wisdom. I aim to become a Data Scientist, ML Engineer. I do not intend to do a PhD\nMy current background is aligned with LLMs, Peft, Prompt Engineering, Task specific models. \nSome of the questions I am looking for answers are:\n1) Generalized learning about various topics vs taking one topic and becoming an expert\n2) Learning new skills, focusing on fundamental concepts vs improving my tech stack and expertise in various ML tools. \n3) How important is other CS fields like DBMS, System Design, Algo DS for ML Engineer/Data Scientist roles/Applied Scientist roles?\n\nAny other generic advice also is welcome. Thanks in advance!",
    "created_utc": "2024-11-02T08:03:43",
    "num_comments": 13,
    "comments": [
        "Fck it method: join any faang company as SE -> every team is doing some type of ml -> ask manager to join ML sub-team/project/whatever -> do kinda good -> repeat previous steps -> join full ml team -> done",
        "The easiest way? With your great resume plus if you already know how to code, IMO get a job as a software engineer somewhere smaller (e.g. startup or scaleup) which allows you more flexibility in how to build. Then, with the security of a salary you can do the \"barbell method\" (h/t Nassim Taleb) - spend ~80% of your time on standard day to day core software engineering you've been hired to do, spend the other 20% of your time experimenting with new ML/AI tools on the job. If your company is open to this (I've seen many that are) then it allows both you and the company to work on cutting edge things without risk of core day to day being compromised, all while you get paid to do it.",
        "1. Have solid fundamentals. You need to fully understand how some of the common optimizers, tokenizers, architectural improvements (eg : why people use Relu attention instead of softmax, why rope embeddings etc) work. You should know the sota but you should ABSOLUTELY know the fundamentals and the fundamentals in ML is a wide area cuz most models use a mix of things that build on top of each other. Can't learn diffusion without knowing VAEs and variational inference and SDEs/ODEs. \n\nFundamentals allow you to develop Intuition over time. I have seen first hand \"experienced\" senior software engineers working at reddit who think they know ML after tweaking open AI apis but don't even know enough to know that they don't know anything lol. So focus on fundamentals. Deep learning book by bengio goodfellow is a good start. \n\n2. Nobody cares about you knowing ML tools outside of recruiters because it is understood that you will learn it on the job and keep up. So yes you need to be comfortable with them and for recruiters might want to have a few projects showcasing some uses. But it forms a small part of your journey. Different companies use different tools. Rather be aware of MLOps life cycle etc instead. Tunnelling into learning mlflow is the wrong move. \n\n3. They are all important. For research engineer and applied scientist roles ML theory is THE most important followed by software engineering and data wrangling skills. For software engineering DBMS especially nosql ones and algo + DS basics for work and intermediate to advanced for interviews.",
        "1. Generalized learning about various topics vs taking one topic and becoming an expert\n\nYou need both. No one knows where the field is headed, you need to know the current trends and have a solid generalized background to hop on any new technique.\n\n1. Learning new skills, focusing on fundamental concepts vs improving my tech stack and expertise in various ML tools.\n\nAgain, both. Everybody has the fundamentals, anyone can follow one of the many online class covering ML basics. Everybody has the basic tech stack Python/Numpy/Pandas/PyTorch/HuggingFace. Expand beyond that and you'll do ok.\n\n1. How important is other CS fields like DBMS, System Design, Algo DS for ML Engineer/Data Scientist roles/Applied Scientist roles?\n\nFor DS a lot since that's what you'll do. For MLE CS is fairly needed, you beat your rivals through engineering, not so much through fundamental research. For applied scientists you might not need it so much but a strong CS knowledge can be useful, nowadays scientists are expected  to have a higher standard of implementation quality.\n\nNow to get into Google Deepmind or FAIR, you need to be an exceptional researcher / research engineer. On top of being exceptional you'll need to have grasped the attention of the researchers inside these labs. You won't get in by applying online. They barely post any of their openings, they recruit through the network of their researchers. Be vocal, build your online presence, try to network with these people and you'll have done 90% of the job to get in.",
        "Hey, I think the easiest way is to start building ML models of your own. I started by using model generation through [https://www.plexe.ai/](https://www.plexe.ai/) and iterated on the output and used it in Kaggle competitions",
        "“Fuck it, I guess I’ll just join a random FAANG company.”\n- General Custer",
        "Haha yeah that’s true too. Except I don’t have a solid profile for software engineering, all my work more or less has been in ML. I am okayish at leetcode and can tackle medium questions tho. I’m not sure that’s enough to crack SDE roles, or even get callbacks from FAANG companies!? Correct me if I am wrong.",
        "That’s a good route too. I appreciate the answer! Let me see how everything works out when I graduate :)",
        "This helps a lot! Thank you so much!",
        "Hi. Thank you so much for your answer. I appreciate it. I’m really interested to become a research engineer/applied scientist at somewhere like Google or FAIR and this does give me a fair idea about how to get there!",
        "Oh I’ve never heard of that, I’ll check it out. Thanks!",
        "Imo ML is harder to learn/get good at than general interview level SE so it's not like you have no shot. With like a month grind of leet code, you'd be in good shape to pass faang SE interviews. The ML probably even gives u an edge over others when applying to general SE roles.",
        "Alright, that gives me a bit of hope. Thanks!"
    ]
},
{
    "submission_id": "1ghyafb",
    "title": "Convert speaker diarization to CoreMl for on-device use.",
    "selftext": "Hello there, first of all I need to mention that I am a newbie in machine learning, I am an IOS developer by profession, I am trying to find the way to convert any existing speaker diarization model to CoreMl for on-device use, so far I could only find some sort of pipelines like Pyannote-audio diarization pipeline, maybe anyone can give me any advices on proceeding with this challenge, Thank you in advance!",
    "created_utc": "2024-11-02T07:57:48",
    "num_comments": 0,
    "comments": []
},
{
    "submission_id": "1ghy73i",
    "title": "Check out the \"Role of SQL in Data Analysis\"!",
    "selftext": "[https://youtu.be/sA4yybFHAOo](https://youtu.be/sA4yybFHAOo)",
    "created_utc": "2024-11-02T07:53:29",
    "num_comments": 0,
    "comments": []
},
{
    "submission_id": "1ghxrne",
    "title": "Fast.ai 2022 version course ",
    "selftext": "I am planning to go through the fast.ai 2022 course, just curious that I should take previous versions as well or can I start with this version? ",
    "created_utc": "2024-11-02T07:33:16",
    "num_comments": 1,
    "comments": [
        "Start with latest version available"
    ]
},
{
    "submission_id": "1ghxdl0",
    "title": "Part Time Masters",
    "selftext": "Hello, \n\nI have the opportunity to get reimbursed for advancing my education. I work in a data science team, dealing primarily with natural language data. My knowledge of what I do is based solely on my background in behavioral sciences (I have an MS degree here) and everything that I needed to learn online to perform my job requirements. I would love to get a deeper understanding of the concepts involved in the computational tools I use so I can be more flexible and creative in using the technology available.\n\nThat said, I am looking for a part time masters program that specializes in NLP. It has to be part time as I would like to keep this job, and they only reimburse 6 credits per semester. Ideally, I am looking for something that can be done online but I am also open to relocating to other states in the US.\n\nDo you have any recommendations or are you in a program you like? Would love some to get your input.\n\nThank you!\n",
    "created_utc": "2024-11-02T07:14:18",
    "num_comments": 0,
    "comments": []
},
{
    "submission_id": "1ghx2pi",
    "title": "I need advise for Creating Hotel advisory tool depending on NLP and Machine learning",
    "selftext": "Hello,   \n  \nI want to prepare one project which is depending on large dataset (around 500 000 rows) of booking user comments. This data includes hotel names, negative and positive comments that users wrote.   \n  \nI want to create one model which will find out my keywords from sentence, for example : \"I want to find hotel which has lake view and suit room and breakfast included\" and model needs to find keywords such as 'lake','breakfast','suit room' and etc. then matching it with that user comments and recommend me the name of hotel.   \n  \nI already preprocessed my data and took keywords from that comments and did some sentiment analysis but I think I don't need that.   \n  \nAny ideas how can I do it? \n\n\n\nhttps://preview.redd.it/0bi9zb5gvhyd1.png?width=1742&format=png&auto=webp&s=26e6caf1cbb0eb000107b4e8a2925b20106091f1\n\n",
    "created_utc": "2024-11-02T07:00:19",
    "num_comments": 9,
    "comments": [
        "If some experienced data scientist could help and show me the path, I would be very grateful. Thank you :)",
        "Do you have a repo?",
        "You could train an NER with the keywords and the character position in the documents",
        "I would have to see it! Also have seen the stanford course on NLP https://youtube.com/playlist?list=PLoROMvodv4rMFqRtEuo6SGjY4XbRIVRd4&feature=shared",
        "[https://github.com/ayxancy/hotelAdvisoryToolTest.git](https://github.com/ayxancy/hotelAdvisoryToolTest.git)",
        "can you describe little bit more?",
        "[https://github.com/ayxancy/hotelAdvisoryToolTest.git](https://github.com/ayxancy/hotelAdvisoryToolTest.git)",
        "https://github.com/dreji18/NER-Training-Spacy-3.0/blob/main/NER%20Training%20with%20Spacy%20v3%20Notebook.ipynb",
        "\nI see you've posted a GitHub link to a Jupyter Notebook! GitHub doesn't \nrender large Jupyter Notebooks, so just in case, here is an \n[nbviewer](https://nbviewer.jupyter.org/) link to the notebook:\n\nhttps://nbviewer.jupyter.org/url/github.com/dreji18/NER-Training-Spacy-3.0/blob/main/NER%20Training%20with%20Spacy%20v3%20Notebook.ipynb\n\nWant to run the code yourself? Here is a [binder](https://mybinder.org/) \nlink to start your own Jupyter server and try it out!\n\nhttps://mybinder.org/v2/gh/dreji18/NER-Training-Spacy-3.0/main?filepath=NER%20Training%20with%20Spacy%20v3%20Notebook.ipynb\n\n\n\n------\n\n^(I am a bot.) \n[^(Feedback)](https://www.reddit.com/message/compose/?to=jd_paton) ^(|) \n[^(GitHub)](https://github.com/JohnPaton/nbviewerbot) ^(|) \n[^(Author)](https://johnpaton.net/)"
    ]
},
{
    "submission_id": "1ghx0ot",
    "title": "For loop sentence encoding - running out of GPU memory",
    "selftext": "I'm new to this and I have a small transformer model which I want to use to build my own SentenceTransfer neural network. As a result I'd like to encode each sample sentence using the word embeddings obtained from my transformer and then subsequently train on similarity via a neural network. \n\nThe problem I'm facing is that I've got almost a million sentences and the way I am encoding each sentence seems to throw a GPU out of memory error:\n\n        def create_sent_vecs(self):\n    \n            lst = []\n            for s in self.sentences:\n                stnsor_mean = self.encode_train(s)\n                lst.append(stnsor_mean)\n    \n            stnsor = torch.stack(lst)\n            return stnsor\n\nand self.encode\\_train is defined as follows:\n\n        def encode_train(self, sentence):\n    \n            wrd_lst = []\n            for w in sentence.split(\" \"):\n                try:\n                    idx = self.chars.index(w)\n                    tnsor = self.wrd_embds[idx]\n                except:\n                    tnsor = torch.zeros(600).to(\"cuda\")\n    \n                wrd_lst.append(tnsor)\n    \n            s_tnsor = torch.stack(wrd_lst)\n    \n            return torch.mean(s_tnsor, dim=0)\n\nI'm sure there is something obviously inefficient about encoding through a loop and storing tensors in lists and I'd really appreciate any guidance on how to move forward more efficiently as the dataset is only going to increase going forward. \n\nThanks a lot!\n\n\n\n\n\n",
    "created_utc": "2024-11-02T06:57:30",
    "num_comments": 2,
    "comments": [
        "Use better GPU with more VRAM",
        "I've actually come across a solution that seems to work, namely to detach tensors before adding them to the list of all tensors."
    ]
},
{
    "submission_id": "1ghwdt8",
    "title": "I really need to hear feedback on my learning path",
    "selftext": "Hi, I started learning ML about a week ago, I started with Andrew Ng course on supervised machine learning: regression and classification. While I learned a lot from this course generally, I feel like I missed a couple (if not lot) of math concepts especially in logistic regression, I blame this on not practicing enough with learnt resources and being burnout by all this functions that hardly made sense after moving on to another topic, so I have decided to go over the whole course again.\n\n  \nThe reason I am posting this is to really hear feedback on what I could do to learn ML as **efficiently** as possible. I would also receive recommendation on ML related book resources and pretty much anything that can help me get into this field.",
    "created_utc": "2024-11-02T06:25:40",
    "num_comments": 9,
    "comments": [
        "You can read the book 'hands on machine learning with scikit learn' . Very good book for ml",
        "What is your background? \"as efficiently as possible\" means different things depending on your starting point.\n\nAlso, everyone learns differently but I heavily discourage just redoing courses or reading textbooks cover-to-cover until they click. Once you do a general sweep through the fundamentals, you need to start building things. When you get stuck, then you dive deeper until you're unstuck. Rinse and repeat. Otherwise all you're doing is learning terminology. That's my opinion, ymmv",
        "It ultimately depends on your learning style. However, in general people find it better to learn by remaking the wheel. For example doing a mini pytorch and figuring out why it works. You can also look at the examples you were given and talk to yourself out loud explaining how it works. You will develop questions about things you don't know and at that point, just look them up. Hope this helps.",
        "The only way when you don't get a thing is first to rest a little and reiterate: i.e. spaced repetition. Plus you've got the option of Cgpt and YT: research as much as you can, hell, get the same concept from other sources as well.",
        "Andrew Ng have discouraged noobs to learn maths for ml until his maths specialization got released. My advice is before taking any machine learning course, get solid grasp of pure maths fundamentas, linear algebra, statistics and calculus.",
        "I'd recommend the Hands-on ML Book that someone also mentioned here. You can also watch Steve Brunton's videos for mathematics. I've found them really useful.",
        "Hey, I think the easiest way is to start building ML models of your own. I started by using model generation through [https://www.plexe.ai/](https://www.plexe.ai/) and iterated on the output and used it in Kaggle competitions",
        "Which edition would you recommend?",
        "I come from Full-stack web development,(frontend, backend, devops). By efficiently I only mean to not miss any vital concept. I want to know what you would recommend for me to practice in? jupyter notebook? or just ide with python. What do people, who work on machine learning models, use?"
    ]
},
{
    "submission_id": "1ghvu8b",
    "title": "chat gpt api (token use )  integration on max Xcode ",
    "selftext": "",
    "created_utc": "2024-11-02T05:58:00",
    "num_comments": 0,
    "comments": []
},
{
    "submission_id": "1ghugku",
    "title": "🚀 Mastering Supervised Learning: Essential Classification Algorithms 📊",
    "selftext": "Unlock the power of supervised learning with this must-watch playlist! 🔑 Dive into hands-on tutorials covering key classification algorithms: Logistic Regression 📈, Decision Trees 🌳, Support Vector Machines (SVMs) ⚙️, and Naive Bayes 📐. From core concepts to practical applications, gain the skills to build accurate, effective models for real-world challenges. Perfect for anyone looking to boost their data science toolkit! 🎓💼.",
    "created_utc": "2024-11-02T04:36:20",
    "num_comments": 0,
    "comments": []
},
{
    "submission_id": "1ghucxe",
    "title": "Few queries around NLP",
    "selftext": "Folks, please assist me by choosing to answer any 1 or all of the below queries.  \n\n1. Could you please suggest a great modern reference book to learn NLP with Pytorch that also has a github page. Something that includes transformers is what I am looking for. I have some older references (4-6 yrs old) from O'reilly/Manning/Packt on NLP, but I am not sure if they'd still be relevant. Comment if I can use these. \n\n  \n2. Can someone also demistify if I should continue learning to build stuff using Pytorch and transformers lib (which I believe is the richer format for learning) or should I learn FastAI. I really am not looking forward to rapid prototyping atm but everyone tells me its relevant. \n\n3.  How did you teach yourself to build NLP projects? Any insights into the process are welcome. How does one build project today - is it all about pre-trained models? what's the better thought process?\n\n  \nBackground - I understand theoretical concepts around NLP (and deep learning in general) but I am not well versed with the recent developments after the transformers. I am also comfortable writing code with Pytorch. Looking forward to build basic to advanced projects around NLP in a systematic and an organized learning format in order to develop skill. \n\nApologies in advance if I have asked too much in a single post. Thanks in advance. \n\n",
    "created_utc": "2024-11-02T04:29:35",
    "num_comments": 4,
    "comments": [
        "u/reminderbot",
        "Ok that didn't work, OP please remind me",
        "When?",
        "As soon as you get a good reply to the questions"
    ]
},
{
    "submission_id": "1ghtdlo",
    "title": "RL FOR DYNAMIC ENVIRONMENTS ? [R] [D]",
    "selftext": "\nHello,\n\nI'm new to the field of reinforcement learning (RL), and I’m working on a deep RL project where the environment is constantly changing. In this setup, the state and action spaces not only change based on the actions taken at each step but also vary due to external factors in the simulation.\n\nAre basic DRL methods and algorithms, like REINFORCE with epsilon-greedy decay for exploration, sufficient to enable the agent to converge in this dynamic environment? Or are there other techniques and literature I should explore for such cases?\n\nThank you!",
    "created_utc": "2024-11-02T03:20:58",
    "num_comments": 0,
    "comments": []
},
{
    "submission_id": "1ghsz87",
    "title": "Oasis : New AI model that can generate playable video games",
    "selftext": "Oasis by decart and etched has been released which can output playable video games and user can perform actions like move, jump, inventory check, etc. This is not like GameNGen by Google which can only output gameplay videos (but can't be played). Check the demo and other details here : https://youtu.be/INsEs1sve9k",
    "created_utc": "2024-11-02T02:50:52",
    "num_comments": 0,
    "comments": []
},
{
    "submission_id": "1ghs3fd",
    "title": "🚨 Decision Trees EXPOSED: Unlock Predictive Power in Just 5 Minutes! 🌳📊 The Ultimate Guide!",
    "selftext": "🔥 Unleashing the Power of Decision Trees: Transform Your Data Game! 🚨\n\nWelcome to SyntaxGrid! 🎉 In this video, we dive deep into the world of Decision Trees—one of the most effective and intuitive algorithms in machine learning! 🌳 Whether you're a beginner or an experienced data scientist, this comprehensive guide will equip you with the knowledge to leverage decision trees for powerful predictions. 📈\n\n🔍 What You’ll Learn:\n\nThe structure and mechanics of decision trees, including nodes, branches, and leaves. 🌿\nThe difference between classification and regression trees and when to use each. 🔄\nStep-by-step guidance on building your own decision tree model using Python and popular libraries like scikit-learn. 🐍💻\nKey hyperparameters that can enhance your model's performance. ⚙️\nThe pros and cons of decision trees to help you understand their strengths and limitations. ⚖️\n💡 Plus, we’ll explore real-world applications and see how decision trees are transforming industries today! 🌎✨\n\nDon't miss out on this opportunity to supercharge your data skills! 🚀 Hit play and let’s unlock the full potential of decision trees together! 🔑🔥",
    "created_utc": "2024-11-02T01:40:02",
    "num_comments": 0,
    "comments": []
},
{
    "submission_id": "1ghrxa9",
    "title": "Looking for contract based projects (CV, ML, Robotics and IoT)",
    "selftext": "",
    "created_utc": "2024-11-02T01:26:16",
    "num_comments": 0,
    "comments": []
},
{
    "submission_id": "1ghrf1h",
    "title": "Performance Differences Between Mac Models for Local Training",
    "selftext": "I'm looking to upgrade my laptop sometime in the near future and plan to buy one of the new M4 Macbooks. I intend to do most larger model training tasks in a cloud server, but I would like to have the option of running smaller things locally. For those with experience using the past few M-chip Macs, which should I expect to give a greater performance boost: going with the Pro chip with less GPU cores while spending more to get 48gb of RAM, or going with the Max chip with a better GPU and sticking with 36gb of RAM?",
    "created_utc": "2024-11-02T00:45:11",
    "num_comments": 3,
    "comments": [
        "RE: Pro chips, no one will know yet. The largest that has been available is 36 and for the MacBooks specifically they will only go up to 48 this gen without the Max. The 64gb M4 Pro will only be found in Mac Minis and is likely a configuration intended for server clients. ",
        "Ah oops I didn't realize the Pro maxed at 48gb. Just edited the post to reflect that number. I also didn't realize that there wasn't an option to push the Pro's memory cap above the base spec of the Max in the past. Definitely going to be interesting to see how they compare once people are able to benchmark them. Maybe we'll see the 48gb Pro being able to outperform the base Max for larger models?",
        "From the ML/LL model benchmark graphs I remember seeing it was usually hard to tell the difference between pro and max if any Nvidia card was on the same graph, they’re just comically faster in comparison since CUDA has a virtual monopoly on GPU compute at this point. \n\nAs a general rule of thumb more ram will allow larger models and datasets so things that otherwise outright wouldn’t work can be done, they just might take longer "
    ]
},
{
    "submission_id": "1ghrbat",
    "title": "Support Vector Machines | Supervised Learning | AIML",
    "selftext": "🎉 Welcome to the Wonderful World of Machine Learning! \nIn this video, we’re diving into Support Vector Machines (SVM), a super cool algorithm that helps us classify data like a pro! 🌟 Ready to learn? Let’s go! 🚀\n\n👉 5 Key Things You’ll Learn:\n\n1. What is SVM? 🤔 Learn how this powerful algorithm separates data with the perfect boundary.\n2. Why SVM is a Superhero 🦸‍♂️ in machine learning! We’ll show you how it tackles complex data classification.\n3. The Math Behind the Magic 📐 (Don’t worry, we’ll make it fun and easy to follow!).\n4. Real-World Applications 🌍 – How SVM is used in everything from facial recognition to text classification.\n5. How to Implement SVM 💻 in Python step-by-step! You'll get hands-on in no time.\n✨ Whether you're just starting or leveling up your ML game, this video will make SVM your new best friend! 😎 So, hit that play button, grab some snacks 🍿, and let’s make learning joyful! 😄\n\n👉 Don’t forget to like 👍, subscribe 📲, and hit the notification bell 🔔 for more fun tutorials!",
    "created_utc": "2024-11-02T00:36:38",
    "num_comments": 0,
    "comments": []
},
{
    "submission_id": "1ghr6fx",
    "title": "Naive Bayes | Supervised Learning |AIML",
    "selftext": "🎉 Welcome to SyntaxGrid, where data meets excitement! Today, we’re taking a thrilling ride into the world of Naive Bayes! 🚀✨ This incredible machine learning algorithm might sound simple, but it packs a punch when it comes to making predictions! 🤖💡\n\nIn this video, you’ll discover:\n\nWhat is Naive Bayes? 🤔 Let’s break it down in the simplest terms!\nHow does it work? 📈 We’ll unravel the magic behind its calculations and why it’s so effective!\nReal-world applications 🌍 — From spam detection to sentiment analysis, see how Naive Bayes is used in everyday tech!\nHands-on examples! 🛠️ Get ready to roll up your sleeves and dive into coding with practical demos!\nGet ready to supercharge your data science skills! 💪💻 Whether you're a newbie or looking to enhance your toolkit, this video is packed with insights, tips, and a dash of fun!\n\nSo, grab your favorite snack 🍿, settle in, and let’s embark on this exciting learning adventure together! If you love what you see, there’s so much more to explore on this channel! Let’s make data science enjoyable! 🌟🎈",
    "created_utc": "2024-11-02T00:25:38",
    "num_comments": 0,
    "comments": []
},
{
    "submission_id": "1ghr2c5",
    "title": "What are the Best Approaches for Classifying Scanned Documents with Mixed Printed and Handwritten Text: Exploring LLMs and OCR with ML Integration",
    "selftext": "What would be the best method for working with scanned document classification when some documents contain a mix of printed and handwritten numbers, such as student report cards? I need to retrieve subjects and compute averages, considering that different students may have different subjects depending on their schools.\n\nThe classification will also be domain-specific, hence, I will be collecting the documents and have them labeled and trained. These are the categories student information (students filled it up), certificate of enrolment, medical certificate, and the report cards. I also plan to develop a search functionality for users to retrieve the documents.\n\nI am considering using a Large Language Model (LLM), such as LayoutLM, but I am still uncertain. Alternatively, I could use OCR combined with a machine-learning model for text classification.",
    "created_utc": "2024-11-02T00:16:48",
    "num_comments": 1,
    "comments": [
        "PaddleOCR is very fast and accurate"
    ]
},
{
    "submission_id": "1ghqd4i",
    "title": "I need some help and advice on how best to design a connect 4 AIAgent that is insanely good enough for a contest",
    "selftext": "I have a contest ending in a week and I need to design a connect 4 AIAgent to compete other people's AIAgent.\n\nCurrently, I am near the bottom of the rankings with just a minimax algorithm. I also only ask this because I cannot understand much of the techniques and literature of the ideas that are online.\n\nI can make use of any ML/AI techniques (AI Search, decision trees, logistic/linear regression, SVM, NN, etc) but need to ensure that each move by my agent is made under 1 sec.\n\nI am working with the following dataset currently\n\n[`https://archive.ics.uci.edu/dataset/26/connect+4`](https://archive.ics.uci.edu/dataset/26/connect+4)\n\nthe first few lines of the dataset look like this:\n\nb,b,b,b,b,b,b,b,b,b,b,b,x,o,b,b,b,b,x,o,x,o,x,o,b,b,b,b,b,b,b,b,b,b,b,b,b,b,b,b,b,b,win\n\nb,b,b,b,b,b,b,b,b,b,b,b,x,b,b,b,b,b,x,o,x,o,x,o,o,b,b,b,b,b,b,b,b,b,b,b,b,b,b,b,b,b,win\n\nb,b,b,b,b,b,o,b,b,b,b,b,x,b,b,b,b,b,x,o,x,o,x,o,b,b,b,b,b,b,b,b,b,b,b,b,b,b,b,b,b,b,win\n\nb,b,b,b,b,b,b,b,b,b,b,b,x,b,b,b,b,b,x,o,x,o,x,o,b,b,b,b,b,b,o,b,b,b,b,b,b,b,b,b,b,b,win\n\n\n\nI have dabbled alittle with the dataset and implement a lousy NN model that did not really improve the performance of my agent. \n\nI need some detailed and comprehensive advice on how to integrate ML into my agent and use it to improve it. Things such as type of model to implement, how to make use of the dataset, integration with the AIAgent to improve performance.\n\nI really appreciate any advice that people can provide. Please I'm desperate :(",
    "created_utc": "2024-11-01T23:23:24",
    "num_comments": 3,
    "comments": [
        "Pretty sure connect 4 is a solved game? I wouldn’t train an ANN",
        "I would recommend taking a look at the notebooks on kaggles [connect x](https://www.kaggle.com/competitions/connectx/code) competition. You’ll see loads of approaches. Plus don’t worry you can genuinely really quite far with minimax. If you want to look at RL maybe have a go at PPO",
        "I get that it is a solved game, but how do I leverage on this fact to implement my AI agent? Also note that I only have 1 second time limit for each move so its not like I can search to great depth for ideal moves. \n\nTIA."
    ]
},
{
    "submission_id": "1ghq8t1",
    "title": "Why charmed kubeflow requires minimum of 32GB of RAM and 50GB of storage ",
    "selftext": "I am wondering why charmed kubeflow that uses juju and microk8s needs 50gb of storage and 32GB of RAM. Can anyone explain me why it need too much resources? I thought k8s is lightweight kubernetes distribution lol",
    "created_utc": "2024-11-01T23:14:37",
    "num_comments": 8,
    "comments": [
        "1. Wrong channel. You can get more helps at r/mlops\n2. \"I thought k8s is lightweight kubernetes distribution lol\" -> What are you talking about? k8s is an abbr of kubernetes.\n3. kubeflow is not just k8s (kubernetes) but an entire MLOps platform and of course it requires a lot of resources.",
        "32GB of RAM is lightweight lol.\nTry Openshift Local with under 192 GBs of RAM.",
        "2. My bad it's should be microk8s",
        "Yeah I mean even if I want to create a very simple pipeline. 32GB is too much, isn't it",
        "Microk8s is now deprecated in favor of kuberenetes charm",
        "If you want to create a very simple pipeline, you don’t need to install all of kubeflow though. Kubeflow Pipelines is a [standalone component](https://www.kubeflow.org/docs/started/installing-kubeflow/#standalone-kubeflow-components).",
        "But latest canonical docs still force users to use microk8s for Charmed kubeflow",
        "Have you tried deployKF?"
    ]
},
{
    "submission_id": "1ghp8a7",
    "title": "Does any one use charmed kubeflow ? ",
    "selftext": "I want to practice kubeflow locally but so I tried to install charmed kubeflow. But it is very unstable and I could not get it into usable condition. There are too much bugs and canonical support doesn't exists. Is there any easy way to run kubeflow locally? ",
    "created_utc": "2024-11-01T22:03:27",
    "num_comments": 0,
    "comments": []
},
{
    "submission_id": "1ghp25i",
    "title": "Need career advice in machine learning ",
    "selftext": "I (24M) post graduate in physics from average college in India. But i building myself into phyton and machine learning because i did my research in semiconductor works. But people in India says, only IIT background getting jobs in INDIA. so i decided to Crack gate and net, but it will take another on year. So i really need your advice that i should crack exam or i can go with tha research intrest in companies with my own studies. Because i am at the 50% state in learning ML. Please guide me",
    "created_utc": "2024-11-01T21:52:27",
    "num_comments": 6,
    "comments": [
        "I would advise sticking to semiconductors if finance is your primary motivation. if you are moving into ML for money you will be disappointed because this field has a lot of passionate people not to mention it is kinda saturated. I don't have much idea about the semiconductor industry but I read some news about govt spending a lot of money here. \n\nOf course some people are paid a lot of money in ML but a lot of it is still unrealised and based on a lot of potential payoff later",
        "Holy shit, fix your English first. On a kinder note, try to see if you can leverage simple semiconductor problems into AI/ML problems before moving on to large ones.",
        "Hey, I would suggest that you start doing Kaggle problems and learning more about ML/AI models and build your portfolio of playing around with building such models. You can try [https://www.plexe.ai/](https://www.plexe.ai/) to use their AutoML solutions to get a base model and run it on Kaggle competitions. I generally build on top of the models they provide and has helped me a lot to get better at work too. \n\nI work as MLE 3 at a tech company :)",
        "[deleted]",
        "Thanks",
        "Your answer has no relevance to the question. ",
        "Are you drunk?"
    ]
},
{
    "submission_id": "1ghnafx",
    "title": "Need Team Member",
    "selftext": "I wish to participate in this LLM Agent Hackathon \n\nLink: http://rdi.berkeley.edu/llm-agents-hackathon/\n\nLet me know if anyone is interested. ",
    "created_utc": "2024-11-01T20:07:06",
    "num_comments": 2,
    "comments": [
        "Hi, I am interested and excited to work together!",
        "track 1 and 5 only"
    ]
},
{
    "submission_id": "1ghn1v3",
    "title": "Fine tune or rag?",
    "selftext": "So I have a dataset of 300 samples of text mapped to code. That code is limited in the sense that they are some basic lines of codes that are not going to be changed for later inference. To be more precise if I have those 300 lines of code, with their text description, then for inference, we're expecting one of those lines given the parameters provided in the text. For example: `(\"print the first index of this string s\", \"print(s[0])\")`, this is just a hypothetical example. What we expect, is an inference for something like: `\"display the 0 index of string j\"` to give `\"print(j[0])\"` .\n\nThere are two ways to go about this, either to fine-tune an llm, preferably smaller one, while increasing the model complexity if the smaller one works but doesn't capture all of the complexity.\n\nOr, we do RAG, because basically we're doing knowledge here to be memorized, and basically for inference, we're just asking for one of those patterns in the initial dataset, and thus it s more of a knowledge memory. Am I right with this?",
    "created_utc": "2024-11-01T19:53:46",
    "num_comments": 8,
    "comments": [
        "Why not use a model that’s already trained for coding?",
        "It sounds like you're only concerned about the model being able to do the 300 operations in your dataset? You dont expect it to be much better at coding problems in general than the baseline model, and you just want it to be really good at memorizing this dataset? \n\nIf so I'd say 100% RAG. Finetuning would take way more training compute, since the baseline llm will have way more knowledge than you are emphasising.",
        "I don t gave python, that s juat an example, we have a new language",
        "Yes, exactly! I fine tuned and got a 90% accuracy, should RAG give 100 accuracy?",
        "Wow that's surprising. What was the baseline accuracy without fine tuning ? I could never say 100% accuracy. 90% is already high accuracy. Whether RAG would give more I'm not sure, I would need to know the baseline to make an informed guess.",
        "Baseline: 0.027",
        "Is that the accuracy of the pretrained model without your fine tuning? Because thats what i meant.\n\nIf so, i am very impressed by your results , and I can't comment on whether RAG could improve much more than that.",
        "Yeah we ll see."
    ]
},
{
    "submission_id": "1ghhzzm",
    "title": "ML question: Training times",
    "selftext": "I've been working with chatgpt to build a model that will inform trading bots on trend direction.  I'm attempting this be using correlations and image recognition.  The trainer begins but the projected timeframe to finish is massive.  I've done some work to optimize it, but i'm sharing the details of what I'm doing in the hopes that someone can tell me if these train times are way too long, or appropriate. Depending on batch/chunk ratio, i'm having a difficult time finding an optimal timeframe. Can anyone advise or point me in a direction?\n\n# Project Overview: Training a CNN on Market Data Using TensorFlow\n\n# Dataset and Structure\n\n* **Data**: 50,000 images representing market price data (candlestick charts).\n* **Input**: Each training sample consists of 3 images per UUID, one per instrument (Gold, Silver, Copper).\n* **Image Specs**: Each image is **400x200 px**, preprocessed to RGB and normalized to `[0, 1]`.\n\n# Model Architecture\n\n* **Model**: A CNN with three branches (one for each instrument). Each branch applies the following layers:\n   * **Data Augmentation**: Horizontal Flip, Rotation, Zoom\n   * **Convolution Layers**: Two Conv2D layers with 128 filters and Batch Normalization\n   * **Pooling**: MaxPooling followed by Global Average Pooling for spatial adaptability\n* **Dense Layers**: After concatenating the branches, a dense layer (256 units) with L2 regularization is applied, followed by a dropout for regularization, and finally a sigmoid output.\n\n# Hardware Setup\n\n* **System Specs**:\n   * **CPU**: 22 vCPUs (Intel Xeon E5-2680 v4)\n   * **RAM**: 148 GB\n   * **GPU**: 3x NVIDIA V100 (16 GB each, running with TensorFlow MirroredStrategy for multi-GPU training)\n\n# Training Parameters\n\n* **Batch Size**: 18 (to balance memory and compute usage)\n* **Chunk Size**: 100 samples (for processing in manageable sets)\n* **Epochs**: Up to 4 per chunk, though early stopping is enabled to avoid overfitting.\n* **Time Limit**: An adjustable cap (e.g., 1 hour) to control training runtime.\n\n# Execution Details\n\n* **Chunked Training with Time Limit**: To handle memory and runtime constraints, training is executed in chunks, allowing partial completion based on the time limit. Each chunk is trained with validation on \\~20% of the dataset.\n* **Parallelism**: GPU-accelerated training using TensorFlow’s MirroredStrategy, alongside parallel image preprocessing on CPU cores.\n* **Debug Options**: Adjustable `debug_file_limit` to control data scale for testing.\n\n# Performance and Efficiency\n\n* **Approx. Runtime**: With batch size 18 and chunks of 100 samples, training typically takes several hours depending on target epochs and time limit.\n* **Challenges**: Managing data input size vs. GPU memory, minimizing file load delays, and optimizing GPU utilization.\n\nThis setup enables efficient training while controlling memory usage and training time, leveraging TensorFlow’s GPU capabilities and multi-processing for preprocessing on a high-powered CPU.\n\n",
    "created_utc": "2024-11-01T15:41:36",
    "num_comments": 4,
    "comments": [
        "It’d help if you put the question at the end, but good job on the extra info. \n\nWhat’s your accuracy and similar metrics?\n\nI haven’t seen the model or data but 4 epochs is quite low in my experience. If early stopping is causing a count similar to that, I’d revisit your model architecture. ",
        "Hey, I started by using model generation through [https://www.plexe.ai/](https://www.plexe.ai/) and iterated on the output and used it in my business",
        "Here's the full code, I'm just trying to understand what to expect from training times for a project like this, or if it's likely that I'm doing something very wrong.\n\ncode has been posted here: [https://pastebin.com/1dV6JVLX](https://pastebin.com/1dV6JVLX)   \ncan't post all images, but they are 400x200 png",
        "I think I've made significant progress. There was a bottleneck in loading images but I built a tensorflow dataset which really helped the loading process"
    ]
},
{
    "submission_id": "1ghfplr",
    "title": "Studying Advanced Math",
    "selftext": "",
    "created_utc": "2024-11-01T13:57:00",
    "num_comments": 2,
    "comments": [
        "Whatever your grad program is, there will generally be prerequisite courses listed in syllabi. I'm wondering if you cannot just take those at the school before you start your core classwork in earnest? I know it sucks to think \"ugh I have to do SCHOOL before I can do SCHOOL?\" but also, if you want to learn ML at some fundamental level, there is no way around understanding what matrix decomposition, eigenpairs, multivariate calculus, etc are.\n\nI am kind of in the same boat as you, getting a masters after being away from school forever. I will say in the one class I shirked the prereq I got my ass roundly kicked, and I think if I had it to do over again, I'd take the pre-req. My experience is oddly opposite of yours though - I am okay at math and absolute shit at coding. Anyway - from one old dude to another - best of luck, you got it!",
        "Thanks, appreciate it. Same to you, good luck!"
    ]
},
{
    "submission_id": "1ghfcqo",
    "title": "Neural Network Learning - Inner Layer Visualization",
    "selftext": "",
    "created_utc": "2024-11-01T13:41:01",
    "num_comments": 0,
    "comments": []
},
{
    "submission_id": "1ghf4h2",
    "title": "Guidance needed for Andrew Ng cource.",
    "selftext": "Hello AI community. I want to learn AI from scratch, so I started Andrew's course. I have completed 1 week of the course, and it's good. The only problem I find is that I'm a student and just want to learn the course, so I'm auditing it. Hence, all the labs and optional labs are locked. (So I have downloaded them from some GitHub repo.) The only thing I think I'm lacking is practice. Right now, I'm on week 1.5 (feature scaling methods). Is it too early to start practice projects? If not, please guide me.",
    "created_utc": "2024-11-01T13:30:43",
    "num_comments": 3,
    "comments": [
        "I just finished up Andrew Ng's Machine Learning Specialization. But, I've already been building simple ML models in Kaggle and participating in competitions. The specialization gave me further understanding of what I was doing in Kaggle.\n\nSo, if you want to get in more of that hands-on practice, I suggest you start with the Kaggle mini-courses instead and jump right into the basic Kaggle ML competitions.",
        "Im new with kaggel too...can u suggest .me few of the mini cources...",
        "Complete all of them--under the Kaggle \"Learn\" section."
    ]
},
{
    "submission_id": "1gheygx",
    "title": "How to start breaking into machine learning as an undergrad?",
    "selftext": "Hi everyone,\n\nI’m currently a sophomore studying computer engineering and physics in college wanting to break into machine learning to hopefully get an internship in the field this summer. However, I feel like I’m a little behind in general. I recently switched my major to computer engineering at the end of my freshman year, so I only have real experience coding in Python, but I don’t have much experience with data structures and algorithms so I’m trying to teach myself that now. I’m also currently trying to learn PyTorch in hopes of making some contributions to a research lab I joined, but it’s a lot of new information at once. On top of this, many internships want people with knowledge of C++, but I have very little knowledge of the language and want to learn it as well. A lot of people say to learn through projects, but I feel like whenever I want to make a project, I need a hand to hold me throughout the entire coding process instead of being able to build one on my own. I don’t fully understand the syntax of C++ yet either. I think what I need right now is some structure and advice so I can set myself on track to be in the best position to get an internship this summer. If someone has useful resources for learning, that would be great! Anything is appreciated, thank you so much! ",
    "created_utc": "2024-11-01T13:23:07",
    "num_comments": 6,
    "comments": [
        "C++ is a bit difficult, and IMO a generally bad language, so it'll take you some dedicated effort to learn it. I think you might just be applying for the wrong internships? Most don't require it these days i think. Many job ads will include C++ among a bunch of other stuff that they're interested in, but you should interpret that as \"you're probably qualified if you know C++\", not \"you must know C++ to be qualified\".\n\nAlso the job market sucks now, for everyone, so don't blame yourself if getting an internship proves to be tough.\n\nA few pieces of advice in no particular order:\n\n- Try to learn one thing at a time and be patient. You can't drink the entire ocean. Pytorch is a fine place to start.\n- Do very well in your classes and go out of your way to talk with your professors regularly and seek their advice. They're the ones who can help you with undergraduate research opportunities, or with getting into grad school.\n- Learn as much math as possible. If you want to do ML, and especially deep learning, you need a lot of math and there's no such thing as too much.",
        "Heyy! With the rise in agentic AI - I would say the best way to learn is actually using Claude or existing AI tools to code with. I found [**https://www.plexe.ai/**](https://www.plexe.ai/) really interesting as I was learning. I used their service to generate models for Kaggle competitions and they gave me a really good base model that I could build on and hone my skills",
        "OP: please do your due diligence please before posting this question. There are a lot LOT of questions exactly like yours. And there are even more comments with solid advice. Just search this godamn sub.",
        "Thank you so much, this really helped ground me. I feel like I was trying to do exactly what you said — drink the whole ocean. I should probably take a step back and approach it one step at a time!",
        "Thanks, I’ll look into this!",
        "No problem! I would say just start with using this for Kaggle problems and slowly you will understand more of ML as you start solving more complex problems"
    ]
},
{
    "submission_id": "1ghd8hn",
    "title": "What monitoring methods and tools help you tackle challenges when deploying AI to production?",
    "selftext": "Considering the complexities of deploying AI in real-world environments, how do you handle issues like hallucinations, data drift, and model security? I'm curious to hear which tools and approaches work to keep models stable and reliable.",
    "created_utc": "2024-11-01T12:07:41",
    "num_comments": 2,
    "comments": [
        "It depends on how you’re deploying the AI into production. Every use case is a little different. If you’re just wanting to learn about AI observability, check out WandB Weave, Evidently AI, and Fiddler.ai.",
        "Thank you for the insights and recommendations! I appreciate the mention of WandB Weave, Evidently AI, and Fiddler.ai—I'll check them out. Considering how different use cases might require tailored monitoring solutions is helpful."
    ]
},
{
    "submission_id": "1ghcgeq",
    "title": "I'm starting with Andrew Ng's course on coursera about machine learning? Is it right step? ",
    "selftext": "Basically I'm a second year engineering student. \nI know engineering mathematics & data structures,  programming and little bit of stats.\n\n",
    "created_utc": "2024-11-01T11:33:44",
    "num_comments": 18,
    "comments": [
        "It's a good introduction to the topics. Make sure to do the newer python version instead of the old Matlab one.",
        "Try to connect that course with the fundamentals that you are learning on your engineering degree. Is a great course, I prefer the Stanford lectures, more depth on the math explanations.",
        "Great! Even I did Andrew Ng's ML course after completing my Engineering 1st year. Because you know Calculus(M1) and Linear Algebra(M2), trust me it's more than sufficient.\n\nI would also suggest to do his Deep Learning Specialisation course if you like the Neural Networks in the ML course, and then NLP specialisation if your interest grows further.",
        "Yep",
        "I would say focus on all the Math first. The Ng course deesn't cover core Mathy concepts. That can be a big hinderance if you intend to gain an in-depth understanding.",
        "Hey, I think the easiest way is to start building ML models of your own. I started by using model generation through [https://www.plexe.ai/](https://www.plexe.ai/) and iterated on the output and used it in Kaggle competitions",
        "No. Study the mathematics first.",
        "Andrew Tate course ?",
        "Yeah I'm doing python version, what should i do next?",
        "You mean Octave one?",
        "Thanks, can i dm?",
        "How can i understand maths can you tell me some resources?",
        "Can i dm you?",
        "Ig not octave.",
        "Ofcourse",
        "Check out my profile. I have answered this multiple times.",
        "3blue 1brown neural networks",
        "Sure!"
    ]
},
{
    "submission_id": "1ghc6v8",
    "title": "Can modern LLM models generate compilable C++ code nowadays?",
    "selftext": "Hi,\n\nI wanted to have ChatGPT/Anthropic generate a small Qt C++ library based on my 4 paragraph prompt which basically defines the requirements. I simply feed the prompt to the APIs. Both generated code which didn't compile. Is this the state of the art or am I missing something?\n\nthanks",
    "created_utc": "2024-11-01T11:22:55",
    "num_comments": 4,
    "comments": [
        "No llm can generate reliable code unless you know how to read and debug it. In English 80% correct is good enough. In software 80% correct is worthless.",
        "It’s not only C++, Python, Java, Go, Flatter also don’t run without tweaking and fixing llm output.",
        "What I am looking for was getting llm generate the source code files and having the possibility to download them via the api. However even with the anthropic API, I had to extract the code from a single text output and  create .h and .cpp files myself. Looks like the artifacts are not available for the API yet only for claude. \n\nCursor did a much better job for files but generated code was not usable still.",
        "[deleted]",
        "I am trying to understand what LLMs are capable of for real-world use cases."
    ]
},
{
    "submission_id": "1ghazl8",
    "title": "Is a MS in DS or AI worth it? ",
    "selftext": "I have a bachelors in compsi comp engineering with a couple of co-ops under my belt and am interested in a MS in ai/data science. I have the opportunity to add a year to my degree to get one. I really enjoy ML and DS to the point where I’d consider getting this degree even without the economic benefits. \n\nI guess my main question is: what are the benefits to getting a MS in AI or DS? I talked to my team lead at my current co-op and he said that you can hit a ceiling later in your career that a MS can help with. My hope is a MS will make me a stronger applicant to SWE jobs and will give me the ability to make the swap to more lucrative AI jobs. Have people found that to be true?\n\nPs. I also have been enjoying SWE and back end data design/algorithms. Maybe it’s not worth to switch because I like what Im doing already? I enjoy ML, but idk if I’d enjoy it in a corporate setting? ",
    "created_utc": "2024-11-01T10:31:31",
    "num_comments": 1,
    "comments": [
        "When I was about to get my BSc I started looking at AI jobs and about 90% asked for a masters or higher. I was offered a scholarship for research work, so I went in that direction. I feel like I've learned a huge amount in the 1.5 years I've been in the program, so it's been rewarding for me."
    ]
},
{
    "submission_id": "1gha40f",
    "title": "Learn Hopfield Networks, the model that won the 2024 Physics Nobel Prize",
    "selftext": "",
    "created_utc": "2024-11-01T09:54:01",
    "num_comments": 0,
    "comments": []
},
{
    "submission_id": "1gh9f6e",
    "title": "Best Machine Learning Courses for Beginners to Advanced",
    "selftext": "",
    "created_utc": "2024-11-01T09:24:57",
    "num_comments": 0,
    "comments": []
},
{
    "submission_id": "1gh8axo",
    "title": "Machine Learning for Finance",
    "selftext": "How many of you have worked with ML in the financial sector? And in trading? What has the experience been like? What skills are needed for someone who already works with ML to enter the industry?",
    "created_utc": "2024-11-01T08:38:23",
    "num_comments": 0,
    "comments": []
},
{
    "submission_id": "1gh6r2m",
    "title": "Beginner in ML: Is This Roadmap Complete or Missing Anything?",
    "selftext": "",
    "created_utc": "2024-11-01T07:30:27",
    "num_comments": 75,
    "comments": [
        "Overkill as hell thanks to a ton of redundancy.",
        "and then people train a deep model where a decision tree would do",
        "Just a quick look and this looks more like a Deep learning roadmap to me. You mention \"Deep learning frameworks\" but no other ml frameworks. I also think there is very little here about understanding and managing data.",
        "I think as a beginner the main thing is to actually get started. The more you learn, the more you learn about what you need to learn. It's very easy to waste time looking for the perfect roadmap",
        "I would say this is definitely an overkill roadmap because machine learning is vast and very hard to master, even when deep-diving into a specific topic or framework.\n\nThe best way to learn ML is to define your purpose: what are you learning ML for? If your focus is on research, you'll definitely need a lot of math. However, if you're learning ML to build pipelines or use it in software development, you don't need such a deep dive into math.\nThen, you can follow a specific roadmap. For example, if you want to focus on coding and building ML pipelines or implementing machine learning functionality in the background, you should follow a roadmap to learn MLOps.\n\nSome suggestive tips:\n\n - Don't deep dive into math if you don't have the time and patience. Just understand the concepts of math and equations, and try to implement them from scratch in Python. By following this approach, you'll gain a good understanding in a short time.\n - Refer to books; they're the best way to learn ML.\n - Understand frameworks and their functions deeply.",
        "This is no where near a complete road map.\n\nThis is the best single roadmap I've seen for machine learning basics for anyone who wants to cover all their basics and see interaction\n\nhttps://whimsical.com/machine-learning-roadmap-2020-CA7f3ykvXpnJ9Az32vYXva\n\nI'm not even joking around this, it's the single best resource I had for catching up on my basics earlier this year when I was interviewing for some pretty big companies for MLE and helped me ace them \n\nThe process itself understanding that, and the thinking that this roadmap helps build, will be key for interviews.",
        "too much bs, not enough stats and maths",
        "It's pretty thorough, I suppose.\n\nI would put Database and Data Eng into its own box though. Thats a big one.",
        "This roadmap is great actually idk why people hating\n\nHere's what I wrote on a different post tho:\n\nI would suggest a topic+Implementation oriented approach that allows you to follow any source to implement them. Don't get me wrong I love those books but I haven't read them front to back as it's unnecessary for my current research/projects.\n\nLearn these in sequence (some can be done simultaneously) (reply if you want resources)\n\nLearn these first:\nPython and Math (Linear Algebra, Probability theory, Calculus)\n\nThen the classical methods (basically optimization)\n1. SVM (support vector machines) and PCA (Principal Component Analysis) for classification\n2. Curve fitting (both gradient based and Bayesian) regression\n3. Fuzzy inference system\n\nThen deep learning (d2l.ai is a gem btw)\n1. MLP (Multilayer perception) aka Neural Net\n2. CNN (Convolutional Neural Nets)\n3. Sequence models (Recurrent Neural Nets, Long short term memory Nets)\n\nAfter this I don't think it matters what sequence you follow anymore. Let your interests guide you. but some topics that I think are important in general are,\n1. Autoencoders\n2. Transformers \n3. Causal inference \n4. Graph based models\n5. Mixture models\n6. SOM (Self organizing maps)\n\nLearn Pandas on a need to know basis:\nLearn Pandas as you go. Nobody really knows when exactly you'll need to master pandas and Numpy but you will at some point need it extensively, though not at the beginning. You'll only need some simple functions at first so maybe take a short crash course or just read the quick start docs. When the time comes when you feel like you should start taking an in depth view of pandas (in depth view of pandas just means you read the user guide front to back, which is not long lol), start doing that.\n\nI really hope this doesn't overwhelm you. This list of topics should get you to a point where you can just look at a book/paper/video, skim and say \"Hey I already know this\". \n\nAbstract thinking is key:\nThis field is fully abstract so be prepared and comfortable to think in the abstract all the time and accept it when you can't. Maybe it'll click at a later point of your education.\n\nPatience is key(I think you know this already):\nPeople think they can just hop into this field in a few months and understand everything. That's only possible for mathematicians and physicists. If you're not one of these two, be prepared to be in the long game. Have patience. Every line of math and code has some amount of thought behind it so it takes time.\n\nThe last two things are two of many reasons I love this field!",
        "You don't need to know ALL of those things. \n\nJust pick a problem, start to solve it, make mistakes and build on your knowledge. That flow chart you posted is complete overkill.",
        "Lots of unnecessary stuff. You don’t need to learn Optuna, Hyperopt and that Microsoft tool that all do similar stuff. Neither do you need to know commercial monitoring tools like Weights & Biases.\n\nYou also don’t need all of Java, Python, Golang, Kotlin. Focus on mastering on, probably Python, rather than knowing just a bit of each.\n\nBiggest red flag here is that any foundational mathematical understanding of machine learning is completely absent from this roadmap.",
        "YARML: yet another roadmap to machine learning.",
        "You forgpt \"Prompt engineering\" for one semester",
        "I'm so tired of seeing these roadmaps for \"how to be this\" in \"x amount of unit time.\"\n\nWho even defines what is a (insert AI/ML/Data) (insert Engineer/Scientist/Analyst/Loser)? Companies do. And as a matter of fact, they usually post what they want you to know in the job description.\n\nThe best thing, in my opinion, is to scrap job descriptions and visualize the technologies listed. Math and algorithms are already a given.\n\nAlso, focus on the industry or specific thing you want to do. As an example, if you want to learn computer vision, and you like the ocean, go chuck an ROV underwater and make a project. You may not learn on a linear path, but boy, will you learn how to build a streaming application with gstreamer and c++ to collect data for underwater photogrammetry. Pair that with an off the shelf textbook or Coursera course on computer vision fundamentals, and your set.",
        "Remindme! 3 days",
        "Remindme! 3 days",
        "What are you trying to be?   Your organizational size will matters and roles I list are for a larger org.  \nYou have MLOps that maintains the ML environments and how ML is done to Org standards.   Then data engineers that get the data ready optimize data pipelines.    Data scientists that experiment and select the model and train or tune it.  Then ML engineers that need to productionize to your org standards, think software engineer specializing in ML.   You could lump in red teams etc for extra steps.   If it is a small organization you could do many of these roles.",
        "To hell with this roadmap! \nChances are you would spend all your time refining this shit, rather than do something.\n\nI would recommend \n\n1) Get good with coding : At least be able to code what you can think of,  you will need this to make projects. \nFor ML, I would suggest getting started with python, follow a good playlist from YT.\nAlso, practice database questions on LC using pandas\n\n2) Get familiar with ML basics: Follow a good finite source, I would recommend the HOML v3 book.\n\n3) BUILD PROJECTS & DO INTERNS \nYour previous journey was meant to serve you on this step, start with small projects then go big, learn step by step.",
        "This is just a bucket of topics .. which you have no chance of covering in their entirety.\n\nDo you think hiring managers have more than a couple of these topics in their heads?\n\nA useful chart would contain just a handful of core topics.",
        "This assumes you already know calculus and it doesn’t include anything about genAI/LLMs, in case you were interested in going that route.",
        "Human life is not long enough to learn these",
        "It's just a bunch of names that don't really resemble any job out there. you don't learn LIME isn't the same as learning Tensorflow. What the hell is Sacred? Feature store is more in the field of MLOps.",
        "you're going to get bored following a roadmap like this. as karpathy recommends, learn in a \"depth-first\" fashion: decide what you want to build, and then learn whatever is needed to build it.",
        "This roadmap is missing a lot:\n\n* It doesn't tell you what to learn about any given topic\n* It doesn't say much of anything about dependencies between topics\n* It distorts the importance of one-off topics compared to fundamentals\n\nThink about it this way. We all start at the same/similar places when we graduate high school, then immediately branch off into different majors in college, even farther picking electives, and then a million different directions in the working world and grad school. It's easy enough to work from the top down and trace somebody's path back to the start, but you lose out on whatever context and uncertainty was at every step of the way. It's better to start with broad steps that don't preclude you from doing what you want and make more specific ones as you progress.",
        "As mentioned by many before what you need to learn depends on who you want to work for. But as to what comes to basic machine learning and general data-analytics you want to cover the basics. Like [Bayes theorem](https://youtu.be/HZGCoVF3YvM), linear algebra, numerical simulation (inverse problems), etc. I have a strong recommendation for [StatQuest](https://www.youtube.com/@statquest/playlists). Then you can apply the learned theory to datasets using [scikit-learn](https://scikit-learn.org/stable/) and [Kaggle](https://www.kaggle.com/).\n\nWhy you **should not** deep dive to deep learning and neural networks is because a huge majority of machine learning problems can be solved using traditional non-learning methods faster, more efficiently, more robust and more accurately than with huge neural networks. And even when the traditional tools of statistics are not enough basic ML is usually well enough, such as methods and tools available in scikit. Only after every other options is exhausted should one go for deep learning.",
        "You can’t be serious… you only need about 1/3 of these. Much of these are not in ML engineer job description",
        "So I can follow the stand running up the middle and avoid studying all those? Great news!",
        "Complete? it's a lot than actually what is required",
        "More than half of the stuffs you listed on this chart can be replaced by ChatGPT",
        "I honestly think this is a great roadmap to have in your mental model of the curriculum. IDK why people are saying you won't need most of these. Like Im pretty sure a handful of people from deepmind and Microsoft research, sakana AI follow these. My lab uses all of these lol. \n\nTbh this subreddit might not be the best place to seek advice on a full fledged curriculum on ML engineering and AI. Try seeking answers from stack exchange.",
        "Break it down to these simple components (in no particular order, as each person will say something different, and I’m not going to start a war in the comment section):\n\n1. Projects\n2. Code\n3. Math\n\nWork on each of these every day. Working on a project and coding can sometimes be the same thing; however, you can also take courses and read documentation for libraries, which is what I mean by “code.” Math is something I’ve done every day for years—it should just become a way to expand your skills and strengthen your understanding of the algorithms you work with. \n\nMost importantly, interleave your practice of these things. Try to get a bit of each done each day instead of spending a week on one and stopping for a month. Keep it simple; overlearning can be your worst enemy. I say that as someone with a tendency to over-optimize.",
        "My 2 cents: join a team, spend a lot of time understanding how things work, regardless of the tech stack that they use. The important skill is to learn how to solve hard problems, and that can be in many of these boxes",
        "Not enough stats and maths.",
        "Never used Kotlin or Golang. Maybe I'm just a crappy data scientist, but for me, this seems bloated",
        "Wow this list sucks\n\nLooks like someone just put everything related to ml in one list. You get jack of all trades master of none in a scope of 5 or 10 years. When most of those skills won't be relevant anymore.",
        "FYI despite this being styled to look like roadmap.sh, it is not one of ours.",
        "I am an ML Engineer for some time now. I don’t know 50% if things written here and haven’t even heard of 20%z",
        "Hi everyone! I’m a beginner in machine learning, and I have a basic understanding of Python. I recently came across a roadmap and wanted to check with more experienced folks here: Does it cover the essential topics for someone starting in ML, or is there anything I should add or approach differently? Any insights or advice would be super helpful. Thanks in advance!",
        "Also a lot of obsolete and irrelevant boxes.\n\nLooks like someone scraped resumes from 2022 and took all the keywords.",
        "Is there any roadmap or steps you would suggest me to take or look at?",
        "How about this one? [https://edu.machinelearningplus.com/s/pages/roadmap](https://edu.machinelearningplus.com/s/pages/roadmap)",
        " or a simple logistic model will very well do",
        "Do you have any roadmap recommendations? I am a beginner and found this roadmap on github. 😭",
        "I started learning python and its been about 2-3 months and I was wondering what I should learn next and what would be a good source. I was hoping to see a good roadmap to get insights on what I would be needing to learn to become a machine learning engineer.",
        "Thanks a lot. That makes sense. Do you have any books that you would suggest to get into ML?",
        "Which place are you from?",
        "Thanks I am getting mixed reviews which is making it complicated. Will definitely do that you said",
        "Thanks for the help",
        "Is it still an overkill if I stick to the yellow boxes?",
        "I will be messaging you in 3 days on [**2024-11-04 14:40:13 UTC**](http://www.wolframalpha.com/input/?i=2024-11-04%2014:40:13%20UTC%20To%20Local%20Time) to remind you of [**this link**](https://www.reddit.com/r/learnmachinelearning/comments/1gh6r2m/beginner_in_ml_is_this_roadmap_complete_or/luv3tah/?context=3)\n\n[**CLICK THIS LINK**](https://www.reddit.com/message/compose/?to=RemindMeBot&subject=Reminder&message=%5Bhttps%3A%2F%2Fwww.reddit.com%2Fr%2Flearnmachinelearning%2Fcomments%2F1gh6r2m%2Fbeginner_in_ml_is_this_roadmap_complete_or%2Fluv3tah%2F%5D%0A%0ARemindMe%21%202024-11-04%2014%3A40%3A13%20UTC) to send a PM to also be reminded and to reduce spam.\n\n^(Parent commenter can ) [^(delete this message to hide from others.)](https://www.reddit.com/message/compose/?to=RemindMeBot&subject=Delete%20Comment&message=Delete%21%201gh6r2m)\n\n*****\n\n|[^(Info)](https://www.reddit.com/r/RemindMeBot/comments/e1bko7/remindmebot_info_v21/)|[^(Custom)](https://www.reddit.com/message/compose/?to=RemindMeBot&subject=Reminder&message=%5BLink%20or%20message%20inside%20square%20brackets%5D%0A%0ARemindMe%21%20Time%20period%20here)|[^(Your Reminders)](https://www.reddit.com/message/compose/?to=RemindMeBot&subject=List%20Of%20Reminders&message=MyReminders%21)|[^(Feedback)](https://www.reddit.com/message/compose/?to=Watchful1&subject=RemindMeBot%20Feedback)|\n|-|-|-|-|",
        "You kinda have to learn these in a year or two when doing a PhD lol",
        "^[Sokka-Haiku](https://www.reddit.com/r/SokkaHaikuBot/comments/15kyv9r/what_is_a_sokka_haiku/) ^by ^SuperTankMan8964:\n\n*More than half of the*\n\n*Stuffs you listed on this chart*\n\n*Can be replaced by ChatGPT*\n\n---\n^Remember ^that ^one ^time ^Sokka ^accidentally ^used ^an ^extra ^syllable ^in ^that ^Haiku ^Battle ^in ^Ba ^Sing ^Se? ^That ^was ^a ^Sokka ^Haiku ^and ^you ^just ^made ^one.",
        "i found it on github. Is there any suggestion you would like to give. any help would be appreciated.",
        "Roadmaps that are all encompassing are a waste of time. The breadth of subjects covered would take someone a decade or longer to complete.",
        "Thanks for the insight. So would suggest me to stick this roadmap and add the changes you guys talked about?",
        "Sadly no I don't have any roadmap available. There is a lot of useful stuff there but there is also a lot that u think is overkill to start thinking about before you have some experience (and maybe not even then).",
        "Make something, anything.\n\nI read half a book on python, then got to work on my goal: multi-gpu sdxl fine-tuning script.  Big enough that it actually accomplished something, small enough that I could pull it off.  And it was relevant to my current job.  It was enough to get experience with some libraries and actually start to understand some of what other devs were talking about.\n\nFor there, you'll easily recognize what you're missing.  For me that's bayesian statistics and it's relationship to machine/deeplearning models.  So now I'm reading a book on bayesian statistics so that I can better understand other books on deep learning.",
        "Look up Andrew Ng's ML course. I think it's still free and it's one of the best intros out there. If you feel you're missing some of the skills or knowledge you need for it, you can always pause the course and work on those. But if you've got python and basic maths, that's the best next step",
        "Make a start with the math if you haven't already. It won't always be clear why you're learning what you are, but it will eventually make sense",
        "Good luck on your learning journey, my friend.\n\nThe only other thing I would change is to put the \"start building\" flag first, because step 1 to learning is building something.\n\nAnd get rid of the finish flag, because learning is a life long journey 🤓",
        "My brother in christ you haven't defined jack shit.\n\nLet's pick a \"yellow box\" as you put it. Let's pick \"Python\" - how much Python do you need to learn before you allow yourself to progress on to the next \"yellow box\"?\n\nYou've over thought this so much. Just stop. Pick a data set. Solve the problem and start learning.",
        "Deep learning is an active field with lots of active research areas. It is the same case with all the rest of the fields in the image. You cannot really learn \"deep learning\" in one or two year. To read all the papers published in a year in deep learning probably takes a lifetime. Clearly there's a difference between memorizing the common buzz words in a field and learning a field. The latter to me means mastering a subject / being in the top 5/10% of the people who \"know the field\"",
        "Pick a single tech stack and start doing projects. You only really need to learn one Linux os and one cloud platform and one monitoring solution. It’s good to have the compsci and math background but it’s not required. Same with leetcode, I’d skip that shit honestly until way later.",
        "what do you recommend?",
        "I dont think thats the purpose of a roadmap, obv that's very time consuming. But you need to know enough so you can focus deeper on specific areas and solve problems that matter.",
        "Start with statistics first: frequentist and Bayesian. That’ll take anywhere between 3-6 months depending on your background. Then calculus, then linear algebra. Once you got it under your belt move to mathematics of deep learning. Next start writing ANN from scratch using Python, if that’s what you’re comfortable with. If not dive deep into Python. Once you’re past this point you won’t need any roadmap. You’ll know where to go from there. Nobody will tell you better than your own experience.",
        "thats great to know. thanks i'll do that",
        "“good to have the math background but not required” is why the space is shit right now",
        "The maths and comp sci is absolutely required if you want to do anything innovative, or even useful.\n\nPeople who blindly apply algorithms to datasets will be automated in the next few years.",
        "Elaborate bit more please",
        "Roadmaps are a complete waste of time 👍",
        "> Nobody will tell you better than your own experience.\n\nAnd then you get to interview \"self taught\" candidates with massive holes in their knowledge failing to explain basics\n\nIt takes a really solid background to know what you don't know and steer your learning in the right direction.",
        "the shift from bayesian from frequentist can take years for a granular understanding i believe",
        "ML Ops dont need the math at all. RS need all of it. MLE need a concept level understanding at best. It all depends what you’re using ML for.",
        "Agreed. My point is to start with foundations."
    ]
},
{
    "submission_id": "1gh4x1f",
    "title": "Would you recommend a masters degree in data science?",
    "selftext": "I'm currently working as a data quality and I want to do masters degree in data science and machine learning. Will it be worth it to have a masters degree in this field or not? and if not what would you recommend?\nI'm 24 M, graduated business school",
    "created_utc": "2024-11-01T06:06:09",
    "num_comments": 13,
    "comments": [
        "From what i've gathered, the consensus on here is that it is better to get a Master of Statistics or Computer Science and learn what you are missing. A lot of data science Master degrees are cash grabs, not to say that there aren't good ones, but you are likely better off with one of the other previously mentioned.",
        "I got one and it opened many doors for me. Look at all of the classes that the program offers and try to determine if they focus on technical skills and theoretical skills. Look for a reputable school. \n\nHonestly you already have a foot in the industry. Part time masters plus taking on additional data science related work might allow for an easy pivot into a data science role. Work that position for a few years and then you have a degree and work experience in the field (on both sides data quality and data science). \n\nIt's the breaking into the industry that's hard. I think you have a solid case for it and a clear path to make it happen instead of rolling the dice and hoping for a new company to bite.",
        "Anything that's easy to get into is not worth the money.",
        "Definitely research your program if you do get a MS. Some are cash grabs and don't teach you anything functional but others are worth every penny. I would say read up more on what skills you want to learn first and then see which ones would fit that criteria.",
        "My take on this will be to go for the higher education. Since you are still young, you can grasp it fast.",
        "No",
        "Hey, I think the easiest way is to start building ML models of your own. I started by using model generation through [https://www.plexe.ai/](https://www.plexe.ai/) and iterated on the output and used it in Kaggle competitions",
        "so you're saying it's better to have a masters in CS or statistics in order to understand the background then I could go for a data scientist job?",
        "I'm looking for scholarships mainly because I heard prices that's quite shocking",
        "I think getting a masters in cs is better because you can become a data scientist with a cs degree, but it is harder to get a swe role with a masters in ds. So it’s limiting your options a little.",
        "If you want to be a data scientist, then the adequate degrees for this are: (1) statistics (2) econometrics (3) data analytics or data science, given that the curriculum has a vast overlap with a master’s in statistics.*\n\nCS is an irrelevant advice for data science. It has literally nothing to do with data analytics or data science, and even if they try to marketize themselves with “ML/AI specialization”, they still remain mostly irrelevant, due to the lack of:\n\n* advanced probability theory / deep dive into probability distributions; graduate level mathematical statistics; regression analysis; monte carlo; bayesian methods I-II; stochastic processes; time series analysis I-II; network science; multivariate analysis; causal inference; etc. etc. etc.",
        "I think your statement could be refined. \n\nThere are computer science departments that let and encourage students to take statistics courses as electives or even as a major sequence. Even taking mathematical statistics at a less rigorous level, like Wackerly’s text, is pretty useful with CS knowledge. \n\nHowever, yes, computer science doesn’t immediately signal knowledge of statistics. \n\nAlso for a business major it’s easier to get into a rigorous statistics department after multivariable calculus, linear algebra and probability than the many more missing prerequisites for computer science",
        "Thanks for the refinement."
    ]
},
{
    "submission_id": "1gh4ktk",
    "title": "When choosing an LLM proxy server like TensorFlow, LiteLLM, Helicone, or MLflow, what features or capabilities are most critical for your use case?",
    "selftext": "I’d love to learn from others’ experiences. When it comes to scalability, monitoring, security, and integration capabilities, are there specific features that you found essential for making your projects successful?",
    "created_utc": "2024-11-01T05:49:33",
    "num_comments": 7,
    "comments": [
        "Tensorflow is an llm proxy server?",
        "Uhh, why do you need a proxy server? What problem are you trying to solve, or are you just trying to build your own competitor",
        "I thought Cloudflare tunnels, Ngrok, [https://pinggy.io/](https://pinggy.io/) , perhaps even tailscale are proxys. How is TensorFlow a proxy server?",
        "Hey! Co-founder of Helicone here—happy to clarify.\n\nFirst, **TensorFlow** and **MLflow** are more focused on traditional ML workflows, not directly designed as LLM proxy servers. For LLM-specific needs, LiteLLM and Helicone are better suited.\n\n**LiteLLM** acts as a unified gateway, letting you seamlessly switch between various LLM providers through one interface. **Helicone** specializes in observability and improving LLM applications, we offer a gateway with many features, and tools for prompt engineering, evaluations, etc.\n\nLiteLLM and Helicone work well together: many users utilize the LiteLLM SDK and log traces directly to Helicone.\n\nHope this helps!",
        "Love these half assed questions",
        "Yeah mlflow vs tensorflow? Are you comparing them because they have flow in the name?"
    ]
},
{
    "submission_id": "1gh3hhn",
    "title": "Quickest method of Neural Network inference?",
    "selftext": "I’m in a situation where I need to make predictions from a loaded NN Model many times.\n\nNote, I am only calling .predict() on small batches of data at a time. Not sure if this is relevant, but it may be.\n\nI am essentially running a simulation (~10k iters) where on each iteration I am making a call to predict from the network.\n\nI need to speed this up massively. \n\nIn terms of machinery, I am CPU bound (no GPU/TPU).\n\nThe current best solution I have found is to convert my Model to an ONNX model, create one InferenceSession object, and pass this object to my various workers (to distribute the work among my CPUs). \n\nONNX is optimised for inference so this is decent enough. I have tinkered with the inter/intra op thread properties and so on.\n\nI’ve also tried quantising my model, which helps a bit but not much.\n\nAre there any better ways? Any other techniques anybody is aware of?",
    "created_utc": "2024-11-01T04:49:24",
    "num_comments": 2,
    "comments": [
        "Use openvino toolkit",
        "Use the sliding windows method, or a hashmap. /s"
    ]
},
{
    "submission_id": "1gh3ahm",
    "title": "Bert Classifier ",
    "selftext": "Can anyone please share any blog or tutorials for multi label classification using BERT?\nGonna make a project around it for a hackathon",
    "created_utc": "2024-11-01T04:37:53",
    "num_comments": 5,
    "comments": [
        "be honest, did you google it?",
        "I would first start with your multi label architecture without BERT. Then, using BERT is as simple as pre-pending your architecture by using it as an embedding. \n\nA general multi class classifier you may consider is an encoder-only recurrent network with a classifier head. Depending on your dataset size, for your encoder you may choose among RNN, LSTM, stacked LSTM, any of the previous with attention mechanisms, transformers, etc.",
        "Yes yes found some resource too",
        "hmm🙌🏻",
        "Yes, I also suggest this, understand the whole pipeline from scratch using LSTM or GRUs then just switch the elements for BERT, this will be very easy"
    ]
},
{
    "submission_id": "1gh1n08",
    "title": "microk8s vs minikube, which is better for kubeflow? ",
    "selftext": "Which kubernetes distribution is most recommended for learning local kubernetes and running local kubeflow pipelines?",
    "created_utc": "2024-11-01T02:45:03",
    "num_comments": 0,
    "comments": []
},
{
    "submission_id": "1ggzx8t",
    "title": "\"What Makes GPUs so Powerful? Matrix Multiplication!\"",
    "selftext": "\n\nhttps://preview.redd.it/xm1m3w4as8yd1.png?width=1280&format=png&auto=webp&s=f7ddc7e1e90137325b0653d931a7f807f027c70e\n\nGPUs have become one of the most essential (and pricey) pieces of hardware today. In my latest video on Vizuara’s YouTube channel, I explore why GPUs are so powerful, what they do differently, and how they went from boosting gaming graphics to transforming AI: [https://www.youtube.com/watch?v=wYXARXhSoSs&feature=youtu.be](https://www.youtube.com/watch?v=wYXARXhSoSs&feature=youtu.be)\n\nBehind GPUs’ power lies their ability to perform massive matrix multiplications quickly and in parallel. Matrix multiplication is at the heart of 3D graphics rendering and AI model training. \n\n\n\nIn gaming, each 3D object is broken down into vertices and triangles, and every time the game’s scene refreshes, the GPU has to rapidly recalculate positions, textures, and lighting using matrix math. A high-quality game renders millions of vertices and triangles. Without a GPU, gaming as we know it simply would not be possible.\n\n\n\nBack in 2008, I remember trying to run GTA San Andreas on an old PC with no graphics card. I had to lower the resolution just to make the game playable. Meanwhile, friends with dedicated graphics cards were enjoying seamless high-res gameplay. That was my first real exposure to what GPUs could do. \n\n\n\nAt the time, NVIDIA was still mostly focused on enhancing gaming visuals. But the same matrix multiplication operations that GPUs used for graphics ended up being perfect for AI once deep learning emerged.\n\n\n\nIn fact, as AI researchers began training larger neural networks, they found that the same type of repetitive matrix math used for 3D scenes also applied to neural network computations. NVIDIA was able to transition smoothly from focusing on gaming to being at the forefront of AI hardware. When I first heard about NVIDIA's growing role in AI, I was unsure if it would really take off. Now, with their stock soaring and AI demand at an all-time high, it is clear that they found gold in an unexpected market.\n\n\n\nIn today’s AI world, companies like Google and OpenAI are chasing advancements in AI like miners in a gold rush. NVIDIA, however, supplies the “shovels” – the GPUs that make this AI revolution possible. \n\n\n\nThese GPUs, priced from $10,000 to $40,000 for high-end models, perform the matrix multiplications and parallel computations needed for training AI models at scale. With only so many chips available and the demand rising, NVIDIA has quickly become one of the most valuable companies in tech.\n\n\n\nFor anyone curious about why GPUs are so vital to both gaming and AI, my video breaks it down. I cover the specific matrix operations and transformations GPUs handle, explaining why these devices are worth every penny for both gaming enthusiasts and AI researchers.\n\n\n\nThe journey of GPUs from niche gaming hardware to AI powerhouse is an inspiring story of innovation, adaptation, and surprising new applications. If you are interested in the deeper mechanics behind these breakthroughs, check out the full breakdown in my video: [https://www.youtube.com/watch?v=wYXARXhSoSs&feature=youtu.be](https://www.youtube.com/watch?v=wYXARXhSoSs&feature=youtu.be)",
    "created_utc": "2024-11-01T00:26:28",
    "num_comments": 10,
    "comments": [
        "Wow 🐭 linkedin's post on reddit",
        "They are not actually powerful.\n\nThe only difference is the quantity. A simple CPU  has like between 6-10+ cores while a GPU has like in ten thousands.\n\nWhich helps in multi processing. Now since they have large quantities of cores in order to be feasible their power is very much less than a single CPU core.\n\nWhich separates their use-cases.  CPU can easily do general complex calculations while GPUs can run multiple weak things at once. These include common background updations such as rendering in games and AI training .\n\nSince these tasks do not require a powerful processor but a large number of cores, GPU are perfect choice in these.\n\nWhile if you have something(a single task which requires  heavy complex calculations) you will use a CPU.",
        "Wow. Such news",
        "Not only that, but massive multi-threads by block for any kind of code.",
        "I asked this question in an interview for an ML engineer position and the candidate (who had several ML systems and certs under his belt) didn’t know. Common knowledge is not so common.",
        "Common but also trivial for 99% of the ML field. Unless you're righting cuda interfacing stuff or actually working on GPU hardware this is just a not so fun factoid.",
        "I wasn’t looking for a factoid. To be able to correctly answer, the candidate would need to have an understanding of how ML works at the most fundamental levels, and explain how tensors work and build it all the way up the stack.",
        "How is this relevant to how tensors work?",
        "Then ask them explicitly what matrix/tensor operations are. Asking an open-ended question (about GPUs of all things) like this leads some to believe that you’re asking for specific, technical mechanisms like caching and VRAM (not relevant to MLE unless you’re writing CUDA/HPC code).",
        "Thanks for your unsolicited advice. In fact, HPC and parallelization was critical for the role."
    ]
},
{
    "submission_id": "1ggzsqi",
    "title": "How to label Object if the object are rectangular object taken from an angled, slightly elevated perspective, giving a partial view of the top and one of the sides",
    "selftext": "I am a students currently learning about computer vision, in my class i have been assigned to train yolo-obb (oriented object detection) using custom dataset\n\n\n\nwhen i try to label my own dataset, i realized that its not fully flat giving some partial view of the sides\n\n\n\nshould i label all the visible parts (image 1) or just the top parts like image 2?\n\n\n\napologize if my english is bad\n\n[Image 1](https://preview.redd.it/ym3rvb5dq8yd1.jpg?width=1600&format=pjpg&auto=webp&s=e7c4f9a52b1aa6352f0ebe7edb0aade77a744369)\n\n[image 2](https://preview.redd.it/vl65t07fq8yd1.jpg?width=1271&format=pjpg&auto=webp&s=8b4940a1d9025ea25520433248034f95b3ddf2de)\n\n  \n",
    "created_utc": "2024-11-01T00:15:40",
    "num_comments": 0,
    "comments": []
},
{
    "submission_id": "1ggyct5",
    "title": "Looking for book/courses that includes: training VAEs, overview of main machine/deep learning models, ideally in pytorch",
    "selftext": "I'm currently reading \"Machine Learning for Dummies\", and had planned to read \"Deep Learning for Dummies\".  But the book is terrible.  Yeah, it covers the basics.  But the descriptions and the code are in completely different chapters.  By the time you reach the code, you've already forgotten the description.  Plus, they barely explain the code.\n\nSo, I'm looking for a replacement book/course to cover all the basic machine/deep learning models.  Ideally using pytorch, since I plan to use native pytorch FSDP (as apposed to something like accelerate) in future projects.  And also ideally in-depth covering of training VAEs, since that is a current topic I'm really interested in understanding.\n\n[http://neuralnetworksanddeeplearning.com](http://neuralnetworksanddeeplearning.com) looks kinda basic.  I'm familiar with most of those concepts from reading a few chapters of \"Understanding Deep Learning\".\n\n[d2l.ai](http://d2l.ai) looks good, but I didn't see anything on VAEs in the outline.\n\nBackground: I'm a hobbyist, with plans to later fine-tune LLMs & text-to-image models for work use.   Math learned: linear algebra, calculus, basic statistics/probability.  Currently reading \"A Student's Guide to Bayesian Statistics\".  Can program some basic stuff in python. Like I've written a multi-gpu sdxl fine-tune script with diffusers & accelerate FSDP, with custom dataset classes & validation functions and other custom stuff.  Currently working on porting that to sd3.5 medium, and CLIP fine-tuning (I don't like the idea of training CLIP through the UNET).\n\nThanks in advance!\n\n",
    "created_utc": "2024-10-31T22:24:11",
    "num_comments": 0,
    "comments": []
},
{
    "submission_id": "1ggxm3q",
    "title": "Should I post my notes/ blog on machine learning?",
    "selftext": "\nhey guys,\n\ni am a masters student in machine learning (undergrad in electrical and computer engineering + 3 years of software/web dev experience). right now, i’m a full-time student and a research assistant at a machine learning lab.\n\nso here’s the thing: i’m a total noob at machine learning. like, if you think using APIs and ai tools means you “know machine learning,” well, i’m here to say it doesn’t count. i’ve been fascinated by ml for a while and tried to learn it on my own, but most courses are really abstract.\n\nturns out, machine learning is a LOT of math. sure, there are cool libraries, but if you don’t understand the math, good luck improving your model. i spent the last few months diving into some intense math – advanced linear algebra, matrix methods, information theory – while also building a transformer training pipeline from scratch at my lab. it was overwhelming. honestly, i broke down a couple of times from feeling so lost.\n\nbut things are starting to click. my biggest struggle was not knowing why and how what i was learning was used. it felt like i was just going with the flow, hoping it would make sense eventually, and sometimes it did… but it took way longer than it should have. plus, did i mention the math? it’s not high school math; we’re talking graduate-level, even PhD-level, math. and most of the time, you have to read recent research papers and decode those symbols to apply them to your problem.\n\nso here’s my question: i struggled a lot, and maybe others do too? maybe i am just slow. but i’ve made notes along the way, trying to simplify the concepts i wish someone had explained better. should i share them as a blog/substack/website? i feel like knowledge is best shared, especially with a community that wants to learn together. i’d love to learn with you all and dive into the cool stuff together.\n\nthoughts on where to start or what format might be best?\n\n\n\n",
    "created_utc": "2024-10-31T21:33:48",
    "num_comments": 23,
    "comments": [
        "Notion sites are cool and free 🙏\np.s. GitHub .md files work just as well",
        "If you do post it please update here. I would like to read them",
        "Do it for yourself. Write as if other people are reading your content, regardless of whether or not its actually useful to other people. Teaching is one of the best ways to fixate new knowledge and clarify gaps.",
        "Yeah I d love to learn them",
        "That's amazing dude. I also had similar experience. I have completed most of the math and did the MITx MAchine learnign via edX. I think we should chat sometime about it. Now I am completing the fundamentals of Molecular Biology with the end goal of building Models on Cancer Genomics.",
        "Would really appreciate the link too! \n\nBy the way, I'd like to ask how your experience was applying for master's degrees in this field. From what you said, am I correct to assume your background wasn't already ML heavy and rather more focused on engineering and software engineering? \n\nHow did you approach the application process especially after graduation? I'm graduating soon and focused mostly on systems programming + software engineering throughout my undergrad, but only recently I've developed more of an interest in machine learning and I'm wondering how to make that transition now that it's a bit late to do undergraduate research in the field.",
        "Obsidian using md files to organize your notes.\n\nhttps://quartz.jzhao.xyz/\n\nOr jupyterbook.",
        "Please do, looking forward to reading them",
        "Please do🙏, and if and when you do, please post the link here as well.",
        "Please go ahead and post them in a platform that allows for interactivity through questions and solving challenges. Notion + Git is a good start as recommended above",
        "Please, do it. I’d be interested to read them as well.",
        "Yes you should my guy",
        "Hey I was in a similar position and I started posting my blogs too (shamless self plug: ym2132.github.io) \n\nIt’s been a very fun process and definitely helped me learn a whole lot more. GitHub pages + Quarto is an easy way to get nice blog pages going!",
        "Please do",
        "If you are interested I've also some notes I can share. They are mostly about ML, LLMs and concepts like RAG etc. Lately I have been thinking about creating some posts articles etc sharing what I have gathered. I don't know if I have the time to post X articles every Y, but I would love to spend some free time to share knowledge. Let me know if I can contribute to your idea",
        "I’d be interested to read them as well.",
        "Notion is great! I have created a few of my notes there i think Notion + Github would be a good combo \nBecause i do want to add exercise problems people might want to solve :)\n\nBut how would i interact with people? I mean if they had doubts/ questions or things i could improve",
        "Amazing HustleHunk, I would love to be a part of the team building the models. I am interested in the application of ML for Medical/Healthcare solutions.",
        "Give us an update if you do.",
        "you can create a form in the website which will take the queries and store them in a database. When you can manage to find time, you may create a separate post on the most common queries.",
        "I’ve been going back and forth about this a lot honestly, and I think for the interactions you’d have to rely on a service. That could be forms or that could be something simpler like an email that’s sent. \n\nIn any case, my issue has always been interactivity, like being able to let people change parameters of a model for example",
        "Excited to see what you’d cook up ! \n\nI’d say a discord server works best."
    ]
},
{
    "submission_id": "1ggxjgf",
    "title": "Lost!!! Help",
    "selftext": "\n\nSo, I'm just starting out with ML and honestly have no idea where to begin! If anyone's got some solid, free resources that can lay things out in a systematic way, I'd be super grateful. And any tips on how I should I start with  would be awesome too.",
    "created_utc": "2024-10-31T21:28:57",
    "num_comments": 21,
    "comments": [
        "I would say start with the Math. All these fancy ML models are nothing up Computational Math aka Math done by the processor.   \nStart with Linear Algebra and Differential equations. For that I would suggest Dr. Steve Brunton's Youtube Channel.\n\nThen move on to Vector Calculus. Again Dr. Brunton. Here are the links:\n\n[https://www.youtube.com/playlist?list=PLMrJAkhIeNNTYaOnVI3QpH7jgULnAmvPA](https://www.youtube.com/playlist?list=PLMrJAkhIeNNTYaOnVI3QpH7jgULnAmvPA)  \n[https://www.youtube.com/playlist?list=PLMrJAkhIeNNQromC4WswpU1krLOq5Ro6S](https://www.youtube.com/playlist?list=PLMrJAkhIeNNQromC4WswpU1krLOq5Ro6S)\n\nThese two playlists are gold mine.\n\nOnce comfortable with these two, move on to Probability and Statistics. I would recommend MIT's offering via the platform edX.\n\nOnce you are comfortable with LA, Calculus, ODE & PDEs, Probability and Statistics, then take a systematic approach to learn Python and associaited libraries. Go for Pandas, followed by NumPY and the rest. All that covered, understand that you are now equipped with the tools of the trade. \n\nResearch and pick an area that interests you the most. Lets say Deep Learning for animal kingdom. Start understanding DL and build a model for it. Slowly but steadily you would have all that it takes to build any model to tackle any problem that fascinates you. \n\nP.S. Understand its a long arduous journey and consistency is the key. By Consistency I mean adaptability as life is non-linear.",
        "Go for Krish Naik and campusx. They have Playlist of ML and DL. Also there is ML Playlist by Andrew Ng on youtube.",
        "Lol I just posted something that could help you\nhttps://www.reddit.com/r/learnmachinelearning/s/jrzpV6OkVl\n\nLet me know what you think?",
        "- Start with Andrew Ng. There’s an updated playlist on YT of his amazing class.\n\n- Also have a goal in mind. Is there some project that you want to do? Is there a particular area that you’re interested in.\n\n- Always keep in mind the first rule of ML - always ask yourself “do I need to use ML for this project?”\n\n- Create a project you’re interested in and try to complete it end to end\n\n\nGood luck. Also here’s an article that has some great lessons for anybody interested in ML, especially someone just starting out: https://medium.com/@levine.seth.p/learning-from-machine-learning-sebastian-raschka-mastering-ml-and-pushing-ai-forward-responsibly-aac39bd4af83",
        "Start with Andrew Tate if you already feel like lost",
        "Hey, I think the easiest way is to start building ML models of your own. I started by using model generation through [https://www.plexe.ai/](https://www.plexe.ai/) and iterated on the output and used it in Kaggle competitions",
        "\\- [https://medium.com/bitgrit-data-science-publication/a-roadmap-to-learn-ai-in-2024-cc30c6aa6e16#aed3](https://medium.com/bitgrit-data-science-publication/a-roadmap-to-learn-ai-in-2024-cc30c6aa6e16#aed3)  \n  \n\\- [https://course.fast.ai](https://course.fast.ai)",
        "Wow, thanks for the answer! I’m actually working on math and stats right now, so  I’ll definitely check out these playlists.",
        "And do you think it's worth watching linear algebra by Gilbert strang over this one?",
        "Could you please share the link of linear algebra playlist?",
        "Thanks for the recommendation I'll definitely check them out.",
        "Cant find the notes, can you ping me when you Will do?",
        "Hey, thanks for linking your post! Honestly, I get that lost feeling with ML too. It would be super helpful to see your notes if you share them somewhere. Having the math and concepts broken down by someone who's been through the struggle would make a big difference, especially for beginners like me.\n\nKeep me posted if you decide to put your notes out there—I'd definitely check them out!",
        "Thanks for the advice! I'll surely keep that in mind and thank you for the resources and the article will surely check out.",
        "Haha, didn't know Andrew tate was teaching ml now.",
        "Cool man. Feel free to reach out anytime ..",
        "The MIT one I use as reference. I would use it future as and when I optimize my models. So yes while learning the math I prefer the applied approach. Hence LA with ODEs and Vector Calculus with PDEs.",
        "The differential equations one is applied LA. Watch the introductory video, Dr. Brunton clearly mentions why he prefers this approach.",
        "Thanks, will definitely reach out if I need anything.",
        "Thanks mate for your response",
        "You are welcome bud! I truly believe one never stops learning the math. Something or the other always crops up as one starts building complex models.\n\nThese days I'm into tensor calculus; the basis of tensors in Deep Learning."
    ]
},
{
    "submission_id": "1ggx4yk",
    "title": "ML Engineer techstack - 2024",
    "selftext": "All help is appreciated.\n\nFor anyone planning to switch to MLE as a job role from big data/backend, what tech stack would you recommend. I understand most of the fundamentals remain the same, but there must be some preference over tools used in most companies?  \n\n\nFor example: in Data, that'd be Spark & Airflow\n\nAny similar ideas for MLE?",
    "created_utc": "2024-10-31T21:03:53",
    "num_comments": 2,
    "comments": [
        "depends on the role/company. will you train models? pytorch mlflow huggingface. will you deploy models? kserve kubernetes triton etc. will you call openai? backend with good prompt skills",
        "Understood. I’m hoping mostly the first 2."
    ]
},
{
    "submission_id": "1ggwzlz",
    "title": "\nPerpIexity AI PRO YEARLY coupon available just for 20USD!!\n",
    "selftext": "I have a few 1 year PerpIexity pro vouchers which give 100% off. I can redeem it on ur email. They work world wide.\n\nI got PayPal g&s , venmo and UPI payments.\n\nPerpIexity ai , has a lot more models than ChatGPT. It has  GPT-4o ,  Claude 3 Opus, new Claude 3.5 Sonnet ,Llam 3.1 305B(Meta) and  Sonar Large 32k.\n\nAnd from image generation models:  Playground v2.5 , DALL-E 3 , and Stable Diffusion XL\n\nText me on [WhatsApp ](https://api.whatsapp.com/send/?phone=%2B919071509078&text=hi&type=phone_number&app_absent=0)to get!\n\n",
    "created_utc": "2024-10-31T20:55:14",
    "num_comments": 34,
    "comments": [
        "Legit. Got myself one from the OP.",
        "Can vouch for this",
        "This is legit, can confirm it works",
        "Legit OP Guy. Confirm it’s working Thank you OP 👊👊",
        "Legit. Was helpful, replied all of my messages within seconds.",
        "Worked for me, thanks!",
        "Interested",
        "Boom, pleasure doing business. Delivery in 5 mins.",
        "It worked for me, also very quick response. Thanks for your help",
        "Legit and worked for me. Got it just now. Cheers.",
        "When this voucher expire?",
        "I was skeptical but it worked and they were really quick!",
        "Sorted in minutes, thanks.",
        "Legít, worked flawlessly in less than a minute, amazing!!",
        "[deleted]",
        "Got it just now. Dude’s legit",
        "Legit!",
        "It is legitimate and provides a quick response.",
        "Legit, love it. Cheers mate 🧉",
        "Legit guy.\n\nTook license. Works perfectly. Thank you",
        "Legit",
        "thank you",
        "thank you for trusting",
        "thank you so much",
        "welcome",
        "pleasure doing buisness",
        "welcome",
        "thank you",
        "after a year",
        "thank you",
        "thank you for trusting",
        "thank you so much",
        "thank you for trusting",
        "welcome sir",
        "thanks"
    ]
},
{
    "submission_id": "1ggtn3n",
    "title": "First time fine tuning model",
    "selftext": "How do I fine tune a model like Llama 3 to extract important information from a given description? Also, do I have to do this process manually? I want It to extract very specific pieces of data and organize It in a special way so I’m thinking I’ll have to prompt It, tell It if the output was correct and keep producing my own data. Is there a way to automate the production of data so I don’t have to always do It manually?\n\nThis is my first time doing this so any tips and guidance would be great. Thanks!",
    "created_utc": "2024-10-31T17:47:24",
    "num_comments": 2,
    "comments": [
        "first of all check if you can solve it with just a good prompt"
    ]
},
{
    "submission_id": "1ggtefu",
    "title": "Train PyTorch DeepLabV3 on Custom Dataset",
    "selftext": "Train PyTorch DeepLabV3 on Custom Dataset\n\n[https://debuggercafe.com/train-pytorch-deeplabv3-on-custom-dataset/](https://debuggercafe.com/train-pytorch-deeplabv3-on-custom-dataset/)\n\nSemantic segmentation can add immense value when coupled with deep learning. Semantic segmentation has several applications in the field of medical imaging, environmental imaging, and satellite imagery. Tapping into any of these areas and carrying out a project can provide a lot of knowledge. We will start with a single-class semantic segmentation in this article. We will **train the PyTorch DeepLabV3 model on a custom dataset**. This dataset consists of **satellite images of water bodies**.\n\nhttps://preview.redd.it/46hc1rwxq6yd1.png?width=1000&format=png&auto=webp&s=0633f6446953c86193b1a03e453ea21657370541\n\n",
    "created_utc": "2024-10-31T17:35:01",
    "num_comments": 0,
    "comments": []
},
{
    "submission_id": "1ggte4u",
    "title": "Does the amount of epochs affect the amount of time to create or graph a pkl model?",
    "selftext": "I'm using ipynb and anaconda to run a code that can create a cnn modle in a pkl format as Visual Studio Code as my editor. It worked for the most part until it reachs to this part of the code: \n\n```\nlearn = vision_learner(dls, resnet50, metrics=error_rate)\nlearn.fine_tune(4)\n```\n\nIt's been almost 2 hours since it started to run this part of the code and it feels like it's making my whole laptop lag hard. I knoe this is more of a programming question but it's why I've been curious to know if the epoch amount (which is 4 as you can see on \"fine_tune\") is what affecting the time it's taking to finish its part. The dataset I'm using for the model is pretty big, with over 15 categories. So I'm not sure if I should increase or lower the amount of epochs. ",
    "created_utc": "2024-10-31T17:34:35",
    "num_comments": 0,
    "comments": []
},
{
    "submission_id": "1ggt3mx",
    "title": "New on 2024 - 2025 are MLX and Transofermers so lets compare Custom Deep Learning Models for iOS with MLX on Apple Silicon vs. PyTorch - day 2 - INGOAMPT",
    "selftext": "",
    "created_utc": "2024-10-31T17:19:29",
    "num_comments": 0,
    "comments": []
},
{
    "submission_id": "1ggrjst",
    "title": "I’m working on a project on using Meta Learners but I have no background in ML( but well versed in Computer Science ) Can anyone suggest some good materials and ramp up quickly? ",
    "selftext": "",
    "created_utc": "2024-10-31T16:02:29",
    "num_comments": 0,
    "comments": []
},
{
    "submission_id": "1ggq6oz",
    "title": "Heavily Skewed features in the dataset",
    "selftext": "Hello fellow learners, I am trying to build a project providing movies recommendations for learning and possibly adding it to my resume for future. \nThe issue I am facing is that my dataset has various features which are useful for recommendations but are heavily skewed. For example, vote_count, revenue, budget.\nI got a 1 Million rows dataset from Kaggle but on further analysis, I discovered most of the movies have 0 to 100 votes, 1-1000$ budget and revenue (more than 90% of the data). The distribution is always left skewed with less frequency for the lower band of values, no matter how many left skewed values I filter out.\n\nWhat are the possible approaches I can take to handle this? Should I create two models to generate the final vectors? One for low quality movies and one for high quality popular movies will the left skewed data still affect the final similarity vectors? or is there a way to unskew this data? Should I just drop these features?",
    "created_utc": "2024-10-31T14:56:19",
    "num_comments": 4,
    "comments": [
        "Wouldnt data standardization solve this?\nEdit,  for knn i guesso, but idk for other models",
        "What happens if you log it?",
        "I am using an autoencoder for finding relationships in between the columns. Standardization would distort the column to affect the relationships of these columns with other columns.",
        "Still heavily left skewed with most values between 0 to 2."
    ]
},
{
    "submission_id": "1ggphgt",
    "title": "Suggestion in roadmap",
    "selftext": "Hi everyone! I’m currently finishing my Master’s in Physics and starting to transition into machine learning. Right now, I work as a junior data engineer, and I’d like some feedback on a roadmap I’ve put together for myself. My goal is either to land a position as a data scientist or ML engineer or eventually pursue a Ph.D. to apply for research positions. For context, I’m from Argentina, so the job market here might be a bit different.\n\nHere’s the roadmap I’ve planned:\n\nI’m currently taking Andrew Ng’s ML specialization and working through An Introduction to Statistical Learning (ISL), doing the exercises.\n\nAfter finishing the specialization, I plan to read Machine Learning with PyTorch and Scikit-Learn while continuing to follow topics in ISL.\n\nThen, I’d like to work on a few projects that interest me, particularly around recommendation systems and classification, but in an end-to-end format (starting with initial analysis in a notebook and then moving towards a production-ready implementation using MLOps tools, etc.).\n\nFinally, to round out the theoretical side, I plan to read The Elements of Statistical Learning (ESL) and Dive into Deep Learning.\n\n\nI’ve set aside around 6 months for this, given that I’m finishing my Master’s while also working.\n\nDo you think this is a good roadmap? Or is it too much theory and reading and not enough coding?\n\nThanks!",
    "created_utc": "2024-10-31T14:24:15",
    "num_comments": 0,
    "comments": []
},
{
    "submission_id": "1ggp3u5",
    "title": "What is the posterior, evidence, prior, and likelihood in VAEs?",
    "selftext": "Hey,\n\nIn Variational Autoencoders (VAEs) we try to learn the distribution of some data. For that we have \"two\" neural networks trained end-to-end. The first network, the encoder, models the distribution q(z|x), i.e., predicts z given x. The second network models an approximation of the posterior q(x|z), p\\_theta(x|z), i.e., models the distribution that samples x given the latent variable z.\n\nhttps://preview.redd.it/ti93kfzsp5yd1.png?width=658&format=png&auto=webp&s=cbe1ff00503ed8dfdd11145ef37fd030e07d0475\n\nReading the literature it seems the optimisation objective of VAEs is to maximize the ELBO. And that means maximizing p\\_theta(x). However, I'm wondering isn't p\\_theta(x) the prior? Is it the evidence?\n\nMy doubt is simply regarding jargon. Let me explain. For a given conditional probability with two random variables A and B we have:\n\np(B|A) = p(A|B)\\*p(B)/P(A)\n\n\\- p(B|A) is the posterior  \n\\- p(A|B) is the likelihood  \n\\- p(B) is the prior  \n\\- P(A) is the evidence\n\nWell, for VAEs the decoder will try to approximate the posterior q(x|z). In VAEs the likelihood is q(z|x), which means the posterior is q(x|z), the evidence is q(z) and the prior is q(x). Well if the objective of VAE is to maximize the ELBO (Evidence lower bound) and p\\_theta(x|z) is an approximation of the posterior q(x|z) then the evidence should be p\\_theta(z) given that q(z) is the evidence, right? That's what I don't get, because they say p\\_theta(x) is the evidence now... but that was the prior in q...\n\nAre q and p\\_theta distributions different and they have distinct likelihoods, priors, evidences and posteriors? What are the likelihoods, priors, evidences and posteriors for q and p\\_theta?\n\nThank you!",
    "created_utc": "2024-10-31T14:07:10",
    "num_comments": 3,
    "comments": [
        "In ELBO, you would refer to p(x) as the evidence, so the whole point is to provide a lower bound to the evidence. This is because the integral for the marginal likelihood is intractable. The ELBO consists of two parts: ln p(x|z) which is sometimes referred to as the reconstruction error since x is encoded to z and decoded back. The second part is the KL divergence (KL[q(z|x)||p(z|x)]), which measures the difference between the variational posterior and true posterior (but we don’t know what that is). This is why when implementing a VAE you usually have a pre-determined prior, which can then be sampled from. Imo the choice of prior is somewhat arbitrary but a good choice will give you better ELBO values."
    ]
},
{
    "submission_id": "1ggnbdg",
    "title": "Planning to become ML engineer, I need help, ",
    "selftext": "I’m currently a sophomore at a university in Korea(I am Korean), majoring in psychology and double majoring in computer science. I plan to become an ML engineer. However, I've heard that university that care capable of  building AI models in Korea is slim to none and most AI grad school is more focused on application and optimization of the model. I'm unsure about my next steps and have been considering the roadmap provided by roadmap.sh. \n\n1. The difference between an AI engineer and an ML engineer that AI engineers use models as a black box for applications, while ML engineers focus on developing those models?\n\n2. Is the [roadmap.sh](http://roadmap.sh) for ML engineers reliable? \n\n3. Is learning frontend and backend development relevant for becoming an ML engineer? \n\nI've only studied basic C, Python, algorithms, and data structures. Should I focus on frontend or backend development, or is it irrelevant for my career goals? Do you have any advice on what I should do next? \n\n4. I'm planning to follow the courses outlined on [roadmap.sh](http://roadmap.sh) during my upcoming vacation.",
    "created_utc": "2024-10-31T12:49:23",
    "num_comments": 3,
    "comments": [
        "1 yes. ”AI Engineer” as I understand it is more like ”AI operator”. I’d assume this title is highly volatile.\n\n2 I’d say it seems like a high level guide. Probably useful to understand the main focus of the different disciplines.\n\n3 backend is definitely useful to an extent. ML engineering is not just a Jupyter sandbox, even if you have ML Ops engineers you should still know how to build, deploy and maintain production workflows imho. Frontend, nah\n\n4 personally I’d find one or multiple passion projects instead of following course plans. But that’s just me",
        "Hey, I think the easiest way is to start building ML models of your own. I started by using model generation through [https://www.plexe.ai/](https://www.plexe.ai/) and iterated on the output and used it in Kaggle competitions"
    ]
},
{
    "submission_id": "1gglx5m",
    "title": "I'm in high school and I want to make my own recommendation/advisory AI, where do I start?",
    "selftext": "I'm in my sophomore year of high school and I'm already stressing about college admissions lol. I want to build a special and unique portfolio for uni bc I'm trying to get into Duke and ik how hard it is these days. So far I've built a website for my school and now I want to build an AI that can help younger students pick their subjects for when they join high school. So far I've only been learning python on freecodecamp and a bit on kaggle. If there's anything I need to know or any resources that will be helpful it would be great if you can help (preferably free obviously since I'm still a student 😅). I work better when I have a clear roadmap.",
    "created_utc": "2024-10-31T11:49:02",
    "num_comments": 20,
    "comments": [
        "Provide people a link to chatGPT.",
        "The reality is it would sense for you to use something like chatbase trained on data about the classes offered and put it on a website, but that’s expensive and not something a high school student can afford. I mean if you make it part of your school website and have your school pay for it though 🤷",
        "Since you’re in high school you probably don’t have the in depth knowledge/debugging skills to build a great model from scratch.\n\nLook into prompt engineering and how to effectively use ChatGPT API. It’s gonna cost money for each request, but you could ask the school to cover this.\n\nI’d recommend make a plan on what are your objectives of the project and how to measure these objectives. Then go to ChatGPT and leverage it to make a detailed plan.",
        "If building a sinple website is like putting up a tent then building an LLM chatbot recommender system is like building a sky scraper.\n\nGo to college for CS or math then consider grad school.",
        ">I'm already stressing about college admissions lol.\n\nHow are your grades?",
        "fuck it we ball. \n\nWhat did you code your school website in? Let's leverage those mf-ing strengths",
        "Hey, I think the easiest way is to start building ML models of your own. I started by using model generation through [https://www.plexe.ai/](https://www.plexe.ai/) and iterated on the output and used it in Kaggle competitions",
        "I like your idea, but using an LLM or an API thereof to materialize would not only be overkill, but also quite vapid and lacking of any originality. I believe it’s best if you look closer to home, to classical math and statistics, as something in those fields may help you more.",
        "seriously people need to stop reinventing the wheel... More chatbots are not welcomed by the market.",
        "That would be hilarious 😂",
        "I do not know or chagbase but that sounds way too overloaded of an approach to me. Easiest approach would be to just build an app using description of classes and conventional llm or not? Direction of RAG?",
        "I’m basing chatbase off because my professor’s site used it, I would explore more tools to find what best fits you. Thats how he uses it, and it’s trained on all of his research papers and course content. \n\nhttps://www.davidjoyner.net/chat-with-dai-vid/",
        "Yes I don't have much knowledge about how to build a model but that's what I'm trying to learn! I was hoping I'd be able to improve my skills and create this model in a few months to a year, there's really no rush.  \nI just wanted to ask you how realistic do you think my idea is? Do you think it's possible for me to just create an AI within the span of a year or so?",
        "wait what ? 'make a model from scratch' ... who can make one?? except the big companies?",
        "in reality just make a wrapper",
        "To my understanding, free use of LLM models tends to be something you can download, train, and run on your own machine.",
        "Alr thank you so much!",
        "Honestly, building a model from scratch while juggling school with your level of knowledge isn’t realistic. Maybe you can create a wrapper on top of ChatGPT within a year, but a model from scratch will be difficult and time consuming. If you’re stuck on this idea, look into building a wrapper on top on ChatGPT. \nBut it will cost you, and students can just use ChatGPT directly which won’t cost money to maintain.\nBut again for your resume, assuming you can get your school to fund it, you should look into building a wrapper on top on ChatGPT.",
        "the wall your going to run into isnt going to be ability its going to be on compute resources training an effective model is computationally hungry. \n\nyour idea is not bad at all but this is a classic case of applying your resources and time well  \n\nlook into fine tuning a public model  on example course descriptions and related interests. \n\nlook into tying a database for RAG to provide additional guidance to an existing pretrained model. \n\nthose are good first steps.",
        "Good luck with getting more feedback, so far I’ve studied machine learning in grad school but I’m covering deep learning next semester my knowledge a little shakey of how you approach doing yourself. With a machine to run it I know free methods exist. You certainly are already looking motivated as a sophomore so I’m sure schools will like that."
    ]
},
{
    "submission_id": "1gglhi6",
    "title": "How many features can the KNN algorithm handle",
    "selftext": "How many scaled (between 0 and 1) features should a KNN algorithm be expected to be able to handle. I know that increasing dimensions with KNN can lead to problems so I'm curious how many dimensions until it's truly problematic? Im currently working with 40 features and curious if this is too much for KNN to find meaningful patterns in the data?",
    "created_utc": "2024-10-31T11:30:20",
    "num_comments": 3,
    "comments": [
        "\\- 40 is not that many.\n\n\\- Just try it out. If it does not work you can move on to something else.\n\n\\- \"too much for KNN to find meaningful patterns in the data?\" KNN does not find any patterns it just stores the training data.",
        "About tree fiddy",
        "Fs got bout fowdy"
    ]
},
{
    "submission_id": "1gghau0",
    "title": "Revisiting ML",
    "selftext": "\nI previously did a Machine Learning MSc at Bristol back in 2013, but ended up working in Finance Software as there weren’t many jobs for non-phds back in the field back then.\n\nObviously, Deep Learning sky rocketed since then.\n\nDoes anyone have advice of what to cover for those of us who have been away for about a decade?",
    "created_utc": "2024-10-31T08:30:31",
    "num_comments": 1,
    "comments": [
        "Hey, I think the easiest way is to start building ML models of your own. I started by using model generation through [https://www.plexe.ai/](https://www.plexe.ai/) and iterated on the output and used it in Kaggle competitions"
    ]
},
{
    "submission_id": "1gggsad",
    "title": "Need a partner (co-author) for Research",
    "selftext": "I am an undergraduate student from India looking for a guy with good knowledge in machine learning (random forest and neural network) to be a part of my research in the field of finance.\n\nDM ASAP IF INTERESTED\n\nPS: the person can be from anywhere in the world :)",
    "created_utc": "2024-10-31T08:08:39",
    "num_comments": 2,
    "comments": [
        "Why are you looking for randos on the internet and not a professor from your uni?",
        "Even I have completed my bachelor's recently and working in a MNC. But can't publish papers with professor now."
    ]
},
{
    "submission_id": "1ggfict",
    "title": "Suggestions on my resume. Or else just Roast my resume 😅",
    "selftext": "https://preview.redd.it/5bxdquh0o3yd1.jpg?width=2550&format=pjpg&auto=webp&s=0d7af24f280a7eaab9649e4b24c014df058fcf89\n\n",
    "created_utc": "2024-10-31T07:13:13",
    "num_comments": 5,
    "comments": [
        "What the fuck literally happened that every single indian on this subreddit had started posting resume reviews here lol there are so many better suited subreddits for this",
        "You are a disappointment... Just like my son",
        "Use less words no one had time to read all that",
        "If you consider HTML “programming” then you’ll also consider cow dung as food.",
        "Oh sorry can you suggest some I can try that man"
    ]
},
{
    "submission_id": "1ggfe01",
    "title": "Tiny ml, help and suggestions ",
    "selftext": "I have an okay understanding of Python and recently dove into embedded systems. I'm currently exploring AI applications for embedded systems but finding it difficult to get proper materials. I’m working mainly with Arduino but also have a Raspberry Pi Pico, and focusing on movement and decision-making applications. I'd love recommendations on any relevant resources, and other areas too.",
    "created_utc": "2024-10-31T07:08:02",
    "num_comments": 1,
    "comments": [
        "Pytorch has software/libraries/something for doing inference on embedded devices: https://pytorch.org/edge"
    ]
},
{
    "submission_id": "1ggeojl",
    "title": "Resume suggestion and tips or roast your wish but help!!",
    "selftext": "",
    "created_utc": "2024-10-31T06:35:48",
    "num_comments": 2,
    "comments": [
        "wrong subreddit, try /r/cscareerquestions or maybe /r/datascience",
        "Damn I helped one guy with a resume post and now resume posts are all I see on this subreddit"
    ]
},
{
    "submission_id": "1ggdmg2",
    "title": "I joined an MIT’s IDSS DSML",
    "selftext": "I was interested in joining a program while I am in high school so that it could help me later on, and well I signed up for this program and it starts on November 11. I have a couple weeks before it starts to see if I like it or not to get a refund, do yall think this program will help with going to a good university/ getting a better job in the field ",
    "created_utc": "2024-10-31T05:45:33",
    "num_comments": 2,
    "comments": [
        "Yeah this looks interesting! I'd like to know if anyone has finished this although I'm sure it is different than last year's course depending on how the professors update the material. Not sure.",
        "What is the cost ?"
    ]
},
{
    "submission_id": "1ggdg40",
    "title": "A Meticulously Guide to Advances in Deep Learning Efficiency over the Years",
    "selftext": "I made a Meticulous Guide to Advances in Deep Learning Efficiency over the Years, which is a detailed story from pre-AlexNet to foundation model training centered on efficient deep learning from a variety of perspectives like the hardware, algorithms, compilers, libraries, scaling laws, and more. \n\nIt focuses a lot on scaling up models (e.g. fused kernels, distributed training, etc.) and scaling down models (e.g. quantization, model pruning, sparsity, etc.) but roughly goes chronologically.\n\nHope you all enjoy, and would love any feedback!",
    "created_utc": "2024-10-31T05:36:18",
    "num_comments": 0,
    "comments": []
},
{
    "submission_id": "1ggafgn",
    "title": "Transfer voice onto song",
    "selftext": "Hello ML folks,\n\nI would like to learn how to transfer a voice onto a song, e.g., my friends voice from an audio recording onto a Beatles songs or whatever, such that it sound alike he would sing it.\n\nI am an experienced coder, but I don't know what is the SOTA tool for this or if there is a particular GitHub repo to start playing with.\n\nThankful for all comments!",
    "created_utc": "2024-10-31T02:27:11",
    "num_comments": 0,
    "comments": []
},
{
    "submission_id": "1gg9pu0",
    "title": "Best hardware for ml",
    "selftext": "I'm thinking in buying a new hardware for ml, but I'm struggling to choose the best configuration.\n\nShould I choose an simple model, Ryzen 5, 32gb ram ddr4, with a good GPU (can't decide which model), or the model with Ryzen 9, 64 gb ram ddr5, without GPU ?\n\n",
    "created_utc": "2024-10-31T01:31:24",
    "num_comments": 1,
    "comments": [
        "Depends if you want to learn, train or just do inference.\n\nLearn: use what you have since qwen 500m or sd1.5 is enough.\n\nInference: get lots of vram so its fast and you can run decent models. If you can afford get a rtx3090 or 2.\n\nTraining: youre probably better off doing it in the cloud, since you need massive amounts of vram. While it is possible to train lora on 24GB you might still need deepspeed to manage it."
    ]
},
{
    "submission_id": "1gg8szl",
    "title": "How to speed up cosine similarity of huge vectors using a GPU?",
    "selftext": "I have 1M vectors and each one is size 768. I'm trying to run cosine similarity on them to get pairwise distance. I'll end up wanting the top 20 or so, so my final output will be size 1M x 20. I have to do this a lot, so I'm looking to speed this up as much as possible.\n\nI wrote some batching in Jax, and then wrote it in pytorch, and even on the GPU I'm not getting great performance. vmap doesn't seem to want to work well with the cosine similarity function... Has anyone computed something similar? Am I at least going in the right direction, or will I need to rewrite this all in something like C?",
    "created_utc": "2024-10-31T00:18:08",
    "num_comments": 9,
    "comments": [
        "Have you tried using faiss?",
        "This should just be a matter of using the linear algebra functionality appropriately. Doing things vector-by-vector is slow, but using matrix multiplication is fast.\n\n1. Form a matrix of all your vectors, it'll be 1M rows and 768 columns. Let's call this matrix 'A'\n2. Do M = A A^T to get a 1M x 1M matrix.\n3. Divide each element by the square roots of the magnitudes of the corresponding diagonal elements. I.e. m_ij -> m_ij/(sqrt(|m_ii|)*sqrt(|m_jj|))\n4. your cosine similarities are now the elements of M. This is somewhat inefficient because the matrix is symmetric, whereas you only need the upper or lower diagonal, but that's kind of a secondary issue.\n\nThis requires a lot of memory, but that is unavoidable since you want pairwise distances for all 1M vectors. It should be much faster because matrix multiplication has its own CUDA kernel that makes it efficient; vmap or whatever is much less efficient by comparison.",
        "Are you trying to cluster? Because this isn't how you cluster.",
        "This is a very cool question have my upvote",
        "Did you try algorithms such as Faiss or Scann? You should be able to get approximations of the closest neighbours at fraction of the cost compared to manual comparisons.",
        "you can use approximate nearest neighbors instead of cosine similarity if approximation is viable in your case",
        "I can batch A to get batch_size x 1M matrix, and then if I do top 20, each batch only has to save batch_size x 20. The trick then becomes how to optimize the batching to get the whole GPU in use constantly.\n\nI was hoping this already existed in code somewhere. But apparently FAISS and SCANN exist, so I'll look into those.",
        "I didn't know about Scann! I will look into that (in addition to FAISS).",
        "Is there a reason you don't want to do the full matrix multiply on either the GPU or CPU? Not enough VRAM or RAM? Maybe there's a way to be more efficient about things but I think the full matrix multiply might be fastest.\n\nYou can also try doing this on a CPU with ordinary multiprocessing, you might be surprised at how well it works."
    ]
},
{
    "submission_id": "1gg8j6c",
    "title": "Can someone please give a clear vision of whats in-front of me if i opt machine learning for studying ?",
    "selftext": "Hi, I'm currently in 12th grade and will be graduating in 2025 from CBSE, India. After 14 years of school, I feel I haven't learned much. I want to master machine learning without attending college. I'm eager to learn and gain expertise, not just for a degree. I'm seeking guidance on how to become a research engineer in machine learning through self-directed learning, attending seminars, startups, and conferences.\nAlso since i find myself feeling more satisfied when learning all by myself,\nI'm starting from scratch and want a clear understanding of the field. Can you help me?",
    "created_utc": "2024-10-30T23:56:38",
    "num_comments": 9,
    "comments": [
        "brother this is ML not web development. you won't get any ML job without a degree here in India (until your uncle is the CEO). go to college and do a Btech or bsc in a quantitative field.",
        "Hey man, I'm also a high school student (grade 11) and I've been chugging at ML for the past year and a half or so. Although I'm still far from mastery I think im at a stage now where I'm confident in my ability and this came through repeated practice and practice.\n\nYou first need to get your fundamentals good. Proficiency in programming (ideally in python), a decent understanding of calculus and statistics go a long way. \n\nWhen you're confident enough in your fundamentals, I'd suggest you start watching introductory college lectures to get a grasp of the theoretical concepts. CS229 is great for this!\n\nFinally you can get cracking at a project. Implementing a decoder only transformer is a fun little project. I'd suggest following one of andrej karpathys videos if you're stuck\n\nAt the end of the day, this is just the path that I followed, and the best thing to do is just start doing something!",
        "A bachelor’s in math would be great if you want to master machine learning (take up statistics if you can as a major)\nBy the time you graduate most of the entry level lucrative tech jobs would be gone\nBesides math will enable you to get into any path you’d like to later specialise in. Good luck",
        "without college degree in india , you want be gaining anything",
        "Hey, I think the easiest way is to start building ML models of your own. I started by using model generation through [https://www.plexe.ai/](https://www.plexe.ai/) and iterated on the output and used it in Kaggle competitions",
        "hey its awesome that you are passionate further than school, this type of thinking is what makes anyone excel in everywhere they go, the practice after the practice. improving your skills and being true to what you want, all people have their own perspective of life and that is ok. you should listen a little bit to everyone though, there's a little bit of truth behind other opinions, and count me too.\n\ni think you should always follow your passions and always have a back up plan, or a parallel plan.  \nbut you should always be true to yourself, don't think like this or that, have fun don't overthink it,  you have your own answers, no one know exactly how to get there, but  where should you go for knowledge relevant to a topic?, i could say a university's you tube channels.\n\ngood luck! i believe in you! you got this!",
        "I know right?! Op watched too many western hustle culture reels. \"College is a waste of money\"\n\nNot in India. Its cheap af to study in India compared to US.",
        "Thank you ChatGPT 🙃",
        "not the best out there."
    ]
},
{
    "submission_id": "1gg7cpd",
    "title": "Roast my Resume (and suggest improvements)",
    "selftext": "",
    "created_utc": "2024-10-30T22:27:48",
    "num_comments": 32,
    "comments": [
        "Oh no here we go again",
        "This isn’t related to learning machine learning. Post this on a resume subreddit. Not the right place for this",
        "\"Machine Learning Intern\" and just after listing \"Led a team of 4\" looks sus af. Where you an intern or a team lead?",
        "run it by chatgpt. even the free ones can give better suggestions than us..",
        "You led a team of four as an intern. Was it a team of four GPUs?",
        "My bs sense is tingling. \n\n> Led a team of 4\n\nAn inter leading a team? No sane company will allow an inter to lead anything\n\n> resulting in 30% reductuon in delivery delays\n\nSuch impact is usually measured over a long time (much longer than duration of internship). You sure this change isn't due to seasonality or sample randomness?\n\nOverall, you made lots of projects in a very short time, most with very significant impact. Either you're claiming credit for projects you barely touched, inflating/ falsely reporting project significance, or both. Or you are this generation's Hinton.\n\nEdit: also googled both hackathons you allegedly won and couldn't find anything",
        "Really sounds like a guy that tries too hard to sell himself trying to take advantage of less tech savvy HR people.",
        "Stable diffusion doesnt need 32GB of vram",
        "To put it in webdev terms, what you got here is HTML with only paragraphs in regular and span, when you should have HTML with proper heading and paragraphs + styling with CSS.",
        "Experience should be after professional summary then skills followed by education",
        "Because you have many roles that were under 6 months, just list the years of each one instead of the specific months. Once you get into an interview, you can discuss the full dates. Getting your foot in the door is priority one.\n\nCategory order: first comes the skill list. That's the main initial qualifier or disqualifier someone will use to screen you. Second, work experience. This validates and supports your skill set. Then, projects. They support the job history. Finally, education followed by publications, which each support the previous categories.\n\nAdd a statement at the bottom that offers references upon request. Do a bit of decent research on each job, and write a thoughtful and brief cover letter acknowledging the best qualities of the company in terms of being a good fit for your next move, and why you're excited and qualified to be their next star player. \n\nDon't apply for jobs that aren't a really solid fit and in the higher tier of pay range. Otherwise you're wasting time on applications that might cause you to settle, or go through interviews that you won't ace, which wastes everyone's time better spent on better fitting candidates. \n\nTo that same point: no matter how good it looks, don't have any skill or technology on your resume that you haven't used at a job, or that you can't support with detailed interrogation and immediate demonstration in a live interview.\n\nKeep your skills sharp between and during jobs by constantly USING them and improving them daily. Tutorials are great, IF you use them correctly, which means you're following along and implementing them immediately. After that, take it a step further by extending beyond the exact process and steps in the lesson, so you ensure that you're not just copying what the teacher did verbatim.\n\nMeditate. Positive self-talk. Believe in yourself. Speak and act from a place of confidence you've earned by virtue of deep preparation. Once you truly deserve the best role you can possibly get, landing that role becomes easy.",
        "You wouldn't supposed to see \" I led a team....\" While you're an intern. Just be qualified, don't upsell that much.",
        "Help needed plz, where did you got this template?",
        "I see the word \"leverage\". I find that it is written by AI. I refuse to look at the other details. I am a simple man.",
        "The average Indian resume. This is not the sub for this stuff, go elsewhere",
        "Use a better font. I cant stand the default latex font. Consider Source Serif",
        "Indian resume, straight into the 🗑️",
        "This reditt is about learning, but commenting anyway.\n\nGood resume overall.\n\nFor Dotlas’ first two bullet points, explaining what models/architectures and tools were used may help. \n\nIn most cases, a very brief description of how you setup the experiment/s will help.\n\nI will hire you if you could explain everything that you have stated 😃",
        "Read my response and tell me if you still think that.",
        "It's all college shit, he studied in an Indian private college, that conducts hackathons within students, it's not that competitive. His internship was probably bogus.",
        "[deleted]",
        "Nothing wrong with that at all",
        "No one has a reference statement anymore.\n\nI’ve also never seen a skill list at the top. Don’t make fluff the first thing people see. The skills should be apparent in the rest of the resume anyway. Work experience first or education if you’re a recent grad. I’d maybe swap Projects and Skills but otherwise their order is fine.  \n\nAnd of course the roles are short. They’re internships. It’d be weirder to try to lie/embellish and say you were working full time while in uni full time.",
        "Haha I haven't seen a reference statement on a professional resume in more than a decade...",
        "here you go: [https://www.overleaf.com/latex/templates/jakes-resume/syzfjbzwjncs](https://www.overleaf.com/latex/templates/jakes-resume/syzfjbzwjncs)   \nit's called jakes resume template.",
        "Zombo.com",
        "Yeah to some extent you would want to upsell yourself, but for my own mental health I would never take a position where I feel I wouldn't be qualified for. But I was just roasting his resume, his projects sound like school assignments.",
        "Thanks alot ❤️",
        "lol are you serious. why are you linking some garbage website to us",
        "Thank you!",
        "[deleted]",
        "u/Zestyclose_Time3195 bro its available on word as a free template you can get it easily",
        "Same man",
        "Thank you!",
        "Ahh I feel I should also mention jakes resume template on overleaf.  \nif you're willing to put in some work, that's the way to go for a professional resume",
        "Okie thank you!!\n\nI didn't knew that, thanks alot!!"
    ]
},
{
    "submission_id": "1gg76f8",
    "title": "Multimodal classification related question(audio and image data)",
    "selftext": "This is my first time posting anything here and I just started learning how to run models. Recently I was trying to run a multimodal classification model designed for a specific dataset(image data and inside and outside audio data of metal parts/made for detecting defects of the metal parts).\nThe providers provided ViT_audio_train file and ViT_vison_train file for those image and two audio datasets. The thing is that the audio data are converted into spectrogram and image data are extracted from them so that those images can be integrally used with the image dataset for classification task(there are several classes classified and one of them is a normal class). \nMy question is, regarding the ViT_audio_train file, when running it with inside audio data, it produces outside_audio_model.pth, which will be later used for the classification. When running it with outside audio, inside_audio_model.pth is created. Later in the classification, inside audio test data is used on the inside_audio_model.pth and outside audio test data is used on the outside_audio_model.pth(vision_model.pth created from the image dataset and is used with the image test data/these three are all together used for classification). \nAbout the audio part, it seemed a bit strange so when I looked more closelt, the inside_audio_model.pth is created by using the label information of inside audio data AND image (created from the spectrogram of audio data) of outside audio data.\nFor the outside_audio_model.pth, image derived from inside audio(image from spectrogram of the inside audio) AND outside audio label info.\nI don't understand why they used like this in sort of a cross manner. I looked into transfer learning but wasn't sure whether this is the case. \nCan anybody help me understand this?\nIs this a normal thing in multimodal classification?\nOr is this just wrong?\nI cannot get any respond from the providers...",
    "created_utc": "2024-10-30T22:16:27",
    "num_comments": 0,
    "comments": []
},
{
    "submission_id": "1gg6xu2",
    "title": "any projects i can do as an aspiring radiology resident ",
    "selftext": "\ni’m currently in med school and i’ve been interested in radiology since the beginning. i’ve seen machine learning being brought up often when mentioning the ways to boost cv. is there anything i can start doing ? any actionable projects i can do that would make me stand out?  it is required of me to be an “exceptional” applicant as a non us student and i feel like this would be one of the ways i can be that. ",
    "created_utc": "2024-10-30T22:00:56",
    "num_comments": 9,
    "comments": [
        "probably not, you need like 2 years or so learning how to code and maths... Why is that everybody thinks they can do ML without leaning anything??",
        "Can do some yolo object detections for tumors",
        "hey u can use it for various stuff like tumour lesion detection , image conversion for scans etc\n\ni am working in bio ML / AI domain since long along with working on research paper on various use case of ML in brains lesions , u could dm in case u wanna ask something , would be grateful to help !",
        "You might be want to do machine learning for tabular data first. I would recommend that you do the breast cancer detection dataset first. This would give you some exposure to Pandas for data manipulation and exploratory data analysis, Matplotlib for plotting, the basic ML algorithms, and model metrics like precision and recall scores to avoid False Positives and Negatives. Afterwards you could do some basic deep learning for images (also known as computer vision) to detect tumors, which is probably what you're after for.",
        "i am in my first year lol so i have like 5 years to figure it out",
        "i actually have no coding knowledge lol should i learn python for all this ?",
        "you basically have to double major to get it done.",
        "Yes that would help",
        "There are a few notebooks in tensorflow where you can upload a dataset and have it work. Some tutorials in youtube are like 39 minutes short."
    ]
},
{
    "submission_id": "1gg66l9",
    "title": "3d model to blue print (FLOOR PLAN)",
    "selftext": "[image ](https://preview.redd.it/cmh2elkan0yd1.png?width=881&format=png&auto=webp&s=076a07ee87d1415606dd7cd8bd7bc128fa748eb0)\n\n stumbled up on this problem statement  a ml model that takes 3d floor plan as input and give back its blueprint/2d floor plan . i have very basic knowledge of ml and related stuff(Still learning ) i have worked mnist and dogcat detection models but this is a very big piece for me to chew how should i go around making it what is the start point and what are some pothole i could fall into  and has someone made it in the past any kind of insight will be very help full and appreciate",
    "created_utc": "2024-10-30T21:13:16",
    "num_comments": 5,
    "comments": [
        "how about using cyclegan or pix2pix for this task ? they are used for image translations task   \nrefer this image about applications of pix2pix u will get a good amount of idea about pix2pix   \n[https://phillipi.github.io/pix2pix/images/teaser\\_v3.jpg](https://phillipi.github.io/pix2pix/images/teaser_v3.jpg)",
        "Github repo?",
        "I am not sure how they work , I went through them it's like converting an image into a form where only outline is visible right ? It's documentation is in tensor flow ig",
        "basically they can map and convert input image to output images , so when trained on data of 3d floor plan and 2d floor plan , they can convert 3d to 2d just like they do with satellite map to outline map \n\ncons - need excessive hardware , need large amount of data (preferably paired 2d and 3d data for pix2pix , for cyclegan u don't need paired data but results might not be as good as pix2pix )\n\n The *CycleGAN* presents an approach for learning to translate an image from a source domain X to a target domain Y in the absence of paired  data",
        "Thankyou for the reply sir .So it will be very hard to pull it off solo right ?  some small company gave me this problem statement they said they will hire me if I am able to solve 🙂.  By the way I am just starting out currently making a Small classification project alongside This and doing zero to hero ml by Andrej and CS229 what other resources should I follow &what projects I should make and when should I start reading and understanding papers , thankyou for your time again🥺"
    ]
},
{
    "submission_id": "1gg3xwb",
    "title": "OLA prep for MLE entry level roles?",
    "selftext": "Should I just expect typical leetcode style questions? My background is in data science.",
    "created_utc": "2024-10-30T19:11:05",
    "num_comments": 0,
    "comments": []
},
{
    "submission_id": "1gg2z73",
    "title": "Should I continue DSA in Java or Shift to Python",
    "selftext": "I'm interested in machine learning and working on it for past six months,and I've been doing dsa in java for more than one year.But now I want to pursue my career in machinelearning,Should i continue doing dsa in java or shift to python.I have only 4 months left for my placements",
    "created_utc": "2024-10-30T18:22:15",
    "num_comments": 6,
    "comments": [
        "depends, if you want to do it just for getting a job then Python, if you really want to learn it then switch to C++.",
        "In java u have the total understanding and its far more structured than python. So if u want to learn the concepts clearly. I'd recommend java. If you have the basic concepts perfectly. You can apply it to any programming language you want.",
        "C++, can i know why would you recommend it ?",
        "Will it be a problem if I write the code in Java in dsa interview for machine learning position?",
        "there is a reason why most CS programs around the world use C++ for DSA courses, is because it forces you to actually understand what is going on on memory, so actually understand the structures and how they affect performance.",
        "Well, technically yes, because for a machine learning position you need to have decent python knowledge and most of the technical interviews are based on python too if you are looking for a ML/AI engineer position. But as I said earlier if you want to get the basic concepts well , go with Java, understand the concepts and then apply it to Python.  From my past experience i'd say that most of the ML based interviews ,They don't focus on the basic data structures but more like ML based algorithms, so if you need to have the understandings of those Algorithms you need to have those basic concepts well. Therefore you can apply it to anywhere easily. Hope this helps! :)"
    ]
},
{
    "submission_id": "1gg1ho9",
    "title": "Are Xgboost outputs \"binned\" or is it a sliding scale like logistic regression?",
    "selftext": "I want to build an Xgboost binary classification model but I would like the outputs (probabilities) to be a continuous sliding \"scale\" like how logistic regression is. Is this how Xgboost outputs are or are the probabilities \"binned\" based on the leaf that the observation falls into? ",
    "created_utc": "2024-10-30T17:10:55",
    "num_comments": 3,
    "comments": [
        "It's a tree-based model. All outputs in a given leaf receive the same predicted value. You can control the size of the leaves through pruning hyperparameters.",
        "You can treat it like \"binned probabilities\", because all cases under the same leaves will get the same probabilities, even though they have slight variations in the input values.",
        "So it pretty much depends on how many leaves I have in the model?"
    ]
},
{
    "submission_id": "1gfz20g",
    "title": "Deep Learning in 2024: Continued Insights and Strategies – day 1\n",
    "selftext": "",
    "created_utc": "2024-10-30T15:21:44",
    "num_comments": 0,
    "comments": []
},
{
    "submission_id": "1gfyl35",
    "title": "Difference between neuronal biases and scalar buases",
    "selftext": "Or did chatgpt just make this up? doing a project for school, mostly basi, stuff, was wondering if there was a difference between representing biases as extra neurons or as just added values,  if that makes sense. Can someone help me out?",
    "created_utc": "2024-10-30T15:01:04",
    "num_comments": 6,
    "comments": [
        "There’s no difference but it’s not really helpful in anyway to think of it as an extra neuron",
        ">but it’s not really helpful in anyway to think of it as an extra neuron\n\nI'm not sure I agree. Thinking of the bias as an extra neuron with a constant (of 1) input helps with understanding neural networks, especially when calculating back propagation of the bias. \n\nThat's why older MLP diagrams visualized the bias as a disconnected neuron. Nowadays, networks are too deep for that kind of detail.",
        "Its doesn’t help because it just makes the purpose of the bias less obvious.     \nA single layer of a NN is supposed to take a vector from R^n -> R^m if we treat the bias as a separate vector which we add to the output then it shows that the bias is simply just translating the output.     \nIf you treat the bias as an extra neuron then it’s the same as taking R^(n+1) -> R^m but one of the dimensions in the input is always 1. Doing this doesn’t really make it obvious to the purpose of the bias.",
        ">If you treat the bias as an extra neuron then it’s the same as taking Rn+1 -> Rm but one of the dimensions in the input is always 1.\n\nBut, this is true and if you are looking at the weight matrix without remembering this, then you might wonder why the weight matrix is larger (by one) from the input.",
        "Both interpretations are true (the maths works out the same in both cases). I just see no reason to include the bias along side the weights when they don’t really serve the same purpose. The weights represent a linear transformation from the input space to the output space and then the bias is applying a translation, this allows it to be nicely written in the form Wa + b which again isn’t shown as obviously when you treat the bias as an extra neuron.",
        "It's just a matter of opinion. \n\nI believe it's more useful to consider it as a neuron, especially when you are programming or solving a neural network by hand. It's cleaner to include the bias in a ( R^n+1 x R^m ) weight matrix. \n\nIt also allows you to have a more accurate diagram that matches the equations like in this from scikit learn https://scikit-learn.org/1.5/_images/multilayerperceptron_network.png\n\nAlso, about z = wx + b, it doesn't necessarily say that the bias isn't a neuron. Back in the day, we used to sometimes write it as z = wx + w_0."
    ]
},
{
    "submission_id": "1gfydjl",
    "title": "Mac's unified memory is superior for AI reasoning?",
    "selftext": "Some people claimed that Apple's unified memory is superior and faster for AI reasoning due to large amount of memory that can access up to 192GB while even RTX 4090 can only have up to 24GB of VRAM. I know that Apple Silicon chip's GPU performance isn't great and the bandwidth is actually not that fast but the amount of memory seem superior.\n\n  \nBut can anyone test with M3 Max and RTX 4090m for AI training and reasoning? I'm curious to know the performance difference.",
    "created_utc": "2024-10-30T14:52:00",
    "num_comments": 12,
    "comments": [
        "I mean I can tell you that from even a 3090 to an M1 Pro with 32gb it works But it’s a fair bit slower.",
        "What does 'unified memory' mean? The 'MPS' Metal Performance Shaders?",
        "GPU and CPU memory is not separated out but all from the same pool. This eliminates interconnect bandwidth bottleneck between GPU and CPU.",
        "It's been a long time since I've thought about machine architecture so bear with me here. What bottleneck is this eliminating, exactly? Why does the GPU need to load data stored in system memory? Can't the GPU load data directly from disk using direct hardware access?",
        "Ok consider a gaming PC - I’m riffing here long time since I thought about this in detail also - data needs to be moved from disk and network io by the CPU to the GPU and back - this goes via the bus to the GPU card with its VRAM. \n\nDon’t have the numbers for you but it’s obvious that there’s a two-step between CPU and GPU in these computations. I don’t know enough about this but I don’t think the CPU even with DMA can access VRAM without using the bus. I don’t see how that would happen. \n\nIn case of AI inference it’s the model that has to be moved from disk to GPU then inference results from GPU back to CPU.  For training it’s much much worse since you’re moving TB of data from disk. \n\nAll this is not 100% accurate but approximately so.",
        "At the scale of micro-electronics the speed of light is a limiting factor. With a 4.0 GHz (4,000,000,000 cycles per second) CPU, the speed of light can only travel 7.5 centimeters per clock cycle, which limits the size of the computer. Any further, and the signals from the previous clock cycle may interfere with the next clock cycle. Disk read and write is many factors of 10 slower than accessing directly from a cache for this reason.\n\nIf the CPU and GPU had the same memory cache, the CPU and GPU can operate at higher speeds without sync issues.\n\nGPU's have become exponentially smaller over the past decade. The power of large gaming rigs a few years ago can now fit on a single chip today.",
        "Yeah, I'm super rusty on machine architecture that I might have straight up imagined a feature that doesn't exist.\n\nWhat I'm envisioning is a setup where a GPU directly loads data from disk, bypassing CPU access. With PCIe being a point-to-point interconnect I thought that this might be a possible feature, with some sort of guards implemented by the CPU that allows this direct access under specific scenarios.\n\nBut I'm not sure if this is a thing that actually exists. I can't think of a term for this type of access which is probably a hint that it's not really a thing.",
        "Sure, but my question is about the assumptions behind this claim. Under what scenarios would a GPU need to access data that the CPU has already loaded from disk into system memory?\n\nLet's go back to first principles. When the computer first boots up, no data is loaded into memory. So all data has to be loaded from disk (or, I guess more precisely, nonvolatile storage, since SSDs aren't exactly disks, but I'll just use the term disk as shorthand). So loading once from disk can't be avoided. What you want to avoid is loading it from disk again. So this presupposes a scenario where the same data is being accessed by the CPU and GPU. Since the CPU has already loaded the data into memory, you don't want the GPU to incur the cost of loading the same data again.\n\nIn a traditional architecture the CPU would transfer that data to GPU via whatever interconnect the CPU and GPU share. In a unified memory architecture, the CPU can instead mark that region of memory as shared and hand the GPU a pointer to that memory region, without transferring any data. I think this is the bottleneck elimination that the post I replied to is talking about.\n\nBut going back a bit, this bottleneck assumes that the data was first loaded by the CPU. But what if the CPU never loaded this data in the first place? Then we can't avoid the first trip to the disk. Then we may as well just have the GPU load the data directly into memory from disk, bypassing any work by the CPU.\n\nSo my question is mostly about the nature of machine learning workloads, I suppose. Are they structured such that it is common for GPU to depend on data that either 1) has been loaded into memory by the CPU from disk directly or 2) has been computed by the CPU and stored into memory? If so then I can see the bottleneck being eliminated here. On the other hand if the computations can be structured such that the GPU can fulfill the entire workload end-to-end, starting from the data on the disk, then I don't think this bottleneck is relevant for machine learning workloads.",
        "For “normal” setups, the cpu is the broker of data into and out of the GPU. Accessing the disk directly would require, I would think, running an operating system.         \nIn large systems like the Memphis Supercluster, it sounds like the GPUs use RDMA to create/access shared memory. https://www.datacenterdynamics.com/en/news/xais-memphis-supercluster-has-gone-live-with-up-to-100000-nvidia-h100-gpus/        \nStill plenty of cpus around in each rack, don’t know the details.    \nVideo of the facility: https://youtu.be/Jf8EPSBZU7Y?si=H01PNS3cMDZmckDy",
        "Yes, what you describe exists: https://developer.nvidia.com/blog/gpudirect-storage/\n\nFor an enterprise system running gpu clusters with TB of collective VRAM, this is the best solution.\n\nHowever, for personal computers, it doesn’t solve the problem of limited and expensive VRAM. Most “affordable” would be an A6000 with 48GB, costing $4200… for half that at only $2000, you can get a new Mac Mini with M4 Pro and 64GB of unified memory. For a little more at $4700, you can get a MacBook Pro with M4 Max and 128GB. Granted, the proper GPU will run faster on any given model that can fit, but the unified memory allows for larger models the single GPU simply isn’t capable of…",
        "It depends on how you do data processing I guess. For use cases when you don’t need cpu data processing, this can be better (thus making unified memory unnecessary). However, wouldn’t unified memory just be  more generalizable to more use cases (i.e. cases where you want to manipulate the data with the cpu)? For example, the majority of LLM pipelines probably postprocess the nn output using cpu.",
        "When you need a solution to terrible design"
    ]
},
{
    "submission_id": "1gfy94p",
    "title": "Training models--local or cloud?",
    "selftext": "I’m studying ML theory and plan to apply it with Python and relevant libraries. I’m also considering a master’s in AI/ML in about half a year.\n\nAre there economical cloud-based options for small datasets and ML applications? I think it’s unnecessary to rely on a local environment for ML learning, right?",
    "created_utc": "2024-10-30T14:46:32",
    "num_comments": 2,
    "comments": [
        "Small datasets can be trained locally, especially when its not a deep learning model. Otherwise, you can use Google Colab, and Kaggle to get access to GPUs. I pay for Colab pro (started in undergrand, now use it during my masters), as you get a pretty good amount of access to GPUs. Sometimes it can be annoying however when Google Colab shutdowns randomly after a few hours. \n\nIn the context of learning ML, you likely would be able to get away these solutions for most problems from school. Doing anything larger you would likely want to start up a server in the cloud.",
        "Thanks, I appreciate your input on this! I'll look into Google Colab for anything too power-hungry to handle locally.\n\nI was mostly curious because a colleague who was getting into ML (perhaps it was a deep learning model; I didn't ask at the time) said he had damaged his work-issued laptop and had to get it replaced. Google Collab looks like a great solution to that problem, especially as I'll be running these models on my own computer."
    ]
},
{
    "submission_id": "1gfwqj5",
    "title": "[P] PyTorch Quantization of model parameters for deployment on edge device",
    "selftext": "",
    "created_utc": "2024-10-30T13:41:32",
    "num_comments": 0,
    "comments": []
},
{
    "submission_id": "1gfvn2p",
    "title": "[D] I’m an ML/programming educator - I was invited as ceo of codesmith to Berlin Global Dialogue (tech/AI insider conference) - see what they said behind closed doors - AMA",
    "selftext": "",
    "created_utc": "2024-10-30T12:55:07",
    "num_comments": 0,
    "comments": []
},
{
    "submission_id": "1gfvmc3",
    "title": "Why is Stable Diffusion image generation single GPU only?",
    "selftext": "I have a RTX 3060 and RTX 4070 in the computer I built. I can use both GPUs simultaneously with just about any LLM model and it works reasonably well.\n\nHowever, I have not found any Stable Diffusion image generator software that supports multi-GPU except maybe a sequential batching model where generation requests get routed to different GPUs for each to generate a complete image independent of the others.\n\nIs there something fundamental to the architecture of Stable Diffusion models that makes multi-GPU generation impossible?",
    "created_utc": "2024-10-30T12:54:17",
    "num_comments": 3,
    "comments": [
        "Comfyui with multi-gpu nodes to load unet on one gpu and vae and clips on the other. Won’t have access to the compute from both but you can get some large models into vram. \n\nSwarmUI for parallel batch",
        "I'll have to look into the ComfyUI nodes. That looks helpful since for some models I'm using I hit the 12GB VRAM limit. Thanks.",
        "Yeah getting the vae and clips on another gpu should give you more headroom on your primary, and you’ll notice a speed boost since it won’t have to unload and reload those smaller components."
    ]
},
{
    "submission_id": "1gfvb9d",
    "title": "The Only Book You Need to Master AI Tools and Advance Your Career",
    "selftext": "Traditional methods no longer keep up in a world that’s changing fast due to artificial intelligence. In the book *Mastering AI Tools*, you'll discover practical ways to use the latest AI tools to grow your skills, broaden your career prospects, and achieve real success. This isn’t just theory; it’s a guide that blends knowledge with real-world applications, putting today’s essential AI tools into your hands—the ones trusted by leaders and creators in the industry.\n\n*Join a community of professionals who’ve used these techniques to change their careers.*\n\nIf you’re ready to shift your path and stay in sync with today’s digital landscape, this book could be your opportunity. Start learning now and equip yourself with skills that matter, becoming part of the next wave of digital professionals.[Get your copy now!](https://skillsetnow.com/mastering-ai-tools-ebook/?&shield=45c583l-o5nhj84czcwcwcx-8h&ad=Chatgpt2)",
    "created_utc": "2024-10-30T12:41:00",
    "num_comments": 4,
    "comments": [
        "Nice try diddy",
        "bro posted an ai generated post promoting his ai tools book",
        "hahaha.. Good try but it ain't fooling no one ;-)",
        "*his ai-generated ai tools book"
    ]
},
{
    "submission_id": "1gfv79z",
    "title": "By how many multiples can I expect an Nvidia RTX 4090 to outperform a Macbook 2023 Pro M2 with 96 GB memory.",
    "selftext": "Trying to find out if its worth getting a dedicated station to do the training or doing in on the MacBook Pro is good enough. The goal is to speed up the time of the machine learning. The software used is Anaconda and used on Windows.\n\nThe system will have\n\n1600 Watt System\n\nVideo Card NVIDIA® GeForce® RTX 4090 24GB GDDR6X (3-Slot) (1xHDMI, 3xDP)\n\nMemory Capacity 24 GB\n\nProcessor GeForce RTX 4090 (AD102-300)\n\n**Memory**\n\nTechnology DDR4\n\nType 288-pin DIMM\n\nCapacity 6 x 64 GB\n\nSpeed 3200 MHz\n\nError Checking ECC\n\nSignal Processing Registered\n\n**Processor** \n\nProduct Line Ryzen Threadripper PRO Socket sWRX8 Clock Speed 4.0 GHz\n\nI know its a general question but a ball number is sufficient. Like will outperform by x10 or x2?  what to expect? ",
    "created_utc": "2024-10-30T12:36:11",
    "num_comments": 6,
    "comments": [
        "Depending on the task with optimized code it can be upwards of 100x. If it's LLM inference or something that requires a lot vram then the m2 can beat the 4090. That said if memory isn't too big of a bottleneck during inference, 4090 with transformer engine and tensorrt + kernel fusion with fused GEMM, Conv, residual add etc ops will run laps around m2.",
        "[deleted]",
        "I would always go for the Nvidia setup.        \nIn addition to the speed above Mx (ram isn’t everything), you can expand your system in the future, if you want. Add a second 4090, for instance. With Apple, you’re locked in with no upgrade path.",
        "It's hard to answer without more specifics, but I've personally seen a 100x speedup for matrix-vector multiplies on an Nvidia GPU vs intel CPU in pytorch. In other cases, my CPU is 5x faster than my GPU. This is all in PyTorch.\n\n\"Anaconda\" is a package manager, so it's hard to know what you're actually doing by this. For LLMs such as llama-8.1B, I've seen a 2x-5x speedup using a GPU. Also, folks will suggest Linux if you get a dedicated system.",
        "Thank you, is there a source for those bench marks you are referring to?",
        "The 100x is correct on the PyTorch for GPU vs CPU, but they wont be using the CPU, they’ll be using Metal on the GPU of the M2 Max. On my tests something that took 10 minutes on the CPU of my M3 Max, took about a couple of seconds in the GPU. PyTorch maths code.",
        "Oh wow that is significant. Sounds like a very similar test to what I am looking for. I think that them means it worth it for us to do this. 10minutes to a few seconds. Lets hope this is the case here as well"
    ]
},
{
    "submission_id": "1gfup68",
    "title": "Resume review (for new grad ML/DS roles 2025)",
    "selftext": "https://preview.redd.it/gpgu6d1u0yxd1.png?width=927&format=png&auto=webp&s=6be8d0a002512f731fe7f709c30745e4dc041d37\n\n",
    "created_utc": "2024-10-30T12:14:46",
    "num_comments": 4,
    "comments": [
        "Wall of text littered with buzzwords",
        "Any particular suggestions to improve it? These are things I have actually worked on..",
        "I mean no one is going to read this, it’s literally text from start to finish and you haven’t even graduated yet. So you need to condense this somehow. 1 bullet per project and publication describing it, instead of 2 perhaps. 3 bullets for experience. Remove the technical skills where you’re just listing courses that no one cares about. Also, Jupyter notebook is not a platform. \n\nEvery single bullet for your professional experience lists a ton of buzzwords and claims to accomplish fancy things, but doesn’t say a single thing about what business value you actually drove. It’s cool that you were the first on your team to implement LLM fine-tuning and inference pipelines, but what did you actually achieve by doing this? How did this drive actionable business value? Is the value you created from this quantifiable? If so what is a conservative estimate of the value you created? \n\nThese are the things that people care about in the job market today. Not that you can rattle off every buzzword related to LLMs and computer vision models, regardless if you’ve worked on these things or not. \n\nThat’s my opinion, take it for what it is. Best of luck to you.",
        "Thank you for your suggestions. I'll try to roughly quantity my achievements. About the technical skills section, I'm not a fan of it either, I was told to do it to beat the ATS automated resume ranking bots. How do you think I would be able to beat that"
    ]
},
{
    "submission_id": "1gfu2kx",
    "title": "Resume Tips = Also for beginners",
    "selftext": "I must've have tried at least 100 different resume designs till now. Some of them got me interviews and some did not. I am going to share with you briefly what my learnings were.\n\nBackground - I am a digital marketer and UI/UX designer, at least now I can say that but when I started out looking for jobs during my college. It was a dreadful task to create resumes and cover letters since I was never satisfied and I did not have much idea about Adobe & Canva at that time.\n\nI started out with Canva- Those resumes were not T-shaped, extremely colourful and had no consistency.\n\nI started adding more content with proper headings, subheadings etc. Started to get more precise with which content should go where to make sure that it was consistent throughout.\n\nFast forward, my resume started to look more professional and gained some weight.\n\nI could never afford to pay someone else to make my resume but if you can, I will strongly advise you to do it. It will save so much of your time and effort. I did it on my own but this experience came in handy. Also, those online resume templates are aesthetically pleasing but I can almost never fit my stuff into those and if I change anything, their formatting explodes. I stopped using those.\n\nRecently I tweaked it again a few months ago and now my resume gets me interviews very easily and I am quite satisfied with it. It took a lot of effort and research to get to the most optimal resume I have right now but who knows I might change it again.\n\nMy Key Learnings:\n\n1. I have heard that MS Word resume works and I tried it during college. Honestly, it might have worked 10 years ago but now recruiters want more than just text. Think about it, as a company recruiter which resume would you prefer- a plain black and white Word document or a good-looking designer-made resume which shows effort and skills? No need to guess!\n\nA lot of small companies still might accept the old word resumes but I have almost never seen any senior in a reputed company go with that.\n\n2. The resume should be such that the reader is prompted to go through it thoroughly. If there are many colors, different fonts or some parts are bold and some italic or different unnecessary lines in between, the viewer gets confused as to where to start looking.\n\nA good resume should be top to bottom, easy to read and not straining to the eyes. Yes, it is possible for the recruiter to read the whole resume or at least some parts of it. And it's your job to guide them to do so.\n\n3. For an entry-level professional- one page is ideal and for seniors, 2 pages are considered good. But don't compromise on your experience to fit it in. It's more important to showcase your capabilities, no of pages are the least important.\n\n4. Sequence of section-\n\nContact details -> Career summary -> Skills -> Work experience -> education -> Projects & certifications -> references (if any).\n\nThese are all important sections- don't skip any. If you've not done any projects or courses, make it up.\n\n5. Always add timelines to the job experience and certifications.\n\n6. Keep going back to your resume sporadically, read it again. Bet you will find something to improve.\n\nLastly, these videos might help - [https://www.youtube.com/watch?v=a5qUeeT2m8k](https://www.youtube.com/watch?v=a5qUeeT2m8k)\n\n",
    "created_utc": "2024-10-30T11:48:27",
    "num_comments": 0,
    "comments": []
},
{
    "submission_id": "1gfsg3s",
    "title": "Recurrant Neural Network question",
    "selftext": "How do I know how many layers my RNN should have and how many neurons per layer I should implement? Is it purely trial and error or is there a more \"correct\" way of going about this? I need to design a model that will predict anomalies in a gearbox system. I have a bunch of sensors on many components within said gearbox (e.g. vibrations of a shaft, temperature of a bearing etc.) and need to use this data to create a RNN model that will tell me (as early as possible) when a failure will happen. Any help is appreciated.",
    "created_utc": "2024-10-30T10:41:27",
    "num_comments": 1,
    "comments": [
        "trial and error"
    ]
},
{
    "submission_id": "1gfr8ae",
    "title": "5 Interesting Deep Learning Research Pattern",
    "selftext": "",
    "created_utc": "2024-10-30T09:51:12",
    "num_comments": 0,
    "comments": []
},
{
    "submission_id": "1gfqgnv",
    "title": "Improving My Content-Based Recommendation System: Feedback Needed!",
    "selftext": "Hello everyone,\n\nI’m currently working on developing a content-based recommendation system that leverages audio features. I have implemented some code, and while I’ve achieved some results, I’m eager to get feedback on potential improvements or any mistakes I may have overlooked.\n\nAny insights or suggestions would be greatly appreciated!\n\n    import ast\n    import os\n    import numpy as np\n    import pandas as pd\n    import tensorflow as tf\n    from sklearn.metrics.pairwise import cosine_similarity\n    from sklearn.preprocessing import MinMaxScaler\n    from tensorflow.keras import layers, losses\n    from tensorflow.keras.models import Model\n    os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"-1\"  # Disable GPU\n    class Autoencoder(Model):\n      def __init__(self, latent_dim, shape):\n        super(Autoencoder, self).__init__()\n        self.latent_dim = latent_dim\n        self.shape = shape\n        self.encoder = tf.keras.Sequential(\n          [\n            layers.Flatten(),\n            layers.Dense(latent_dim, activation=\"relu\"),\n          ]\n        )\n    \n        self.decoder = tf.keras.Sequential(\n          [\n            layers.Dense(\n              np.prod(shape), activation=\"sigmoid\"\n            ),  # Use np.prod instead of tf.math.reduce_prod\n            layers.Reshape(shape),\n          ]\n        )\n    \n      def call(self, x):\n        encoded = self.encoder(x)\n        decoded = self.decoder(encoded)\n        return decoded\n    \n    def load_track_features(path, max_rows=None):\n      df = pd.read_csv(path, delimiter=\"\\t\", nrows=max_rows)\n      # Apply ast.literal_eval on each cell that contains list-like strings\n      for col in df.columns:\n        df[col] = df[col].apply(\n          lambda x: (\n            ast.literal_eval(x) if isinstance(x, str) and x.startswith(\"[\") else x\n          )\n        )\n      df.drop(columns=[\"tags\"], errors=\"ignore\", inplace=True)\n      return df.to_numpy()\n    # Data preparation function\n    def prepare_data(data):\n      scaler = MinMaxScaler()\n      data_normalized = scaler.fit_transform(data)\n      return data_normalized, scaler\n    \n    def recommend_similar_tracks(track_id, encoded_items):\n      # Calculate cosine similarity between the target track and all other tracks\n      sim_scores = cosine_similarity([encoded_items[track_id]], encoded_items)[0]\n    \n      # Sort by similarity and get most similar tracks\n    \n      sim_track_indices = np.argsort(sim_scores)[::-1]\n      sim_scores = sim_scores[sim_track_indices]\n    \n      # Create a mask to exclude the input track from the recommendations\n    \n      mask = sim_track_indices != track_id\n    \n      # Filter out the input track from the indices and similarity scores\n      filtered_indices = sim_track_indices[mask]\n      filtered_scores = sim_scores[mask]\n    s\n    if __name__ == \"__main__\":\n      # Example data - replace with your actual data\n      R = load_track_features(\"../remappings/data/Modified_Music_info.txt\", 30000)\n      # Prepare data\n      data_normalized, scaler = prepare_data(R)\n      # Train autoencoder and get item feature matrix\n      latent_dim = 16  # Adjust as needed\n      input_shape = data_normalized.shape[\n        1:\n      ]  # Assuming data_normalized is (num_samples, num_features)\n      autoencoder = Autoencoder(latent_dim, input_shape)\n      autoencoder.compile(optimizer=\"adam\", loss=\"mean_squared_error\")\n      autoencoder.fit(data_normalized, data_normalized, epochs=10)\n      # Get the encoded items\n      encoded_items = autoencoder.encoder.predict(data_normalized)\n      track_id_to_recommend = 0\n      similar_tracks, similar_tracks_scores = recommend_similar_tracks(\n      track_id_to_recommend, encoded_items\n      )\n      recommend_id_example = similar_tracks[2]\n      print(\"Recommended Track Indices:\", similar_tracks)\n      print(\"Similarity score:\", similar_tracks_scores)\n      print(\"score of recommended item similarity of encoded item: recommend_id_example\")\n      print(\n        cosine_similarity(\n          [encoded_items[track_id_to_recommend]],\n          [encoded_items[recommend_id_example]],\n        )[0][0]\n      )\n      print(\"score of recommended item similarity of item: recommend_id_example\")\n    \n      print(\n        cosine_similarity(\n          [data_normalized[track_id_to_recommend]],\n          [data_normalized[recommend_id_example]],\n          )[0][0]\n      )",
    "created_utc": "2024-10-30T09:18:38",
    "num_comments": 0,
    "comments": []
},
{
    "submission_id": "1gfqdsg",
    "title": "This is the course outline of the machine learning course . Anyone with an idea about how good the course is?",
    "selftext": "Title : Complete A.I. Machine Learning and Data Science: Zero to Mastery",
    "created_utc": "2024-10-30T09:15:19",
    "num_comments": 14,
    "comments": [
        "Depends on your objective. Is this for an introductory course? Because I can guarantee you that there is an insane amount of things you have to ingest in ML before being anything close to an « expert ». I know ZTM and I don’t think they’re the best for ML related contents. I would rather chose Coursera. But it can be a good introductory course if you’re starting. If, at some point, you really want to go deep into a subject though, books are the way to go imo.",
        "I took ML course 6 years back, it had almost same syllabus… \nI would say ,to start as novice it’s good..\nBut not for mid or advance learners.It’s not even close to where the technology is heading..\n\nIf you want to take this course,check how much they’re charging for this.. I am sure all of these topics you can learn free on internet.. or cheaper on Udemy..\nIf they’re giving any value add ,then only it’s worth…and I highly doubt that..",
        "It's bothering me that these don't appear to be in either sequential or alphabetical order....",
        "I'm a novice myself so take anything I say with a grain of salt but I believe your time is better spent learning Pytorch instead of tensorflow. It's not so hot right now.",
        "Everything here is very basic practical stuff that seems surface level and something you’d get at a really poor university. Completely missing linear discriminant functions or back propagation or any probability like Bayes. How are you going to learn SVM without any basis studying linear discriminants? I’d give this course a 2/10. Just buzzwords without building real theoretical background.",
        "Conceptual understanding is a lot easier with torch. TF is not maintaned anymore and useless, while torch opens the entire industry for you. \nIt does not even mention some of the hotest NLP libraries out there!",
        "The second last one ,using GPU in colab? what's the big deal in it? I'm a novice too\nEverything else looks good to me",
        "Just use YouTube bro. You'll learn the rest though and internship/job",
        "I'm just a novice to machine learning. But can you recommend some books for me",
        "I am starting as a novice , but true I'll check other free channel up",
        "I don’t get super hung up on tech, it’s much more about conceptual understanding to me. I’d take you if you had tensorflow or Keras experience so not a disqualification but by the same token I’d agree, PyTorch is where things currently are at. So it definitely wouldn’t be the best choice necessarily to choose those for new learning now. It’s one thing to have existing experience it’s different to learn suboptimal tools by choice.",
        "Oh okay thank you for this advice",
        "Thank you",
        "There are many, many excellent books on this topic. I can recommend Elements of Statistical Learning (which is often mentioned) or Machine Learning with PyTorch and Scikit Learn by Sebastian Raschka. The main problem when you’re starting ML is that if you’re lacking in maths, it’s gonna be hard and frustrating. Online boot camps often abstract the maths for more practical experience which can be a good way to start."
    ]
},
{
    "submission_id": "1gfq2nj",
    "title": "intro to superposition",
    "selftext": "We made a video to introduce superposition: a foundational concept enabling parallel computing. \n\nWhile classical bits are either 0 or 1, quantum bits (qubits) can exist in a probabilistic combination of both states at once, allowing for powerful, simultaneous computations beyond the reach of classical methods.\n\n[https://www.youtube.com/watch?v=6I8JcA0H\\_-E](https://www.youtube.com/watch?v=6I8JcA0H_-E)",
    "created_utc": "2024-10-30T09:02:32",
    "num_comments": 1,
    "comments": [
        "and if you want to understand qubits first, start here: [https://www.youtube.com/watch?v=258-IkgWekA](https://www.youtube.com/watch?v=258-IkgWekA)"
    ]
},
{
    "submission_id": "1gfpsw6",
    "title": "How good is this online course ? ",
    "selftext": "",
    "created_utc": "2024-10-30T08:51:14",
    "num_comments": 89,
    "comments": [
        "I have a hypothesis: if you can get a PhD in something, anyone saying you can master it in 3 months is full of shit.",
        "I feel like the course description is written by someone who doesn't really understand machine learning at all. Someone who would label themselves as \"prompt engineer\" and \"AI enthusiast\" on linkedin.\n\nOn a more serious note I'm really concerned on the lack of mathematics and especially numerical mathematics. No mention of statistics, numerical simulation, probability theory, linear algebra which are subjects that took me 3 years of uni to start understanding, while this course says it only takes 3 months to master that and the technologies: python, pandas and preferably pytorch.\n\nI do not know that many learning platforms for AI/ML, but I can recommend **Kaggle**. Seeing what codes other users have made for a specific dataset is really helpful and to take their code and play with it teaches a lot. Outside of that I would recommend a semi strong foundation in numerical mathematics because that is what all of AI/ML stuff is built on. Something like **scikit-learn** (which is really shamefully not mentioned in the course) is excellent ML library, that can be used through Kaggle for understanding basic to intermediate level machine learning which in real life when mastered can be used to solve real problems where most \"AI enthusiasts\" would resolve to unnecessary complex neural networks. Scikit-learn also has a lot of learning datasets and tools for generating datasets to try out different stuff.",
        "3 months 🤡",
        "\"modern\" \"up-to-date\" and it mentions tensor flow 💀",
        "Take course by andrew ng on coursera it's more in detail and worth the time",
        "If I'd do it all over again, I'd recommend the machine learning specialization course on coursera. I've been doing ML adjacent stuff for a few years now, and I finally feel like I'm understanding how everything works in detail. Once you finish that, you can do the hands on ml book which has lots of practical applied examples.",
        "I bought a few of those ZTM Courses on Udemy and they are mostly garbage.  \nBy far the worst courses I've ever bought.  \n\nWith that being said, their PyTorch course with Daniel Bourke is nice if you want to get started and have no prior experience, just know right away that it might be a bit outdated as ZTM doesn't really update anything on Udemy\n\nIf you want to check it out before buying, he's got the first 25h of the course available for free on his YouTube Channel.  \nHere's the link:  \nhttps://youtu.be/Z_ikDlimN6A?si=xuc71H047zJ1FUOt",
        "Take a course in PyTorch",
        "Look into the Data talks club. They have free courses on ML, MLops, data engineering, and more. You can do it at your own pace or jump into their current ML course. Not magic, but it gives a solid experience AND helps identify areas you need to research and study more on your own",
        "Can you post the outline of the course? Like, how do the section the ML or AI part. What stuff they highlight",
        "I did this course on udemy, they teach numpy, pandas and matplotlib then Sklearn and then tensorflow, it was okayish course, most of the stuff they teach can be found free on the internet.",
        "I took it for the first 4 chapters/sections. The instructor is obnoxious and thinks he is super funny but he isn't. It has also zero math background, its just running code on a Jupyter Notebook to \"see what it does\" and if it seems to work then you're done. \n\nBooks are a better alternative in my opinion especially those that cover the mathematical background needed and apply the math.",
        "Tensorflul",
        "You might be interested but one of their courses is actually free on YouTube under the freecodecamp channel. It’s their “PyTorch for Deep Learning Bootcamp” course. \n\nVideo Link: https://youtu.be/V_xro1bcAuA?si=iWzHS8XkdG15f8SX",
        "I took this course to prepare for the ML course in my online masters. I really liked it a lot. Keep in mind it’s not deep learning, but instead Scikitlearn.",
        "Never heard of the company. Also, most ML engineering jobs require master’s degrees in comp sci or data science. This course sounds like a waste of $. You could probably learn the same stuff from YouTube. Personally, if you’re looking for certification courses, check out DeepLearning.AI. Start with the machine learning specialization, then do the deep learning specialization, and decide what type of AI you want to focus on (traditional ML, NLP, neural networks, computer vision, image/video generation, etc.).",
        "It's actually pretty good. Focuses more on practice as opposed to the theory and math behind it. If you're a complete beginner it's a good intro course.",
        "I don't think it's possible to master ML from scratch in just 3 months. It ain't that simple.",
        "Bad. It says AI and ML is the same.",
        "I'd say it's obvious that 3 months isn't enough to master one of the most difficult areas of CS.",
        "it will get you to a point where you can do some basic of data manipulation with pandas, some basic coding with python, basic data visualization and some application of ML model with tensorflow or torch or sklearn.\n\nWill not help you master machine learning/data science by any means.",
        "Use Medium extensively, paired with Kaggle and Andrew Ng’s courses",
        "I’ve taken it. It’s not a bad overview of topics you need to learn, but it’s *nowhere* near enough for actual mastery. I’d say, take a look at the outline and find other courses to study the material.",
        "Don’t bother",
        "wow that's a bargain xD \n\n3 months when some people learn it in 3 years !",
        "Anything that says mastery in a year or less is a scam",
        "Sounds too good to be true, you're not going to become an ML Engineer just with this, even a Masters degree lasts 1-2 years, that isnt enough to become a true expert in ML since the field primarily looks for PhDs. Looks like a scam to me.",
        "I’ve taken this course and I can say, you pick a few nifty tips and tricks from this one. \n\nBut you need a pipeline of courses + projects to be actually comfortable in DS",
        "No chance without classic algorithms, that are not even mentioned.",
        "If you read tensorflow as a DL framework. RUN…!",
        "I did the same course actually, it didn’t took me 3 months though more like 1. It teaches you the core concepts of machine learning and data science and how to make use of these tools in a practice levels showing common problems people in the field have to go through on a daily basis.\nBut it totally lacks the depth needed to grasp the mathematical foundations behind them. It’s good to get a general understanding of how everything works, but if you want to really learn you will have to dig deeper.\nStill id recommend it, it was valuable to me and pushed me into learning more.",
        "ZTM has some good stuff. \n\nFor AI/ML DeepLearning.AI has their courses which are industry recommended. Andrew Ng is renowned for AI/ML.",
        "I watched the course. And I honestly got so much value from it as a beginner in the field. Totally recommend.",
        "You might learn some stuff, but you won't get a job from this. Nearly everyone in the field at least has a master's degree. That's your competition.",
        "It’s a good course to get up with the absolute basics. Don’t expect to get a job from this course though.",
        "ML and AI bootcamps are bullshits; That's a PhD level thing, unless you're looking for an AI engineer role.",
        "claim: If you see a full course with homework and midterms available from a University you'd dream getting into like MIT/Harvard/Stanford FOR FREE - it's a good idea to pass them instead of some dodgy courses for which you have to pay for and ask strangers for reviews.\n\n1. Theoretical classical ML - [https://www.youtube.com/watch?v=Bl4Feh\\_Mjvo&list=PLoROMvodv4rNyWOpJg\\_Yh4NSqI4Z4vOYy](https://www.youtube.com/watch?v=Bl4Feh_Mjvo&list=PLoROMvodv4rNyWOpJg_Yh4NSqI4Z4vOYy)\n2. Deep Meta and Multitask Learning - [https://www.youtube.com/watch?v=bkVCAk9Nsss&list=PLoROMvodv4rNjRoawgt72BBNwL2V7doGI](https://www.youtube.com/watch?v=bkVCAk9Nsss&list=PLoROMvodv4rNjRoawgt72BBNwL2V7doGI)\n3. Machine Learning with Graphs - [https://www.youtube.com/watch?v=JAB\\_plj2rbA&list=PLoROMvodv4rOP-ImU-O1rYRg2RFxomvFp](https://www.youtube.com/watch?v=JAB_plj2rbA&list=PLoROMvodv4rOP-ImU-O1rYRg2RFxomvFp)\n4. [https://www.youtube.com/watch?v=XZ0PMRWXBEU&list=PLoROMvodv4rPOWA-omMM6STXaWW4FvJT8](https://www.youtube.com/watch?v=XZ0PMRWXBEU&list=PLoROMvodv4rPOWA-omMM6STXaWW4FvJT8)\n5. Deep Bayes - Bayesian methods in Deep learning - fantastic course, very advanced IMHO - [https://www.youtube.com/watch?v=wA8UMzhGv7o&list=PLe5rNUydzV9QHe8VDStpU0o8Yp63OecdW](https://www.youtube.com/watch?v=wA8UMzhGv7o&list=PLe5rNUydzV9QHe8VDStpU0o8Yp63OecdW)\n6. Deep Learning - [https://www.youtube.com/watch?v=PySo\\_6S4ZAg&list=PLoROMvodv4rOABXSygHTsbvUz4G\\_YQhOb&pp=iAQB](https://www.youtube.com/watch?v=PySo_6S4ZAg&list=PLoROMvodv4rOABXSygHTsbvUz4G_YQhOb&pp=iAQB)\n7. Again some general ML/AI - a bit more widespread than the MIT course by Cornell - [https://www.youtube.com/watch?v=MrLPzBxG95I&list=PLl8OlHZGYOQ7bkVbuRthEsaLr7bONzbXS](https://www.youtube.com/watch?v=MrLPzBxG95I&list=PLl8OlHZGYOQ7bkVbuRthEsaLr7bONzbXS)\n\nyou get the gist. Do these. They're free. There's course material available. There's also courses on NLP, Transformers, Computer Vision  and more that I can't be assed to get now. All free on YouTube.\n\nEDIT:\n\nAlso the required Math. Gilbert Strang's - Linear Algebra is godsend, Herb Gross' Calclus 1 to Differential Equations is Godsend. There's also two courses for real analysis and functional analysis.\n\nThere's course on Statistics / Probability from Harvard.\n\nWe live in the best timeline for learning. Don't pay for shit that you can get for free, legally and in a MUCH higher quality. \n\nAlso, yes, I'm totally piggybacking a top comment to shill for top universities :D",
        "You're right . I never believed in any of those 3 months expert talks",
        "Thank you for this insight",
        "Could you DM me  link for your profile on GitHub or linkedin? I really want to get a job as I am in my 3rd year and don't know much about it",
        "The description is written to target the kind of audience they are hoping to separate from their money.",
        "Not good ?",
        "OP I'll train you to be a master in Theano in 2 weeks for only $420",
        "Wait what tf is still popular in the industry",
        "Is it free??",
        "Thank you for this information",
        "Yeah planned on doing that . Thank you",
        "Thank you very much",
        "Heya! Another beginner here.\nShould I start with this data talks club course or Andrew ng ml specialization?",
        "Right",
        "Haha you're absolutely Spot on.",
        "Thank you",
        "Okay",
        "Okay",
        "Okay",
        "Hahaha 😂",
        "Thanks",
        "I'm just a novice.  Thank you",
        "Thanks for the list. Side note, are you an AOE2 fan?",
        "Yeah man! I completely second you. I am on track with resources like the ones you have provided. I would like to add University of Tubingen's material to the list. Its fantastic. Check out the playlists on their channel.\n\n[https://www.youtube.com/c/TübingenML/playlists](https://www.youtube.com/c/TübingenML/playlists)",
        "That's really helpful.",
        "Trust me bud! These 3 months promises are made by snake oil salesmen. Please don't fall for their gimmicks.",
        "I don't like sharing personal information. Ask here in reddit anything you think I can help you with.",
        "It surely is, but mostly because of legacy. Pytorch is being used much more frequently for new stuff, in both academia and industry",
        "You can audit the course for free, but you will only gain access to the videos. If you care about certificate and lab exercises then it's paid",
        "No but it's not that expensive pretty affordable",
        "You get what you pay for.",
        "Price-wise, DTC is better: since it's free, you can learn ML without spending money to see whether it's your field at all. Also, it's practical. I haven't done Andrew Ng's ML course, but I imagine it's more rigorous",
        "I would instead suggest doing a Coursera course like the others said, or use DataCamp which has more hands on coding, however I don’t think DataCamp certificates have much value. So if certification is important, go with Coursera, and if its really important than go with a university course",
        "Not just an AOE2 fan, a Daut fan :D",
        "How good should I be with mathematical concepts? I am in 2nd year of my uni and I am kinda overwhelmed by all this.",
        "Can you list your major projects brother as I only make movie recommendations system 🥲. Please help",
        "Sure, but I don’t think it can be called outdated at this point. Especially for a bootcamp-like course",
        "I tried auditing but I was told to input my credit card details . Which I stopped",
        "No it's free, you can audit it and I think it's one that doesn't remove any exercises then or even if they are available on deeplearnings github",
        "Oh oky",
        "Yeah thanks",
        "Thank you very much",
        "depends. for ml engineer/ml ops not much more than a regular dev tbh. for research and academia tho def a ton of math. like \"majoring in math might be the smarter idea\" ton. went into my cs ms directly from my cs undergrad and i felt like i was reading hieroglyphics my first year. to this day i struggle a little bit, but it gets easier over time.",
        "Really depends on what kind of career are you looking for. My career is on industrial data science which mostly consists of timeseries data analysis and regression models. Looking at kaggle there is [Kaggle's own learning platform](https://www.kaggle.com/learn/time-series) and [this industrial time series dataset](https://www.kaggle.com/datasets/edumagalhaes/quality-prediction-in-a-mining-process) looks really interesting. Solve those problems, try out different methods and maybe post the results to you linkedin if you want to build portfolio.\n\nMaybe you can format your posts as business recommendations based on data analysis observations which is really common in this field. Load data -> preprocess -> filter -> analyse -> understand -> report observations and recommendations -> $$$",
        "Some functionality of tflite (or whatever it's called nowadays) is broken. Tf support on windows is dropped. Possibly lots of other issues too that I'm not aware. I think that's a sign that a tool is slowly becoming outdated. I've read that google internally is prioritizing jax over tf too.\n\nPlus bootcamps shouldn't really set standards for anyone. It's typical for them to use outdated stuff",
        "Do not try to enroll into specialisation as a whole. If a specialisation have 5 courses then search the name of course individually then you can audit them",
        "You can also just apply for financial aid and get it for free",
        "I am not so sure about whether I wanna get into mlops or research I have been trying to get my hands on projects related to nlps rn",
        "I just randomly search for machine learning projects and do them on my own , but you are helped me a lot. Thank you very much. Can you just tell me how to improve my portfolio or CV (sry that you are looking at these stupid questions but I genuinely to know from a person who is the one I want to become)",
        "I’ll have to read up on it then I guess. I would think that learning the basics in either tf or pytorch doesn’t really matter at the end of the day, I never had an interview where they made an issue about me being more familiar in tf even if they use torch",
        "Okay thanks",
        "I'm afraid I really cannot help with CVs or portfolios. I got my job through being at the right place at the right time and meeting my current CEO. The first year I did front end development because they didn't know what to do with me. But for the second year we had discussions and I had to explain and teach them about my field. What is data science, what is machine learning, and what can I do that will generate profit... and I'm still learning more on those every day. There really isn't any guaranteed sales pitch to sell yourself as data scientist",
        "Thx for your help  it means a lot"
    ]
},
{
    "submission_id": "1gfomtb",
    "title": "The Power Combo of AI Agents and the Modular Data Stack: AI that Reasons",
    "selftext": "",
    "created_utc": "2024-10-30T08:02:26",
    "num_comments": 0,
    "comments": []
},
{
    "submission_id": "1gfniiz",
    "title": "Best way to translate English sentences to various languages ",
    "selftext": "Our application is supported in various languages and it’s an enterprise one. We usually give the new sentences to various people who are expertise in different languages to translate which costs the company a lot.\nWe are exploring ML models to automate this. \nOur team has suggest OpenAI to do this which is probably a decent idea. I am wondering if there is any other way to try this.\nTranslation won’t be a live translation, translation will happen and we will update on the relevant files and it will get released later. \nI tried a local model like Marian, it didn’t do well even for basic sentence Japanese. \n\nI am not new to ML, been learning for sometime but lack experience. ",
    "created_utc": "2024-10-30T07:13:59",
    "num_comments": 2,
    "comments": [
        "Thought of using Google translation api?",
        "I had good results with EasyNMT (open source)"
    ]
},
{
    "submission_id": "1gfn7j5",
    "title": "Need advice and resources for the software and ML part of this project. (I'm a dummy who doesnt know how to work on AI/ML).",
    "selftext": "[So what it does is... using the esp32-cam and ai it identifies the plants, gives co-ordinates to the servos to point at the plants and shoot water on them. then well create an app to control\\/monitor the device.](https://preview.redd.it/29c8z202gwxd1.jpg?width=720&format=pjpg&auto=webp&s=e110f546521c20cf398af00a1deed354410db745)\n\n",
    "created_utc": "2024-10-30T07:00:22",
    "num_comments": 0,
    "comments": []
},
{
    "submission_id": "1gfmrhr",
    "title": "What AI/ML Models Should You Use and Why? - Jozu MLOps",
    "selftext": "",
    "created_utc": "2024-10-30T06:39:36",
    "num_comments": 0,
    "comments": []
},
{
    "submission_id": "1gfmmjx",
    "title": "Exploring the Role of AI in Data Labeling Solutions",
    "selftext": "Hey everyone! \n\nAs AI continues to evolve, one area that’s seeing significant improvement is data labeling. This blog I found dives into how AI-driven data labeling solutions are changing the landscape and making the process faster, more accurate, and scalable. \n\nHere are a few highlights: \n\n* **Efficiency Gains:** AI-powered tools can automate repetitive tasks, speeding up the labeling process while reducing human error. \n\n* **Improved Accuracy:** Machine learning models continuously learn and refine labeling, which can enhance the precision of labeled data over time. \n\n* **Scalability:** AI-driven solutions make it easier to manage and label large volumes of data, which is especially crucial in industries like healthcare, autonomous vehicles, and e-commerce. \n\nThe blog also discusses the challenges and limitations, so it’s not all rosy—but it’s exciting to see how AI is paving the way for better data preparation and ultimately better insights. \n\nYou can check out the full article here if you're interested: [Exploring the Role of AI in Data Labeling Solutions](https://itchronicles.com/artificial-intelligence/exploring-the-role-of-ai-in-data-labeling-solutions/) \n\nWould love to hear your thoughts! Have any of you tried AI-powered data labeling tools? What’s been your experience? ",
    "created_utc": "2024-10-30T06:33:11",
    "num_comments": 0,
    "comments": []
},
{
    "submission_id": "1gfkvt7",
    "title": "Coats pants ",
    "selftext": "",
    "created_utc": "2024-10-30T05:04:47",
    "num_comments": 0,
    "comments": []
},
{
    "submission_id": "1gfks2e",
    "title": "Fine-Tuning GPT-4o on legal text classification dataset",
    "selftext": "",
    "created_utc": "2024-10-30T04:59:30",
    "num_comments": 0,
    "comments": []
},
{
    "submission_id": "1gfjmqd",
    "title": "I Built an AI to Help Businesses Interact Directly with Their Data—Here’s What I Learned",
    "selftext": "Hi everyone! I’ve been working on a project called **Cells AI** that uses NLP to make data more accessible for businesses. The goal is to let users ask questions directly from their data, like “What were our top-selling products last month?” and get an instant answer—no manual data analysis required.\n\nThrough this project, I’ve been experimenting with various NLP and ML techniques to enable natural language queries. It’s been an incredible learning experience, and it made me think about how ML can be applied to bridge the gap between complex data and everyday business users who might not have technical skills.\n\nIf anyone is interested, I put together a demo to show how it works. **Happy to share in the comments.** \n\nI’d also love to hear from others working on similar projects or learning ML—what has been your most interesting application so far?",
    "created_utc": "2024-10-30T03:48:39",
    "num_comments": 16,
    "comments": [
        "See demo here:  [**https://drive.google.com/file/d/1k5fyNqNCkV67MW4AFUxMl\\_DcWpyZv\\_Md/view**](https://drive.google.com/file/d/1k5fyNqNCkV67MW4AFUxMl_DcWpyZv_Md/view)\n\n  \nFind out more about Cells AI at [usecells.com](http://usecells.com)",
        "Cool project! I'm working on something similar but for a particular domain and that involves interfacing with databases directly, so it's nice to see others getting good results for NLP on structured data.\n\nI'm curious about how you got it to generate complex queries with JOINs on multiple tables, GROUP BYs etc accurately. Did you do any extra processing or logic to make it easier for the LLM to determine the correct relationships?",
        "this is a nice project and has very useful application, i also do AI / ML  but my focus area is computer vision",
        "Wonderful man,do share the link",
        "This is fantastic",
        "Well... what did you learn? Lol",
        "Question - I assume this only works with structured data ? Is this a true assumption? Just watched the demo. Really love the work",
        "The project seems very cool! \nIs it by any chance open source? I would love to look at the codebase and understand the working of your product.",
        "I still have no idea what you learned 😅\n\nShare some more about your experience! What businesses did you talk to? Did they like the product? What were the biggest complaints? Did you close any sales?",
        "So, from what I see in the demo, you built a nice interface to the various LLMs via their APIs.",
        "Yep, it's able to generate join queries and even CTA expressions. \n\nIt's all about efficient parsing and preparation of data + giving LLMs the necessary context and it's able to generate complex queries pretty efficiently.",
        "Thanks!",
        "Yes correct.  \nWe have roadmap to also support other data sources such as PDFs / databases.",
        "We're still early stage and just launched.   \nAnd are currently talking to various businesses and the response has been very positive. \n\nA complaint obviously is regarding data security which we're working to make your data secure and encrypted. \n\nTrajectory is very good to close some good leads!\n\nAs a first time founder, there has been so much learning on the tech (langchain, python, LLMs) end and the business (sales, SEO, marketing, content) end.",
        "I have given you a follow. And will send you a DM"
    ]
},
{
    "submission_id": "1gfj9bc",
    "title": "Project for internship ",
    "selftext": "What are the best NLP projects I can build to get internship at any good company with good stipend.\nHardware is not the issue.",
    "created_utc": "2024-10-30T03:23:33",
    "num_comments": 4,
    "comments": [
        "Project ideas generator",
        "how can hardware be not an issue but you are looking for an internship",
        "yeah",
        "It is not like I can train a billion parameter model, I use colab so that's why I told for small models hardware is not an issue."
    ]
},
{
    "submission_id": "1gfivwk",
    "title": "AI and ML Course offered by Universities",
    "selftext": "Hi, I have 15 years of programming experiences in mainframe and looking to change the career to AI/ML. I just started learning python. I am looking for colleges or universities, offering AI/ML courses.\nPlanning to complete the course in 6 to 8 months. Could you please advise? ",
    "created_utc": "2024-10-30T02:58:08",
    "num_comments": 1,
    "comments": [
        "I’d recommend omscs (Georgia tech) to do part time next to your job but that would take much longer than your timeframe. Thats a bit short for how much there is to cover."
    ]
},
{
    "submission_id": "1gfim5f",
    "title": "Roast my Resume (and suggest improvements)",
    "selftext": "",
    "created_utc": "2024-10-30T02:37:45",
    "num_comments": 34,
    "comments": [
        "Looks like an excellent range of technical skills and achievements but no soft skills.\n\nIt’s all well and good if you’re a genius, but if you’re not great at working with others effectively, you’re cooked.\n\nNot saying you’re not, just putting it in a way that a potential employer looking at it might think.",
        "I think what you've done is really interesting but there's not a lot of depth to it. For instance, how did you take those models into production and turn them into something that generates revenue. I guess all of these have been academic, or fun projects so perhaps you're missing that aspect but that is the gap for me. If I want an ML engineer, I need somebody that can design and train a good model but I also need this person to work with wider dev teams to reimplement it in Rust or something practical for deployment either on mobile devices or cloud rather than Python. It's about getting value from the model. Who's to say whether the accuracy scores are actually useful until you turn them into a product.",
        "This is full of ML metrics, but no mention of business value. You will not be hired to get high accuracy on models. You will be hired to increase revenue, decrease customer churn, decrease costs… etc. \nyou need to be able to articulate the business value you create not just ml eval metrics",
        "This is general. If you're looking for a specific company, pitch to them. You don't want to use a shotgun when you need a scoped hunting rifle.\n\n1. One page is the goal. It's not like you have a Ph.D or ton of years in the field.\n2. Limit the use of bold. I get that you're trying to quantize things to show impact and highlight it by using bold. But your using bold in even cases where it's of low value -- like '**100 people'.** Plus using it for every buzzword possible. Plus years. And project titles. And metrics like **F1-score**\n3. If you want to be anonymous, don't half-ass it. It took me less than 15 seconds to get your github account name. Either trust people will try to help or do a good job at keeping it private.\n4. From that -- and this is key -- I can see that you did projects as a group but none of that shows on your resume. When I hire people, I want them to be team players not just solo artists.\n5. Kaggle is not a developer tool.\n6. Move Education down if your applying in the US. Not that many know about IIT or IIIT.\n7. Achievements needs a line space above it. Remove or combine a line from above.\n8. \"with**real-time\"** (oooh, real-time! No wonder why that, like 1/2 of the page, is in bold) needs a space.",
        "\\- Make it 1 page - I have 20 years of experience up to a VP level and I still just have a 1-page resume\n\n\\- Create a mission statement - what are you looking for?\n\n\\- When I look through DS/ML resumes I almost always care most about how you found/generated -> cleaned -> maintained -> versioned the data. People that can do ML models are a dime a dozen these days. People who can manage the full stack of a data lifecycle are hard to find - those skills are probably more important for you now that you can train algorithms with some deep learning libraries.\n\n\\- Business impact and value - I would rather have an if/else statement in production at 80% accuracy than a deep learning model in production at 90% accuracy (depending on the volume/value margins/etc). They're way easier to debug/manage/track/maintain. Why do any of these models matter in the context that they're used?",
        "The amount of bot like responses in the comments. \nIt's obvious that OP hasn't had one internship to demonstrate the business value that he's contributed to a company. \nHow can you critique a profile based on it, if the resume is intended to get him to a place where he can start producing business value?",
        "As others have said; you are not showcasing any actual work done in a professional environment. What you are showcasing, is that you are able to code.   \nThis is not enough for people looking to hire or take on interns. Why? I see no proof that you are able to perform a real world problem from start to finish. ML is about much more than finding a good model for specified data. It's about analysing the data, analysing the results, their implications, and how to deploy them and provide certain guarantees.   \nAll your projects were on existing datasets, with a lot of background and previous projects that have worked with the exact same data; thus making you show your ability to incrementally improve these approaches.  \n   \nThis is normal: you are  still a student. Don't be discouraged.  \nHowever, one way to get this type of real experience on new problems, or developing new ideas or frameworks, is by working with your professors. Ask them if you can assist them; you have substantial proof of your coding ability, and professors are very often looking for extra people to work on their ideas.\n\nAlso, to be clear; you haven't specified what your ambitions are. In which direction do you want to go? Developing new concepts/frameworks/methods, or solving real world problems by applying what you've learned.   \nIn both cases, a research internship can be a good start. What you do afterwards, however, will depend on what your goals are.\n\nWishing you the best of luck!",
        "One page, skills at the top, projects using those skills after, education at the bottom.  Lose the achievements.",
        "I feel like these comments don't realize OP is a 3rd year student...\n\nOP I'm not sure how different the recruiting process is for indian firms but I've helped HR at my current job with hiring (data scientist in finance) and this looks like one of the better resumes I've seen for internship roles (which is what I'm assuming you're applying to). Most the suggestions are related to formatting (it should definitely be 1 page) and your description of your projects uses too many buzz-words. Like I don't know what \"competitive performance against state of the art methods\" means.\n\nRather than focusing on results just give a quick, quantifiable metric and then explain some of the more technical components of the project. E.g. explaining tools you used like spark or elaborating on any pre-processing you had to do. That gives me a better idea of what the project is and gives me something to talk about in the interviews, and also gives me a better approximation of your technical abilities.\n\nAlso your achievements are very impressive, when I was in undergrad I never participated in challenges like that so props to you.",
        "Some specific advice:\n1) Make it one page long\n2) Don't put the Amazon school with other official education. If you want, put together with skills/technologies.\n3) Remove projects and keep only the two/three that are more relevant.\n4) Add a section of personal interests (music, sports, books, etc) at the end.",
        "I think the CV is good. Much better than most IIT CVs I think. India is kinda lagging behind hard in ML. \n\nSome roast would be to not put libraries and packages beside project names. It's kinda redundant imo. And pytorch-lightning is one word. Only embolden words that are very important I think you have used it a bit too richly. Also, you are a recent grad, keep length of CV to one page.\n\nEdit : FYI I don't agree with people who emphasize business value and soft skills on a technical resume. I have a very technical resume and have never had problems landing interviews. You have a cover letter for that. I typically draft my resume for a hiring manager and cover letter for recruiter.",
        "So you put your skills in the second page header and the rest of the page is empty?",
        "Cg to acchi h oncampus ho jayega accha",
        "What tool/template did u use?",
        "Sorry no feedbacks here but I am from Ranchi as well what a coincidence",
        "It is important to have group projects",
        "If you are on LinkedIn, check out https://liroast.web.app/ for a fun and informative roast of your profile.",
        "hey. \n\ni like the template that you are using. would you mind share it with me?\n\nthanks",
        "hey i really liked your template, can you share it?",
        "It's not s CV. It's a list of projects",
        "Thanks for your suggestion.\nAlso how should I add it in the resume? (I use resume wordded for resume review and it keeps complaining when I add words like team work, hard workong etc.)",
        "To be fair, this was from their time at school and those projects tend not to have any business value unless they come from a intership.",
        "OP is a student, where exactly are they going to get opportunities to demonstrate \"business value\"",
        "Hey that’s a good suggestion. Care to elaborate on your experience? Would like to know more and if you are from a ML side, I could use your help on my previous posts",
        "I agree with you on this ! Am also an undergraduate student HAVENT done much projects but this guy resume and projects is sumn else 🔥 motivated me to work on my skill sets too!",
        "Why add a personal interests section? What purpose does that serve the company?",
        "Bhai abhi 1 company aayi thi, specialy bola ki DS-AI wale bacho OA me nahi baithenge baaki sab baitho. 😢 😭",
        "It’s from Overleaf.com",
        "I have a small section, maybe 1/4-1/3 of a page on the front page that lists 3-5 key skills that have been asked for on the job I’m applying for. I’ll bullet point them in bold, then write a sentence or two next to each explaining a few key experiences I’ve had that demonstrate it.\n\nI’d say the main thing though is not to go for a one CV fits all approach and subtlety tailor each submission to the job description. That includes soft skills they’re asking for such as time-management, stakeholder-management etc. as well as technical skills specific to the job.\n\nI also think that one page of technical experience very relevant to the role is better than 2 including the relevant stuff, but padded out with things the employer doesn’t care about. You have an impressive resume with a lot of things on it, but you want an employer screening hundereds of CVs to see yours as one that sticks out. To make that easy for them, (to an extent) less is more.\n\nDoesn’t mean you can’t draw on these in interview, but the way that I see a CV is that it’s main aim for me is to get me to interview.\n\nHappy to clarify anything further if it helps :)",
        "use overleaf",
        "Shows that you are a person with interests beyond your job. Of course, you won't be hired because of that, but from my experience, it sometimes helps to create points in common with your interviewer. In addition, depending on what you put, you can show leadership, discipline, etc."
    ]
},
{
    "submission_id": "1gfijbe",
    "title": "Where to find example of Llama2 code (no langchain)",
    "selftext": "Hi all, \n\n  \nI want to learn how to write an LLM class (like LLama 2 using HuggingFace pretrained checkpoints). I don't want to use langchain since I want to have free access to all component. \n\n  \nI was searching something like: an initialization part, a generate function etc. \n\nDo you have some autors or github repo of person who write good quality code an learn from those?",
    "created_utc": "2024-10-30T02:31:37",
    "num_comments": 0,
    "comments": []
},
{
    "submission_id": "1gfievm",
    "title": "Kaggle, Projects, or Certifications? What Matters Most for Data Science Internships?",
    "selftext": "For those experienced in hiring or interviewing for entry-level data science internships: What truly stands out on a candidate’s profile? I’m trying to make the most of my limited time by balancing several things—building a meaningful Kaggle profile (thoughtful notebooks, quality contributions), working on personal projects, completing online courses, and pursuing certifications(Hackerrank). From your experience, which of these elements makes the strongest impression? How should I prioritize my time to have the best chance of landing an internship?\n",
    "created_utc": "2024-10-30T02:22:05",
    "num_comments": 16,
    "comments": [
        "First, certs are almost completely useless. There are no widely accepted standards in ML and accordingly no reputable certification. Cloud certs could help you, but that's more about infra and not ML.\n\nSecond, kaggle and projects are quite similar but have few important differences.\n\nKaggle: offers lots of very different datasets, problems, challenges. Also plenty of existing solutions from which you can learn. The problem is that everything is given to you on a silver platter (necessary data and a formulated problem), in real world this rarely happens.\n\nProjects: potentially more interesting and unique than popular kaggle challenges. But you have to come up with your own problem. Forget about housing prices/mnist/iris/titanic. Find a problem personal to you, collect and process data (annotate if needed), train/test several models, select the best one, deploy the solution somewhere, document the whole process in your github repo.\n\nKaggle is much better if you're new, as learning from examples is easier. Proper projects are more difficult since all decision making is on you and that requires significant imagination and intuition, but you can also learn more depending on how difficult you want to make it for yourself. But again, it's for a proper project. Nobody cares about lazy unoriginal projects.\n\nKeep in mind that these things won't guarantee you a job, they are more of a second priority addition to a resume for a recent grad or professional with little experience. Work experience and education are still key.",
        "University degree matters most. You didn’t share where you stand on that.",
        "No experience hiring or interviewing.\n\nBut this is what helped me and a few off the cohort who got placed well.\n\nProjects > Kaggle > Certifications.\n\nNo one cared about certifications. Uni reputation and Projects seemed to be the most priority as we had to discuss challenges and how we worked around them. \n\nKaggle and Github was rarely asked but it did pop up every now and then.\n\nCertifications never.",
        "RemindMe! 3 days",
        "RemindMe! 3 days",
        "RemindMe! Tomorrow",
        "RemindMe! 3 days",
        "Hey, I think the easiest way is to start building ML models of your own. I started by using model generation through [https://www.plexe.ai/](https://www.plexe.ai/) and iterated on the output and used it in Kaggle competitions",
        "This 💯💯💯💯💯🙌",
        "this is so much game. thank you so much",
        "Okay got it thank you so much",
        "\\+1",
        "I am pursuing my B.Tech in Computer science from the second oldest engineering college of india.(Iiest shibpur)",
        "I will be messaging you in 1 day on [**2024-10-31 09:45:42 UTC**](http://www.wolframalpha.com/input/?i=2024-10-31%2009:45:42%20UTC%20To%20Local%20Time) to remind you of [**this link**](https://www.reddit.com/r/learnmachinelearning/comments/1gfievm/kaggle_projects_or_certifications_what_matters/luhv05o/?context=3)\n\n[**1 OTHERS CLICKED THIS LINK**](https://www.reddit.com/message/compose/?to=RemindMeBot&subject=Reminder&message=%5Bhttps%3A%2F%2Fwww.reddit.com%2Fr%2Flearnmachinelearning%2Fcomments%2F1gfievm%2Fkaggle_projects_or_certifications_what_matters%2Fluhv05o%2F%5D%0A%0ARemindMe%21%202024-10-31%2009%3A45%3A42%20UTC) to send a PM to also be reminded and to reduce spam.\n\n^(Parent commenter can ) [^(delete this message to hide from others.)](https://www.reddit.com/message/compose/?to=RemindMeBot&subject=Delete%20Comment&message=Delete%21%201gfievm)\n\n*****\n\n|[^(Info)](https://www.reddit.com/r/RemindMeBot/comments/e1bko7/remindmebot_info_v21/)|[^(Custom)](https://www.reddit.com/message/compose/?to=RemindMeBot&subject=Reminder&message=%5BLink%20or%20message%20inside%20square%20brackets%5D%0A%0ARemindMe%21%20Time%20period%20here)|[^(Your Reminders)](https://www.reddit.com/message/compose/?to=RemindMeBot&subject=List%20Of%20Reminders&message=MyReminders%21)|[^(Feedback)](https://www.reddit.com/message/compose/?to=Watchful1&subject=RemindMeBot%20Feedback)|\n|-|-|-|-|",
        "LOL. No body cares about college age. \nTier ?? \nAlso your fate will be most likely be decided by campus unless u find an ML job in your own.",
        "I think second tire."
    ]
},
{
    "submission_id": "1gfi13u",
    "title": "Need advice on building a model to classify typefaces",
    "selftext": "Hey everyone, I'm trying to build a model that can classify typefaces into serif and sans-serif categories (and even subcategories like in Vox-ATypI classification, see more in here [https://en.wikipedia.org/wiki/Vox-ATypI\\_classification](https://en.wikipedia.org/wiki/Vox-ATypI_classification)), but I'm having trouble figuring out the best way to proceed. I could really use some advice!\n\nMy first approach was to convert each glyph of the font to SVG format and train on an SVG dataset. The problem is, I'm stuck when it comes to finding a library or method to effectively train on SVG data directly. Most resources I've found focus on image-based training, but I'd like to maintain the vector nature of the data for more accuracy if possible. Does anyone have any suggestions on libraries or frameworks that can help me work directly with SVG data?\n\nThe second approach I'm considering involves using FontForge. FontForge can give me instructions on how to draw each glyph in the font, and I was thinking of creating a dataset based on these curve instructions. However, I'm unsure if this is a viable route for training a classifier, and I'm also wondering if anyone knows if it's \"allowed\" in the sense of common practice or standard methods.\n\nAny pointers, advice, or resources would be super helpful! Thanks in advance :)",
    "created_utc": "2024-10-30T01:52:20",
    "num_comments": 1,
    "comments": [
        "Please, any help would be really awesome!"
    ]
},
{
    "submission_id": "1gfhs27",
    "title": "New RL internship at Meta FAIR CodeGen Team\n",
    "selftext": "",
    "created_utc": "2024-10-30T01:31:58",
    "num_comments": 39,
    "comments": [
        "I assume this is akin to a postdoc position? PhD internship is crazy.",
        "You need a Phd to be an intern these days, crazy",
        "I have no experience with math or numbers.  Can someone share a playlist of YouTube videos that will prepare me for this position?",
        "[Research Scientist Intern, Reinforcement Learning (PhD) | Meta Careers](https://www.metacareers.com/jobs/1738282493595185/)",
        "Wow in other fields, after PhD you can go to postdocs positions. I mean, it's kinda like internship-ish but idk",
        "How much does it pay?",
        "kinda frustrating how interns are phds ..... i am in bachelors degree rn , seems like there is no space for ppl like me in the ML space , guess i would just f\\*ck myself with LLM prompting",
        "Most ML PhDs at good schools take industry internships. Not a postdoc since it's seasonal.",
        "No. These are tailored for PhD students.\n\nPhD level internships pair students with supervisors who give guidance/direction. A postdoc would be expected to produce results without direct supervision. \n\nIf anything, I’ve met professors at my school who’ve stated that they don’t put as much weight on papers that come out of internships when evaluating a student’s ability to do independent work because of the prestructuring of many industry internships (i.e. interns are often made to work on topics or even preexisting work instead of pursuing their own direction like in academic research).",
        "it's a research internship you always needed Phd for those",
        "No, it’s an internship for someone currently doing a PhD. Pretty normal in ML. My team pretty much just hires PhD and some MS interns (job itself requires a PhD or MS plus 5 years research experience).",
        "Fr. Everyone needs a phd to be an intern. Like give us ug some chance.",
        "That too for Prompting, which is even more surprising",
        "PhD interns typically are on E3 level salary at Google and Meta (equivalent to junior SWE). You can probably find pay ranges for that on levels.fyi or similar websites.",
        "It’s a research position internship. You’re a bachelor student, it is unlikely that you have done research in the academic sense",
        "Yea pretty sad \nI am interested in doing research but I am not in good financial condition",
        "Not exactly true in other areas, someone with a Phd should be a researcher not an intern, someone who is still finishing his Phd degree could be an intern.",
        "you have no idea what prompting is (or RL or anything in ml is)",
        "It's more likely to be using outputs of LLMs for reinforcement learning. Prompting will just be a small portion.",
        "So how much would that be in USD? \n\nEdit: Why can't anyone be straight about this? Is it really that egregious to ask how much a job is paying on a job listing now?",
        "It depends on where you go, to be honest. My schools ura program is strong it isn't uncommon for undergraduate students to publish. Also, in the 4th year, there are a lot of research courses and crosslisted courses with phd/masters. It is still rare, but there are possibilities of bypassing these requirements. If you did the latter, this is still rare, thought. I do agree with the sentiments, though that a PhD. being  needed to do research does limit things, but we need to find ways to incorporate research skills more frequently earlier in academics.",
        "Yeah you're right but I'm pretty sure this position is for people who are currently in the middle of their PhDs(even if they let people who have completed to apply), all the people I follow who get those internships do so during their PhDs. I don't think they'll hire someone who completed their Phd for these posts unless they're special circumstances.",
        "the role is for current PhD students",
        "Butthurt? And for some reason you think you are the torchbearer of ML. Looking at your posts, your knowledge is apparent.",
        "wow that was harsh ..",
        "a PhD position for junior SWE pay is ridiculous levels of BS that can't even be quantified. They are not saying how much it is because it's BS.",
        "FAANG are world wide. It would depend first where the job is (as in what country) and then what CoL in that country, if it's in the US. So ... there's no one answer. Better to look up ranges as noted by the person you replied to.",
        "It’s possible to publish as an undergrad, but it requires a serious amount of work if you want it to be counted in your favour when you apply for a research scientist position (internship or full time graduate roles). Being a URA and publishing is not the same as doing academic research. Most of the heavy lifting has been done by a supervisor, either a prof/post doc/pdra or a PhD student. \n\nI don’t know what courses you’re talking about that would be taught to PhD students. In my institution (European, so probably very different if you’re in the states), PhDs have no taught element unless you’re on a 1+3 program. Undergraduates are at university for a fundamentally different reason to PhDs (and research-based) masters students, so expecting research output from them is inappropriate.",
        "Why are people getting so uppity about this?\n\nWhen I was working in research labs pretty much every intern was masters level doing work for our supervisors who were PhD students/candidates. These supervisors were also considered 'interns' despite being there for a year or two.\n\nIt's also different now because being a PhD student studying machine learning was almost a niche field 10 years ago. It's where everyone wants to be now so it's much more common to have lots of applicants.",
        "Considering you thought this position was about prompting, I know far more about ML than you will ever know. Also you have very little self-awareness to talk about profiles considering yours lol",
        "It’s an internship. PhD students without industry experience simply are junior. \n\nThe PhD degree unlocks possibility to get hired into research roles. It doesn’t suddenly make you not junior, only experience will do that.\n\nIt’s still FAANG though. These junior level roles will pay more than most non-FAANG mid-career or senior level SWE roles.",
        "Yeah, I am talking about edge cases. I do agree with you. I think people are misunderstanding my comment. I went through the phd. program, bustI'm talking about taking crosslisted masters course. I am thinking more along the lines on what is necessary to be a machine learning researcher and a phd programme imo is overkill as I am in that boat phd. programme with a research position. So I'm saying the core research fundamentals for a lot of industry jobs can be taught without a phd, in reality it is just understanding research papers and applying a lot of the time and knowing how to test the methods of the research paper on said problem. I think it is very appropriate because we need more researchers and a phd is too much of a financial burden for those not born in privilege.",
        "Tbh don't know either, my original comment was obviously a joke on the job market in general, but some people decided to take it too seriously. I totally agree with you, and i think it's awesome that ML got to the point where it has a lot of visibility and applicants to the point Phd is required.",
        "The way you talk about knowing it all, your immaturity and unawareness is pretty evident. I think you need to introspect. I have been working in this field for past 2 years and from the looks of it, you do seem like a know it all brat. Learn to be humble kid.",
        "I don't think you understand much about the research and academic world. They are the pinnacle of knowledge...they do PhD to PUSH the understandings...they are the ones who make the research papers others use to further their progress. That's not \"without industry experience\" because they are at the forefront...in this case RL research...it's the industry that is behind.\n\nSo your argument is...like the pay...also BS.",
        "> I don’t think you understand much about the research and academic world.\n\nI have a PhD degree and work at FAANG as research scientist (but not at Meta).\n\nAnd no, having a PhD degree doesn’t make you senior. It just gives you research competences, which are must-haves for research jobs.\n\nPhD grads take a while to adjust to industry and learn how to actually contribute to an industry researcher team and make their research knowledge useful.\n\nJust like how a bachelor graduate would take some time to learn how to contribute in industry in a software engineering team and get to the point of making their engineering skills useful to the company.",
        "Likewise. However there's a difference between a bachelor CS and a PhD...and when it comes to RL or other sorts of DL etc.. there is no \"industry standard\" or \"differences\"...it's STILL RESEARCH. Unless you're hired to write a ton of code in a language you don't know all of a sudden....then such a pay is unjustified.\n\nWhat getting used to? More research? \\*Different tools\\*..yeah maybe for deployment but they are not even in that quadrant of engineering.\n\nIt's simply unjustifiable...no amount of getting used to will equate a BachelorCS to a PhD.\n\nP.s. all the friends with PhD's that jumped into an \"unknown\" industry...like from CERN straight into the banking world had no pay reduction \"because getting used to\". Likewise for lithography field OR automotive. Preposterous.\n\nP.s.s. Your argument is literally: \"You need a Bachelor for flipping burgers...\"",
        "Dude calm down, we’re talking about FAANG salaries here. Nobody is talking about burger flipping salaries.\n\nYou seem pretty inexperienced in tech industry. Which is fine. But maybe match your confidence level to your level of experience.",
        "Fortunately for me, all the people and friends with PhD's I know had a great start in their respective industries they've chosen....so I don't have to defend some bullshit belittling argument about what to accept in life...smart people know their worth and don't fall for BS positions such as OP's post.\n\nHave a great day."
    ]
},
{
    "submission_id": "1gfhm5k",
    "title": "Help for finals project",
    "selftext": "Hello everyone, I'm a student working on a report for my machine learning final course and I really want some comments, inputs, opinions about the content for improvements or insights. I'm working on a mental health corpus of \\~27k samples with 2 columns 'text' and 'label', column 'label' have 2 classes 1 and 0. Here is the recap of the content of my report:\n\n1. Analyze and clean the Corpus, feature extraction with TF-IDF.\n\n2. Analyze further with LSA (Latent Semantic Analysis) and visualize with a biplot combined with the TF-IDF extracted matrices reduced to 3 components with LSA.\n\n3. Use K-Means and GMM to cluster the documents in the higher dimension TF-IDF extracted matrices, then reduce to 3 components with LSA for visualization (From testing, both algorithms return clusters eerily similar to the original class distribution)\n\n4. Evaluate and comprehend classifiers: KNN, Random Forest, Logistic Regression, SVM,  all will use the inputs extracted by TF-IDF on the cleaned corpus. We will perform grid search where applicable to better understand their behaviors. (With hindsight all classifiers performed at around 80%-93% peak)\n\nI'm unfamiliar with text data so any input is highly appreciated, thank you very much!",
    "created_utc": "2024-10-30T01:19:06",
    "num_comments": 2,
    "comments": [
        "This seems like a good way to start. If you have extra time here are some other things to try.\n\nOther methods of extracting features from the data; TF IDF is very simplistic. Maybe try using text embedding models, for example:\n\n> from sentence_transformers import SentenceTransformer, util\n\n> model = SentenceTransformer('multi-qa-MiniLM-L6-cos-v1')\n\n> text_embedding = model.encode('How big is London')\n\nThe above will give you a feature vector for text using a language model.\n\nDoes dimensionality reduction actually help at all? Try classification with and without it and see what happens.\n\nTry using XGBoost, a library for gradient boosted decision trees. It'll be better than the scikit learn tree models. In fact it'll probably be better than every model you've tried so far.\n\nDo cross validation, so that your test metrics don't depend on just a single arbitrary choice of test data.",
        "Thanks for your input! Because of the scope of the course, my focus is not really about NLP but more so the classic ML algorithms like the specified clustering and classification models. TF-IDF is mostly just a fast way for me to vectorize the data so i can work with them with the specified models. Yes dimensionality reduction does not help with this kind of data indeed! I explored LSA mostly to include some implication to dim reduction because of a quota, it was helpful for cluster visualization later on tho, and combining with term-document biplot does help with interpretation somewhat. Thank you!"
    ]
},
{
    "submission_id": "1gfh0lc",
    "title": "Advice on breaking into the Machine Learning field as mech eng",
    "selftext": "I'm a third-year mechanical engineering student studying abroad in Malaysia. As a foreign student with limited opportunities back in my home country, this is important later in my post.\n\nI've always been interested in AI, and I've taken courses on machine learning and computer vision. During an internship with a manufacturing company in the O&G sector, I got hands-on experience working on data science projects, including one machine learning project. This experience taught me a lot about analyzing and managing data, and it really fueled my enthusiasm for the field.\n\nNow, I’m self-studying LLM models while continuing my mech eng degree. For my final year project, I’m planning to focus on a computer vision topic, which my university thankfully supports.\n\nI’d love some guidance on next steps. A master’s degree is financially out of reach unless I can secure aid which is doable, and as an international student, getting a job in Malaysia can be very challenging but it is doable. Any general advice would be appreciated.\n\nThanks in advance!",
    "created_utc": "2024-10-30T00:31:44",
    "num_comments": 2,
    "comments": [
        "Ehh... well, based on your constraints, I'd say you're not gonna be able to for the time being. The masters degree makes the most sense as that's really the bare minimum to break into these days. Is there any chance of an employer paying for your masters?",
        "If my cgpa stays up I can apply for a grant in a local uni here, but my problem is Im not sure if a masters degree is worth my time, or is it better used in searching for a job."
    ]
},
{
    "submission_id": "1gff18s",
    "title": "Exam scheduling using Neural Networks ",
    "selftext": "\n# I'm exploring the use of machine learning for exam scheduling, but most previous research primarily employs genetic algorithms. I'm interested in insights on applying neural networks for this problem. Has anyone encountered approaches where NN have been used successfully in exam scheduling.\n",
    "created_utc": "2024-10-29T22:02:40",
    "num_comments": 0,
    "comments": []
},
{
    "submission_id": "1gfeq2b",
    "title": "Free Unlimited AI wallpaper python generator using Stable Diffusion code walkthrough",
    "selftext": "Create unlimited AI wallpapers using a single prompt with Stable Diffusion on Google Colab. The wallpaper generator : \n1. Can generate both desktop and mobile wallpapers\n2. Uses free tier Google Colab\n3. Generate about 100 wallpapers per hour\n4. Can generate on any theme. \n5. Creates a zip for downloading\n\nCheck the demo here : https://youtu.be/1i_vciE8Pug?si=NwXMM372pTo7LgIA",
    "created_utc": "2024-10-29T21:42:23",
    "num_comments": 0,
    "comments": []
},
{
    "submission_id": "1gfe65v",
    "title": "How many % can help me in my learning of machine learning the book Mathematics for Machine Learning  ?",
    "selftext": "I know that the book is important and can be helpful but I would like to know wether I read the book a bit faster, like a refresh of mathematics or I really need to read it in deep cause machine learning needs mathematics for everything.\n\nI am not expert in maths but I know some from university and school.",
    "created_utc": "2024-10-29T21:08:10",
    "num_comments": 3,
    "comments": [
        "Idk what that means, can you understand the reasoning and math behind the machine learning concepts? If yes, then can you understand if a or b changes what would happen, if so, that's all good. If not go in a bit more detail.\n\nAt the end of day the math helps build fundamentals, after that it's all exposure and practice.z\n\nYou can do years of theory but without practice you won't understand its use well.",
        "Hey, I think the easiest way is to start building ML models of your own. I started by using model generation through [https://www.plexe.ai/](https://www.plexe.ai/) and iterated on the output and used it in Kaggle competitions",
        "got it, thank for your response."
    ]
},
{
    "submission_id": "1gfdd9l",
    "title": "what should i do to get a job as ML engineer?",
    "selftext": "I am currently working as a C# developer and i don't see any future in my current role and company. I am thinking about learning ML . what is the fastest way to learn and what are the resources for that. Also i am learning maths from Coursera but i am thinking should i skip maths and learn simultaneously with machine learning course to speed up the process. Please help me i want to change my job in 3-4 months. I am willing to put in the effort to achieve this goal. Thank you everyone.",
    "created_utc": "2024-10-29T20:21:05",
    "num_comments": 29,
    "comments": [
        "Why do you want to be a MLE? Also switching companies into MLE role without graduate education or relevant work experience will be very difficult.",
        "Not going to lie, you’re almost starting from zero. Unless you’re lucky I don’t see how you can become a MLE in the next year or two.\n\nBeing a C# developer and being a ML engineer have only in common that you do some code? Also switching because you don’t see a future in your company, to a field that’s pumping more and more Master and PhD graduates every year is short sighted.",
        "3-4 month timeline is unrealistic",
        "I pivoted to data engineering from C#/SQL development with a pit-stop in machine learning. It took me five years. I already had a decade of experience by then. \n\nThis is not a fast transition. Start with the fundamentals. Good luck.",
        "I noticed every comment here basically is saying its impossible or unrealistic. Is there any constructive criticism to be said where you can give a realistic timeline or any path to do so? Or is OP jus fucked and he should go a different direction?",
        "Forget C#",
        "Unrealistic timeline imo. Over a span of two years is more realistic. This field is inundated with folks who have masters and phds so unless you are built from the same DNA as the big three, knowledge you gain in 3-4 months while working a ft job is not going to be enough in most situations. Perhaps a less ML and more data science oriented position can be attainable. I would say deep learning specialization from coursera + some stats and data wrangling should be enough for entry level data science.",
        "why not ask on indian subreddits ?? are you looking exclusively for remote jobs?",
        "Maybe try to get an SWE job at a company that has an ML team, keep learning ML in your spare time, then can try to internal transfer after a year there\n\nIt's very tough to get an MLE role with your background",
        "Switching for SWE to ML,DL, and AI roles is an easy task for someone who has graduated with a computer science degree or related field from a university. You just need a good resource and good advice to start. I recommend you focus on the Gen Ai fields but first you need to understand the basics of ML, DL,NLP...\nHere are some links to the top useful tutorial I found after a deep research on the net:\nFor machine learning, this is the only course you need:\nhttps://mlcourse.ai/book/index.html\nFor deep learning, this is the only course you need:\nhttps://course.fast.ai/\nAfter that you can focus on gen AI, gen ai is a new hot field and few good resources are available but I could recommend this roadmap by Krish Naik, has also a YouTube channel:\nhttps://github.com/krishnaik06/Roadmap-To-Learn-Generative-AI-In-2024/tree/main\n\nHope this is helpful ☺️🙏",
        "Hey, I think the easiest way is to start building ML models of your own. I started by using model generation through [https://www.plexe.ai/](https://www.plexe.ai/) and iterated on the output and used it in Kaggle competitions",
        "I don't understand why everybody says that the transition is hard ? If you studied CS with a good foundation of maths like it's the way in 95% of good universities, the transition to ML is actually easy , three months all in is more than enough. People here make it sound like the maths behind it is difficult, it is not unless you want to be a researcher which you don't. I would make the point that going from somebody who only understands maths and \"codes\" in python to a good software engineer is way harder than the opposite because you actually need a good base in SE IN 90% Of ml positions. \n\nDon't listen to people here, also I don't think transitioning to ML is better than sharpening your skills as a backend dev, if anything I think it's the opposite, an excellent backend dev will alwayys be more relevant",
        "I think maybe on the side after 3 months try to join non profit or an early stage startup to gain experience",
        "Good luck with that",
        "i have done my bachelor's and working for almost a year as a c# developer for a machine manufacturing company.i write software for machine. And i am based in India.",
        "For entry level rigorous 16,17hr daily ...",
        "i already know this is an unrealistic goal. but i still have to try , it's not like my world is going to end if i didn't achieve this in 3 months. i just need help from everyone to Fastrack my progress.but everyone is just criticizing .",
        "no just wanted some insight here as i have been on this sub reddit for quite some time.",
        "What would make it easier with that background? A masters?",
        "thank you for your help",
        "The issue is that most cs programs don’t have what anyone would call a good foundation in math outside of their dedicated ml tracks. That’s why ml is filled with electrical/mechanical engineers or physics/math majors over cs people. The comfort and familiarity with math just isn’t there.\n\nEdit: and I whole heartedly disagree with the proposition that it’s harder to learn programming than math. That’s such a completely unhinged take. Plenty of people teach themselves to program from YouTube. Very few people teach themselves math especially at the level required to be good at machine learning.",
        "thanks for the word of encouragement.",
        "Because a lot of us work in the field, have relevant experience, and know the market. You absolutely cannot transition to a ML engineer role in a few months. It takes years of experience. It's not an entry level position.",
        "Industrial/embedded software development is a completely different \"machine\" than ML. The only overlap is that both jobs require coding (albeit in different languages), so you are almost starting from zero.",
        "Mate, we're not just trying to put you down but the way you approach the problem is doomed. There's no fast and easy way to get into the field. It's full of hype, so many people want to have their share and many will just jump to the next thing (you can see so many Web3 \"experts\" becoming AI \"experts\").\n\nIt's really difficult to say what are the true fundamentals that are required to have a good career as there's so many changes every year. So we end up just recommending learning everything and learning everything takes time.\n\nShould you learn the math and build bottom-up? Yes, understanding what's happening under the hood is necessary to pick the right loss functions, the right models, the right assumptions, interpreting the results properly when trying to solve a new problem.\n\nShould you learn the concepts first and build top-down? Yes, many problems can be solved by high-level frameworks and APIs that will hide all the nasty bits and give you good to great results quickly.\n\nShould you do both at the same time? Yes, the high level stuff will help you build a portfolio of small demonstrations and get you familiar with the tools you'll probably use, while learning the maths and fundamentals will help you understand why it works.\n\nNow, you'll be competing against quite a lot of other people for the jobs. Many of whom have taken the same path of free Standford/MIT/Berkeley/Harvard classes online, Coursera/EdX/DeepLearning.AI certificates, HuggingFace/Fast.AI/D2L.ai online classes. Some of the other people will have degrees in related STEM fields and have the maths fundamentals already acquired while learning to code and learning ML. Some other will have a degree in CS and need to learn ML + complete some of their math knowledge. Some will have a ML-focused degree and will have everything except experience. Some will have a masters/phd and some research experience in ML.\n\nEven if you worked 12h a day, 7 days a week for 4 months I doubt you will stand out in that crowd (and you'll burn out). Maybe if you focus on things you can show: innovative demos/product oriented stuff.\n\nThe problematic part in your statement is the \"i want to change my job in 3-4 months\" (and the opportunistic undertone but maybe you didn't express your interest in ML correctly). Otherwise there's nothing really preventing you from succeeding.",
        "Possibly, not to discount the fact that during masters you can still take on internships",
        "I did within three months and I'm no genius, I just work and understand , and then I switched back to SE.",
        "i do know that , computer vision is used in the automation process for context , i am already learning things like python and maths i just need some insight to best method to cover fundamentals quickly. so that i can work on projects to learn more.",
        "thanks for the insight , i know there is lot to learn and i put it incorrectly as i want to cover the fundamental and starts building projects to showcase. This is not a deadline i just want to learn as much as possible as soon as possible. But thank you for this long indepth reply"
    ]
},
{
    "submission_id": "1gfd0l8",
    "title": "Thoughts",
    "selftext": "Machine learning journey is so difficult. There are so many things that I don't understand.\n\nWhen I learn the fundamentals, I just asked myself why am I even studying this. I try to do some project after studying fundamental but I cannot even implement it.. But when I try to Google, the explanation is so difficult to understand. When I use ChatGPT, it gave me some weird prompt and spit some stupid code out.\n\nWhen I watch youtube video, there are so many errors. I just want to give up.\n\n  \nThanks guys for reading this post.",
    "created_utc": "2024-10-29T20:01:56",
    "num_comments": 3,
    "comments": [
        "if it is easy it wouldn't be worth it to learn it",
        "too difficult more rewarding",
        "Hey, I think the easiest way is to start building ML models of your own. I started by using model generation through [https://www.plexe.ai/](https://www.plexe.ai/) and iterated on the output and used it in Kaggle competitions"
    ]
},
{
    "submission_id": "1gfbsnk",
    "title": "How do I get better? What job positions should I look for?",
    "selftext": "I'v been struggling to find a job as a new grad for my MS AI program. Published couple papers, but not in a super great conference (Minor IEEE conference and ACC). I really want to get better, but I've been feeling so stuck lately. I couldn't get any internship last year because of family issues. What is the best course of action given my current condition where I want to get started in finding a job in AI/ML, DS, or even SWE? Am I doomed?",
    "created_utc": "2024-10-29T18:58:09",
    "num_comments": 1,
    "comments": [
        "Hey, I think the easiest way is to start building ML models of your own. I started by using model generation through [https://www.plexe.ai/](https://www.plexe.ai/) and iterated on the output and used it in Kaggle competitions"
    ]
},
{
    "submission_id": "1gfb1if",
    "title": "Questions about undergrad research",
    "selftext": "",
    "created_utc": "2024-10-29T18:19:44",
    "num_comments": 0,
    "comments": []
},
{
    "submission_id": "1gfaktw",
    "title": "AI/ML Engineer Jobs",
    "selftext": "Hi, I'm 23/M, recently graduated (Jun' 2024) in CS engineering degree. I don't have the job at the moment. I gave few interviews but couldn't succeeded. \nI want to start my career with AI/ML Engineer. \nPlease let me know how can I proceed from here.\nI searched PAP courses but I'm not financially good.\nI don't want to burden my parents.\nPlease help me. \n\nThanks.",
    "created_utc": "2024-10-29T17:56:38",
    "num_comments": 15,
    "comments": [
        "This is a hot and very competitive field right now. The skills you need to stand out are not taught in class, though, because what most jobs actually require is MLOps or Data Engineering, with a dash of AI/ML.\n\nWhat a realistic path for you might look like is: Jr Software Eng > Data Eng > Data/AI Eng\n\nBasically, dont expect to get in the field out of the gate, unless your uncle is on the board of OpenAI or something.  Instead, work on building background knowledge and transition into it.\n\nAnd importantly, as others mentioned: build open source projects that actually work. Showcase what you can do.",
        "Build cool stuff and keep learning, money will come.",
        "I am same age but graduated a year earlier than you. I started as DS for half a year, transitioned to DE from reorg. Then became MLE through internal transfer by interviewing. I think easiest way without masters+ is internally showing you can do the work. I am also working on my masters while working.",
        "It’s usually easier to get a job as a ML Ops Engineer. If you could consider doing that for a few years you’d build a foundation that is really valuable when eventually applying and working as a ML Engineer.",
        "As marconancer pointed out it can be challenging to land your first job in AI/ML, and more realistic is to start as data oriented software engineer. One of the reasons being how relatively new AI/ML/Data science is for majority of the companies in the world. They don't know what to do with us and how to make us generate profit, which is why starting as data oriented software engineer let's you get inside and learn more about the different needs of the companies through AI/ML glasses and allow you to start suggesting upgrades, projects and other stuff where you can use your skills and passion. -*Story of my life.*\n\nAs to what you should learn and study is really hard to pinpoint, but working with real datasets on real problems is the best practice you can get. Websites like kaggle have tons of real world datasets which are dirty, inconsistent, frustrating and lack proper documentation... just like in real life. I would roughly estimate about 80% of my time as data scientist goes to cleaning, preprocessing and understanding of the datasets. Only after mastering those can you really start responsibly training models.",
        "Keep learning the AI/ML part but don’t forget the engineering part. Do some good projects, don’t restrict yourself to just AI/ML. The job market is really bad right now, do open source.",
        "Hey, I think the easiest way is to start building ML models of your own. I started by using model generation through [https://www.plexe.ai/](https://www.plexe.ai/) and iterated on the output and used it in Kaggle competitions",
        "[https://course.fast.ai](https://course.fast.ai) \\>> Finish this course >> Do What Jeremy Howard Says to do >> Apply for Jobs everywhere...",
        "If OP wants an MLE job, DE isn't really a prerequisite. Part of skills do overlap (which is a case for most data jobs), but one doesn't naturally come after the other. These two jobs are quite different - MLE depends heavily on ML fundamentals, DE usually doesn't have that at all\n\nOP, I recommend two paths: SWE > MLE, or DA/DS > MLE. Really depends on your priorities",
        "I would second this, machine learning is NOT an entry level job, unless you want to take up a software job integrating APIs\nMachine learning is more of applied software engineering\nSo getting software engineering experience is a good idea\n\nAnother path is get into ML research right after college, work on a few ML projects",
        "This\n\nAnd start from GitHub",
        "Any tips on finding roles in ML Ops? Seems like they’re masked as jobs with different titles, or I don’t know where to look.",
        "I agree. Once you get a good SWE, DA, or DS position; pivoting into MLE becomes possible. I did DS -> MLE\n\nGoing from SWE -> DE or DA/DS -> MLE is an unnecessary step (but can still be very useful experience)",
        "Hm try searching mlops in one word. The most similar roles are probably devops/SRE or data engineer but they are rarely focused on ML environments."
    ]
},
{
    "submission_id": "1gfa6ql",
    "title": "Looking for 2-10 Python Devs to Start ML Learning Group",
    "selftext": "[Closed] \nNot taking anymore applicstions :). \n\nLooking to form a small group (2-10 people) to learn machine learning together, main form of communication will be Discord server.\n\n**What We'll Do / Try To Learn:**\n\n* Build ML model applications\n   * Collaboratively, or\n   * Competitively\n* Build backend servers with APIs\n* Build frontend UIs\n* Deploy to production and maintain\n* Share resources, articles, research papers\n* Learn and muck about together in ML\n* Not take life too seriously and enjoy some good banter\n\n**You should have:**\n\n* Intermediate coding skills\n* Built at least one application\n* Understand software project management process\n* Passion to learn ML\n* Time to code on a weekly basis\n\n**Reply here with:**\n\n* Your coding experience\n* Timezone\n\nI will reach out via DM.\n\nWill close once we have enough people to keep the group small and focused.\n\nThe biggest killer of these groups is people overpromising time, getting bored and then disappearing.",
    "created_utc": "2024-10-29T17:37:53",
    "num_comments": 38,
    "comments": [
        "I’ve worked on some machine learning projects, including tree-based models, distributed computing with Spark and Hadoop, building APIs, and creating web apps for ML. My most recent project was an English-to-SQL Query translator using a language model. I’ll give you a URL for it on DM.\n\nAcademically, my topic of interest is in computer vision. I’ve got no professional experience but had some hands-on experience through two internships.\n\nIn my free time, I play around with local LLMs r/LocalLLaMa and still teaching myself building a RAG system.\n\nAfter working on a few personal projects, I’m now looking to collaborate with others, which is why I’d like to join.\n\nCoding experience: Python (6 years), C/C++ (2 years), CUDA (1 year), HTML/CSS/JS (1 year)\nTimezone: GMT+7",
        "hi! i’m a hobby coder but i’m interested! time zone is PT",
        "I will pass out from my uni with a masters and over the academic years I have collaborated with fellow classmates to create web applications but I never got to do a major thing I'd like to spend my time now working on projects like you mentioned and ofcourse become a good team member again by meeting you guys. I will be very glad if you let me become part of this team as I still have a lot to learn I feel.\n\nI am in Australia (Sydney/Melbourne time) lemme know if there's anything else needed. Cheers!",
        "i don't have any professional experience as of now... biggest thing I've built is a 1000 lines of code banking CLI application which im not even done with yet. Timezone is GMT+3 btw. my passion in ML really started with chatbots, like chatGPT. these things really helped me in various things, it changed the way my life flows. the desire within grows bigger everyday as i use AI tools to make my life easier. this desire is to make my own models that is of use to other people. and i demand achieving it with or without company. let me know if i sound passionate enough",
        "I’m still a student in university, but I’d like to think im a pretty proficient programmer. I’ve been interested and learning ml for a couple of years now but I’m always down to learn more and refine my skills/knowledge. I’ve worked a couple of internships but they were at small companies or startups so nothing too crazy.",
        "i don't have any experience in machine learning, even in python I don't but i know how to code in c and JavaScript so i think I'll learn python fast\n\nand my timezone now Will be changed after two weeks so I'll give u the new one \nit's (GMT+3)\n\nI've done a calculator and i made it as a website but i forgot the link of it",
        "I'm a ML coder, I have some experience building pipelines in Python. My timezones is +1 CET",
        "Im interested.",
        "I'm in. Please add me into group.",
        "I'm interested, GMT + 4",
        "I’m interested if there is a room",
        "Interested.",
        "I'm interested. Are you planning to focus on more theoretical aspects or more practical use cases?",
        "- I have 2 years of experience in Python, SQL,Power BI\n\n- I have worked upon 2 ML projects Store sales forecasting, Market mix modelling.\n\n- I have build automations in python, build data preprocessing scripts for corporate.\n\nI am looking forward to join your group.\n\n\nTimezone : GMT +5:30",
        "Interested \ni'm new in ML, trying to replicate yt projects.\nGMT+6",
        "interested please add me",
        "I have a several years experience in backend python development, IIoT, frontend development, but never worked in ML. Interested",
        "Hi, I'm a master's graduate in Data Science and Engineering, experienced as Java dev and Data eng for a few years.  \nmy academic works are related to optimizing DL models, and few of computer vision. I'm interested in the field of CV and ML optimization.\n\nI'm interested in your project, are there any spaces open?   \nCoding experience:  java (3y), python (5y), pythorch/tensorflow (1y), pyspark(1y)\n\nTimezone: GMT +1",
        "Hey, \nI am a professional backend developer (Java) since 6 years and recently started working on designing a recommender model for users on our website using python. I am also interested in the learning group.\n\nTime zone : GMT+1",
        "DM'ed",
        "DM'ed",
        "DM'ed",
        "Thats fine, i don't have any professional experience either!\n\nI'll DM you",
        "DM'ed",
        "DM'ed",
        "Dm'ed",
        "DM'ed",
        "DM'ed",
        "DM'ed",
        "Dm'ed",
        "DM'ed",
        "Probably both, I mean personally I prefer to learn the theory so I can be practical with it. But if people just want to do theory or be practical, that's fine too.",
        "DM'ed",
        "DM'ed",
        "Dm'ed",
        "Dm'ed",
        "are you going to DM me too? :D",
        "Yep, sorry, just did!"
    ]
},
{
    "submission_id": "1gf9u60",
    "title": "Recommendations on applying data science to the new social network I'm building",
    "selftext": "I am an indiemaker who is building new SaaS websites. The latest project I'm building is a new social media network. A different one and a useful one.\n\nI already created a beta version of it [here](https://www.pollquester.com/), and it is up and running. I can **DM you the link and more information** if you are interested to take a look. Just comment here or send me a msg here.\n\n# What kind of social network is it?\n\nIt is a social platform for creating and participating in polls in a fun way. On top of that, I'm planning to use AI and data science techniques to extract interesting and fun facts from each of the Polls and other polls users have answered previously.\n\nFor example: if a user participates in a poll and choose their age group, then the same user can participate in other polls and because we already know its age group, we can extract interesting facts for the second poll. (something like 43% percent of people who said 'yes' are between 15-35 years old)\n\n# What is my question?\n\nI do have a background of ML and data mining, and I am familiar with most of the ML algorithms. however, I wanted to ask your feedback and opinion on how I can achieve this?\n\nIs there any algorithm or system currently available that can relate information from different datasets?\n\nOr any other feedback you might have for me.\n\nAny comments or helps would be appreciated.",
    "created_utc": "2024-10-29T17:21:04",
    "num_comments": 0,
    "comments": []
},
{
    "submission_id": "1gf9l8b",
    "title": "Am I the only one struggling to understand langsmith UI?",
    "selftext": "I was learning langchain and langsmith came up. But it is so hard to understand all the UI components of Langsmith and the purpose of every element. Am I the only one who feels this way?",
    "created_utc": "2024-10-29T17:09:10",
    "num_comments": 0,
    "comments": []
},
{
    "submission_id": "1gf9g9g",
    "title": "Deep learning studying tips",
    "selftext": "I’m a data analyst trying to transitioning into data science. I’ve recently studied most machine learning algorithms and worked on some NLP stuff as well. But when I started studying neural networks, it goes above my head. Question I have is, for my DS folks, how often do you use deep learning - neural network stuff at work? And any tips on learning from scratch.",
    "created_utc": "2024-10-29T17:02:38",
    "num_comments": 1,
    "comments": [
        "Hey, I think the easiest way is to start building ML models of your own. I started by using model generation through [https://www.plexe.ai/](https://www.plexe.ai/) and iterated on the output and used it in Kaggle competitions"
    ]
},
{
    "submission_id": "1gf5th4",
    "title": "Study book question: model type vs model class",
    "selftext": "Hi,\n\nThis might be a really silly question, but I can't seem to get past it.\n\n  \nI'm following the \"Introduction to machine learning\" course on the MIT Open Learning Library. In the first chapter, six characteristics are given by which machine learning problems and their solutions can be described:\n\n>We can describe problems and their solutions using six characteristics, three of which characterize the problem and three of which characterize the solution: \n\n>1. Problem class: What is the nature of the training data and what kinds of queries will be made at testing time?\n\n>2. Assumptions: What do we know about the source of the data or the form of the solution?\n\n>3. Evaluation criteria: What is the goal of the prediction or estimation system? How will the answers to individual queries be evaluated? How will the overall performance of the system be measured?\n\n>4. Model type: Will an intermediate model be made? What aspects of the data will be modeled? How will the model be used to make predictions?\n\n>5. Model class: What particular parametric class of models will be used? What criterion will we use to pick a particular model from the model class?\n\n>6. Algorithm: What computational process will be used to fit the model to the data and/or to make predictions?\n\n  \nI thought these six characteristics were very useful to create some order in my self-taught knowledge of machine learning. \n\nBut I do not understand what the author means with \"model type\", and where this differs from \"model class\". I think \"model class\" would be something like \"neural networks\" and \"support vector machines\". But what do they mean by \"model type\"? \n\n  \nThey go on to explain \"model type\" by \"no type\" and \"prediction rule\", but I still don't understand...\n\n  \nThe course notes can be found here: [https://openlearninglibrary.mit.edu/assets/courseware/v1/2481f8f2964716032b134db99e369b81/asset-v1:MITx+6.036+1T2019+type@asset+block/notes\\_chapter\\_Introduction.pdf](https://openlearninglibrary.mit.edu/assets/courseware/v1/2481f8f2964716032b134db99e369b81/asset-v1:MITx+6.036+1T2019+type@asset+block/notes_chapter_Introduction.pdf)",
    "created_utc": "2024-10-29T14:18:42",
    "num_comments": 0,
    "comments": []
},
{
    "submission_id": "1gf48ct",
    "title": "Quickly learning VMLs?",
    "selftext": "I’m currently a freshman student at university and I recently got the opportunity to chat with a grad student to weigh whether I’m a good fit for a machine learning lab. A project they are considering on letting me contribute to works with VMLs like Llava. Does anyone have good resources on quickly getting up to speed with this architecture, conceptually and at a code-level? Thank you. ",
    "created_utc": "2024-10-29T13:12:37",
    "num_comments": 0,
    "comments": []
},
{
    "submission_id": "1gf43dz",
    "title": "Seeking Ideas for a Captivating AI Exhibit at a Trade Show – Will Showcase the Final Project!",
    "selftext": "I’m working on an AI exhibit for an upcoming trade show and have been given a lot of creative freedom to make it something truly captivating. We’re aiming to design an interactive experience that draws people in, from AI enthusiasts to newcomers, and really showcases the potential of AI in an engaging way.\n\nIf you've seen or built an AI exhibit that stood out, I’d love to hear about it! Any ideas on themes, setups, or interactions that highlight AI’s practical applications or visual appeal would be greatly appreciated. And if we decide to incorporate any ideas from this post, I'll be sure to share the final project with you all here!\n\nI have access to a variety of technology partners and devices like Hailo-8 accelerator modules etc, so implementation is not a real problem, this is more about spit balling ideas. \n\nThanks in advance for your inspiration and advice! ",
    "created_utc": "2024-10-29T13:06:54",
    "num_comments": 0,
    "comments": []
},
{
    "submission_id": "1gf2wrp",
    "title": "My company fucked me up?",
    "selftext": "I'm 25M Working as a data scientist for the past 2 and half years, everything is great but the main problem is that my company is a really small company and even though I have the role of a data scientist, I have been doing some python programming related stuff for the past 2 years and I don't have any experience in Deeplearning. The only thing i am good at is Machine learning and i have also worked on a couple of LLM related projects.\n\nCurrently i am focusing on learning MLOps since i am confident with ML.( I wont get to work on any MLOps projects since my company is good with what they have and doesnt really need a standardized process or flow.) However i am learning so that i could get a job in some other company.\n\nSo, i need some honest advise guys, if you were inmy shoes. What would you do? ",
    "created_utc": "2024-10-29T12:17:56",
    "num_comments": 39,
    "comments": [
        "Sounds like they are giving you experience. How is this a fuck up?",
        "The percentage of companies with a data maturity + a necessity to work (and I mean really work, not just 'import OpenAI') with Deep Learning is very small.\n\nMost companies just need classic ML models and a fair amount of solid analysis. If you want to brush up your DL skills, you can do so on your own, but most companies (aside from really big ones) don't have the need for such models on a regular basis.",
        "If it is a good job, then how did your company f*k you up?\n\nYou are in a job, not in school or internship. Your company will assign you tasks important to their goals. If they align with your own, great. If not, look elsewhere.",
        "> My company fucked me up?\n\nin what way?",
        "Bro seriously just go out enjoy the sun talk to someone and relax.",
        "The value is in execution, not in building a fancy model... Most companies will want you to execute well and at low cost.",
        "Most places u will be a sql and dashboarding monkey. I’ve worked at a company as a data scientist for 6 years I’ve only done ML 2x count your grace",
        "The more I read anything on this sub the more I'm convinced that ML is no better than crypto lmao\n\nLike, what the hell are you talking about. Just apply to a different job",
        "If I were in your shoes, I would stop blaming the company. You own your career. Don’t just jump on deep learning or LLMs without knowing that you have a problem that needs it. You should find problems the company has and fix them. You decide which area of ML you want to work in and either make successful projects or prepare to leave for somewhere else.",
        "In my experience, except companies like openai, nobody works with fundamental dl models. Forget ml, they get excited with heuristics based methods that are really easy to build and work well. Even if they work with dl, small companies don't want to do anything beyond finetuning and maybe change small components here and there. Bigger ones are happy enough to use tools like azure for their work which again isn't building models yourself. They just want things to be built fast that won't take much time. If you have a manager with a research background, atleast for a couple of years you will be peaceful since they even understand the effort taken for failed experiments. But ultimately the higher management will force them to become short sighted just for survival. I have seen a small company try to build a number plate recognition system for the last 15 years and they are still running around in a short sighted manner. Sometimes,  they even close the groups and there are layoffs. So unless you can become good enough to work in places like openai or a core research group, that is how it will be, sorry.  On the constructive side, do work on dl projects in your free time and add them to your profile. If you want to work for bigger companies, learn the tools and get a certification. If it is feasible,  do try for a research paper. Also, attend some online talks whenever you can.",
        "Brother you should at least try to build a DP model. There is a guide in kaggle, start with the very basic and that is artificial neural network. Try to understand the inner workings of it I mean the math, the thing with DP is easy to work with because most of the time you get a high performance model and other times it’s just overfitting. Depends also on how you fine-tune the model. You mastered ML and I am pretty sure you can do it with DP.",
        "You need some tough love. Grow up and stop comparing yourself to what others have.\n\nYou're in the top 0.1% so try to be a bit more grateful.",
        "Is your company working with visual or text data? Or is it mostly relational databases?",
        "Learn to be humble first. Guess university and courses didn’t teach you that.",
        "Right tools for the right jobs. As some already mentioned only the big companies have the data maturity and scale to really go into LLMs and other large models. Even then you only use large neural networks if the task requires you to. As every ML studied person should know going for bigger tools is always a trade-off. If the task at hand requires interpretability the smaller the better, if the task requires robustness and accuracy the smaller the better, etc...\n\nPersonally having worked as industrial data scientist for couple of years most of my tools are very basic running medians, traditional statistics and hypothesis testing. These tools work the best when the size of the reliable dataset is only in thousands, and even then there might be missing values, frozen measurements, uncalibrated sensors, ...\n\nLarge neural networks would just simply be too volatile and learn individual errors rather than the big picture",
        "You are not the only one. I've been working as Data Scientist for a small company for 4 years. For the past 1 year and a half, I developed mostly Java components that don't have anything to do with a Data Scientist role. They keep giving me java developer tasks just because I am a programmer and of course the are in need of it and don't want to hire someone else. The best solution for me would be to find something better.",
        "I was partially in your shoes. Now, I say I was promoted over my head, but then it was a whirlwind of learning. It was awful, and I dont want to repeat it, but I learned so much during that time, (including the not wanting to repeat it), that it provided tons of transferable skills.\n\nStick with it. Don't make my mistakes and give in. Get better, get through the current economic tide, then move on.",
        "The vast majority of what we’ve found to be useful… stuff that can actually ROI… is classic ML.  You’re not wasting your time.",
        "OP, this is extremely common.  Move towards larger companies (with larger DS activities and ambitions) and you’ll see this decrease.  Like others have said, DL is exceedingly rare and a lot of what they’re having you do, while annoying will give you an edge on your peers.  Some is great, too much isn’t.  Where those lines fall is your choice to make.",
        "Before you worry about relevant experience and future career, let me ask, how's your debt situation? Are you contributing to retirement accounts and building wealth? The better your financial situation, the more choosy you can afford to be about what type of jobs you're taking.",
        "Eh, I'm a living example of not getting the experience I want from any job or university.  Im a loser dropout because I couldn't afford it. Previous company had me replacing fucking components in a stupid fucking angular web application when all I wanted was to work on computer vision projects.\n\nSo take control. Build your own projects. That's what I do. And I try to take on projects I know I can solve. At my experience level, I'm not going to reimplement a photogrammetry pipeline. But I can train some instance segmentation and key point detection models. Or fine tune a stereographic depth model.",
        "I’m literally in a very similar condition",
        "Yes, i am getting experience but not relavant to the field. One day i am doing some backend level python programming and the next day i will be doing some random stuff related to finding out how an entire app written in c# works and prepare a report.",
        "The thing is, i really think that i have the experience for a 2.5 yr Data Scientist. So ratherthan focusing on DL i am thinking that i should go with what i am good at. ML and Learn MLOps and specialize in that. What do you think about the potential of that",
        "Can these basic models you talk of learnt from just uni courses? I know a few classical ML models, scratch implementation and also using it for development, but I was hoping to aim for MLE so I started with DL and have a I think decent proficiency in them, (qlora fine-tuning and RAG framework are my best works) is there more to classical ML that I skipped over?",
        "Lmao. 100%",
        "It doesn’t always work like that, company has specific targets and resources limitations. You’d be investing to learn on this on ur own in ur own free time, if ur vision doesn’t align w them.",
        "DP = Deep learning?",
        "Exactly, anyone can build a model, but not everyone can understand it's flaws and fix it, oftenly falling in the underfitting/overfitting scenario. Having a good Data Science experience is good since the way the data is fed to the models is extremely important for its perfomace. \n\nWith such a background, Deep Learning will be just another step for OP. Good luck.",
        "Sure man. Thanks!",
        "There is not even a database most of the time. I am asked to work on things that a backend developer should do",
        "Sad to hear that pal",
        "Learning ml ops isn’t a bad idea if only because it’s more tech and can’t hurt. \n\nBut also, learning deep learning isn’t too difficult and should be another tool under your belt as a data scientist.",
        "I mean, with enough practice, you can do both. Of course, it will depend on your willingless to be a lifelong learner.\n\nPersonally, I wouldn't just decide my life based on redditor's comments, but here are my 50 cents:\n\nYou should not be thinking only about \"what skills should I focus on for tomorrow?\", but about \"what skills should I have under my toolbelt as a DS?\" and \"where in my career do I want to be in 5/10/15 years from now?\" as well.\n\nIf you want to become good in MLOps, Deep-Learning and other tools, it might no be feasible in 1 year, but in 3? It's completely doable. If you set your learning goals on a very short term, you won't be able to accomplish them (at least not in a way that sticks), and will probably mentally punish yourself for not being able to. \n\nSo set your goals for the long term, and focus on solidifying what you know and consistently building new knowledge on top of it.",
        "Strengthen your weakness instead of leaning on the skills you are already comfortable with.",
        "It's a double penetration model. An old school DL architecture based on two-path principle - you have a network which branches out into 2 parallel sets of layers, but there are several neurons that are connected to both which transfer knowledge between paths.",
        "Yes man it’s deep learning",
        "It’s difficult to interpret since it is a black box. When I see a ML model I can only think of a mathematical model because you can easily interpret how the model reached to a particular result. As with DP it’s very challenging, overfitting is an issue in DP, like you said you need expertise of how to fine tune without causing the model to overfit.",
        "Same broski"
    ]
},
{
    "submission_id": "1gf2so9",
    "title": "Which fields/areas of ML use mathematical optimization the most?",
    "selftext": "",
    "created_utc": "2024-10-29T12:13:11",
    "num_comments": 7,
    "comments": [
        "Like the whole ML is an optimization problem.",
        "Quite an expected answer, but: Optimization theory. \n\nStuff like SGD, AdaGrad, Adam, RMSProp, stochastic Polyak step size.",
        "I work in AI for operations research problems, and we use a lot of discrete optimization tools and knowledge alongside our ML approaches.\n\nMight not be exactly what you're looking for, but it's definitely mathematical optimization-heavy.",
        "Convex optimization is way more common than neural networks and has even more math because you derive convergence guarantees and second order methods are used more",
        "Adversarial robustness certification is very optimization-heavy.",
        "I know but some areas do require more explicit knowledge of mathematical optimization like working on unsupervised learning or RL require more knowledge than working on GNNs for example",
        "I'm studying OR right now actually, My question was more about what stuff from Optimization(also from OR because they're similar) is used in ML in general, like I know RL uses some stuff from optimization and OR for example"
    ]
},
{
    "submission_id": "1gf2d29",
    "title": "Is this an overfitting or data preprocessing problem?",
    "selftext": "For a while I have been building an AI model to classify digits from brain scans. I have managed to train the model to '99% accuracy' but when I run it on a test dataset it fails. Could somebody with more knowledge and experience take a look at my code to see if it is an overfitting or a problem with my makeshift data processing and help would be very kind.\n\n  \n[https://www.kaggle.com/code/scpcontainment/notebook69e313a46c](https://www.kaggle.com/code/scpcontainment/notebook69e313a46c)",
    "created_utc": "2024-10-29T11:55:33",
    "num_comments": 0,
    "comments": []
},
{
    "submission_id": "1gf27kj",
    "title": "Realistically, how much calculus do I need to know for deep learning?",
    "selftext": "I understand that derivatives and the chain rule are apart of it in a big way. I understand these concepts at a basic level, but sometimes when I'm in my Intelligent Systems class, he starts throwing these derivatives at us that have so many greek symbols I'm unfamiliar with, and sometimes I just don't understand how certain transformations are taking place, and honestly the *why* of why we are taking the derivatives of the sigmoid function and the back propagation algorithm. it really does seem like more than just simple derivations going on. Or maybe my calculus skills are truly that terrible.\n\nAre there any courses online dedicated to calculus study specifically within a ML/DL context?",
    "created_utc": "2024-10-29T11:49:06",
    "num_comments": 16,
    "comments": [
        "It’s mainly differentiation for back propagation. If you can do partial diffs, that’s mostly it.",
        "Partial Derivatives, Directional derivatives and Different types of Differentiations.",
        "“I haven’t calculated a derivative in years.” — Andrew Ng\n\nNot true, of course, because he teaches and occasionally does it in the whiteboard. But he said it in the context of modern libraries that abstract it for you. He used to do it manually (in code), but then created TensorFlow (with others).\n\nYou definitely need to understand the concepts thoroughly, but if you’re using PyTorch and not sitting for an exam, or teaching students, or getting a PhD, do you really need to be 100% proficient in the mathematical operation itself? \n\nIt’s a values question. Would it be nice to have the skill? Sure. Will you use it? Probably not. Would your time be better spent mastering all the other aspects of deep learning, or transformers, or tooling, or optimizing, etc?\n\nPersonally, I’d learn it well enough to get a few sample problems correct, but not enough to be guaranteed to ace an exam. That’s just me though. I suspect there’s some value in working enough sample problems to strengthen your understanding of the principles, which in turn, will probably give you better intuition about related ideas.\n\nEdit: You need to understand the Greek symbols if you want to understand papers in the future. \n\nPersonally, I sought to understand the formulas well enough to explain each part to someone, and why it’s there, but not well enough to write the formulas without accessing my notes.",
        "Calculus? Functional Analysis.\n\n  \nEdit:   \nHow much X is needed to do Y?   \nAs much as, it gets you started with doing Y as soon as possible. After that it is perpetual learning.",
        "Justin waller",
        "Backpropagation is basically an application of the multivariate chain rule to the \"credit assignment problem\" in neural networks. However, this application cannot be done directly, since it results in an exponential time algorithm, which would make the training process infeasible.\n\nFor this purpose, we use a Dynamic Programming algorithm to calculate the chain rule in an optimized way, leading to a polynomial time algorithm.",
        "In the context of backpropagation specifically the application of the chain rule is fairly simple. I'd suggest going through this video (by Andrej Karpathy) to implement a simple backprop engine yourself - the best way to understand something is to build it from scratch.\n\n[https://www.youtube.com/watch?v=VMj-3S1tku0&t=11s](https://www.youtube.com/watch?v=VMj-3S1tku0&t=11s)",
        "If you're familiar with calc 1, this writeup might help bridge the gap to some of the higher dimensional stuff you encounter in ML papers: https://explained.ai/matrix-calculus/",
        "Been in AI for 10 years now. Honestly I know enough to be dangerous and know what to brush up on",
        "And vector calc is important too, i guess ",
        "Is integration needed?",
        "Do you know of any courses online which teach calc specifically within an ML/DL context?",
        "I am indeed getting a PhD. The PhD is mostly centered around \"applied AI\" though, using PyTorch and such, so maybe you're not too far off. Thank you!",
        "IDK why you're downvoted. Vector calc (gradients and Jacobians) are the work horse of neural networks.",
        "you should understand how integration works and how and why for example expectations can be expressed as an integration. Occasionally you will find integrals in the theory behind the models, for example for the Kullback–Leibler divergence (key for variational autoencoders!)\n\nBasically you will find integrals in some numerical and a lot of statistical applications of deep learning, but unless you want to really get deep into the theory I doubt you‘ll need more of an understanding than understanding Fubini‘s theorem.",
        "Nice, thank you!"
    ]
},
{
    "submission_id": "1gf1mea",
    "title": "Suggestions on how to get started and cover things quickly with the right foundations",
    "selftext": "So I am a kind of getting started with machine learning and data science in general. My background is maybe a couple of years working as a backend engineer and have some basic idea on data preprocessing and how it is done.\n\nCurrently I am in a project as an Al/ML engineer tasked with working on generative Al and training models. I am the only person in the team as well. I can read about it, but don't relate much as I do not understand the concepts a lot and need to build up some foundations. I am not sure how to cope up with it and would appreciate suggestions or help with how to get started and what to cover probably practically too in a swift pace.\n\nI feel I need to build up on my data science and machine learning foundations and then my generative Al skills to be able to sustain and proceed in this career path and shift from a backend engineer role moving ahead. Suggestions on roles and jobs combining current project and previous experience is also appreciated.\n\nThanks in advance!",
    "created_utc": "2024-10-29T11:24:48",
    "num_comments": 1,
    "comments": [
        "Hey, I think the easiest way is to start building ML models of your own. I started by using model generation through [https://www.plexe.ai/](https://www.plexe.ai/) and iterated on the output and used it in Kaggle competitions"
    ]
},
{
    "submission_id": "1gf07rn",
    "title": "Thoughts on this response?  Seems like a security issue.",
    "selftext": "https://preview.redd.it/808s7dgqcqxd1.png?width=745&format=png&auto=webp&s=c5de154f248dd8067df794cd489806a7e305ec5b\n\n",
    "created_utc": "2024-10-29T10:27:03",
    "num_comments": 0,
    "comments": []
},
{
    "submission_id": "1gezlht",
    "title": "Need ideas for a mini project for college\n",
    "selftext": "Hello everyone,\n\nI'm new to ML and recently joined a course. I need to submit a mini project that involves using a pre-trained model and training it with a benchmark dataset for a specific use case that I need to finalise. I'm not sure where to begin or which topic to choose.\n\nCould you suggest a simple project idea for a college level submission?",
    "created_utc": "2024-10-29T10:01:19",
    "num_comments": 3,
    "comments": [
        "image models i guess, for pre trained i suppose thats where id start, something like VGG16 on the cats vs dogs dataset",
        "You might want to pick a subject area you already know about. You can draw from your other coursework or hobbies.  Ideally you want to focus on training and evaluating a model, without getting too sidetracked in some subject you don't know much about. Try to keep the task pretty simple, even if it seems a bit artificial. \n\nYou'll need to line up a few things:\n\n\\- data set for training\n\n\\- labelled data for evaluation\n\n\\- task\n\nThere's many examples of these kinds of papers in medical research. One recent example is Asclepius, [https://github.com/starmpcc/Asclepius](https://github.com/starmpcc/Asclepius)\n\nYou could try mixing and matching existing resources from prior works. For instance, take the Asclepius dataset and train an LLM. Then use the tests from the Med-PaLM paper to evaluate it. Compare the base model with your fine tuned version, then compare those with ChatGPT.\n\nOne source to check out is various leaderboards.",
        "Hey, maybe start with some Kaggle competition idea. I started by using model generation through[https://www.plexe.ai/](https://www.plexe.ai/) and iterated on the output and used it in Kaggle competitions"
    ]
},
{
    "submission_id": "1gez4o1",
    "title": "Generate Time-Series data from a defined SARIMA model using Python",
    "selftext": "Hello all, I have been trying to figure out how to simulate/generate a time-series from a defined SARIMA model using Python. My motivation is to have a tool to visualize the \"population\" ACF & PACF of classical SARIMA models at any (p,d,q)x(P,D,Q)s and given an array of AR & MA coefficients.\n\nI have been checking out Python libraries such as darts, statsmodel, nxtla, etc. but their documentation is limited for this application and always require an initial dataset.\n\nI am looking for a Python implementation that is closely similar to R. Below is an example for ARIMA:\n\n`set.seed(1);`\n\n`sim_data = arima.sim(list(order=c(1,1,1),ar=0.5, ma=0.3), n=1000);`\n\n`arima(x,order=c(1,1,1),include.mean=FALSE);`",
    "created_utc": "2024-10-29T09:41:49",
    "num_comments": 0,
    "comments": []
},
{
    "submission_id": "1geyxww",
    "title": " New Mechanistic Interpretability Guide: How to Find a Feature with a Premade Sparse Autoencoder ",
    "selftext": "My friend Sean and I have made, as far as I know, the first step by step guide in How to Find a Feature with a Premade Sparse Autoencoder: [https://beta.ai-plans.com/guide/g7yjq98bhuyhkza](https://beta.ai-plans.com/guide/g7yjq98bhuyhkza)\n\nThe closest that exist atm, seems to be the GemmaScope tutorial and the SAE Lens tutorial, neither of which show how to do this generally, especially for SAEs and models which aren't in the library.\n\nThis will be part of a series of guides on how to do things in Mechanistic Interpretability.",
    "created_utc": "2024-10-29T09:33:54",
    "num_comments": 2,
    "comments": [
        "Wonderful. You might want to briefly expand a bit on what this is an why its useful. You also may want to add screenshots so a reader can see what it does without running anything.\n\nI look forward to trying it out.",
        "Thank you! Will add these today"
    ]
},
{
    "submission_id": "1geyr8p",
    "title": "Suggest some authors for books ",
    "selftext": "I want to read the AI books but I can't find the best author to read, \nI need your help and suggest me  the best authors and latest books for AI.",
    "created_utc": "2024-10-29T09:26:25",
    "num_comments": 0,
    "comments": []
},
{
    "submission_id": "1geyiil",
    "title": "1 SQL command to automatically create and manage vector embeddings",
    "selftext": "Hi, r/learnmachinelearning!\n\nVector embeddings are essential for many AI applications, like RAG and semantic search. However, keeping embeddings in sync with your data can be time-consuming and complex.\n\nWith [pgai Vectorizer,](https://github.com/timescale/pgai/blob/main/docs/vectorizer.md#automate-ai-embedding-with-pgai-vectorizer) an open-source tool for PostgreSQL (which also doubles as a vector database!), you can manage embeddings with just one SQL command. This tool is great for software engineers at any level—whether you’re transitioning to AI or already experienced.\n\nCheck out my new [blog post](https://www.timescale.com/blog/how-to-automatically-create-update-embeddings-in-postgresql/) on automating embedding creation and updates with pgai Vectorizer! I’d love to hear your thoughts and answer any questions in the comments below.",
    "created_utc": "2024-10-29T09:16:25",
    "num_comments": 0,
    "comments": []
},
{
    "submission_id": "1geygpl",
    "title": "For researchers, how do you stay current?",
    "selftext": "I'm still learning and understanding the basics and fundamentals. But, sometimes I will go on arXiv to see what people are researching. But, the machine learning arXiv has hundreds of new papers each day. For instance, [today](https://arxiv.org/list/cs.LG/new) it was 560 papers. How do you juggle your research while also trying to stay current in such a fast-moving field?",
    "created_utc": "2024-10-29T09:14:20",
    "num_comments": 19,
    "comments": [
        "Focus on people not papers.",
        "I follow problems that are relevant to my job. And do literature search on each problem. Sometimes a good solution comes from way back in history. \n\nExample, time series. 5 year old stuff is still super relevant. The new AI doesn’t always work and isn’t yet very tunable (please correct me if I’m wrong). Old ML like LSTM still works great for certain problems and can be trained on custom data more easily. So I’m focusing on making my lstm better for now. When it stops performing, I’ll research the problem again. \n\nMy job is data science, so I look at a lot of fresh projects. They are usually reduced to old problems with old literature.",
        "You don't read every paper.  You skim the AXRIV list looking for stuff that is interesting.  If title looks good, read abstract.  If abstract looks juicy read intro and conclusion.  Only read the middle if it is directly useful to you.  \n\nAnother thing to do is subscribe to journal email notifications. They come out quarterly and can give you a good idea of what cool new ideas are coming out.",
        "Two minute papers on YouTube is a godsend, super digestible and cites sources to investigate further if someone tickles your fancy. myaihub.ai is also pretty good has a lot of current news that you can check out.",
        "You get into the habit of picking out papers you like on a regular basis; for me, I do it about every week or so. This applies to research in general; not just in ML.\n\nIf something *super* important comes up that starts to make waves in the field, you'll probably hear about it pretty quickly. Those also go into the pile.",
        "embed yourself in a community of people who have similar interests and monitor what content is floating around the community. For me, this is:\n\n* researchers I follow on twitter\n* researchers I follow on github\n* the discord server for a research lab I like",
        "Do researchers read every paper ?",
        "im interested in this as well.\n\nhow do people find the state of the art algorithms for computer vision?",
        "your user id🤣 fits perfect in this scenario\n(English translation of his user ID means :\"Bro you could just simply listen to me\")\n\n\nIt's true, progress is happening at a rapid rate and honestly this issue cannot be generalized by any solution. \n\n\nReading arxiv is a great habit, I once had an interaction with an IBM senior researcher (around 11-12 years of experience) and as a student he said he had developed a habit of reading one arxiv paper everyday during his 4 years at college and graduation, condition being that you only read paper of your domain only or you pick a HANDFUL of favourite authors only who publish consistently and their research aligns with your field of interest that you wish to pursue in the long run\n\n\nI believe you could start with that and time will play it's role",
        "In one sentence you show why academic research is broken.",
        "And what if a breakthrough comes from someone you’ve never heard of?",
        "yep agree with this. back then, i had a training with Monas University, friend of robb hyndman became a tutor. sometimes in his exploration, he still used LGBM and perform the best compared to complex NN",
        "May the source be with you.",
        "huh. What if?\nCheck out the author of grassman algebra. Hermann Grassman. \n\n\"\"\n\nHermann Grassmann, a 19th-century German mathematician and polymath, made pioneering contributions in linear algebra and vector calculus, yet his work was largely unrecognized in his lifetime. Grassmann published his Ausdehnungslehre (Theory of Extension) in 1844, introducing abstract mathematical ideas like vectors and multidimensional spaces. However, his peers found his work too complex and abstract, so it was largely ignored.\n\nDespite his groundbreaking ideas, Grassmann faced professional setbacks, teaching high school for most of his life and never securing an academic position in mathematics. It wasn't until decades after his death that mathematicians fully appreciated his contributions.\n\n\"\"\n\nAfter Hermann died, Gibbs came along and he \"discovered\" his texts to father Vector Calculus, to dispose of quaternions.\n\n**Countless such cases in academia.**",
        "Always",
        "I agree that’s common, but that’s more of a reason why you shouldn’t focus on people over people.",
        "You got my argument wrong. But It could be intentionally so. \ngood luck.",
        "If you mean it’s OK to just follow the big shots because they’ll effectively parrot back any major result, that might work for an individual keeping up with the field.\n\nBut it becomes dangerous if everyone does that because then lesser-known folks don’t get their names out.",
        "zeitgeist? whats that? \nmyth of sissyphus? do they care?\n\nI'll leave you with your own answers now. dirty I know. I hope you'll understand why I am doing so and forgive me."
    ]
},
{
    "submission_id": "1gevxye",
    "title": "Best resources for PYTORCH ",
    "selftext": "Hey guys  \nCan you suggest me any best resources for learning pytorch in the most efficient way but with less time(including books, youtube tutorials and other online courses). ",
    "created_utc": "2024-10-29T07:27:20",
    "num_comments": 14,
    "comments": [
        "it's extremely suspect that this post and the [other](https://old.reddit.com/r/learnmachinelearning/comments/1gesndg/about_pytorch_courses/) one from two hours earlier were both posted by month old accounts and both are directing people to this daniel bourke course. maybe it's good material, but the reddit activity I'm seeing screams sock-puppet marketing to me.",
        "It’s good to learn by doing! PyTorch has some built-in ready to go datasets that you could quickly write a class for and start training loops. Would also recommend writing your own dataset class too to learn the working of the data loader and how it requires the dataset class to be constructed!",
        "My favourite is this 25 hours one [link](https://youtu.be/Z_ikDlimN6A)",
        "karpathy s videos from last 2 years",
        "5 minute beginner tutorial with numpy comparison: \nhttps://youtu.be/EB4pqThgats?si=dpdbTOmWI9vZoDQC",
        "I enjoyed PyTorch Step By Step",
        "learnpytorch(dot)io docs\nand official pytorch(dot)org docs\n\nyou can also refer to the 24 hours pytorch tutorial of daniel, the video nothing but a video format of learnpytorch(dot)io docs\nI recommend learning by reading docs and then skim over the chapters of the yt tutorial.\ni am doing the same and it is working for me",
        "[keras.io](http://keras.io)",
        "Does it go in-depth or skims through all possible things? I believe you have completed it. I am asking as I still going through PyTorch.",
        "🎵 🎵 optimer step step step 🎶🎶",
        "Thanks so much going to follow this!",
        "I second that https://pytorchstepbystep.com/",
        "month old account that's totally not a sock puppet says what?",
        "I believe it covers most of the things you need to learn."
    ]
},
{
    "submission_id": "1gevrtd",
    "title": "Question about Anomaly Detection and Prediction in TimesFM Model\n",
    "selftext": "Hey everyone,\n\nI've been working on a project involving Time Series forecasting, anomaly detection, and prediction, and I've come across the TimesFM model in my research. From what I understand, TimesFM doesn't seem to include an auto-encoder, and I'm unsure how (or if) it's possible to perform anomaly detection or prediction with this model.\n\nHas anyone here used TimesFM for these tasks? Is it capable of handling Time Series anomaly detection, or is this feature not implemented yet?\n\nAny insights would be much appreciated! Thanks",
    "created_utc": "2024-10-29T07:20:08",
    "num_comments": 2,
    "comments": [
        "This is self indulgent, but..\n\nThe Matrix Profile/time series are a good anomaly detector, because..\n\n• They have only zero or one parameter\n\n• They are blindingly fast (tested to trillions of datapoint)\n\n• Batch or Online with zero-lag\n\n• Can work with or without training data\n\n• Trivial to have *golden batch* or *amnesic versions*\\*\n\n• The only TSAD algorithm to have been used by at least 100 research teams to solve a real problem\n\n• You *can* incorporate domain knowledge\n\n• Invariant to concept drift\n\n\n\netc\n\n[https://www.dropbox.com/scl/fi/n5o61cle4eszucgkjqozr/DSAA\\_Time-Series-Data-Mining\\_A-Unifying-View.pdf?rlkey=prv2v3cbumdxzxhv1ob71s4dz&dl=0](https://www.dropbox.com/scl/fi/n5o61cle4eszucgkjqozr/DSAA_Time-Series-Data-Mining_A-Unifying-View.pdf?rlkey=prv2v3cbumdxzxhv1ob71s4dz&dl=0)\n\n  \n[https://www.cs.ucr.edu/%7Eeamonn/MatrixProfile.html](https://www.cs.ucr.edu/%7Eeamonn/MatrixProfile.html)",
        "I'll definitely look into it and thanks for such a detailed  comment 🤗"
    ]
},
{
    "submission_id": "1geuhcj",
    "title": "I built \"Cracked Engineers\" – a new platform for technical job roles only (currently a lot of good ML startups posted their job posts)",
    "selftext": "",
    "created_utc": "2024-10-29T06:21:53",
    "num_comments": 3,
    "comments": [
        "Who are you?   Any context?   Why would I use your app?",
        "I used to work at Google DeepMind and Microsoft. Now building my startup.\n\nSee this for more info: https://medium.com/@gordicaleksa/how-i-got-a-job-at-deepmind-as-a-research-engineer-without-a-machine-learning-degree-1a45f2a781de\n\nThought helping people find tech jobs. See the why in my other comment. 🙏"
    ]
},
{
    "submission_id": "1gesndg",
    "title": "About Pytorch Courses",
    "selftext": "Talking about Daniel Bourke courses on Pytorch, theres mainly 3 different sources with different lenghts:\n\nThe book: [https://www.learnpytorch.io/](https://www.learnpytorch.io/)\n\nThe Youtube 1day Video: [https://www.youtube.com/watch?v=Z\\_ikDlimN6A](https://www.youtube.com/watch?v=Z_ikDlimN6A)\n\nAnd the Udemy Course: [https://www.udemy.com/course/pytorch-for-deep-learning/](https://www.udemy.com/course/pytorch-for-deep-learning/)\n\n  \nThey all look similar, but as i said, they are at different lenghts (YT 24 hrs, Udemy 54 hrs), also notice that some examples hi gives on the YT video os not contained on the book and vice-versa.\n\nSo i really wanna know whats the difference between them and which is the most 'complete' so i can go through.",
    "created_utc": "2024-10-29T04:48:35",
    "num_comments": 5,
    "comments": [
        "just go through the tutorial in the pytorch docs like everyone else",
        "it's extremely suspect that this post and the [other](https://old.reddit.com/r/learnmachinelearning/comments/1gevxye/best_resources_for_pytorch/) one posted two hours later were both posted by month old accounts and both are directing people to this daniel bourke course (in the other one, it's a third month old account directing people to the course in the comments). Maybe it's good material, but the reddit activity I'm seeing screams sock-puppet marketing to me.",
        "Interested in this too.",
        "it’s promotional campaign",
        "yes, it is a deceptive promotional campaign designed to present a false impression of social proof, hence \"sock-puppet marketing\"."
    ]
},
{
    "submission_id": "1ger17j",
    "title": "10 GitHub Repositories to Master Natural Language Processing (NLP)",
    "selftext": "",
    "created_utc": "2024-10-29T03:08:49",
    "num_comments": 0,
    "comments": []
},
{
    "submission_id": "1geqlcj",
    "title": "GGUF V/s GPTQ V/s AWS V/s Bitsandbytes",
    "selftext": "",
    "created_utc": "2024-10-29T02:36:55",
    "num_comments": 0,
    "comments": []
},
{
    "submission_id": "1geprvj",
    "title": "Deep Reinforcement Learning Survey",
    "selftext": "[https://github.com/EzgiKorkmaz/generalization-reinforcement-learning](https://github.com/EzgiKorkmaz/generalization-reinforcement-learning)",
    "created_utc": "2024-10-29T01:33:44",
    "num_comments": 0,
    "comments": []
},
{
    "submission_id": "1geouj4",
    "title": "Suggest me Machine learning project ideas ",
    "selftext": "I have to complete a 12-credit final year project for my university. I'm an Electronics major, so could you suggest some project ideas?\n",
    "created_utc": "2024-10-29T00:20:41",
    "num_comments": 9,
    "comments": [
        "Imagine if you could classify survivors/non-survivors from a sunken ship",
        "Hey there! For your final project, focus on selecting something that truly sparks your passion while demonstrating your technical chops in electronics. Don't just pick a project to check a box - choose something that genuinely excites you and allows you to showcase your skills. Consider exploring areas where electronics intersect with emerging technologies like IoT, robotics, or sustainable tech. The key is selecting a project that not only challenges you technically but also reflects your unique interests and potential career direction. But you can also check [this resource](https://www.interviewquery.com/p/machine-learning-projects) out if you really need ideas right away.\n\nBy the way if you're interested, students can get free access to Interview Query if their university partners with them for a bulk license deal!   It would be great if someone could reach out to your career services center or program director to explore this opportunity. Check it out here: [Interview Query](https://www.interviewquery.com/university)",
        "Hey, I think the easiest way is to start building ML models of your own. I started by using model generation through [https://www.plexe.ai/](https://www.plexe.ai/) and iterated on the output and used it in Kaggle competitions",
        "You can create advanced Speech-To-Text system with state-of-the-art NLP algorithms and models. Or a chart builder where model will generate a histogram or a kind of chart based on the information extracted from Excel file. It is like constructing a kind of visual statistics. You can use Seaborn and Matplotlib as instruments for visualizations and GANs for image generation",
        "Here are some of the Beginner Machine learning Projects to do:\n\n* Handwriting recognition with neural networks\n* Breast cancer classification\n* House price prediction\n* Stock price prediction\n* Emotion recognition\n* Image recognition",
        "Will the project be based on CSV dataset",
        "Thanks a lot 👍",
        "Thank you, that's great",
        "Bro I'm looking for bigger projects which  worth 12 credits"
    ]
},
{
    "submission_id": "1geob5m",
    "title": "How manage the models?",
    "selftext": "I have a problem to classify, um trying a lot of models, with variations of data. Is there a way to organize the evolution of models, maintain a historic of the models with the best scores with the data used?\n\nAfter a lot of combinations of data, models and epochs everything is a mess.",
    "created_utc": "2024-10-28T23:37:59",
    "num_comments": 4,
    "comments": [
        "Utiliza mlflow",
        "I recommend clearml, it's free, server-based (accessible from anywhere), and much better UI and functionality than mlflow",
        "Mlflow",
        "mlflow"
    ]
},
{
    "submission_id": "1geo0h6",
    "title": "Need help developing Deep learning model for stock market",
    "selftext": "I am a data science intern at a firm which is involved in trading stocks.The higher ups have asked me to develop a deep learning model to predict the direction of the stock market with upto 80 percent accuracy.Even after my repeated explanation of what are random walks and the very nature of the market makes it unpredictable.they want me to try developing a model for the same.I have tried basic LSTM and hybrid CNN-LSTM models without promising results in [forecasting.Is](http://forecasting.Is) there any resource i can look at which would help me atleast deliver a model with some level of reliability.",
    "created_utc": "2024-10-28T23:15:01",
    "num_comments": 30,
    "comments": [
        "You have been tasked with something that is generally considered impossible by even the world's best quantitative finance experts.\n\nIf you could do what you're being asked to do, you would be paid millions of dollars per year at the world's best trading firms, and could quickly become one of the richest people in the world.\n\nIf I were you, I would try to gather some evidence to demonstrate this to the higher-ups. Someone in this situation is not understanding what's being suggested. Nobody on here can give you advice to help you achieve this.\n\nHeck, show them this comment section. I'm sure I won't be the only one to say this.",
        "Im guessing the “higher ups” dont come from a STEM background?\n\nMaybe MBA, MFin types?",
        "Few points:\n1. If anyone is doing this then it is very unlikely they will tell you how.\n2. You need to consider a timeframe. What is the investment horizon?  Are you talking milliseconds, days or months?\n3. 80% accuracy... good luck with that. Why not aim for something like 52%.\n4. Thats not to say a deep learning model can't be used to identify stocks that are likely to trend with high accuracy... but all stocks... nope.\n5. You are unlikely to come up with anything robust using price data alone.",
        "This stuff already exists for hedge funds and has been employed for a really long time. Most of the stock market already trades off of algorithm. Does your firm not know this already? https://investingnews.com/daily/tech-investing/emerging-tech-investing/artificial-intelligence-investing/pioneering-hedge-funds-artificial-intelligence/",
        "Lol I feel like stock markets and finance are something that everybody in ML goes through at the very start. I also started out with that and even managed to publish a paper in a top conference but quit after learning that it's pointless.\n\nThe stock market is unpredictable. People who have PhDs in math and have been in the stock market for decades can't do it.",
        "That sucks for you. Maybe it's time to look for another job. Being tasked with an impossible job is a no win situation. It ends up making you look bad no matter how you deal with it. Prove to them it's not possible and you look like a complainer. Do your best and fail and you look like a failure. \n\nI was tasked with an AI project over the last year. Worst thing that could have ever happened to my carrier. Nobody is happy with the outcome, it works exactly like I told them it would, but nobody listened and expects it to perform feats of magic. When it doesn't that's my fault. Now I've spent a year leaning all this ML stuff, which is useless in the job market, and that's kept me from keeping up to date with the core technologies I should have been focusing on",
        "So if i leveraged up a 82% prediction rate i could turn my roth into fort knox in under a year.  What theyre asking for is risk free high yield returns.  Explain to them the only way to do this is to buddy up with central bankers, market makers, family fund managers and ask to smell their farts, and trade ahead of them.  Algos cant even do that.  Algos are what all these kingpins use to runnarbitrage on each other.  Let them know they are merely minnows.  Good luck keeping your job tho",
        "Haha good luck",
        "I have a fine tuned model that does that for large cap with 95% accuracy. It's being implemented now after extensive testing. It can predict not only market movements but short term predictions (intraday) of individual stocks with >50% proven accuracy across a 3 month period. Profits are up 50% with modest implementation trying to stay ahead of competition.",
        "I mean, this sounds like gibberish/ nonsense. \n\nEssentially you are either misrepresenting the actual request, or the request is fucking stupid. \n\n“Deep learning model”, “80% accuracy on market direction (?)”\n\nDo you know anything about ML? Does anyone in your firm? \n\nThe simplest answer is: you need to come up with a predictive model, get the data, train the model and then test its accuracy. \n\nThere’s literally dozens if not hundreds of options for every single aspect of the prior sentence. You are starting from square 0 with your post. \n\nMaybe look up a course of ML?",
        "Start with considerations what would make your model more feasible. \n\nOne stock or one sector. \nNot the entire market.\n\nStart with getting it right more than half the time.\n\nPossibly incorporate sentiment analysis and ensure you take into account quarterly reports. \n\nI won't say impossible. I will say highly unlikely with a team of one.\n\nDocument everything you have tried \nGood luck.",
        "Would Deep RL help this? I haven't tried Deep RL myself but perhaps you can since you have already experimented with CNN and LSTMs. You might also try context aware time-series analysis (https://arxiv.org/html/2410.12672v2), might help. Good Luck!",
        "Tell them \"If this works, they will be out of a job\".",
        "I’ve faced not an entirely unrelated problem. \n\nLLMs are expensive for FAANG level companies. You’ve not provided the name of your company, but I guess it’s smaller given the nature of higher ups request.\nPackage this fact and present to higher ups. Make a special accent on unnaturally expensive cost of development and maintenance. Provide some numbers, like training LLM is worth a year of electricity in 2 big cities. Money is the language they will understand.\n\nNow, throw them a lifeline. Suggest a model that specializes in stock trading only. Make an accent how cheap it is compared to LLM. What will it contain, they might ask. A good proven machine learning. Statistic tools that proved useful since before computers. A cherry on top, artificial intelligence is just a subject in machine learning. Why settle for a subject when they can have the field as a whole?\n\nYou’ll have a hell of fun designing such a system and implementing it while being paid. This is a dream request, from a personal standpoint.\n\nOne last thing given you’re an intern. Never quit a job because of someone. Quit only in light of much more lucrative job opportunities.",
        "Many many have tried but failed. If one can build such a model, he will become a billionaire.",
        "You would make millions even if the accuracy is only 55% lmao.",
        "Hey, I think the easiest way is to start building ML models of your own. I started by using model generation through [https://www.plexe.ai/](https://www.plexe.ai/) and iterated on the output",
        "If you succeed building such model, you can just buy the company you're working at and fire your bosses that gave you this task.",
        "I am not sure about this but I guess you will be able to get more than 50% accuracy by analyzing real time data from financial sources using a llm and then combine the results with lstm models. Use them to predict a particular stock will go up or down.",
        "Right? They have essentially tasked an intern with making \"free money\" machine. If OP could do such a thing, they wouldn't be working as an intern at some company where they aren't taken seriously. They would be sipping cocktails on a beach on a their private island.",
        "I have expressed my opinions on it being impossible.But was still told to deliver something which can help them.",
        "Exactly 💯",
        "I’m curious. Why do you think that ML skills are useless in the job market?",
        "What do you think LLMs stand for?",
        "I would ask for clear guidance on the strictness of the 80% threshold, and, for example, the repurcussions on your future at the company for something more reasonable like 40%+. If they can't give you that information, you are in an unfortunate situation for your career and just try your best while planning for other companies",
        "If you happen to be one of the few people who can build a large language model that can compete with the best of them ML skills are super valuable.  That's only a handful of people. If you get two or three years of experience with machine learning you're not one of those people. Also since it takes huge computing power to build one of those models there's very few people that are going to get access to even try. If you know how to use pyTorch or Tensorflow to build recognition or classification models, that's not really an in-demand skill. The people that openAI still needs are people to write Android apps and to write web scraping tools and to do the infrastructure but they don't need people to write simple machine learning code. Most of the projects that aren't large language models, like this time series prediction that the OP was asked to do aren't really going to be successful because the people backing them are idiots that have no idea how machine learning works or what's even possible. This means you're really better off learning to do traditional programming that's still in high demand and everybody actually needs that. A year or two of machine learning experience only takes you away from the useful skills that are marketable and therefore makes it really hard to find a job.",
        "Large language models",
        "Hmm I see. Your point of view seems rather pessimistic to me. I work as a data scientist for an automotive OEM and the field is just beginning to being discovered. Granted, you won’t get to do cutting edge technology like at OpenAI but the applications of what you call “simple machine learning” are huge (driving style, accident detection, remote operations, battery life, etc etc etc…) and since in this industry it’s such an unexplored field, each of those projects bring in huge benefits for the company in terms of money savings, time savings or often both. \n\nAnd automotive is just one field that just discovered data science but there are many more to join (logistics, transport, manufacturing…) Fields that traditionally didn’t have too much room for innovations until now. \n\nSo, all in all, I think there are still reasons to remain optimistic and although most of us won’t get hired by openAI with a 350k salary, our skills are still very much in demand today",
        "What data do you think they were trained on?",
        "I am a little pessimistic due to my experience with it so far. I built a recognition model with 97% accuracy and got the whole thing running on an Arm microcontroller with only 64k of flash for the model weighs and code. People are unhappy because they expected 100% accuracy and for it to act on sensor data it didn't have. I find myself in a position where I may loose my job over this and I'm finding that the ML skills I do have aren't really in demand. I'm just not seeing any job listings for someone who can build a CNN to do some recognition tasks or things like that."
    ]
},
{
    "submission_id": "1genrlr",
    "title": "Should I switch my early career to Machine Learning?",
    "selftext": "I am currently working as web developer internship in a company. By the looks of how AI is evolving, I think there will be very less scope for web development in near future. I am enthusiastic to learn new things and I have recently pursued a CS degree. I can spare 1-2 hours daily and probably 4 hours weekend for learning. my maths is pretty mediocre and i have gaming laptop with just i5 and rtx 3050 4gb, should I start machine learning now or go for another field like devops or cloud?",
    "created_utc": "2024-10-28T22:57:21",
    "num_comments": 27,
    "comments": [
        "No one will replace you if you are good in your field. If you're a top performer, you're not easily replaceable. Instead of changing roles, use AI to improve your skills and work smarter.\nToday you want to switch to Machine learning, tomorrow you will switch to cybersecurity and will never get out of this loop.",
        "> and i have gaming laptop with just i5 and rtx 3050 4gb\n\nFocus on your laptop in the meanwhile. Owning a gaming laptop will define your future career like nothing else could. Have you considered also buying a water bottle? Maybe a water bottle with Pokemon stickers on it?",
        "You're probably not going to be employed over someone who has a master's and publications in top-tier conferences.\n\nYou're not replaceable unless you suck, which you don't need AI for.",
        "I was a machine operator at a milk factory. Now I am learning Machine learning. Today is the last day. Tomorrow will be too late to be cracked into",
        "CS degree plus Machine Leraning can only make you more valuable, in my opinion. I have a CS bachelor's degree and worked as a full stack, mostly web developer, for 20 years. I am now looking into a master's in machine learning.  I believe it's essential for me to understand it in orde to incorporate it into my field since the two are most definitely merging. \nI may be wrong. Thoughts?",
        "Hope everyone switch to Machine Learning so that backend engineer job market is not crowded.",
        "Don’t do it, nowadays everyone wants to switch and you’ll have to compete with people with masters",
        "Yes AI and ML is a good field, suggest you start learning and look for opportunities within the same company, I have been a software developer for over 25 years now and keep learning new things all the time, even within Web development there is a new framework every other day so continuous learning is essential.\n\nYou don't need a new computer, start learning on kaggle, it is free and everything is hosted, only need a browser, once you are good at it opportunities will come",
        "Just do what you find interesting OP, all are good choices. If you want to minmax, you'll probably have a higher ROI if you study devops/cloud because that's closer to your current job.",
        "Just do what interests you. But ML is a lot of math. And with a lot, I mean a lot. Ah yeah and data cleaning.",
        "Hey Reddit!\n\nWe just published an “early days” benchmark evaluation of **Plexe**, our prototype AutoML framework designed to train ML models from natural language problem descriptions, data, or both. In our post, we focus more narrowly on the framework’s performance when producing supervised learning models from datasets.\n\nIn our benchmarks, Plexe consistently achieves either competitive or marginally superior results relative to frameworks like **AutoGluon**, **H2O AutoML**, and **TPOT**.\n\nIf you're interested in AutoML, check out our post for some preliminary findings, code and data. While we are not yet ready to release Plexe to the world, we intend to launch a public release in the next few months. We would love to get some feedback from the community, so please feel free to [join our waitlist](https://plexe.ai/).\n\nIf you want to try Plexe out, we have a discord server where you can message us your ML problem description along with a small dataset and we will create a model for you along with a report about the solutions considered and the corresponding performance metrics.\n\n👉 [Read more here](https://www.plexe.ai/post/plexe-production-ready-custom-ai-from-natural-language)",
        "I'm seeing ads everywhere now for machine learning degrees. Seems over bought.",
        "Hello,\n\nI am in your same position. I was hired as a full stack developer, but ended up getting concerned with how fast AI has evolved.\n\nI got into machine learning and it honestly doesn't take much. Within a year of studying you can understand PyTorch, CNN, feed forward networks, etc. All of the basics that will allow for ML to be a tool in your CS toolbox. \n\nYou are right that web development will have less scope in the future. Web development is one of the easiest CS fields, and it will be the first to be replaced by AI.",
        "Choosing between a career in Machine Learning (ML) and DevOps depends on your interests, skills, and career goals. Here’s a comparison to help you make an informed decision:\n\n**Machine Learning**\n\n**Pros:**  \n**Growing Demand:** There is a high demand for ML professionals across various industries, including tech, finance, healthcare, and more.  \n**Innovation:** ML is at the forefront of technological innovation, enabling advancements in AI, data analytics, and automation.  \n**Creative Problem-Solving**: Involves designing algorithms and models to solve complex problems, which can be intellectually rewarding.\n\n**Cons:**  \n**Steep Learning Curve:** Requires strong mathematical and statistical knowledge, along with programming skills (Python, R, etc.).  \n**Continuous Learning**: The field evolves rapidly, necessitating ongoing education and skill development.\n\n**DevOps**\n\n**Pros:**  \n**Essential Role**: DevOps is crucial for improving collaboration between development and operations teams, enhancing productivity and efficiency.  \n**Versatile Skills**: Involves a mix of software development, system administration, and automation, making it a well-rounded skill set.  \n**Job Stability**: As companies adopt DevOps practices, the demand for skilled professionals remains strong.\n\n**Cons:**  \n**Broad Focus**: While versatility is an advantage, it can also mean that you need to be knowledgeable in many areas, which can be challenging.  \n**Potential for Burnout**: The fast-paced nature of DevOps can lead to high-stress situations, especially in production environments.\n\nhere are some of the resources to start learning ML:\n\n* Machine Learning Specialization\n* Machine Learning for all Supervised Machine Learning regression and classification\n* IBM Machine Learning introduction for everyone\n* Machine Learning A-Z - Udemy\n* Complete Machine Learning Bootcamp - Udemy are some of the [best machine learning courses for beginners](https://codingvidya.com/best-machine-learning-courses/)",
        "I think DevOps and cloud are a way better choice, to be fair 95% of \"data scientists\" don't have the tiniest bit of  a real software engineer 's insight, understanding the machine will be ever and forever more portant . Switching to ML is EASY because the maths behind it are not that difficult for 95% of use cases that's one and two 99% of the time you only train preprogrammed models you don't actually need to reason in maths like a researcher.\n\nI've been in this field for 8 years and I'm telling you, in the end ML engineer, data engineer, data scientists... Are all mediocre software engineers whose work can be done by a CS major and even more within two months.  Adata engineer is basically a back end software engineer. A ML engineer is. A software engineer who writes python code to train models (no need for maths in the majority of cases). A data scientist in 95% of companies is a data engineer actually.\n\n\nJust be an excellent software engineer, focus on understanding the lower level layers especially databases and you'll be better than 99% of people.",
        "> use AI to improve your skills and work smarter.\n\nMost people don't realize that AI is just a tool lol. Probably the same thing happened when a lot of accountants quit after Microsoft Excel was released.",
        "perfect analogy 😂",
        "😂😂😂😂😂😭💀",
        "But how many r's are in \"Moo\"?",
        "Backend will always be crowded lol. Even people in ML need to know about backend if they want to have any use in the real world outside of academia/pure research.",
        "Thank you ChatGPT",
        ">Switching to ML is EASY because the maths behind it are not that difficult for 95% of use cases that's one and two 99% of the time you only train preprogrammed models you don't actually need to reason in maths like a researcher.\n\nTrue for research roles, not true for everything else. Good luck preparing a proper dataset for \"just training\" only to realize in a few months that your data is useless due to some unfixable bias and you have to redo everything. Or when you improperly test a model, deploy to prod, and then get all predictions wrong. Or a million other potential mistakes you can make.\n\nYou're overfocusing on the modeling part (where SWE skills indeed help a lot). DS/ML is often more about data, not models",
        "This is BS, software engineers don't have the statistics skills required for proper data scientist positions. There's a reason why most data scientists have a masters+ in a quantitative field.",
        ">A software engineer who writes python code to train models (no need for maths in the majority of cases).\n\nGood Luck training models without any knowledge about potential risks regarding data inputs. Regarding Deep Learning, you can also make many mistakes by defining the architecture without getting any errors. The modeling part is not even the  \n\n>A data scientist in 95% of companies is a data engineer actually.\n\nI worked at a company that developed AI solutions only. We had data engineers and data scientists. Of course, you have some overlapping skill sets (DS familiar with DE and the opposite). However, we never staffed a DE to a DS position or vice versa. There is a reason for it.\n\n> A data scientist in 95% of companies is a data engineer actually.\n\nMaybe 95% of data scientist roles are not proper data scientist roles...",
        "There are 3 r's in \"Moo\"",
        "Thank you for encouragement",
        "No problem 👍 Just make sure that you're good at what you do. \"Backend\" is an extremely broad field that's filled with different roles and domains."
    ]
},
{
    "submission_id": "1genkjy",
    "title": "Is the \"bigger picture\" a lower or higher dimensional space?",
    "selftext": "This is something I've been thinking about often and I can't seem to find a straight-forward answer on. In colloquial language, people will often refer to seeing the \"bigger picture\" of something. Usually, this means a more zoomed-out perspective that encompasses more variables, but also gets at the \"essence\" of what is motivating some particular system.\n\nMy question is this: does a \"bigger picture\", translated to technical terms, refer to *lower* or *higher* dimensional space?\n\nI would argue that it translate to a lower dimensional space, since the \"bigger picture\" is the latent factors that motivate a system as a whole (e.g. you want to understand the meta factors that drive changes in the prices of many individual stocks). However, I could see why considering the \"bigger picture\" to be higher dimensional space is valid in other contexts (e.g. you are fixated on a single stock price, but not considering how it is changing relative to other stock prices).\n\nSo, if you had to pick one answer for what the \"bigger picture\" in colloquially language, is it typically referring to lower or higher dimensionality?",
    "created_utc": "2024-10-28T22:43:23",
    "num_comments": 11,
    "comments": [
        "I agree, I would say the figurative “bigger picture” is looking at something through a lower dimensional lens. \n\nThe “smaller picture” includes the shades of grey. Microdifferences. Personal feelings and motivations. \n\nYou can relate it to picture quality. A high definition image has so much more going into its description of an image (small picture), while the same picture that is highly pixelated delivers the same story with far less. (Bigger/ more meta picture)",
        "Imo lower dimensional. A removal of detail in order to see an overall trend. That’s why analysis techniques reduce the dimensions like PCA.",
        "I'd say a lower dimensional factorization or compressed representation of true the higher dimensional space.",
        "If you're looking at alot of details you are looking at looking in high dimensional space, but if you zoom out and look at the \"bigger picture\" that's low dimensional. \n\nAt least that's the way I'd think of it",
        "IMO it translates to higher dimensional space b/c, to me, “bigger picture” means you’re taking additional factors and their interactions into consideration with respect to your initial focus area, so now you need higher dimensionality to represent the same problem.\n\nOr put another way, you’re viewing your initial focus area in the context of the greater system that it exists within. You may need to “take a step back” to gain this perspective, but that doesn’t necessarily mean that you lose sight of the finer details in doing so. \n\nJust my 2 cents.",
        "I'd say that if the true underlying pattern/manifold that you're trying to learn for your task and desired performance is truly capturable in an embedding of low-dimensional data (of course, you'd have no way of knowing this without thorough analysis and experimentation), then the lower dimensional space would be the big picture since less parameters takes fewer arithmetic operations to compute. \n\nIn reality, most of the interesting manifolds we aim to learn are either not representable in a lower dimensional space or it would be infeasible for data scientists to find such a lower dimensional representation of the data to learn such a manifold from. In either of these cases, it ends up being more feasible to go with a much higher dimensional representation of the data and throw a shit ton of parameters and compute at the task (e.g. see modern state of the art transformer-driven LLMs).",
        "The bigger picture is whatever dimensional space gives you the results you need. \n\nThere is no philosophical method of determining bigger picture that provides satisfaction in all cases. \n\nApproaching the question from a results based perspective gives you the bigger picture on bigger picture for most use cases. \n\nWhat's the difference between a forest and a large amount land with a bunch of trees on it? It depends on if you're logging, farming, preserving, hiding bodies, or starting fires. Same raw data, but the \"bigger picture\" depends entirely on the use case.",
        "I like the Kabbalah view of dimensionality which in my limited understanding seem to view reality as being like a diffraction lens, with the most subtle influences coming from the highest levels and unfolding into more complex and more dense shadows of the original influence as you go downward. \n\nthe universe seems to have lots of fractal echos, with us being described as being made in the image of god.  I know I mix a lot of metaphor into my soup, but the chakra system seems to denote this as well, and we intuitively structure organizations with hierarchical models of control at the top, so my money is there.  Unfolding as it goes down, the simple seeming reveals itself to be more complex than it appears.",
        "If you talk to e.g. ML researchers vs. system engineers vs. UX designers, there are massive gulfs among them in what they care about or are aware of. All may be essential to a project. That's how specialization should work, with (making the vector space analogy) different specialities covering \"orthogonal subspaces.\"\n\nThe \"bigger picture\" is not losing sight that everything in the full higher dimensional space needs to work for a successful project.",
        "It's the same dimensions. It's 2D. It's just zoomed out.",
        "Renormalization group say what?"
    ]
},
{
    "submission_id": "1gen5z7",
    "title": "Need help with learning ML and hopefully a career of it",
    "selftext": "Good day everyone. First off, I wanna apologize if this topic has been posted here already. \n\n  \nML has peaked my interest and I wanna build a career out of it. For now, I have a few questions.\n\n1. Where to start?  \n2. Can you recommend me a few resources? Paid and free.   \n3. I'm 26 and a Civil Engineering student. Is it too late?",
    "created_utc": "2024-10-28T22:15:31",
    "num_comments": 6,
    "comments": [
        "I recommend to start with Andrew NG courses, you can find them on coursera or DeepLearning.AI , to understand the basics and the fundamentals. \nAlso you can start with some basic projects you can find tutos on YouTube.",
        "Start with campusx 100 days of machine learning YouTube",
        "* Machine Learning Specialization - Andrew ng course\n* Machine Learning for all Supervised Machine Learning regression and classification\n* IBM Machine Learning with Python\n* IBM Machine Learning introduction for everyone\n* Machine Learning A-Z - Udemy\n* Complete Machine Learning Bootcamp - Udemy are some of the [best machine learning courses for beginners](https://codingvidya.com/best-machine-learning-courses/)",
        "Absolutely not too late. I studied physics, started working as a backend developer and only 5 years later started as a ML Engineer. If you know maths well and some coding you can start out as a junior no problems\n\nOh and a good intro is the ”o’reilly scikit learn” book. You can find d the 2018 version online as pdf",
        "It really depends on what you want to do with ML. Create models?\nMLops?\nData engineering side?\n\nML is such a broad term being used for jobs that it really depends on what exactly you are trying to do related to ML",
        "Hey, I think the easiest way is to start building ML models of your own. I started by using model generation through [https://www.plexe.ai/](https://www.plexe.ai/) and iterated on the output and used it in Kaggle competitions"
    ]
},
{
    "submission_id": "1gemhx8",
    "title": "Help with Bird Call Classification: Data Augmentation & Model Consistency Issues",
    "selftext": "Hey all, I'm working on a bird call classification project and could use some advice on a few challenges I’m facing.\n\nI’ve got 41 bird species classes, but the dataset is pretty imbalanced. Some species have over 400 audio samples, while others have fewer than 50. Here’s what I did to balance things out:\n\n1. **Audio Splitting:** All audio files are split into 10-second segments. Clips shorter than 10 but longer than 5 seconds are padded with silence to make them 10 seconds.\n2. **Augmentation:** For classes with fewer than 500 samples, I used time-stretching, phase-shifting, and Gaussian noise to boost the sample count up to 500.\n\nIs it a good idea to augment from as few as 50 samples up to 500? Could that harm the model's generalization?\n\nAlso, I’ve converted these audio files to mel spectrograms for training. The model performs really well with these, but oddly, when I pass raw audio from the training set (processed with the same steps), it gives incorrect results. Any insights into why this inconsistency might be happening?\n\nThanks !",
    "created_utc": "2024-10-28T21:32:19",
    "num_comments": 8,
    "comments": [
        "In reply to your augmentation methods, I think those sound fine, same for 50 to 500 samples being fine. If you want, you can also consider random combinations (additive) if samples.\n\nFor the audio conversion, I'm not familiar with mel files, and I'm not entirely sure I understand that last paragraph. If I understand correctly, you processed the entire dataset (including training and val/test) into mel. Then, after training on mel, the model performs well on the test data. But when you feed the model a mel sample from the training data that it was trained on, it performs considerably worse? If my interpretation is correct, I would not consider that an issue since it seems to be performing well on the end goal task.",
        "Hey, I think the easiest way is to start building ML models of your own. I started by using model generation through [https://www.plexe.ai/](https://www.plexe.ai/) and iterated on the output and used it",
        "UPDATE:  \nApparently, I used both preprocess\\_input from efficientNet and normalization to preprocess the mel spectrogram, which diluted the features from the images, leading to poor performance during testing.   \nTurns out, the preprocess\\_input function handles normalization on its own.  \nThanks everyone!",
        "Thank you for the quick response.\n\nI’m currently using mel spectrogram images—visual representations of an audio signal's frequency content over time—to train an EfficientNet model.\n\nThe model performs very well with these mel spectrogram images. However, when I try testing it with raw audio files (even though I apply the same preprocessing steps to convert these audio files into mel spectrograms as I did with the training data), the model produces incorrect results. In short, despite identical mel spectrogram conversion for both training and test data, the model struggles with accuracy when using raw audio files.",
        "Sorry, cant do. we are planning to publish this project as research in a journal thus I need to jot down model architecture and all.  \nThanks for the heads up tho.  \nAppreciate it.",
        "Ah, so a standard case of good training performance and poorer val/test performance? Have you tried the standard go-tos for this pattern? B/w images, k-fold cross validation for hyperparameter tuning, early stopping, dropout, or if the previous all fail (but still give great training performance), flat out dropping layers/parameters from the network ?",
        "Current architecture (which is basically efficientNet as base model coupled with an output layer) is performing really well with test set as well. I reckon the problem is a classic case of overfitting and perhaps adding Dropout layer and regularizations might help.  \nI will be giving that a try.  \nThanks a bunch!",
        "Not going to lie im now very confused what your question was/is then if it sounds like both training and test do very well, but sounds like you have a good next direction. Good luck\n\nTbh i think i may be a bit ambiguous on what you mean on test set. When you say you're taking raw audio files and converting it to mel, to me that means test. :shrug:"
    ]
},
{
    "submission_id": "1gem51q",
    "title": "What are AI Agents in Generative AI?",
    "selftext": "Right now, a lot of buzz is around AI Agents where recently Claude 3.5 Sonnet was said to be trained on agentic flows. This video explains What are Agents, how are they different from LLMs, how Agents access tools and execute tasks and potential threats : https://youtu.be/LzAKjKe6Dp0?si=dPVJSenGJwO8M9W6",
    "created_utc": "2024-10-28T21:10:34",
    "num_comments": 5,
    "comments": [
        "\"Agent\" is a buzzword that means different things to different companies' marketing departments.\n\n> how are they different from LLMs\n\nUh, one of those is a ML model, and the other isn't.\n\n>  execute tasks and potential threats\n\nWhoa - they can execute potential threats  --- like some terminator-assassin --- maybe this youtube channel's more fun than I expected!",
        "Great video! AI agents are fascinating, especially how they can access tools and execute tasks beyond just language processing. As someone who's always looking to boost productivity, I'm intrigued by the potential of AI agents to automate complex workflows. Have you come across any practical applications of AI agents for entrepreneurs or small businesses? I'd love to explore how they could streamline operations or enhance customer interactions.",
        "Llms can be agents. And they are more specifically DL."
    ]
},
{
    "submission_id": "1geleu3",
    "title": "Machine Learning Integration with Knowledge",
    "selftext": "",
    "created_utc": "2024-10-28T20:29:42",
    "num_comments": 0,
    "comments": []
},
{
    "submission_id": "1gekila",
    "title": "Image segmentation leveraging object graph",
    "selftext": "I’m using image segmentation (e.g. YOLO V7) to segment parts of objects, like vehicles or aircraft (could be almost any object comprised of identifiable parts). In such cases the object could be represented by a graph. Parts that are never adjacent are not directly connected by an edge. Some parts may always be adjacent to others (e.g. head is always adjacent to neck if both are visible). It seems that incorporating this information could help in some cases where otherwise the shape might be ambiguous (often occurring in close up images). \n\nI don’t know how to incorporate this information into training the segmentation model. I’ve found pretty limited papers and no available implementation. \n\nI would be grateful for any information on how to accomplish this or resources and references to which I could refer. ",
    "created_utc": "2024-10-28T19:41:35",
    "num_comments": 0,
    "comments": []
},
{
    "submission_id": "1geikso",
    "title": "So for aspiring ML/DS enthusiasts, Masters, Certs, None?",
    "selftext": "I have about 5 years of experience as a data analyst/ business intelligence professional in financial services. I double majored in Finance and MIS. Took calculus 1-3, linear algebra and statistics in college. I also hold a CQF certification (basically tests you on stochastic calculus, linear algebra, differential equations, ML basics, probability & Statistics, python and quantitative finance).\n\nI'm looking to get into the ML field as I have found that most data science professionals that come from Computer Science or Maths background either just can't get the business or get bored and move on to more 'ITish' roles, so I think my business expertise may become handy when thinking of actual Model implementation. I personally find Al and MI interesting and the future but every time I come to this sub to see what path should one take I get the following: \"MS in Data Science is a waste\",\n\"certifications in data science (AWS MLS)\" are a waste.\nWhat does a person with solid quantitative background and technical skills and BUSINESS knowledge can actually do to break into the field? Not interested in research (I get that you would need a PhD for that) I'm thinking of roles such as ML Engineer or Al dev.\nI should also add that I have 5 years of Python programming, SQL and Cloud Computing experience (AWS and Azure)\nThanks in advance!",
    "created_utc": "2024-10-28T18:04:31",
    "num_comments": 4,
    "comments": [
        "I’m in a similar boat, currently looking at the OMSCS program",
        "I think moving to DS at finance company / bank will be easiest in your case due to your background. Being MLE might be harder due to lack of programming experience (from hm pov). Then internally transfer into MLE role. Another thing about MLE compared to analyst role is the type of interviewing. MLE may require rounds with leetcode style interviews in addition to understating stats and modeling. As well as already understating other programming related things like cloud and devops.",
        "I think none of the ones you listed.\n\nA substantial hobby project would be more impressive -- especially if it comes with a well organized github page.\n\nThat said, I kinda like fruitpunch.ai 's approach of giving certificates for \"AI-for-good\" \"projects\", where they team you with people who may know more, to try to solve a real-world problem.",
        "I finished the OMSCS program and I recommend it!"
    ]
},
{
    "submission_id": "1geg393",
    "title": "Learning path recommendation for Deep Learning + Computer Vision using a project.",
    "selftext": "Hey everyone! 👋\n\nI've been trying to learn deep learning for a while but following courses without a real goal wasn't really working for me. So I thought why not pick a project and use that to learn?\n\nI'm a software engineer (studied mechanical engineering) and I've got these construction diagrams \\[image attached\\] where I want to:\n\n\\- find all the symbols and their text from the legends\n\n\\- count how many times each symbol appears in the diagram\n\n\n\nBut here's why I think this needs deep learning + computer vision and not just CV:\n\n1. architects can use whatever symbols they want - there's no fixed set\n\n2. the same symbol might look slightly different in the diagram vs legend (human eye won't notice but computer would)\n\n3. these diagrams are all different - different symbols, different contexts\n\n\n\nMy thinking is to break it down like:\n\n1. model to spot the legends\n\n2. model to list out all legend items\n\n3. model to get the symbol and text for each item (with OCR)\n\n4. model to find the actual diagrams\n\n5. model to match symbols and count them (dealing with those tiny variations)\n\nI'm a complete beginner in DL/CV but been coding for years. Any tips on where to start? Which framework would be easier for someone coming from software engineering background?\n\n\n\nThanks!\n\nhttps://preview.redd.it/ncukijzxwkxd1.jpg?width=6622&format=pjpg&auto=webp&s=f7fe3d689ebce3e1a87763116a4eb7cad0720fec\n\nhttps://preview.redd.it/hbtwukzxwkxd1.jpg?width=9361&format=pjpg&auto=webp&s=3115c5f6d2a997d7aea380a8717c94a885031067\n\n",
    "created_utc": "2024-10-28T16:09:45",
    "num_comments": 0,
    "comments": []
},
{
    "submission_id": "1gecr87",
    "title": "I've created a video for beginners that explains where ML fits into the full AI knowledge stack. Would appreciate any feedback so I can make improvements or correct any errors.",
    "selftext": "",
    "created_utc": "2024-10-28T13:47:44",
    "num_comments": 0,
    "comments": []
},
{
    "submission_id": "1geb4xh",
    "title": "Enhancing Attention Mechanism to O(log N): A Tree-Based Approach for Optimizing Transformer Models",
    "selftext": "Hello community! \n\n**In today’s world, AI has become one of the most influential tools in our daily lives. However, current Transformer models such as BERT, GPT, and Llama still face unresolved challenges, with a key limitation being the time complexity of the attention mechanism.**\n\nI have reviewed common attention mechanisms used in state-of-the-art (SOTA) models, including self-attention, multihead attention, sparse attention, and factorized attention. Despite their advantages, these approaches have limitations in time complexity, typically ranging from `O(N^2)` and `O(N)` to, at best, `O(N log N)`, where `N` is the number of tokens in the input sequence. I am currently exploring and developing a next-generation attention mechanism that aims to reduce the time complexity to `O(log N)`, where `N` represents the number of sentences instead of individual tokens.\n\nBelow is a summary of the time complexities associated with each of these attention mechanisms:\n\n# Self-attention\n\nSelf-attention, widely used in models like GPT-4, GPT-4o, Gemini 1.5 Flash, Gemini, BERT, and its variations (e.g., mBERT, sentence-BERT), has a time complexity of `O(N^2)`, where `N` is the number of tokens. This quadratic scaling means that as sequence length increases, so does the computational cost, making self-attention less efficient for processing large documents. The computational demands rise due to matrix calculations (queries `Q`, keys `K`, values `V`) and operations like transposition (`K^T`) and softmax normalization, especially challenging when computational resources are limited.\n\n# Multihead Attention\n\nMultihead attention builds on the concept of using multiple attention heads. For example, models like GPT-4o have up to 96 attention heads in their largest variants. While this improves performance, it also increases model size, making it impractical for low-resource devices (e.g., smartphones) without quantization. Converting model weights from `float32`to `int8` can reduce model size but often comes at the expense of accuracy. This method is optimal for high-power devices like PCs and laptops.\n\n# Sparse Attention by Google\n\nGoogle’s sparse attention mechanism combines local and global contexts using sparse matrices, achieving a best-case time complexity of `O(N log N)`. While this is more efficient, it still does not reach `O(log N)` complexity.\n\n# Suggested Improvements to Current Approaches\n\nAchieving `O(log N)` time complexity requires a novel approach that leverages algorithms and data structures with logarithmic time complexity.\n\n1. **Choice of Data Structure**: To manage context vectors and embeddings, sentence-level embeddings are more appropriate than token-level embeddings, as the latter do not lend themselves to `O(log N)` complexity. Tree-based structures like `AVL Tree`, `Red-Black Tree`, or `B-Tree` are promising options, given their logarithmic complexities for insertion, search, and deletion, where `N` represents the number of nodes in the tree.\n2. **Attention Mechanism Based on Trees**: Each tree node can store a sentence embedding and its context vector (or context value). Using binary search, we can identify sentences crucial for specific tasks (e.g., detecting misinformation) and pass only those selected sentences to subsequent processing layers. Looping through entire text sequences is inefficient (`O(N)` complexity), so focusing on key sentences is critical.\n3. **Efficient Selection of Crucial Sentences**: Instead of traditional algorithms like `TF-IDF`, which operates with `O(N log N)` complexity, we need an alternative that maintains the desired `O(log N)` complexity. Selecting and analyzing only a limited number of crucial sentences helps maintain performance while ensuring a focus on relevant information.\n\n# Overall Proposal Summary\n\n1. **Select a data structure** (e.g., `AVL Tree`, `Red-Black Tree`, `B-Tree`).\n2. **Combine algorithms** with `O(log N)` complexity, such as binary search and tree operations (insertion, deletion, search).\n3. **Use a sentence-selection algorithm** to identify regions of interest (ROI) efficiently.\n\n**I would love to hear your thoughts and suggestions on this approach! If you have experience or insights that could aid in implementing and refining this algorithm, please share them. I’ll gladly incorporate any valuable recommendations into the upcoming article on the algorithm.**",
    "created_utc": "2024-10-28T12:40:55",
    "num_comments": 0,
    "comments": []
},
{
    "submission_id": "1ge9ws1",
    "title": "Enough images are enough to train a classifier (vs transfer learning)?",
    "selftext": "If I have 10 thousand images, is this enough to train a classifier for 4 classes? Assuming the images have enough images from each class.\n\nOr is it better to use a pre trained network like vgg-16 and apply transfer learning?",
    "created_utc": "2024-10-28T11:50:53",
    "num_comments": 1,
    "comments": [
        "As always, it depends. Are your images of a similar data distribution to ImageNet? As in, vaguely natural looking photos? Or, on the opposite end of the spectrum, are they black and white images that show the state of a massive board of the game of life? If your images look vaguely like natural images, you are better off using VGG, if not you could definitely train a classifier from scratch on 10k images, it may just be slower."
    ]
},
{
    "submission_id": "1ge9924",
    "title": "We just Open Sourced Promptwright: Generate large synthetic datasets using a local LLMGeneration ",
    "selftext": "Hey Folks! 👋\n\nWe needed a means to generate large synthetic datasets using a local LLM, and not OpenAI or a paid cloud service. So we built [Promptwright](https://github.com/StacklokLabs/promptwright) - a Python library that lets you generate synthetic datasets using local models via Ollama.\n\nWhy we built it:\n\n* We were using OpenAI's API for dataset generation, but the costs were getting expensive for large-scale experiments. \n* We looked at existing solutions like pluto, but they were only capable of running on OpenAI. This project started as a fork of \\[pluto\\](https://github.com/redotvideo/pluto), but we soon started to extend and change it so much, it was practically new - still kudos to the redotvideo folks for the idea.\n* We wanted something that could run entirely locally and would means no concerns about leaking private information.\n* We wanted the flexibility of using any model we needed to.\n\nWhat it does:\n\n* Runs entirely on your local machine using Ollama (works great with llama2, mistral, etc.)\n* Super simple Python interface for dataset generation\n* Configurable instructions and system prompts\n* Outputs clean JSONL format that's ready for training\n* Direct integration with Hugging Face Hub for sharing datasets\n\nWe've been using it internally for a few projects, and it's been working great. You can process thousands of samples without worrying about API costs or rate limits. Plus, since everything runs locally, you don't have to worry about sensitive data leaving your environment.\n\nThe code is Apache 2 licensed, and we'd love to get feedback from the community. If you're doing any kind of synthetic data generation for ML, give it a try and let us know what you think!\n\nLinks:\n\nGitHub: [StacklokLabs/promptwright](https://github.com/StacklokLabs/promptwright)\n\n\nCheckout the `examples/*` folder , for examples for generating code, scientific or creative ewr\n\nWould love to hear your thoughts and suggestions, if you see any room for improvement please feel free to raise and issue or make a pull request.",
    "created_utc": "2024-10-28T11:24:10",
    "num_comments": 3,
    "comments": [
        "Hi! I need a dataset of banking statements (images) in diverse layouts. Desperate to find any I’m starting to think of creating synthetic dataset. Can the library be helpful here?",
        "I am sure it could if you can get them into textual format, maybe try what DigThatData recommends.",
        "use pandoc to convert generated markdown/latex to PDF and you should be able to automate rendering images from that, or otherwise can automate taking screenshots from a PDF reader."
    ]
},
{
    "submission_id": "1ge8xqx",
    "title": "How to improve a retrieval system ",
    "selftext": "Hi everyone, \n\nI have a Kaggle competition where the goal is to build a retrieval system for multiple languages. \n\nFor 7 different languages, we have a corpus of documents and a set a query and for each query we have to compute the 10 most relevant document. \n\nI currently use BM25 and get a score of 0.76 on the test set. But the best score actually achieved is around 0.81 \n\nMy question is: what can I add to BM25  to improve my score? \n\nI wonder if it’s even possible to obtain this kind of score using only sparse retrieval techniques like BM25 and/or TFIDF. Should I try to use DL model? It’s really slow since the corpus is quite big (268k documents) and documents are also really long but I can’t run for hours \n\nThanks for your help :)",
    "created_utc": "2024-10-28T11:11:38",
    "num_comments": 0,
    "comments": []
},
{
    "submission_id": "1ge742h",
    "title": "Stuggling to train/find a model for my project.",
    "selftext": "Hi everyone,\n\nI’m a college student working on an IT project where I’m trying to build a model that can detect and read store signages using the Google Street View API. My approach so far has been to train a custom model with YOLOv8 for detection and EasyOCR for text recognition. But since I’m new to AI/ML, I’ve been learning everything from scratch, and it’s been pretty overwhelming.\n\nAfter training with about 5 custom datasets, my model is only about 10% accurate in detecting signs—nowhere near where I need it to be. With the deadline approaching, I’m realizing I might need a different approach.\n\nI’d love to hear if anyone knows of pre-trained models that could be useful for detecting and reading store signage or if there are resources I should check out. I know some of the model's limitations are likely due to my inexperience, so any advice or guidance would be greatly appreciated!",
    "created_utc": "2024-10-28T09:57:41",
    "num_comments": 4,
    "comments": [
        "I would expect YOLOv8 to do well on something like this. Unfortunate that your finetuning isn't working as you were hoping.\n\nMaybe try something like PaddleOCR and use some heuristics to filter out signs from non-signs (e.g., storefront signs are usually at a certain elevation around the 1-story mark on a building). It certainly won't be perfect but probably much better than 10%",
        "Question: What’s your data like? Have you verified if it’s properly labeled?",
        "Hey, I think the easiest way is to start building ML models of your own. I started by using model generation through [https://www.plexe.ai/](https://www.plexe.ai/) and iterated on the output and used it in Kaggle competitions",
        "**10% accuracy of the model can be potentially associated with the following issues:**\n\n1. **Low Quality of Data**:\n   * If you're using custom datasets and benchmarks while training, it's crucial to ensure your data quality, especially that signages are correctly labeled. Which tool did you use to create your custom datasets? Incorrect labeling often negatively impacts overall model performance. If you’re currently using YOLOv8, consider looking at newer versions like **YOLOv11**, which was recently released. This version offers enhanced feature extraction, which could address the issues you’re facing. [More about YOLOv11](https://docs.ultralytics.com/models/yolo11/#key-features).\n   * For creating custom datasets, I recommend [**CVAT.ai**](http://CVAT.ai) (Computer Vision Annotation Tool) since it automatically converts labels for various model architectures, including YOLO.\n   * For additional data, consider using established datasets like the [COCO-Text Dataset](https://www.kaggle.com/datasets/c7934597/cocotext-v20) or the [Street View Text Dataset](https://www.kaggle.com/datasets/nageshsingh/the-street-view-text-dataset). These datasets include diverse examples of text in real-world settings, which can be helpful for improving model robustness.\n2. **Overfitting/Underfitting**:\n   * Overfitting occurs when the model performs well on the training set but poorly on the test set, resulting in many False Positives or False Negatives. A common way to address this is **early stopping**, which halts training once model performance plateaus on the validation set, preventing overtraining.\n   * For more details on early stopping and other techniques, such as L1/L2 regularization, data augmentation, dropout, and cross-validation, refer to this helpful article: [8 Simple Techniques to Prevent Overfitting](https://towardsdatascience.com/8-simple-techniques-to-prevent-overfitting-4d443da2ef7d). Since you're using a pretrained YOLO model, it may help to focus on **data augmentation** and **early stopping**.\n3. **Hyperparameters**:\n   * Tuning hyperparameters is critical for training. Key parameters include the number of epochs, batch size, metrics like accuracy, precision, recall, f1-score, Intersection Over Union (IOU), the optimizer (Adam, RMSProp, SGD), and the loss function (e.g., Binary CrossEntropy, Sparse Categorical CrossEntropy, MSE). The learning rate can also have a significant impact on convergence. Experimenting with these settings on your specific dataset is essential, as there's no one-size-fits-all solution.\n\nThese three steps can substantially impact model performance and accuracy, so try implementing them for improved accuracy."
    ]
},
{
    "submission_id": "1ge6zrr",
    "title": "Recommended PC Specs for generalized ML/DL",
    "selftext": "Hello all, I'm am getting into ML and DL and plan on buying a new PC soon to run some larger models (and have faster training), as mine is currently a potato. Would some of the more exprienced ML wizards on here give me your feedback on what kind of specs I should be looking for specifically in a PC? What should I prioritize? \n\nI understand that large amounts of RAM allow for larger models (more parameters) and better graphics cards translate into higher training speed, but that's about all of the info I have.\n\n  \nThanks a lot!\n\n",
    "created_utc": "2024-10-28T09:52:49",
    "num_comments": 14,
    "comments": [
        "If you are starting I recommend you to use free services as kaggle notebooks, I've been using it in the last year. Obviously it has some limitations and I would love to have a 4090 but I don't have the money so still is a great upgrade for my personal use (grad project)",
        "As a baseline, you want at least 32gb of RAM, at least 1 M.2 PCIE slot with a >1TB drive to match, and a modern upper-mid range CPU (I7, 9th gen + sounds good. IDK amd but its entirely viable for CPU). \n\nThe actual most important spec is to buy as much VRAM as possible on an NVidia GPU.These get expensive, so you should try to buy as much VRAM as cheaply as possible. You shouldn't worry between 3000 series 12 GB, vs 4000 series 12gb cards. I would suggest 8GB may be the absolute minimum you should use, 12GB recommended min, but if you're planning on getting into high resolution images, video, or LLMs, then even 16gb would be good to get started. New 5000 series cards are coming out, they are expected to have more VRAM than any card available atm. If you're aiming for the cheaper end of the spectrum, keep it to at least the 2000 series, or more recent.  \n\nLaptops are a no-go, desktops are fine, but you should compare to the cost of training on the cloud. There are many resources which will give you X free hours of compute, so those can be very useful when you're getting started.",
        "Here's something else that will be a good reference for you. AI server companies also cater to enthusiasts like yourself with higher-end desktop PCs for local AI development. For example, Gigabyte has something called the AI TOP, it looks like a high-end gaming PC but can handle LLM of up to 400-ish parameters www.gigabyte.com/WebPage/1079?lan=en Obviously you could just buy the whole pre-built machine from them if you've got the scratch, but even if you didn't you could look at which components they're using for the build (scroll down a bit on the page) and then base your own build off of that.",
        "Yeah I've considered cloud-based options but locally run models are what I want to specialize in. Also, I've just started working as an engineer so having a powerful machine for the next few years is an investment I want to make (and can now yay) for multiple other types of projects.",
        "Thanks a lot for all the info! I was planning on purchasing around Black Friday to get the most bang for my buck, so are you saving I should wait for the 5000 series to come out before purchasing? Or would a nice 4070 be enough for most applications?\n\nI would like to ideally get into live image processing at some point so yeah what you're saying does apply to my use case.",
        "Are titan V’s still usable? Been thinking about snagging one to use in an EGPU",
        "This is not good advice for someone interested in \"getting into ML and DL.\" You don't even know if you're going to end up liking the field, but you're prepared to drop a few thousand dollars on hardware that will be out of date in a few years? Worse yet, building and using some monster local PC will not prepare you for ML in a professional environment, where everything is trained and deployed in the cloud on metered compute.\n\nTo OP, have you tried training models with your current machine and actually hit hardware limitations? What about google colab? Kaggle notebooks?",
        "Hell yeah this is great! Thanks a lot, it's a good benchmark",
        "So go for a GPU with 24gb VRAM if you can, prioritize the VRAM, not the RAM",
        "It kind of depends on your personal use and upgrade plans. It should be fine to buy a 12gb card to get started, but you will want to get more. Unfortunately its kind of hard to know what the 5000 pricing and availability will look like, so its hard to say if it'll be smart to wait or not.",
        "Honestly, I'm not sure about the Titan V specifically, or using an EGPU, in terms of performance. The Titan V will work, but there is one important caveat with it comes to comparing VRAM with cards that old. Long story short, 2000 series cards and later are optimized for fp16. This basically means that for specific tasks like image and video, RTX cards are worth approximately double the VRAM of 1000 series cards, and earlier. I think, but am not sure, that the Titan V does not have this optimization.\n\ntldr; Titan V seems to have 12 GB, and a 3000 series 12GB would almost certainly be better. And old/free/cheap titan V will work though. But you know, beware used GPUs and all that.",
        "Hey, thanks for your answer! I was planning on getting a good *laptop* for a while now, so wanted to spend quite a bit on a highly performing PC anyway. As you say, I am a beginner in ML but I am an engineer and would need high specs for many types of projects anyway. Since I'm really taking a liking to ML and am curious about locally run models in general (embarked systems are my jam) I wanted to know the basic needs for not having to upgrade my PC every year. \nSorry for the lack of details in the post!\n\nEdit: typo",
        "Ok noted!",
        "Yeah I'm exploring ML on my personal time and won't be working on enormous projects either, I just don't want to be limited as little as possible by my PC as I'm going forward, to not have to upgrade in a year. Thanks a lot for the suggestions, it's a big help!"
    ]
},
{
    "submission_id": "1ge6fnb",
    "title": "Learn necessary details or upskill ?",
    "selftext": "I am final year B E graduate student, I am currently doing internship.  Most of my internships in my past experience gives us project which required lesser knowledge of pandas. Not too complex.\n\nShould I learn from this playlist to upskill to stand out in competition.\n\n\nLink: https://youtube.com/playlist?list=PLXovS_5EZGh6CpyyB4m7dQDlcocsqIseK&si=2H2Qyfrj8s84MHJX",
    "created_utc": "2024-10-28T09:29:59",
    "num_comments": 4,
    "comments": [
        "No offense, but this will not help you standout in a competitive job market. You’re assumed to know this if you’re applying to data science or machine learning positions.",
        "Hey, I think the easiest way is to start building ML models of your own. I started by using model generation through [https://www.plexe.ai/](https://www.plexe.ai/) and iterated on the output and used it in Kaggle competitions",
        "Thanks for the reply. I understand it now",
        "Thanks"
    ]
},
{
    "submission_id": "1ge4q41",
    "title": "New to recommendation engines, looking for advice",
    "selftext": "Hi everyone,\n\nI've been working as a data scientist for a few years, but l've never worked on a recommendation project.\n\nWe sell vehicles, and for the past year we have focused on creating a master data table which contains a large number of features on each individual in our database (excluding demographic data, so we have e.g., how much they have spent, how many vehicle they own, the models, how long they own them for, how often they bring them in for servicing, how responsive they are with emails), and we have used this to create a propensity-to-purchase model to be able to better target customers for new vehicle releases.\n\nWe are now looking at improving the used vehicles selling system, and want to be able to recommend vehicles that come in to existing owners. The idea is that we will have the vehicle model, colour, transmission etc., and we generate a list of 10 customers who we feel would be interested in the vehicle based on their purchase history.\n\nAfter doing some reading, would I be right in saying that this is a hybrid recommendation engine approach, where both item features and user history is used?\n\nThank you in advance for any responses!",
    "created_utc": "2024-10-28T08:22:07",
    "num_comments": 0,
    "comments": []
},
{
    "submission_id": "1ge3kkg",
    "title": "Help me with pytorch scripts",
    "selftext": "hello everyone just got over with the going modular part of learnpytorch.io where u have to write code in scripts instead of notebook.\nso do u have any other resources or blogs or tutorials that i can follow to get better\ni will be making a project all by myself using scripts ",
    "created_utc": "2024-10-28T07:34:51",
    "num_comments": 3,
    "comments": [
        "Hi, recently I did a project where I implemented a very simple classification model with a very simple dataset (only a few data points). I implemented it first in numpy and then in PyTorch. \n\nI documented it all on YouTube. You can look into it and follow along if you like to. \n\nhttps://youtu.be/EB4pqThgats?si=7vr_v5REusvxWXlk\n\nI am pretty sure that you will be able to implement more complicated skripts after you implemented and understood this one. \n\nIf you have any questions just hit me up via dm\n\nCheers",
        "thanks man 😊",
        "No worries ☺️"
    ]
},
{
    "submission_id": "1ge34im",
    "title": "How to Structure ML Projects for Production?",
    "selftext": "",
    "created_utc": "2024-10-28T07:15:47",
    "num_comments": 0,
    "comments": []
},
{
    "submission_id": "1ge30hb",
    "title": "How Self-Improving Autonomous AI is Transforming R&D: Are We Ready for the Future?",
    "selftext": "🚀 The future of research and development is here, and it's powered by self-improving autonomous AI! From revolutionizing material science to speeding up drug discovery and crafting innovative climate solutions, these intelligent algorithms are reshaping the way we approach complex challenges.\n\nBut with great power comes great responsibility. As we harness this technology, we face ethical dilemmas, technical hurdles, and the need for robust regulations. Can we balance innovation with safety?\n\nJoin the conversation! 🤔 What do you think about the impact of autonomous AI in science? Are we prepared for the ethical implications? Share your thoughts below! \n\n[https://techrevolutiondaily.com/article/self-improving-autonomous-ai-revolutionizing-randd-in-material-science-drug-discovery-and-climate-solutions](https://techrevolutiondaily.com/article/self-improving-autonomous-ai-revolutionizing-randd-in-material-science-drug-discovery-and-climate-solutions)",
    "created_utc": "2024-10-28T07:11:03",
    "num_comments": 0,
    "comments": []
},
{
    "submission_id": "1ge1bxb",
    "title": "I need help.i am new to ml (semister 1)our college gave us a project.",
    "selftext": "Please see this image which includes my implement \n",
    "created_utc": "2024-10-28T05:57:01",
    "num_comments": 9,
    "comments": [
        "There are various resources on Kaggle that can guide you to solve this problem. Check this out: https://www.kaggle.com/datasets/krishujeniya/heart-diseae",
        "What’s your question exactly?",
        "Woah this is weirdly nostalgic, I did a RF classifier for heart disease for my undergrad dissertation.",
        "Can I do this project for my practice ?",
        "I need tips.how to proceed on this project.\nThe jpg are my implemention wanted to do in project",
        "The jpg contains steps.\nAre procedures correct?",
        "Yes you do .it's all basic",
        "Find the data you need on kaggle. Understand the data (format), preprocess the data(you got detailed steps in your paper already) , label the data (if needed) . Implement different machine learning algorithms for comparison.  Feel free to do your own research and learn how to make your project better.",
        "Yh it looks great! V similar to what I remember doing.\n\nWhen I did my VIVA I got told that the professors would have liked to see some more \"creativity\" with designing my model.\n\nAt the time I determined a 3 layer CNN was enough because my old laptop couldn't handle more. I said as much in my paper but didn't cut it. \n\nThis looks like a solid framework though, ticks all the boxes"
    ]
},
{
    "submission_id": "1gdy7xi",
    "title": "Linear Algebra for Machine Learning",
    "selftext": "Linear Algebra is one of the foundational pillars of ML. But how exactly?\n\n \n\nhttps://preview.redd.it/hharse6wygxd1.jpg?width=800&format=pjpg&auto=webp&s=812f02f3a77be4ef27571fbafbe193e40e7994a6\n\nWhen you multiply a vector (read list of features) by a matrix (read weight matrix), you are simply performing linear transformations. Sometimes this transformation takes the vector from a lower dimensional space to a higher dimension or vice versa.\n\n \n\nConsider the following cases.\n\n \n\n1. You may have heard of or studied span and basis and wondered where exactly is this useful.\n\n2. You might have found linear algebra very difficult in the beginning. I certainly did.\n\n3. You wish to learn ML from its mathematical foundations but never got a chance to do so. \n\n4. You know how to run Google Colab notebooks. However, you are not confident about your ML skills because you lack foundational knowledge.\n\n \n\nIf you have experienced any of the above here is a resource for you. For the past 4 months, I have been working on a course to lay the foundations for ML. I have released this course on Vizuara's YouTube channel titled \"Foundations for Machine Learning.\" This will be a 45-hour course with \\~65 lectures.\n\n \n\nHere is the first lecture that introduces Linear Algebra with an emphasis on its utility for ML. Check this out. This is the starting lecture. I am sure you are going to enjoy: [https://www.youtube.com/watch?si=sNBRs1x5PX200Jgo&v=llV4uj58wuA&feature=youtu.be](https://www.youtube.com/watch?si=sNBRs1x5PX200Jgo&v=llV4uj58wuA&feature=youtu.be)\n\n \n\nThere are no prerequisites. If you have basic logical thinking capability and a willingness to dedicate time, consistently, you can follow this course. I have tried to simplify the course content as much as possible by trying to give you a logical and geometric intuition rather than only mathematical steps.",
    "created_utc": "2024-10-28T02:53:18",
    "num_comments": 0,
    "comments": []
},
{
    "submission_id": "1gdx7rp",
    "title": "Can I get straight into machine learning without learning mathematics? ",
    "selftext": "",
    "created_utc": "2024-10-28T01:35:55",
    "num_comments": 37,
    "comments": [
        "No lol.",
        "Can you? Yes.\n\nIs it dumb? Yes.",
        "Lotta gatekeepers here. Yes you can, and you can also be competent and influential. But it certainly narrows your path to success (read: there will be roles that you'll never get) and you'll likely be pushed into less technical roles as you progress.",
        "Pytorch and tensorflow do not need math.\nUnderstanding how to create a nn does not require math.\nUnderstanding where to use which classification technique doesn't require math.\n\nMl is more software engineering than math.\n\nGood luck in your adventures",
        "As a highschooler forced to solve 20000+ calculus problems i still can not understand how it is used in artificial intelligence engineering?\nWhere can i learn how to apply what i know to this field?",
        "Let me translate for you:  \n\"Can I get straight into advanced mathematical concepts with complex computations without learning bathic mathematical concepts first\".  \nmachine learning is 90% math unless of course you just want to deploy already existing models and play around which is totally fine if that is what you wanna do. I however would recommend you at least have some knowledge. Always hard to say what people consider \"learning mathematics\" here. If it means you dont know yet how a matrix is multiplied then yes please learn some math first. Could as well mean you have good foundation but dont know stuff like idk taylor series so yeah. Hard to say much with no information.",
        "**You won't get far without the math, but you can brush up on it as you go**. Calculus and linear algebra are areas that you will come across again and again in machine learning and in deep learning. I'd suggest learning machine learning/neural networks before tackling deep learning, regardless of your math background.\n\n* Mathematics for Machine Learning\n* Pattern Recognition and Machine Learning\n* The Mathematics of Machine Learning: Lectures on Supervised Methods and Beyond\n* Essential Math for Data Science: Take Control of Your Data with Fundamental Linear Algebra, Probability, and Statistics\n* Before Machine Learning Volume 1 - Linear Algebra for A.I: The fundamental mathematics for Data Science and Artificial Inteligence. Here are some of the other [Best Machine Learning Mathematics books](https://codingvidya.com/machine-learning-mathematics-books/)",
        "you can.. you just call the ml models api and apply the models and call yourself ml engineer. like 90% engineers  in India does this",
        "While it's technically possible to start with machine learning without a strong foundation in mathematics, it's generally not recommended.   \n  \nHere are some key points to consider:\n\n1. **Understanding Algorithms**: Many machine learning algorithms are grounded in mathematical concepts. For instance, linear regression involves understanding linear algebra, and many algorithms rely on statistics and probability.\n2. **Interpreting Results**: Mathematics helps you interpret the results of your models accurately. Knowing concepts like mean, variance, standard deviation, and distributions is essential for understanding model performance and validity.\n3. **Debugging and Tuning Models**: A grasp of math is crucial for debugging issues and tuning hyperparameters effectively. You'll often encounter concepts such as gradients and loss functions, which are fundamentally mathematical.\n4. **Building a Strong Foundation**: If you want to advance in the field, having a solid understanding of mathematics will benefit you greatly. Topics like calculus, linear algebra, and statistics are particularly important.\n5. **Resources for Learning**: If you're concerned about the math aspect, many online courses and resources focus on teaching the necessary math concepts alongside machine learning. For instance, some platforms offer courses specifically designed for those with minimal math backgrounds.\n\n# Suggested Path:\n\n* **Start with Practical Applications**: If you're eager to dive in, begin with practical machine learning courses that focus on coding and implementation (like Python-based courses) but also include some foundational math resources.\n* **Learn Along the Way**: As you work on projects, tackle math concepts as they arise. This approach allows you to apply what you learn immediately, reinforcing your understanding.\n\nIn summary, while you can start machine learning without extensive math knowledge, developing that knowledge over time will significantly enhance your ability to understand and excel in the field.",
        "Unfortunately yes, but you'll be a low quality practitioner.",
        "Check you will get it\n\nhttps://www.reddit.com/r/DTU__Delhi/comments/1gdt5ja/for_the_pythonsavvy_beginner_knows_python_might/?utm_medium=android_app&utm_source=share",
        "For sure you can. Get your hands dirty with python basics first. Like what is the paradigm around Python programming. \n\nThen, as per your interest, choose the avenue like Computer Vision, NLP, etc.\n\nSuppose you like NLP, read RNN and LSTM. Work with models that are built on these architectures. Huggingface will come into play for the loading of open source models and datasets. \n\nNext, you may check out deep learning areas i.e. Transformer architecture, self-attention, etc.",
        "Lol",
        "Technically you can but at the best you'll be a \"code monkey\" (pardon the language) whose skills will be as far as importing a bunch of libraries and running code. Which is not that much of a distinguishing factor tbh.\n\nWhat separates a good ML engineer or data scientist, whatever you wanna call it, is math skills. Some people might not like the answer but one of most frustrating things to happen is for a MLE or DS literally not knowing from left to right. When asked the whys and their thinking process why they did this or that, or when asked for ideas on how to improve model performance, they don't have a clue and say something that boils down to \"Idk, the coding tutorials I saw implemented it that way\". Makes debugging almost impossible.\n\nBut at the end of the day, the most applicable answer depends on you. What's your end goal learning ML? Do you wanna get a good paying, technically-oriented job doing ML? Then you better start hunker down and learn maths plus stats. If you see yourself just wanna work a run-of-the-mill data analyst job, for example, and just need convenient/easy ML implementations ala AutoML or something, then you don't need to.",
        "Yes.",
        "Can I become a lawyer without learning the law?",
        "No. Hahahaha c'mon man!",
        "It depends. You don't need to know machine code to work in webdev. But if you want to work on compilers, then you should invest some time learning about machine code. \n\nIn software development there is certain levels of abstraction. And the depth of your knowledge should be on par of that abstraction level. If you want to work on new ML algorithms, on new architectures, etc, then you should invest into math heavily. If you want to work on concrete use cases (=end user products), then I don't think that you need that much of math. Those are two different abstraction levels. If you want to work at OpenAI, invest in math. If you want to make something useful with ChatGPT api, then skip the math. Do meth !",
        "Depends on the context. You don't need to know math to download Foss models to compete on kaggle. It's unlikely you'll be any good.",
        "Absolutely.",
        "Haha oh okay",
        "Ou",
        "lol!",
        "Oh this is the most explainable answer. Thank you very much",
        "How he can read the pytorch or other related libraries documentation without knowing maths? Or even understand the metrics",
        "I see thanks",
        "Thank you",
        "Haha 😂😂",
        "Thank you for this insight",
        "Oh okay. So what deep should the mathematics be to be considered a beginner in machine learning?",
        "This is very insightful. I needed this , thank you very much",
        "Tell me more",
        "Hm",
        "Linear Algebra and multivariable calculus are typically considered basics for ML, the latter especially if want to go towards the LLM hype. I will say if you are a computer scientist who just happened to not study linear algebra, you probably already thought about a lot of the same concepts just not in the sense of math.",
        "You don’t have to immediately dive into complex math, to learn basic of machine learning. Saying that you have to know maths first is just simply gatekeeping",
        "Thank you",
        "Oh okay"
    ]
},
{
    "submission_id": "1gdwos9",
    "title": "OpenAI Swarm : Multi-AI Agent tutorial playlist",
    "selftext": "OpenAI recently released Swarm, a framework for Multi AI Agent system. The following playlist covers :\n1. What is OpenAI Swarm ?\n2. How it is different from Autogen, CrewAI, LangGraph\n3. Swarm basic tutorial\n4. Triage agent demo \n5. OpenAI Swarm using Local LLMs using Ollama\n\nPlaylist : https://youtube.com/playlist?list=PLnH2pfPCPZsIVveU2YeC-Z8la7l4AwRhC&si=DZ1TrrEnp6Xir971",
    "created_utc": "2024-10-28T00:54:10",
    "num_comments": 2,
    "comments": [
        "Wow, this playlist looks super helpful! I've been diving into multi-agent AI systems lately and OpenAI Swarm sounds fascinating. Love how it compares to other frameworks too. As someone always looking to boost productivity, I'm really curious about that triage agent demo. Have you tried implementing any of these concepts in real projects yet? I'd be keen to hear about your experiences if so!"
    ]
},
{
    "submission_id": "1gdwo5n",
    "title": "Master’s student ML internship - how to prepare to land an ML internship ",
    "selftext": "Hey y’all\n\nSo, I was a software engineer for 3 years (web dev mostly), but I always wanted to do machine learning. Finally took the leap and moved to the US this August for a master’s in ML. It’s been a lot, like seriously overwhelming, but lowkey amazing too. The math, the problem-solving, info theory and causality—it’s beautiful. But, I’m a total beginner.\n\nThis semester, I joined a research lab working on process reward models. Never read research papers before, but now because of my lab work I have been making it a habit.\n\nHere’s the thing—I need to apply for summer internships, but my resume feels empty on ML stuff(i have no ML related research papers yet) I’ve only done a few small projects and a RAG project back in June. Meanwhile, job listings expect me to know things I’m only learning now, like… by next month based on my course syllabus. I feel way behind when i look at github but also have many people have research papers and cool ai projects/ startups\n\nAny advice on how to get my act together for internships? Or how to feel less like an imposter when I apply? 😅\n\nThanks for any help!!",
    "created_utc": "2024-10-28T00:52:46",
    "num_comments": 6,
    "comments": [
        "If you are looking to do ML research, can you stay in your current lab for the summer? You are much more likely to get publishable work than in a two-month internship and will be more experienced and competitive. Ask around for summer funding options if your lab doesn't have the money.",
        "Unrelated question, which college and program did you go for?",
        "Hey, I think the easiest way is to start building ML models of your own. I started by using model generation through [https://www.plexe.ai/](https://www.plexe.ai/) and iterated on the output and used it in Kaggle competitions",
        "Hi thanks for your reply, I do think that is a possibility because the current timeline and scope of the research we are doing right now would easily extend 6 months\nI do enjoy the research, it’s challenging and I learn a lot but I don’t intend on doing a PhD. Both the compute and people seem to be in the industry and financially too it would make sense for me to get a job (I am burning through my savings to support my living cost in the states)\nI do believe research will definitely give me an edge over the competition but i see two types of internship roles \n\n1. ML engineering internships\n2. ML research internships (which definitely is intended on research and roles that prefer PhDs)\n\nSo i am not even sure what should i focus on to build my profile",
        "Its a target school in the States, major Machine Learning",
        "I think you can learn things of   ML in  parallel and develop skills required for a job role by doing your lab work too .Well I'm  ML Student but not much experienced ."
    ]
},
{
    "submission_id": "1gdvl0f",
    "title": "Understanding the architecture of various Llama models",
    "selftext": "\\[removed\\]",
    "created_utc": "2024-10-27T23:26:38",
    "num_comments": 0,
    "comments": []
},
{
    "submission_id": "1gdusax",
    "title": "Why is Llama failing where openai works just fine? (code)",
    "selftext": "",
    "created_utc": "2024-10-27T22:27:38",
    "num_comments": 7,
    "comments": [
        "This is r/learnmachinelearning and you’re effectively asking a question that amounts to having a sub do your homework.  Why don’t you learn how to implement these tools so you can tell when the LLMs output is bad?",
        "Right... Why didnt i think of that before? Its almost as if I got stuck in the middle of the learning process? Almost as if i have been doing my own research for this as well while waiting, like any normal person. \n\nYou are so right, one should always find the answer and then ask the question here right? Because we are playing pop quiz and this absolutely aint the place where people stuck on something ask for help... Mindblowing.",
        "Being a jerk when you’re in a sub asking for help is always a winning strategy. I read the other post and call bullshit on you trying to learn.  You’re trying to get an LLM to produce your entire solution.  That’s not learning.\n\nSeriously though you asked people to troubleshoot why two different LLMs gave you different responses, that has to be one of the dumbest questions ever asked here.  Do you think there’s anyone who can truly answer that as gippity is closed source and to really answer it you would need someone who knows what’s going on and is familiar with llama.  That’s probably a handful of people at most if any exist.  ",
        "Good job projecting by calling me the jerk here. You talk a lot of fluff without actually knowing anything. This is the place where people ask for help which is what I did, and you calling bs on your own delusions couldn't be more pathetic.\n\n>why two different LLMs gave you different responses\n\nGreat way to show you read nothing and yapped an essay on it. Me and another dude on the r/learnprogramming discord figured out and solved it together, it was an issue with how tools are bound with different Llms and how llama requires more of a structured system prompt. BUT OH NO! According to you, its a dumbass question! And only a handful of people existing know why that is!! TRULY a divine miracle that one of Meta's most senior lead engineer himself came down to help me out!\n\nJust go back into your ego tripping shell dude\n\n1) You didnt read the post and started commenting on it.\n2) if you believe there are dumb questions, you dont deserve to be in a subreddit for people learning about a field.\n3) you are telling someone to go learn as if that isn't exactly the reason why anyone would end up on this subreddit, i.e. Learning and getting stuck somewhere.\n4) Stop trying to assume you know anything about what im trying to do.",
        "So changing your prompt is solving the underlying difference in two models?\n\nhttps://giphy.com/gifs/a3zqvrH40Cdhu",
        "You must have an allergy to careful reading. Its alright, i hope you get well someday.",
        "Awful cocky for a jr offshore dev who’s using chat gippitty to deliver for your client…."
    ]
},
{
    "submission_id": "1gdu0np",
    "title": "Attempt at new architecture but needs expert advice ",
    "selftext": "Private dm me so I can send the implementation and if you can please provide some feedback.\nI am doing this as a learning experience and could use some expert help if possible.\n\nI’ll call it Incremental Pattern Recognition Network (IPRN), designed to identify the significance of each word and learn structural language patterns progressively.",
    "created_utc": "2024-10-27T21:35:48",
    "num_comments": 0,
    "comments": []
},
{
    "submission_id": "1gdtqc9",
    "title": "Do models forget connections as they are trained?",
    "selftext": "I am curious about the inner working of how AI models build connections between the various hidden layers in regards to inputs and outputs. Ideally, I want to build a \"living model\" which is constantly being trained and producing output as more data is added. \n\nIdeally as more data is added to the model, old unused connections are forgotten and more recent highly associated connections show up. This is much in line with what we see in neuroscience where connections are quickly formed through repeated uses of the connections but gradually get pruned over time due to lack of use.\n\n  \nIs this similarly occurring with these AI models?",
    "created_utc": "2024-10-27T21:17:03",
    "num_comments": 11,
    "comments": [
        "Look up catastrophic forgetting. MoE models with occasional pruning might be a promising modelling approach for this neural capacity maybe.",
        "Every neural network has a certain capacity to store information. This capacity also depends on how the weights are adjusted. But even when used optimally, every system reaches a limit at some point. In a neural network, “forgetting” occurs because the weights no longer react reliably to a certain input in the way that was previously defined during training. The original behavior can be overwritten by continuously changing the weights.",
        "To some extent but you'd have things like the \"dying relu\" problem that will prohibit iterative learning like that fairly randomly.",
        "Well that depends how you build the model.  \nIf you consider reinforcement learning for example the networks totally do forget certain things or at least reduce their value, while constnatly learning from newer data (e.g. as the AI progresses further into the game and encounters new unseen things)  \nIt just kinda depends how you built things. I dont understand the \"forget connections\" though. They just have their weights shifted, they arent really \"forgotten\" in a literal sense as if they were disconnected or reconnected.",
        "You lack significant understanding of artificial neural networks.\n\nYou design the neutral network explicitly in code going from the input nodes to a series of fully connected nodes, to an output node(s).\n\nYou train the model on data, the connections remain the same, yet only the weights / biases of each connection and node is updated. Yes there are some nuances such as activation functions and attention mechanism but generally this is the general thing going on with artificial networks.\n\nSo to achieve your result of being updated constantly, yes, you can do it, but you would just simply be training the old weights/biases (parameters) on the new data. You can start from random weights or you can start from the weights you had from the old data.",
        "Best I can tell- every time you fine tune, it gets better at the new thing but sometimes worse at other things. I think MoE where the experts are all different models good at different things orchestrated in an agentic framework is the best we can do right now.",
        "Well stuff can be forgotten for sure but when you define a model before it's been trained, all the connections are there they just have meaningless. values. As you train it those values start to be good at something. So you are not really loosing and gaining connections but assigning meaning to them.",
        "to explain the \"forget connections\", the idea is that as a particular set of knowledge is no longer needed, the model eventually loses the ability to output that result. In my case, I want to ask it a question like \"best RPG?\" and it would respond differently depending on when it was trained. maybe reviews from 1997 would have Final Fantasy 7 being attributed to best RPG while when training on data from 2016, Maybe Persona 5 is now the response. \n\nAnd as I train, request output, train, request output, and repeat; I would see this transition between the responses.\n\nI could theoretically train on all of the corpus at once and then be able to ask for \"Best RPG in 97\" and \"best RPG in 2016\" but I am more interested in building a system that could change as new data is added as opposed to all at once.\n\nOne thing to note is that some remembering would be optimal as when new data is added, it is possible it could strengthen existing connections making items that were previously insignificant much more so.",
        "I had read that the corpus of words is usually set at the beginning when generating the layers and connections. Is there a possibility to add more later? Is there a model that allows this?\n\n  \nI know the brain does this all the time. It will actively add in new neurons and prune the connections based on usage.",
        "I see. Are the defined parameters a bunch of interconnecting nodes or are they predefined? When building a neural network earlier, I had to make sure all words that would appear in the texts were also present during training.",
        "Parameters are numbers involved in math operations like multiplication. Theyre initially random, then training changes them to encode \"information\" that relates good input and output. \n\nWhen you mention requiring all words to be present during training, that's not really related to this question. That's a convenience shortcut. It's slightly more complex to handle an unseen word during testing, but there are other workarounds. It doesnt have to do with forgetting in the sense of the discussion above."
    ]
},
{
    "submission_id": "1gdt6zk",
    "title": "What do you think about metaheuristic algorithms?",
    "selftext": "I have seen many people use classical machine learning or deep neural network to solve problems but nobody tends to use meta heuristic algorithms. Why is that?",
    "created_utc": "2024-10-27T20:45:18",
    "num_comments": 1,
    "comments": [
        "I do not use them because when it's over it is very difficult to have an idea about the solution quality. Most of the time these algorithm (grey wolf, whales, ant colony) are just fancy toys. But probably I did not encountered a problem where they could take advantage"
    ]
},
{
    "submission_id": "1gdqy53",
    "title": " Do you do hyperparameter search for each setting in ablation study？",
    "selftext": "",
    "created_utc": "2024-10-27T18:40:41",
    "num_comments": 0,
    "comments": []
},
{
    "submission_id": "1gdmtcj",
    "title": "Tufts Online MSCS vs. Georgia Tech OMSCS - Which is better for career prospects?",
    "selftext": "I’m deciding between the online MSCS programs at Tufts and Georgia Tech (OMSCS), and I’d love some advice on how these two stack up, especially in the eyes of recruiters and hiring managers. Cost isn’t an issue for me, so I’m focusing on program reputation, learning quality, and overall career impact.\n\nQuestions:\n\n* Does one of these programs tend to be viewed more favorably by employers?\n* For those who’ve completed either program, how was the learning experience, support, and networking?\n* Any insights on how the programs compare in terms of opening doors for job opportunities and long-term career growth?",
    "created_utc": "2024-10-27T15:17:09",
    "num_comments": 1,
    "comments": [
        "GT and it isn’t close"
    ]
},
{
    "submission_id": "1gdklg0",
    "title": "Good accuracy, wrong predictions on new data",
    "selftext": "Hello,\n\nI am writing with the hope that somebody knows why my ML model isn't working. I have a dataset of \\~300.000 entries, in 4 categories: 0 - benign traffic, 1 - dos, 2 - ddos, and 3 - other. The dataset is balanced, so every category has the same amount of samples. I have extracted the most important features using RFE. I have scaled the data using standardScaler. Furthermore, I have trained a RandomForestClassifier, 99% accuracy. Next, I have tried implementing a hybrid model, for anomalies. I have used a KNN model. It also has 98% accuracy. Furthrmore, I am using a Stacking Classifier (Gradient Boosting) to combine the two models.\n\n    stacking_clf = StackingClassifier(\n        estimators=[('knn', knn), ('rf', rf)],\n        final_estimator=GradientBoostingClassifier(),\n        cv=5\n    )\n\n(Good accuracy as well).\n\nNow, I am trying to test it on other data. I have generated this data, scaled it using the same scaler, and inputed it in the stacked model, but the predictions are all 0. Furthermore, I got some data from the actual training/testing set, predictions are wrong again. What could be the problem? Thank you for your time.",
    "created_utc": "2024-10-27T13:35:43",
    "num_comments": 3,
    "comments": [
        "You don't mention partitioning your data into validation and test, have you done that? If not, and your generated new data is sufficiently different from that of the dataset, your system won't make accurate predictions. \n\nYou could also add this data to the dataset, shuffle it, then partition into test and validation. Both are important to train your system and fine-tune your model.",
        "your model DO NOT know how to \\`generalize\\`. It memorized your dataset ...",
        "try to implement cross-validation. Your model isn't generalizing at all. When same data is used for training everytime, your model just memorized it perfectly and gave you the perfect accuracy score and when new data (unseen before) is used it fails badly."
    ]
},
{
    "submission_id": "1gdkg6p",
    "title": "SUFFERING with algortihms",
    "selftext": "Hello guys, i am a starter machine learning enthusiast and i love the field but algorithms and data structures been getting the best out of me, all the resources out there are really expensive (neet code is fucking 119 dollars) and i cant find a good course with videos that teachs them well. \n\nIf you guys had any reccomendations any resources anything i could practice while also studying Data stuff and ML i would be really grateful",
    "created_utc": "2024-10-27T13:29:15",
    "num_comments": 4,
    "comments": [
        "Just go on youtube and check for DSA series ,  my fav is Abdul Bari you can check out once",
        "Try striver and love babbar , they helped me a lot",
        "Go to leetcode top 150 interview questions study track, it covers most of the data structures and algorithms",
        "I’ll do that man!"
    ]
},
{
    "submission_id": "1gdjk3u",
    "title": "Unfiltered  model",
    "selftext": "Hi. I'm new to machine learning. I wanted to use it for some storytelling but online models have filters for some drastic content. So I installed LM Studio and one of the models. With some tutorials, I added a prompt that allowed it to swear but it is inconsistent. So I am asking how to make LLM that could generate basically some NSFW content focused on more drastic things.    \nMy priority:   \n1. Swearing\n2. Drastic descriptions\n3. Maby NSFW\n4. For it to answer anything: By that, I mean for it to try replaying and not just straight refuse to answer (I know that it's may be impossible but maybe someone has some tricks)\nI'am looking mostly for least restrictive models, prompts that can bend filters and mabybsome other solutions. There are NSFW models on internet Soni know that is possible.\n",
    "created_utc": "2024-10-27T12:49:44",
    "num_comments": 3,
    "comments": [
        "Lmao how many billions of dollars do you have to spend on cloud computing bills?",
        "I know that models have their own built in filters and with the right prompts you can bend it a little and out there are models with more loose filters. So I don't want to create my own model or completely rewrite it (if I understood correctly). Like I said I am new to this so I'm just looking for some solutions like which model has the least restrictions, what prompts I can use or how to create a database with responses to direct it on the right track.",
        "You're not even close to being able to do something like this. There's also a lot of very good legal reasons that something like this *doesn't* exist."
    ]
},
{
    "submission_id": "1gdieoa",
    "title": "What are the best tools for labeling data?",
    "selftext": "What are the best tools for labeling machine learning data? Primarily for images, but text too would be cool. Ideally free, open source & locally hosted.",
    "created_utc": "2024-10-27T11:59:46",
    "num_comments": 18,
    "comments": [
        "interns/undergrads",
        "Labelstudio is one of the best. I love it.",
        "Prodigy is decent but does require some setup that's not super intuitive.",
        "ImageJ is free and open source",
        "I found TRAINSET (https://trainset.geocene.com/) really helpful and easy to use for tasks involving timeseries data labeling.",
        "Labelimg is pretty easy to use",
        "Mechanical Turk or Sagemaker GroundTruth",
        "Anyone have suggestions for TIME SERIES labels?",
        "Eyes and fingers",
        "For images try CVAT, much better than label studio etc",
        "roboflow or label studio",
        "supervisely",
        "😂😂😂gosh !",
        "Try CVAT, much better than label studio",
        "Try CVAT, much better than label studio",
        "Why do you think CVAT is better than LabelStudio for images?",
        "It's available as a website, no need to download etc. \nIt has better navigation facilities. \nComes with a range of keyboard shortcuts. \nHas wonderful AI support like yolo etc. \nWay too easy for doing annotations. \nSupported by multiple browsers. \nBetter community support compared to label studio. ",
        "Can confirm, it's a solid tool, especially considering the cost/benefit ratio. Only downsides are: pretty boring UI, and doesn't work with non-vision data."
    ]
},
{
    "submission_id": "1gdh940",
    "title": "Where to download OpenAI's Whisper model \"standalone\"?",
    "selftext": "I want to download one of the OpenAI's Whisper models \"standalone\". I start looking here\n\n[https://github.com/openai/whisper](https://github.com/openai/whisper)\n\nThere all (?) models are listed but there are no download links. There is something called \"model card\" that links here [https://github.com/openai/whisper/blob/main/model-card.md](https://github.com/openai/whisper/blob/main/model-card.md) but again, no download links, just more details about the models.\n\nVia Google I found [https://huggingface.co/openai/whisper-tiny](https://huggingface.co/openai/whisper-tiny) but it just seems to offer me to run that specific model online.\n\nAt [https://github.com/openai/whisper/blob/main/whisper/\\_\\_init\\_\\_.py#L17-L30](https://github.com/openai/whisper/blob/main/whisper/__init__.py#L17-L30) I found some source code with hardcoded download links but that can't be the \"right\" way to find download links for these files, can it??\n\nI want to download a model so I can convert it to another format.",
    "created_utc": "2024-10-27T11:10:36",
    "num_comments": 0,
    "comments": []
},
{
    "submission_id": "1gdgp3j",
    "title": "How to start",
    "selftext": "Hello, i wanted to learn how to work with machine learning from the bottom up (i set on a quest to not use pytorch, tensorflow nor scikit). I started by reading grokking deep learning, and got to chapter 10 (cnn layers) and got completely stuck. The code felt impossible to understand so i chose to move on. I tried making a digit recognition system but failed. So i decided to take a step back to learn the whole thing properly from the ground up. I read some articles that said that there are some complex math concept involved, but from the knowledge i had all the concepts i studied felt pretty simple (dot multiplications and activation functions\\[...\\])\n\nIs all that math really required if i wanna go under the hood? or should i follow a guide more programming-oriented? or just start off with tensorflow or pytorch and then dig my way down from there? I'm really lost. Thank you",
    "created_utc": "2024-10-27T10:46:38",
    "num_comments": 3,
    "comments": [
        "Also, i saw some other interesting books:  \nIntroduction to statistical learning, \n\nHands on machine learning with sci-kit learn and tensorflow\n\nI really like the idea of using an ai to recognize stuff, which book would be better to start?",
        "I'm learning as well and I think doing it completely from scratch is overkill. You can build a simple backprop engine and build up to an MLP along with training from scratch but beyond that I think it's diminishing returns unless you plan to go to the CUDA level later on.\n\nI recommend just working with PyTorch tensors instead. So try to implement your own forward and backward passes on top of just tensors without using nn.Module stuff or optimizers etc. It will also be more useful since PyTorch is most popular in research and production nowadays.\n\nThe Andrej Karpathy zero to hero neural nets playlist on YouTube is great for this sort of stuff and if you like NLP.",
        "Hey, I think the easiest way is to start building ML models of your own. I started by using model generation through [https://www.plexe.ai/](https://www.plexe.ai/) and iterated on the output and used it in Kaggle competitions"
    ]
},
{
    "submission_id": "1gdghn6",
    "title": "Revolutionizing Radiology: How AI Startups Are Transforming Healthcare",
    "selftext": "Imagine a radiology department overwhelmed with scans, leading to delayed diagnoses and critical treatment hold-ups. Enter AI-powered diagnostic tools—today's superheroes in healthcare! 🚀\n\nStartups like Zebra Medical Vision, Aidoc, and [Viz.ai](http://Viz.ai) are harnessing artificial intelligence to tackle these backlogs head-on. By automating the initial analysis of images, these innovations are slashing diagnostic times by up to 50%, ensuring patients receive timely and accurate diagnoses. Just think: earlier detection of diseases like cancer can boost survival rates from 90% to an astounding 99%! 🎉\n\nBut it’s not just about speed; it’s about saving lives. These AI solutions are helping radiologists identify critical conditions faster than ever, revolutionizing patient care across hospitals. Yet, we must tread carefully—ethical concerns and data privacy issues are part of the conversation.\n\nWhat do you think about the role of AI in radiology? Is it a game-changer or a cause for concern? Join the discussion on Reddit and let’s dive into the future of healthcare together! 💬",
    "created_utc": "2024-10-27T10:37:34",
    "num_comments": 0,
    "comments": []
},
{
    "submission_id": "1gdg8mw",
    "title": "Has Anyone See this before",
    "selftext": "[This comes up on first boot after installing Anaconda](https://preview.redd.it/4no1pjmt2cxd1.png?width=461&format=png&auto=webp&s=c8be81f49f4de23aa6c9fdd07eb66af5b95e184f)\n\n",
    "created_utc": "2024-10-27T10:26:48",
    "num_comments": 1,
    "comments": [
        "No. But I’d guess that your anaconda installation has been moved/deleted"
    ]
},
{
    "submission_id": "1gdg1k5",
    "title": "python ML for football scores",
    "selftext": "^(Hello chaps, I am wondering how I can map 4 input values to 2 output values. I am trying to apply machine learning to football scores.)\n\n^(Could anyone suggest any models, and how i would go about doing this in python?) \n\n^(cheers)",
    "created_utc": "2024-10-27T10:18:07",
    "num_comments": 2,
    "comments": [
        "You need to give more information",
        "I'm sorry - I have 4 Input values ( home team avg goals , away team avg goals , home team avg goals conceded , away teams AVG goals conceded) and two outputs ( home goals scored in game , away goals scored in game). Does this help?"
    ]
},
{
    "submission_id": "1gdfmg3",
    "title": "RAGs - a deep dive into each major component",
    "selftext": "Hello! Just sharing a video I made covering the major components in RAG and the leading techniques researchers/engineers are using to make powerful LLM pipelines. Hope yall enjoy! I ",
    "created_utc": "2024-10-27T10:00:31",
    "num_comments": 0,
    "comments": []
},
{
    "submission_id": "1gdf917",
    "title": "Suggestions for my private data science workstation",
    "selftext": "Hey guys,\n\nI'm in the process of buying a laptop that I want to use mainly for my personal data science projects.\n\nI have decided on the Lenovo P16 G2 with the following specs:\n\n\\- Intel® Core™ i7-13700HX\n\n\\- 16 GB ram\n\n\\- 1 TB SSD \n\n\\- Intel® UHD build in GPU \n\n\n\nDo you have any suggestions for alternatives or improvments?\n\nMy price limit would be between 1500 and 2000 and it should also be upgradeable in terms of ram and memory.\n\nIf you have any suggestions please let me know.",
    "created_utc": "2024-10-27T09:44:21",
    "num_comments": 14,
    "comments": [
        "This is not a good buy. Firstly, you should never buy a laptop to do ML compute on- the cooling is bad, the chips are low powered, and the hardware can't be replaced easily.   \n  \nSecondly, this machine doesn't even have an Nvidia GPU- you will get zero benefit from the intel built in GPU. The 16 GB of ram is about half of what you'd want, and the 1TB SSD is fine, but you really want an M.2 PCIE drive, in addition. \n\nBasically, nothing about that machine is a good choice to actually run ML compute on. However, if you're looking for a laptop to take to class and do basic things on, I suppose its fine.",
        "If you want the device to be upgradable, go for a Desktop machine. I’d also use a full ATX mainboard.\n\nRest of the components will depend largely on your preference. \n\nA desktop configuration, especially if you build the pc yourself, will be a lot cheaper. You could likely get 64GB of DDR5 RAM, a core ultra or 14th gen i7 more storage and a GPU (if you want to do ML in the foreseeable future) while still staying below 2K",
        "Try and get something where the deep learning libraries support GPU acceleration. Currently; Nvidia GPU, AMD or Mac Silicon chips. Try and get some decent RAM (mainly desktops) if you want to work with large models\nA desktop where you can upgrade the computer as needed could be a good option!",
        "Chromebook + `ssh`",
        "For this price tag your best bet would be to find a good condition used MacBook M2 Max if you want a laptop. You can find some good ones for around 2K. The CPU is a beast, it will have at least 32 GB of RAM. The GPU is ok, not great, but it has more VRAM than anything else you can find in a non Mac laptop, so if you wanna go into big models, this will run bigger models than a desktop 4090. Definitely won’t be as fast but they’ll fit, while NVIDIA GPUs, and especially laptop ones, are soooo VRAM limited.",
        "If you really were serious, you'd need something that more resembles a high-end gaming PC (like Gigabyte's AI TOP: www.gigabyte.com/WebPage/1079?lan=en) or even an entry-level enterprise machine (like this one that runs on Ryzen: www.gigabyte.com/Enterprise/Tower-Server/W331-Z00-rev-100?lan=en) Of course all these might be beyond your budget, but at least try to get something closer to the aforementioned AI TOP.",
        "Not a good choice. Trust me when I say: You will be surprised how quickly 16 GB RAM gets full. You want at least 32 GB. The CPU is okay. The Intel UHD GPU is crap, there is no hardware accelerator that will run on it. You would want an NVIDIA GPU with a decent amount of VRAM.\n\nPersonally, I’d say forget about the idea of using a workstation for actual computations. If I had to buy a new laptop, I’d choose a MacBook Pro with 32 GB RAM, knowing that I’d do the hard stuff in the cloud.",
        "Start doing those \"personal data science projects\" right now on whatever you have access to.  Look at what's holding you back.  Spending over $1.5K that way isn't a very good use of scarce resources, IMO, unless you know your *real* needs.  If someone is willing to pay right now, but maybe not in the future, that's a different (and sad) story.\n\nLenovo's are quite nice, but I got a AMD six core Ideapad with Nvidia for $300 on open box clearance and found $50 64mb ram on Facebook marketplace, and it works great with an external 4k 40\" display with Win11 WSL2.",
        " Take a good look at hat you already have and what you can access for free, and be sure to use up all the free time possible.  Once you have a better feel for which direction you want to go in ML/AI you can then revisit the personal system needs.  Earlier versions of the RTX series may be enough to satisfy you requirements...inmy case an RTX 2070 12 GB with another 32gb ram is enough, I do time series analysis.\n\nAlso consider that using object storage is a skill as is running models in small spaces, online ML, batch.... There is plenty to do with limited resources and these are very valuable skills.",
        "Thank you for your advice! I’ve actually changed my mind and decided to invest in a desktop PC instead of a laptop. I realized I was too focused on the portability factor, but I agree with you—it would have been a waste of money in the end. I managed to put together a desktop setup within the same budget that gives me double, maybe even triple, the performance of the Lenovo laptop.\n\nThanks again; I really appreciate your time and thoughts!",
        "Totally true! I’ve put together a build with an i7 13th Gen processor, GTX 4060 Ti (CUDA support), 32 GB DDR5 RAM, and a 2TB M.2 SSD—all within my budget. Plus, I have the flexibility to upgrade the RAM or GPU if I want even more power in the future.\n\nThanks for your insights;",
        "I’ve decided to go with a desktop PC after all and have already found a few CPU and GPU options that I’ll probably pick up around Black Friday. In the end, you’re totally right—a desktop PC is the best setup for my needs.\n\n  \nThanks for your thoughts",
        "I’ve actually changed my mind and decided to go with a desktop PC instead. I’m considering a setup with:\n\n* GTX 4060 Ti 16GB with CUDA support\n* i7 13th Gen (K series)\n* 32GB DDR5 RAM\n* 2TB M.2 SSD\n\nThank you for the recommendation—this setup seems like a much better fit for my needs! I’ll just need to do a bit more research on the components to be sure.",
        "Sounds good! Definitely the best bet to not be constrained by your hardware as much. Enjoy!"
    ]
},
{
    "submission_id": "1gdevgz",
    "title": "I need help in my homework ",
    "selftext": "",
    "created_utc": "2024-10-27T09:28:23",
    "num_comments": 6,
    "comments": [
        "Circle true or false.",
        "Do it yourself",
        "Study your notes and then try to answer cold, and then go back and crosscheck with your notes. Study regularly. And I'm really not an expert in any of that, but most of it seems just knowledge, isn't the goal to be able to be competent and answer technical interviews? \n\nLearning is struggling...",
        "This is not homework lil bro, this is an exam",
        "Lmao dm me. I'll help",
        "And they don’t need help, they want someone to solve the entire thing for them."
    ]
},
{
    "submission_id": "1gdes9g",
    "title": "I'll take your mock interview, just show me the project you did. No charge, it's free",
    "selftext": "I am taking mock interviews and if you are preparing and wanna practice, go for it.\n\nP.S. - We'll do it on Youtube Live",
    "created_utc": "2024-10-27T09:24:37",
    "num_comments": 0,
    "comments": []
},
{
    "submission_id": "1gdelva",
    "title": "[Help Needed] Guidance for Final Year Project on Diabetic Retinopathy Classification Using Segmented Fundus Images.",
    "selftext": "Hello everyone,\n\nI’m a final-year engineering student working on a project to classify Diabetic Retinopathy using segmented fundus images. Unfortunately, I’m feeling quite lost and struggling to make significant progress. I have about 1.5 months left to complete this project, and I’m not getting the guidance I need elsewhere, so I’m turning to this community as a last resort. Any help or advice would be highly appreciated!\n\nProject Goals:\nTask: Classify Diabetic Retinopathy using segmented fundus images.\nApproach: I’m aiming to use efficient models for classification. I’ve considered using CNNs for feature extraction but am open to other suggestions for models or techniques.\nTimeframe: I have about 1.5 months to complete this project.\nChallenges:\nI’m struggling to find a suitable dataset that provides segmented fundus images. I would appreciate any recommendations for publicly available datasets.\nI’m looking for guidance on model selection and approaches that would be feasible within my limited timeframe.\nI’m feeling overwhelmed and unsure about the implementation steps. If anyone has sample code or resources related to Diabetic Retinopathy classification, it would help me get started.\nWhat I’ve Tried:\nI have some background in machine learning and have worked with CNNs in the past, but I’m unsure how to apply this to segmented images.\nI’ve been exploring different models but don’t have a clear plan on what would be most efficient and effective.\nAny help with the following would be greatly appreciated:\nSuggestions for datasets containing fundus images for segmentation and model training.\nRecommendations for suitable models or architectures for Diabetic Retinopathy classification.\nSample code or tutorials that could help me get started with this specific problem.\nGeneral advice on how to manage and complete the project within the timeframe.\nThank you in advance for your support and guidance. I’m really hoping to get this project back on track, and I would be grateful for any pointers or resources you can share!",
    "created_utc": "2024-10-27T09:16:52",
    "num_comments": 0,
    "comments": []
},
{
    "submission_id": "1gdedxh",
    "title": "Detecting/ counting unique humans in a video feed",
    "selftext": "I'm new to machine learning, about to start an IBM course. I have a specific goal though: want to be able to define a rectangular area on a video feed and have the machine count how many unique people pass-thru/ venture into it.\n\nKnowing a lot has been done in the ML/AI space, sending this out with the hope someone can direct me to some ready solution (preferably open source self-hosted, but paid online service if need be) to leverage. This would immediately meet my practical needs while I learn the deeper theories in the course as to what makes it work.\n\nI do have some prelim exposure to ML terminology can likely make use of some services out there if directed.",
    "created_utc": "2024-10-27T09:07:04",
    "num_comments": 0,
    "comments": []
},
{
    "submission_id": "1gddkyn",
    "title": "Reinforcement Learning: An Evolution from Games to Real-World Impact - day 77 - INGOAMPT",
    "selftext": "",
    "created_utc": "2024-10-27T08:31:54",
    "num_comments": 0,
    "comments": []
},
{
    "submission_id": "1gdd2y0",
    "title": "What to look for when Choosing or Shortlisting Masters in AI ( Curriculum )",
    "selftext": "I heard many master's degrees in AI are not up to date with the current industrial methods used there. What must I look for in the Curriculum to determine whether the course is up to date for the current time ( Oct 2024 )?\n\n  \n",
    "created_utc": "2024-10-27T08:09:14",
    "num_comments": 0,
    "comments": []
},
{
    "submission_id": "1gdciq4",
    "title": "Good short YouTube videos for public to learn about AI",
    "selftext": "I'm volunteering at a few educational events for the general public in the coming days, and I need to present about AI / Generative AI.\n\nAs I don't have much time to prepare for it, my plan is to use the time to show some videos about AI use cases across different industries - education, healthcare, government etc. and talk through them.\n\nExample: Khanmigo in the education space.\n\nDo you guys have examples of recent (in the past year) and positive (no doomsday messages) YouTube videos of AI or GenAI use cases that a person with minimal AI knowledge would be interested in and make them excited about the future possibilities?\n\nOpen to all suggestions!",
    "created_utc": "2024-10-27T07:43:22",
    "num_comments": 1,
    "comments": [
        "https://youtu.be/N_Nvr4ztBXs?si=ltb4mxJy_RIt-rAp"
    ]
},
{
    "submission_id": "1gd9j7u",
    "title": "Seeking Advice on Improving Robustness of Instacart Basket Analysis Models Dominated by Single Feature ",
    "selftext": "Hi everyone,\n\nI’m working on the Instacart Basket Analysis project on Kaggle, focused on predicting repeat purchases. I engineered a feature called `num_of_ord_purch_p_prod`, representing the number of times a specific user has purchased a particular product. While this feature is highly predictive, it heavily dominates feature importance across models, raising concerns about potential over-reliance and model robustness. Here's a link to my feature engineering notebook for more details: [Notebook](https://www.kaggle.com/code/deepsutariya/instacart-dataset-transformer-cv2/edit/run/202830778).\n\n**Project Details**:\n\n1. **Class Balance**: The target class (reordered status) is balanced.\n2. **Evaluation Approach**: I tested the models on two separate test sets, each containing only the latest orders for a more time-based validation.\n3. **Model Performance**:\n   * My best LightGBM model achieved an AUC of 0.85, with **100000x** gain importance on `num_of_ord_purch_p_prod` over the second-best feature.\n   * My best XGBoost model achieved an AUC of 0.72, with **20x** gain importance for the same feature over second best feature.\n4. **Feature Importance**: SHAP analysis confirmed the high importance of `num_of_ord_purch_p_prod`, even after applying regularization techniques. In XGBoost, regularization reduced its dominance but also lowered AUC.\n\n**Features in Use**: Beyond `num_of_ord_purch_p_prod`, I’ve included features such as:\n\n* `frequency_of_reorder` (frequency of product reorders by user)\n* `product_mean_of_position` (average product position in user’s orders)\n* `prob_of_being_reordered` (probability of reorder based on past purchases)\n* `count_ord_no_prev_purchased_items` (count of new items in each order)\n* Distribution counts of product orders per day of the week, among others.\n\n**Request for Advice**: Given the dominance of a single feature, I’m looking for suggestions to enhance the model’s robustness and generalization with descent  auc ( > 0.80).",
    "created_utc": "2024-10-27T05:10:02",
    "num_comments": 0,
    "comments": []
},
{
    "submission_id": "1gd991i",
    "title": "Project recommendations ",
    "selftext": "Tell me some projects to have on my resume? Can someone also tell me some must do projects and tell me projects of each difficulty.",
    "created_utc": "2024-10-27T04:52:54",
    "num_comments": 2,
    "comments": [
        "RemindMe! -7 days",
        "Hey, I think the easiest way is to start building ML models of your own. I started by using model generation through [https://www.plexe.ai/](https://www.plexe.ai/) and iterated on the output and used it in Kaggle competitions"
    ]
},
{
    "submission_id": "1gd8o2n",
    "title": "I applied for many companies and can’t get a job",
    "selftext": "Just like the title said, I applied but still cant get a job, what’s the issue am 17 years old really love machine learning/AI.\nAs you can see from the CV i built many projects, whats the issue is it because of the Age or my projects are not impressive?",
    "created_utc": "2024-10-27T04:15:12",
    "num_comments": 40,
    "comments": [
        "Hey so il give you some feedback from the perspective of an engineering manager taking a look at this CV\n\nFirst off the layout is pretty bad. You’ve got the right sections in there but it looks and feels low effort. You could take a look at some Cv templates online and really make this stand out a bit. \n\nYour summary is not helping you.  If I see a 17 year old claiming 3y experience and no work history it’s going in the bin. I have enough candidates applying that I don’t have time to pick through and figure out what actual experience you have. \n\nYou should change the summary to something like “A highly motivated high school student who has been developing skills in X over the last 3 years. Looking for (internship /jr position) where I can use these skills and continue to grow)\n\nYou have a lot of projects which are good. But they don’t tell me much.  Make sure you don’t have any projects that were just tutorial following - then maybe drop a few projects and focus on the strongest ones giving more information about what you did and challenges you overcame. \n\nIf you have group projects or anything like that emphasise teamwork / collaboration / organisation skills. Also any unrelated to IT part time jobs are good on there - real experience working for someone else. Those are good to demonstrate soft skills / show you’re reliable etc. \n\nOverall for an internship this CV would not hold up and get you to the next round compared to other intern CVs I review and certainly wouldn’t get you past screening for a junior role.  But with some improvements I think you have a strong shot at least at getting an internship from this",
        "You are 17. You don't have 3+ years of proven experience or expertise.\n\nGithub is not a version control tool, GANs and CNNs are not AI techniques, they are neural network architectures. Plus your resume layout and content is pretty bad.\n\nIf you are interested in ML/AI, go to college, and a masters degree as well. Focus on getting into a good university, do internships/research in the field as a student, and keep learning.\n\nBest of luck! :)",
        "What roles are you applying for? If you’re applying for ML Engineer, that’s a position for PhDs in the field. I understand that the industry is lead into this space where they hire engineers who do backend and deployment of models, however, that’s not a ML Engineer, that’s mostly devOps. I would suggest by starting as a normal SWE and then working towards an ML Engineer. Also, fix your resume to be in one page. R/engineeringresumes is probably a good place to get an idea of how a resume should look.",
        "Any formal certifications? \n\nI don’t doubt your skills, just wondering what the hiring team might be looking for",
        "I see you’ve completed many projects, but there are no links to the codebase.\n\nIf you’re interested in a role with Company A, start contributing to their open-source repositories and projects. Consistent, valuable contributions often lead to interview opportunities, and if your work aligns with your interview performance, it can significantly increase your chances of receiving an offer.",
        "Everyone tries to apply to ML positions, ML master degrees (and even then its getting really hard), ML researchers (PhD in ML), maybe PhD/MSc in statistics, math, physicists, scientists... its competitive enough for people with a formal education, let alone for someone with no formal education beyond high school. For a high school student who has done international competitions (IMO/IOI) i guess it may happen for an internship?",
        "Lord, give me this audacity :)\n\nImpressive for a high school student but completing a tutorial/bootcamp does not automatically make you skilled or fit for the job.\n\nYour coding skills might be good but it does not mean you are skilled in ML. You need to understand the inner workings, particularly needed in real life scenarios to understand what you need, what is working, what is not working etc. I am not even getting to the ethics and all. You just can't, I repeat, can't list tutorials as projects/experience. Nothing in your CV tells me you developed critical thinking skills needed for ML.\n\nIt does not mean high schools students can't create great things, but I fail to see them becoming employees, and certainly not with a CV like this. It is better to have a paragraph that says, hi I am 17 and I am passionate about this and this.\n\nGo to events, career fairs, hackatons sponsored by companies, or better find start-ups, approach people and ask for an internship. Or build your own products and release them.",
        "I have 25 years of experience at life, doesn't mean I will write the same in resume. Isn't it?",
        "You just completed a bunch of tutorials and put them under projects/experiences. I would be annoyed.",
        "As others have pointed out, claiming three years of experience is a major problem. When employers are asking about years of experience they're talking about professional experience, i.e., from work. You don't have that so, it is going to get thrown out with certainty.\n\nAlso, numpy, Pandas, MathplotLib, and PyQt5 are \\*not\\* programming languages. They're libraries. Those being listed in languages would get it thrown out because it indicates you don't know what you're talking about.\n\nThe layout is not great. There is \\*way\\* too much white space. I would include links to your projects rather than one link to a github. You have to make it easy for the reviewer. I would strongly suggest finding a nice CV template.\n\nAlso, I didn't look at your project thoroughly, but make sure your terms right. If the terms are wrong, then it is going in the garbage because it will indicate you don't know what you're talking about. Also make sure everything on there is \\*true\\*. If at the interview stage it is revealed that some of those are not true, then you're done. A bit of puffery is sometimes ok, but not outright falsehoods. Based on the phrasing, there are some of those that I would find suspicious. Image Classification... this is a MAJOR research problem that you're just casually say \"Oh yeah. I solved that with 80% accuracy.\"\n\nFinally, make sure you're applying for something appropriate. An internship for example makes sense, a junior (or higher) AI engineer does not.\n\nGood luck!",
        "you’re not going to find a job by doing baby projects on your own, unless you’ve built something truly ground breaking. what you need is  an internship or apprenticeship.",
        "Your github username is weird. Wouldn't put that or  maybe change it",
        "First things first, update it to following something more in line with this: [https://github.com/alexcalabrese/techResume](https://github.com/alexcalabrese/techResume)\n\nOtherwise no one will take this resume seriously.",
        "Other people have already pointed out the layout issues so here's a quick way to fix that: head over to r/resumes, pick one of the standard resume templates that's linked in the pinned messages and fill it out with your details.\n\nBest of luck to you.",
        "Seems I need to work harder. I’m also 17 and I have no project.",
        "It’s prob ur age, lack of degree and the projects are not impressive",
        "When I look at this CV, I see someone who attended some bootcamp program and/or did some online courses \n\n\nProblem is, these people are dime a dozen. On the other side you have some college graduates who apply for the same beginner level jobs, and of course I will pick them. I know it is really fashionable to claim college is useless but it is not. It is a place to get more social skills, get basic skills to be able to do some research and project development. It is free time for young people to also think, read and research. moreover most colleges force their students to get fundamental courses related to their field: calculus, linear algebra, statistics, basic computer science and programming classes. Bootcamps and online courses are all great but I have yet to see that gives that depth of knowledge.\n\n\n\nWhen the market was hot, people were being hired left and right but this is not the case anymore. \n\n\nMoreover AI is currently killing some junior level work. I have seen this happening. Instead of hiring 5 juniors, people hire one or two.\n\n\n\nSo the unfortunate case as your CV screams, you are at lower levels of the skill and experience levels compared to other people applying for the same jobs. \n\n\nBut more importantly, you are not even aware of this. This is normal. You are a high school student. But the lack of awareness, not knowing what you don't even know is the biggest elimination reason when someone is hired. \n\n\nIn old times a 17 year old would go into business and learn as they did. If you can find someone to hire you through some networking, good for you. But just know what you did and know is just the tip of iceberg compared to the business problems. We rarely think.and talk about what algorithm is the best. We spend more time about the business approach at work. ",
        "First off, get someone to help you properly format your resume and remove all projects that don’t have a working demo. If you claim to have built it, I need to see it, so link directly to a live demo or video (short) and the repo. HR/Hiring managers will spend less than 60 seconds looking at your resume. If nothing catches their eye, the resume get’s trashed. If something does catch their eye you may get a few more minutes of their attention and you may get an interview.\n\nI’ve gone through the process of hiring many devs including ML focused devs. The big problem I see is that you claim to have experience, but I don’t see any *professional* or *team* experience on your resume. Personal, non-paid solo work is not experience, it’s education. Working on personal projects is fantastic to see and should be highlighted when the end result is impressive, but professional development is different, you’re part of a team, a cog in a machine. I’ve interviewed many self-taught solo developers who are very highly skilled, but have no idea how to work as part of a team on larger projects.\n\nThat being said (and maybe it was too harsh) you’re only 17, so I wouldn’t expect you to have professional experience. I also wouldn’t expect you to be applying for anything other than internships. Your experience should be more than enough to land you a great internship at 17. Get an internship to get some real experience under your belt. Prove you can hack it in a professional dev environment. The more impressive the company the better. THEN apply for entry level jobs with the internship as your experience and your best personal projects as proof of skills/passion.\n\nJust my two cents.",
        "Skill issue, everybody knows python",
        "why your github respositories dont have readme😭",
        "Can u understand the mechanism behind ML or just edit those open-source code package and change few lines? it's great achievement of u to create such projects (at an early age) tho",
        "Your knowledge is too specific bro. If you were doing the easy web stuff you'd have been hired by now.",
        "Lol who the f is this clown?",
        "Thanks for telling me about the layout.",
        "I started coding at 14 (wasn’t really into it but i learnt something the first year and completed a udemy course which has a certificate)",
        "I didn’t really care about the position applied for SWE ML engineer and AI but still can’t get a job, thanks for telling me.",
        "No not really am a high school student, but I completed a 100 days coding bootcamp which has a certification (dont know if the certification really helps)",
        "There is a link at the top, the code looks to be almost all copy and pasted",
        "Oh you're 17. So of course your resume is being thrown in the \"bs\" trash bin.",
        "Experience means real commercial experience not when you started learning. You can reword it to show you’ve been building things for years but you can’t say you have 3y experience",
        "I started coding at 14 too, and at that age developed a bunch of games that you can still find and play on newgrounds.com.\n\nThen got my PhD in ML at age 29, had a 2-day per week software engineering job throughout the whole 10 years of Bachelor’s, Master’s, and PhD, and also did a few full time 4-6 months internships during this period. \n\nNow I have been working full-time in an applied ML in industry for 7 years now, since graduating PhD.\n\nThe experience that I list on my resume is 7 Years of Experience. That is really only the full time employed part. Not the part time jobs, not the full time internships, and especially not the hobby coding that I have been doing since 14.\n\nIf I would go by your approach of counting experience I would list 24 years by now. Nobody counts experience like that.\n\nJust go to university, get your degree, and find some company in your university town where you can get some experience for a day or two per week.",
        "Jesus Christ, this is painful to read. You’re 17. Listen to the adults here pal",
        "That is irrelevant to an employer.\n\nParticipate in Kaggle or other such prestigious open competitions and show your skills there. Build something that is beyond your course exercises and are of actual practical value.",
        "i started coding at 11, i don't think i qualify for 13 years of experience (i would hope so). Experience is when you have a job point. That sucks cause you need experience in order to get experience...",
        "Most of the rejection tends to come from not applying to positions suited for you. Read the requirements and see if you’re resume is a fit for the position. Applying on a mass scale will lead to higher rate of rejection which isn’t good from a mental standpoint. Also, if you really like ML try to do research in the area. A lot of ML is just math and you probably should get a Master’s at least to qualify for a ML position. Try to author papers if you do go to a university and get started with a research group as soon as you can.",
        "I don’t see why they wouldn’t give you an internship level position. You should keep applying. In the mean time perhaps look for formal certification routes once you finish high school. \n\nAlso another factor here is maybe those companies have a policy against hiring people without a high school diploma / underage etc",
        "The links should direct straight to the project page.",
        "They used the word \"job\" and not \"internship\"\n\nThe resume greatly exaggerates the scale of what they have built, suggesting lack of understanding.\n\nThe person is on the right track but not yet there for someone to take a risk on them.",
        "Thanks really appreciate it!."
    ]
},
{
    "submission_id": "1gd8jl3",
    "title": "Rant: word-embedding is extremely poorly explained, virtually no two explanations are identical. This happens a lot in ML.",
    "selftext": "I am trying to re-learn Skip-Gram and CBOW. These are the foundations of NLP and LLM after all.\n\nI found all both to be terribly explained, but specifically Skip-Gram.\n\nIt is well-known that the original paper on Skip-Gram is [unintelligible](https://arxiv.org/pdf/1301.3781), with the main diagram completely misleading. They are training a neural network but in the paper has no description of weights, training algorithm, or even a loss function. It is not surprising because the paper involves Jeff Dean who is more concerned about protecting company secrets and botching or abandoning projects (MapReduce and Tensorflow anyone?)\n\nHowever, when I dug into literature online I was even more lost. Two of the more reliable references, one from an OpenAI researcher and another from a professor are virtually completely different.\n\n1. [https://www.kamperh.com/nlp817/notes/07\\_word\\_embeddings\\_notes.pdf](https://www.kamperh.com/nlp817/notes/07_word_embeddings_notes.pdf) (page 9)\n2. [https://lilianweng.github.io/posts/2017-10-15-word-embedding/](https://lilianweng.github.io/posts/2017-10-15-word-embedding/) Since Skip-Gram is explained this poorly, I don't have hope for CBOW either.\n\nI noticed that for some concepts this seems to happen a lot. There doesn't seem to be a clear end-to-end description of the system, from the data, to the model (forward propagation), to the objective, the loss function or the training method(backpropagation). Feel really bad for young people who are trying to get into these fields.",
    "created_utc": "2024-10-27T04:06:38",
    "num_comments": 27,
    "comments": [
        "IMO the skip gram paper was perfectly intelligible. I have no idea what you want when you ask them to describe their “weights”. They do describe the training algorithm. I don’t think loss functions are always provided explicitly when they can be easily inferred by most researchers; cross entropy loss is the default for these sorts of classification tasks. ",
        "Wow. I get that you're frustrated because what you're trying to learn probably requires a paradigm shift in how you look at the topic, and this is a difficult hurdle to surmount, but... just...\n\n> It is not surprising because the paper involves Jeff Dean who is more concerned about protecting company secrets and botching or abandoning projects (MapReduce and Tensorflow anyone?)\n\nDude. Chill. There's no call for a personal attack against anyone, and the projects you are trying to cite as some sort of derogatory example are just making you sound like a child. MapReduce and Tensorflow were two of the most impactful and influential technologies of the past decade. Neither was \"botched\" nor \"abandoned\", and if the skip-gram paper was unintelligible it wouldn't be recommended reading.\n\nYou are frustrated. It's coming out in your post, and it's not conducive to inviting support. It's the weekend. Have some coffee. Take a walk. Try to relax. Touch grass.",
        "Found [58 relevant code implementations](https://www.catalyzex.com/paper/arxiv:1301.3781/code) for \"Efficient Estimation of Word Representations in Vector Space\".\n\n[Ask the author(s) a question](https://www.catalyzex.com/paper/arxiv:1301.3781?autofocus=question) about the paper or code.\n\nIf you have code to share with the community, please add it [here](https://www.catalyzex.com/add_code?paper_url=https://arxiv.org/abs/1301.3781&title=Efficient+Estimation+of+Word+Representations+in+Vector+Space) 😊🙏\n\nCreate an alert for new code releases here [here](https://www.catalyzex.com/alerts/code/create?paper_url=https://arxiv.org/abs/1301.3781&paper_title=Efficient Estimation of Word Representations in Vector Space&paper_arxiv_id=1301.3781)\n\n--\n\nTo opt out from receiving code links, DM me.",
        "I mean this completely genuinely - you should be more humble, learn more, and stop assuming that just because you didn't understand something, it must be gibberish. ML is difficult for beginners to get into, but every field is difficult if you don't know much about it.",
        "Funnily enough this is probably one of the most well written and concise paper that i've read in a while",
        "You’re getting roasted in these comments (and I do get why) but to offer something helpful: \n\nGenerally academic papers are written for an audience of the authors’ field colleagues and peers. If you’re just getting into this, you’re not a peer to the Google researchers who developed this technique way back in 2013. That’s okay. I’ve worked in AI for nearly a decade and I’m not either. \n\nBut it does mean that when we read an academic paper like this, we may have to do some digging to understand the assumed prior knowledge we just don’t have. When I was first getting into it, same as you now, I’d be frustrated by the vagueness on how they trained the neural net. “Isn’t this supposed to be the official documentation of the method?”\n\nBut now that I’ve trained a hundred neural network models, I don’t need to pause the paper I’m reading to learn up on that as a stepping stone. By this point, I’d actually be a little annoyed(!) if I saw that in a paper on a new NLP method. “You don’t have to explain to me how neural net training works, that’s not why I’m reading your specific paper right now.” \n\nEnd takeaway: it’s okay that this paper contained a lot of stuff you didn’t know. It’s normal that this paper didnt even attempt to explain some of those things you didn’t know. You’re still gonna make it in ML, but this is how it goes",
        "Embeddings in one sentence: A model trained such that operations like these become possible: King - Man + Woman = Queen\n\nthank you for coming to my ted talk",
        "Tbh these are not the foundations anymore since nobody uses them for embeddings nowadays and they are not your typical neural networks so you can just skip them",
        "It's almost as if this is not trivial. Almost like this is something of an art still.\n\nRemain calm..it's early days yet. It will all be ok, trust me",
        "In addition to the good responses here, stanford has an excellent Deep Learning NLP course that's updated every year. I remember watching the version from 2017 or so. One lecture  covered skip-gram and it was excellent. They explained in detail the training process.\n\nAh, here it is: [https://www.youtube.com/watch?v=kEMJRjEdNzM&list=PLoROMvodv4rOhcuXMZkNm7j3fVwBBY42z&index=2](https://www.youtube.com/watch?v=kEMJRjEdNzM&list=PLoROMvodv4rOhcuXMZkNm7j3fVwBBY42z&index=2)",
        "for a basic understanding try statquest then go deeper from there",
        "I know somebody wrote a small , relatively beginner friendly pdf explaining embeddings, and I feel like they did a good job. I can try hunting it down if you want \n\nI feel like embeddings are something everyone takes for granted that they understand, but nobody really understands.",
        "If you have a word and use a vector to represent it, the next step would be to get the model to internalise a representation of it. The idea is the model self organizes the space to represent the words as vectors in the space. The space is the embedding space. It outputs embedding vectors, though really maybe it should be known as embedded vectors.  CBOW and skipgrams aren't really used, tell me if I'm wrong, and sliding window word2vecs aren't used as well, since transformers embed words and provide far more utility. In my mind you're in a much better place, because at the time there were far less examples or explainer media or even code to answer any questions.",
        "I think that generally papers are for learning the concepts like word embeddings, loss and optimizers, etc., and to learn actual applications you can use minimal code implementations such as [https://github.com/karpathy/minGPT](https://github.com/karpathy/minGPT)  \nThe authors of these papers focused on what was new and not the entire stack of their model. It's possible to plug embeddings into SciKit-Learn and do a classical classifier, perceptron, etc. for a smaller-scale project if you want to go back in time.",
        "Seems to be a trend in AI these. I assume most white papers are full of lies by default and are there just for publicity.",
        ">Generally academic papers are written for an audience of the authors’ field colleagues and peers\n\n100% this is the audience for papers",
        "That’s good for illustrating the concept of word-level embeddings, but embeddings are a much bigger and more general idea than that. The king queen example is helpful for gaining intuition, but it doesn’t directly translate to every type of embedding.\n\nEmbeddings are simply vectors in some space such that similar vectors encode similar meanings. In this explanation, “meaning” can refer to any number of things, including not just words but also individual characters, entire sentences, images, and almost any unit of unstructured data.",
        "Now explain how this property magically manifests as a consequence of a simple unsupervised objective that wasn't specifically engineered to behave this way ;)\n\nSpoiler: https://papers.nips.cc/paper/2014/file/feab05aa91085b7a8012516bc3533958-Paper.pdf",
        "Ohhhhh someone knows their stuff",
        "> It’s possible to plug embeddings into SciKit-Learn and do a classical classifier, perceptron, etc.\n\nCase in point, I just recently designed a system which extracts common topics from a database of texts based on embeddings. The texts are embedded and then the embeddings are clustered into groups which (in theory) correspond to topics.",
        "You seem to have misunderstood the concept of peer-reviewed journals.",
        "> but it doesn’t directly translate to every type of embedding.\n\nweirdly... it kinda does. This is why you can compose LoRAs linearly: if you unrolled the model into a vector, you could treat the whole thing as a kind of embedding, and so you can actually perform this kind of semantic algebra in the parameter space of the entire model, not just whatever you consider its embedding layer. Pretty sure the reason why is that modern DL training necessarily ends up producing objects with certain properties because the training objective acts as a measure which imparts a particular topology on the geometry of the parameter space: https://en.wikipedia.org/wiki/Reproducing_kernel_Hilbert_space",
        "Thanks, interesting paper. I’ll have to study it more closely, is an area I’m pretty interested in.",
        "I can't fully agree with the commenter's sentiment, but I didn't think \"white papers\" are typically intended for journal/conference/peer-review. They are just technical documents.\n\nAnd the most cited version of \"Efficient Estimation of Word Representations in\nVector Space\" *is* on ArXiv as a preprint. It was a workshop paper at ICLR 2013 (I think). It's reviews are less-than stellar https://openreview.net/forum?id=idpCdOWtqXd60\n\nSilver-lining: poor peer-reviews and rejects no longer mean okay-papers-with-a-good-idea will be lost. And if you have the fortune of working for a big research lab, people won't care in the end and will hear about it through other means.",
        "The peer review system is far from perfect. Plenty papers get submitted  with closed code bases and BS claims",
        "That’s a really interesting idea. I never thought about a model itself being a kind of embedding. I realize that’s stretching the analogy pretty damn far, but I do wonder if it’s meaningful to take the delta between two different model’s weights (same architecture, of course).\n\nThat said, when would anyone ever want or need to combine LoRA adaptors? Given a model X with adaptors Y and Z fine-tuned on different tasks. Why would it ever be beneficial to merge the adaptors? What downstream task would Y + Z actually correspond to? Can you explain with an example?",
        "People already do this all the time, it's extremely common in the text-to-image AI art community. LoRA's in that context are typically used as a parameter-efficient way to fine-tune a base model like SDXL on a concept, like a particular person or artistic style. Users of these models then treat the entire LoRA as if it were a text token in the prompt, up-down weighting the LoRA's contribution to the denoising process exactly like they would up-down weight concepts represented in natural language.\n\nAnother application here is in \"model forgetting\". If you train a model on a particular concept and subtract the difference in weights from the original model, you're effectively erasing the concept (or at least impeding the original's model ability to represent it)."
    ]
},
{
    "submission_id": "1gd6ta7",
    "title": "How to learn the mathematics for machine learning from scratch",
    "selftext": "I built some machine learning projects involving regression, neural networks , classification algorithms like KNN and clustering. I always end up using the built-in python modules and only have an high level overview of how the algorithm works and nothing much about the math in it. What books/youtube playlists are good for the mathematical aspects?",
    "created_utc": "2024-10-27T02:00:47",
    "num_comments": 16,
    "comments": [
        "This is what I recommend: CS 109 by Chris Piech, any linear algebra class by Gilbert Strang, ENGR 108 and EE 263 by Stephen Boyd. \n\nAfter these courses, you will know enough to learn the material in CS 229 and EE 364a. The last two classes will teach you all of the ML theory.\n\nYou should be able to find video lectures for all the classes above.",
        "[https://mml-book.github.io/book/mml-book.pdf](https://mml-book.github.io/book/mml-book.pdf)",
        "https://www.youtube.com/@3blue1brown",
        "Look up StatQuest on YouTube!",
        "Mathematics for Machine Learning and Data Science Specialization from Coursera by deep learning.ai",
        "* Mathematics for Machine Learning\n* Pattern Recognition and Machine Learning\n* The Mathematics of Machine Learning: Lectures on Supervised Methods and Beyond\n* Essential Math for Data Science: Take Control of Your Data with Fundamental Linear Algebra, Probability, and Statistics\n* Before Machine Learning Volume 1 - Linear Algebra for A.I: The fundamental mathematics for Data Science and Artificial Inteligence.  Here are some of the other [Best Machine Learning Mathematics books](https://codingvidya.com/machine-learning-mathematics-books/)",
        "What is your background?",
        "Whats EE364a?, I would suggest focusing on the matrix algebra in detail ig, basic ml math is only linear reg and knowing certain terms definitions.",
        "Seems like a great text. Thanks for the link",
        "Electronics and Communication Engineering",
        "That's Convex Optimization. A lot of the theory is useful for ML.",
        "how strong are your maths? ML is a lot of linear algebra.",
        "Pretty good because of my engineering background \nI did some coursework on linear algebra too",
        "if you feel confident in the math then you could jump into a course. I did andrew ngs free course.\n\nnvm looks like its not free anymore. :/",
        "I'll check it thankyou!",
        "it does look like theres still a bunch of videos and stuff but there was a whole free course on coursera. Really disappointing they took that away. :/"
    ]
},
{
    "submission_id": "1gd6mmt",
    "title": "Does anyone wanna study stanford's machine learning course or mentor to guide me ?",
    "selftext": "I'm about to start \" Stanford's machine learning course by Professor / Andrew \" course\n\nand I need someone who wants to study it with me or a volunteer mentor to give me his experience or guide me on the track\n\nand I'm open-minded to any advice from U guys thanks in advance",
    "created_utc": "2024-10-27T01:46:59",
    "num_comments": 8,
    "comments": [
        "Data scientist @ Mercedes-Benz here. I completed both the coursera version as well as the CS229 on YouTube quite a few years ago.\n\nI happen to offer paid ML coaching, if that is interesting to you guys.",
        "I would be interested to join you … what is the cost of the course",
        "I am interested in doing this with you as well!",
        "When do you start?",
        "Hey, I think the easiest way is to start building ML models of your own. I started by using model generation through [https://www.plexe.ai/](https://www.plexe.ai/) and iterated on the output and used it in Kaggle competitions",
        "Sorry buddy ,I'm looking for a volunteer.\n\"We're livingin the poor countries🥸🥸\"",
        "It's free course from stanford's University \"cs229\"\nIf u wanna a study mate dm me\nBut I need u to be serious",
        "DM me bro , I'm gonna start soon ,by the end of this week"
    ]
},
{
    "submission_id": "1gd4hks",
    "title": "Did I make an Ok buy? Dell 5820, 128GB Ram GTX A5500",
    "selftext": "Cost an arm and a leg (for me) but I'd like to get into AI/ML and run my own virtualization using ProxMox as the base OS. \n\nIdea is to virtualize the AI/ML with GPU passthrough and run file serving and other necessary services in containers. I think I could have done better for the price, but it was new, under warranty still. \n\nShould I rethink my purchase and build something from scratch? I have $3k budget. I will need the 10Gbe ports on this server. \n\nThanks! ",
    "created_utc": "2024-10-26T23:05:38",
    "num_comments": 1,
    "comments": [
        "what kind of AIML are you trying to run?, are you building from scratch? Are you doing ImageNet or are you trying to run an open source Llm"
    ]
},
{
    "submission_id": "1gd3xqf",
    "title": "I want to build a face anti spoofing detector. It will take a face depth image captured from true depth camera of iPhone as input and will say if the image is spoofed or not. On which data set I need to train the model. ",
    "selftext": "In my company they are using pre-trainned a core ml model for this task. It was trainned several years back and the dev who made it doesn't mention what type of data set he used for training the model. It works but it is not accurate. It takes a depth face image captured using iphone true depth camera as input and gives a dictionary with two keys \"Human\", \"Spoof\" and their values representing the respecitve confidence value. The model works but it is not accurate some times it fails. When I try to feed in a normal colour image the model detect it as spoofed. So I think the model was most probably trained on depth face images . Where can I find a data set for depth face images.",
    "created_utc": "2024-10-26T22:25:39",
    "num_comments": 0,
    "comments": []
},
{
    "submission_id": "1gd3p9a",
    "title": "\nHow can I determine if my F1-score and Learning Curve is performing well in a machine learning model?",
    "selftext": "https://preview.redd.it/p1153a25f8xd1.png?width=586&format=png&auto=webp&s=91f463e42b2908bc6d0a11e2f5f776951832d214\n\nhttps://preview.redd.it/pm4u2nq7f8xd1.png?width=911&format=png&auto=webp&s=a38de25c7e889d4325347689ba7c25ebb92b3f7a\n\n",
    "created_utc": "2024-10-26T22:08:55",
    "num_comments": 1,
    "comments": [
        "Depends."
    ]
},
{
    "submission_id": "1gd2knl",
    "title": "Master's in AI or SWE",
    "selftext": "I’m at a crucial point in my career and could use some advice. I have a background in mechatronic engineering, solid Python skills from university, and I’ve taken several machine learning courses. My long-term goal is to become a Machine Learning Engineer, and eventually, I’d love to pursue a Master’s at Georgia Tech or even a PhD.\n\nRight now, I’m working at a company that gives me some freedom to develop projects in AI, which has been fantastic for hands-on experience. My plan is to balance my time between these AI projects at work and studying one of the two master’s programs I’m considering (after my workday) and improving my English to complement my learning.\n\nRight now, I’m deciding between two master’s programs at Universidad de los Andes in Latin America:\n\n1. **Master’s in Artificial Intelligence**\n\n2. **Master’s in Software Engineering**\n\nHere’s a quick overview of the curriculum for each:\n\n**Master’s in Artificial Intelligence**\n\n**Semester 1: Foundations in AI**\n\n• **Cycle 1**: Introduction to Contemporary AI, Principles of Machine Learning\n\n• **Cycle 2**: AI Ethics, Mathematics for Machine Learning\n\n**Semester 2: Deep Dive in AI**\n\n• **Cycle 1**: Supervised Machine Learning, Unsupervised Machine Learning\n\n• **Cycle 2**: Reinforcement Learning, Deep Learning Techniques\n\n**Semester 3: Elective Component**\n\n• **Cycle 1 & 2**: Choose 4 electives, like Computer Vision, NLP, Reinforcement Learning, Embedded AI, Biomed AI, etc.\n\n**Semester 4: Elective + Practical Component**\n\n• **Cycle 1 & 2**: Project Development and Deployment + more electives in specialized AI topics\n\n**Master’s in Software Engineering**\n\n**Semester 1: Principles of Agile Software Engineering**\n\n• **Cycle 1**: Essentials of Agile Software Development, Software Design and Architecture\n\n• **Cycle 2**: Automated Testing, Software Engineering for Web Apps\n\n**Semester 2: Agile Development Competency**\n\n• **Cycle 1**: Agile Development Processes, Agile Software Architectures\n\n• **Cycle 2**: Software Engineering for Mobile Apps, Cloud Development\n\n**Semester 3: High-Performance Engineering Teams**\n\n• **Cycle 1**: Native Cloud App Development, User Experience Improvement\n\n• **Cycle 2**: DevOps, Project Management\n\n**Semester 4: Elite-Level Competency Preparation**\n\n• **Electives**: IoT, Data Analysis, API Development, Software Quality Metrics, Process Analytics, and more\n\n• **Final Project**: Applied project over two cycles\n\nI’m trying to decide which path would set me up best for an ML Engineering role. AI would dive deep into ML fundamentals and applications, but I wonder if the solid foundation in software engineering might also be beneficial. For anyone who’s been through something similar, what do you think is the better choice? Would the AI-specific focus provide a stronger start in ML Engineering, or would the broader software engineering skills be more versatile in the long run? \n\n&#x200B;\n\n[View Poll](https://www.reddit.com/poll/1gd2knl)",
    "created_utc": "2024-10-26T20:57:34",
    "num_comments": 0,
    "comments": []
},
{
    "submission_id": "1gd1m6g",
    "title": "How should I start if the goal is to create a simple NN chatbot?",
    "selftext": "I have a C/Python programming background but no ML/NN/AI training. Let's say my goal is to create a simple AI (not algorithmic) chatbot. How should I start? I.e. what resouces/books/tutorials that focus on practical learning first, and theory second?",
    "created_utc": "2024-10-26T19:59:34",
    "num_comments": 13,
    "comments": [
        "/r/LocalLLaMA \n\nIf you have a reasonably beefy computer you can run small language models locally. If you only have a crappy school laptop you can call a paid LLM API.",
        "If you want to create a chatbot, then use a pretrained model as others have suggested.\n\nAlthough it seems to me that you want to learn the fundamentals of language generation.\n\nYou say that you want resources that focus on \"practical learning\" first and \"theory\" second. I assume you mean the coding when you talk about \"practical learning\" - be aware that the coding part of ML is the easiest part by far. To truly understand ML, be able to make improvements to your models, requires a good understanding of the fundamentals. If you understand the fundamentals, then the \"practical learning\" becomes easy.\n\nYou will want to learn about transformers, specifically decoder-only models, which are the style that ChatGPT uses. To summarise simply, a transformer takes in \\`n\\` tokens (words) and outputs the (n+1) th token, based on what the model thinks is the most likely according to its own probability distribution. A transformer is essentially nothing but matrix multiplications with a few nonlinear functions like softmax thrown in here and there, as with most neural networks. \n\nA word of caution: **ML is not a field of computer science. Your knowledge of C / data structures and algorithms / cybersecurity will not help you at all with machine learning. At it's core, machine learning is essentially statistical modelling. The theory is all derived from linear algebra and statistics. ML is only erroneously called a field of CS because you need a computer to churn out the calculations. By the same logic I could argue that arithmetic is a field of CS because I need a computer to do calculations for me instead of me doing it by hand.**\n\nA very nice resource for getting started with deep learning, which goes deep into the theory whilst forcing you to create your own implementations, are the ARENA notebooks: [https://www.arena.education/](https://www.arena.education/)\n\nI warn you though, the notebooks are very hard and long, but if you persist you will learn a great deal.\n\nBest of luck with your journey my friend!",
        "What sort of chat bot? \n\nI would learn about neural networks first (coursera or MIT open course ware or even just clicking through random articles until you get the drift), then transformers, and make sure you are able to program stuff in PyTorch. \n\nOther resources depend on what your end goal is. ",
        "What do you mean by \"create\"? Do you want to rent a bunch of cloud gpus and train a language models from scratch yourself as a learning project? It's possible. Or do you want to run an LLM locally, and build an interface around it? Or just an interface slapped over a third party provider's REST endpoint, like Open AI? The last is the easiest option.",
        "Make a bot that can chat.",
        "Second this! There are many, many models. You should search for one that fits your requirements and then build on top of that.\nYou WON'T be training one, maybe fine-tune (but that goes beyond basic, but not hard). The easy way would be to get a \"runtime\" or sort of engine that will run the model, the most famous is llamacpp, due to been the first one, but ollama is also a top choise also and keeps things simple. It was a library or repository of models that you can run with a simple command. With that you should be done with the \"implementation\", you type in the terminal and the LLM answers back.\nIf you don't have a good computer or don't have an Nvidia gpu, you can start with small models like the recent Llama3.2, they have a version that is 1b and 3b I think (where \"b\" mean billions of parameter, ignoring quantizations, dont worry about that for now, that would mean 1Gb of RAM or VRAM for each billion parameter). These models due to been small can run fast on the CPU, but are kind of \"dumb\" for task that ChatGPT, dont expect having the same quality, but for experimenting is enough.\nAn extra would be to install OpenUI that allows you to have a web page that runs locally and is similar to ChatGPT. You \"link\" it or configure it to have access to Ollama and you can start chatting with it.",
        "I do have a reasonably modern rig with 6GB Nvidia RTX card and 16 GB RAM. Not much by AI standards, but enough to comfortably run Stable Diffusion and Google's Flan LL models. But is it enough for training in any capacity?",
        "What do you mean by \"create\"? Do you want to rent a bunch of cloud gpus and train a language models from scratch yourself as a learning project?\n\nThat is the ideal path that I would like to take. To train a very small model from scratch to get the foundational concepts down and learn possibilities and limitations of LLMs. Then likely switch to a pretrained model and finetune it to generate answers to some simple questions.\n\nUltimately, the goal is to have a small LLM answering FAQ questions from people on insta-msg boards like discord or Telegram. Yes, Im aware that there are ways of implementing the FAQ chatbot without LLM, but the goal is to learm LLM and its applications ;)",
        "I do have a reasonably modern rig with 6GB Nvidia RTX card and 16 GB RAM. Not much by AI standards, but enough to comfortably run GPT-2large, Flan-t5-large and Stable Diffusion 1.5 (not SDXL). But is it enough for training in any capacity?",
        ">But is it enough for training in any capacity?\n\nNot even remotely close. As of 2024, training a foundational language model costs millions of dollars worth hardware / compute. You need hundreds of gigabytes of VRAM equivalent, even terabytes (even then, the training takes weeks, and typically a team babying it). So you can't feasibly train a model from scratch locally. You can use pretrained models (see /r/localllama ) and there are solutions to use your system RAM + limited VRAM to run models of various sizes. You can also feasibly finetune such pretrained models on the limits of consumer hardware (like with a few 4090s etc.) but renting the hardware is what most people do for those cases.",
        "This comment can give you more concrete info about requeriments: https://www.reddit.com/r/MachineLearning/s/FktKdh7RmL\nYou would probably end up using a cloud service to do the training.\nUnless you want to train a really small model then you could probably do it there is a tutorial where they train from scratch GTP2.\nEdit:\nHere is the tutorial: https://youtu.be/l8pRSuU81PU\nBtw, how fast did sd1.5 ran in your pc?",
        "I kind of expected this, so guess it is not a shocker. Thanks! Any advice on what are the good online services offering compute power for AI training? And whether they exist at all for beginners/students proficiency?**:)**",
        "Ok, I understand. Training takes some heavy duty hardware, but fine-tuning can be done on consumer PCs with decent Nvidia card. Fine tuning should be able to achieve what I'm aiming to do for now. Thanks!\n\n  \nTo answer your question about StableDiffusion:\n\nI routinely run SD generation in batches of 8 with 1024x1024 resolution + ControlNet. This gives me 8 output pics in under 5 min (4:00-4:59). 1024x1024 image takes under a minute on average.\n\nIf I use -xformers option, then I can get 2048x1024 images without crashes even on 8GB Ram that I had before upgrading, but it takes a good 5 mins per picture minimum.\n\nSDXL models just plain run out of memory and crashes on 8GB generating 1024x1024 unless I use -lowram  along the -xformers and 12GB swap file on an SSD."
    ]
},
{
    "submission_id": "1gd02c0",
    "title": "N-gram probabilities in a Naive Bayes model",
    "selftext": "Hi there! I have some confusion regarding text classification with bigrams in a Naive Bayes model.\n\nLet's take this toy set:\n\nhttps://preview.redd.it/hqnx0jf8a7xd1.jpg?width=954&format=pjpg&auto=webp&s=336bdb1ede63deed3c49298a69ee89a401a44a7f\n\nAnd the test sentence \"good company\".\n\nIn determining the class assignment of \"good company\", which has no occurrences in our toy data set, am I right to assume that \"good company\", despite having unigrams that overlap with two entries in our data set, would look like this (with add-one smoothing):\n\nP(\"good company\"|positive) = (0+1)/(11+2) = 1/13\n\nP(\"good company\"|negative) = (0+1)/(10+2) = 1/12\n\nThe principle question is about the 0+1 part (0 for occurrence rate, 1 for smoothing). The existence of unigrams existing in the data set overlapping our test sentence, does not affect its probability, right? Thank you in advance!",
    "created_utc": "2024-10-26T18:27:42",
    "num_comments": 1,
    "comments": [
        "So, if you have a bigram-only model and you have a bigram that’s never been seen before/in training in either class, then that bigram itself basically doesn’t push towards one class or another and the decision/classification falls back to/is made basically by base rate (most common class) when you add in P(positive) vs P(negative) as part of calculating the classification. This is modified a bit by the smoothing, but overall the base rate (or prior) becomes the main thing pushing the decision in absence of data that has a signal. \n\nBut also, keep in mind that in general with N gram models you wouldn’t have a *bigram-only* model making classifications/decisions. Rather, an overall bigram model would also include info from the unigrams involved, or be balanced/combined with a separate unigram model somehow. (So an N gram classification where N is 2 would mean using unigrams AND bigrams together somehow to classify. More generally, N gram based models use all the grams from N down to 1 together: a trigram model uses 3, 2, and 1 grams together. Etc etc for higher N. The terminology might differ here; some would say the bigram model is the specifically and only bigram part, others would use the phrase to mean the bigram model plus the unigram model to be the complete thing that does the classification.) So in this case the unigram “good” would still push the model one way even if the bigram “good company” didn’t because it was unseen in training, and the class prediction would be a balance of the unigram AND bigram terms (and priors). How exactly to strike that balance would be a tunable (with cross validation) hyperparameter. The combination could happen in different ways, eg all the terms in one model or separate models combined afterwards, but you probably wouldn’t be classifying with *just* bigrams."
    ]
},
{
    "submission_id": "1gcz57n",
    "title": "Is probability calibration for probabilistic Neural Networks just another form of regularization? ",
    "selftext": "I've read that for neural networks that output a probability it's a good idea to calibrate the network's output probabilities on a separate subset of data (different from the validation set) because the probabilities may be over or under estimates of the true probability. Why does this happen in neural networks? Is this basically another form of regularization for overfitting? ",
    "created_utc": "2024-10-26T17:36:26",
    "num_comments": 2,
    "comments": [
        "from what i can tell, yeah, pretty much.  they call it calibration instead of regularization, probably because it's a single final step after a neural network is trained to rescale the classifier scores, while regularization is applied during training.  but it's still aimed at reducing overfitting, which is the underlying thing causing the confidences/classifier scores to be over estimates (usually over confident).  regularization is also more general, tho, not just on nn that output probability scores.",
        "I wish it was talked about more. I only recently learned about probability calibration but it seems really cool and useful for accuracy, interpretation, and a good remedy for overfitting."
    ]
},
{
    "submission_id": "1gcz4v5",
    "title": "Is an Information Science degree a good idea?",
    "selftext": "Hello everyone,\n\nPlanning to change my major and most of our NLP folks are in Information Science but some are also in CompSci. Is Information Science the degree to get for machine learning?\n\n",
    "created_utc": "2024-10-26T17:35:53",
    "num_comments": 4,
    "comments": [
        "No",
        "My brother did masters in information studies but he isn't a technical person at all. I can name one of his profs who does ML. I on the other hand study cognitive science, chose to specialize more in comp sci and right now I'm doing machine learning.",
        "do Data science or artificial intelligence degree",
        "My college has Information Science with a data science emphasis. They do have a degree in Statistics and Data Science."
    ]
},
{
    "submission_id": "1gcu41w",
    "title": "Web Development Wizardry: HTML & CSS Course For Beginners. | Free Udemy Coupons",
    "selftext": "",
    "created_utc": "2024-10-26T13:27:14",
    "num_comments": 0,
    "comments": []
},
{
    "submission_id": "1gcsxj2",
    "title": "Choose only one book- Machine Learning with PyTorch and Scikit-Learn: Develop machine learning and deep learning models with Python by Rashka ......OR....... Hands-On Machine Learning with Scikit-Learn, Keras, and Tensorflow: Concepts, Tools, and Techniques to Build Intelligent Systems by Geron",
    "selftext": "As per the title- if you were to only read one ML/Deep learning book between these two- which would cover more topics and be better for learning?\n\n# Machine Learning with PyTorch and Scikit-Learn: Develop machine learning and deep learning models with Python- by [Sebastian Raschka](https://www.amazon.co.uk/Sebastian-Raschka/e/B00J1DHHFS/ref=dp_byline_cont_book_1) \n\nOR\n\n# Hands-On Machine Learning with Scikit-Learn, Keras, and Tensorflow: Concepts, Tools, and Techniques to Build Intelligent Systems Paperback –by [Aurélien Géron](https://www.amazon.co.uk/s/ref=dp_byline_sr_book_1?ie=UTF8&field-author=Aur%C3%A9lien+G%C3%A9ron&text=Aur%C3%A9lien+G%C3%A9ron&sort=relevancerank&search-alias=books-uk)\n\nI know I'm going to choose one of these books to supplement my learning for ML & deep learning for production-level systems, but I'm unsure which one yet, and I also know that I will only read one, not both. Please explain your choice in the comments.\n\n[View Poll](https://www.reddit.com/poll/1gcsxj2)",
    "created_utc": "2024-10-26T12:32:01",
    "num_comments": 3,
    "comments": [
        "Adaptive Predictive Modeling by Kuhn and Johnson",
        "Voted for Tensorflow. It’s what I am familiar with, and I find its syntax/API more intuitive than PyTorch.",
        "Just noting that book was published in 2013, and uses R instead of Python."
    ]
},
{
    "submission_id": "1gcsehd",
    "title": "Current Choices for Open AI Voice Generation",
    "selftext": "Hi, I am starting to work on local voice generation und used tortoisse-tts, its ai-voice-cloning webui and discovered piper:\n\n[https://github.com/neonbjb/tortoise-tts](https://github.com/neonbjb/tortoise-tts)\n\n[https://github.com/JarodMica/ai-voice-cloning](https://github.com/JarodMica/ai-voice-cloning)  \n[https://github.com/rhasspy/piper](https://github.com/rhasspy/piper)\n\nSo far my user experience is, that I was not actually working on the voices, but working on fixing depnendencies and trying to find any kind of documentation. So evem though I got the tortoise-tts repos running and generated some files, my experience so far is kind of lackluster. I think this will improve as I solve problems and learn things, but the lack of a good documentation is a warning sign for me, that this could become a time sink.\n\nDoes anyone know, what is the current best local AI Voice Generation Tool, that is Open-Source?",
    "created_utc": "2024-10-26T12:07:10",
    "num_comments": 0,
    "comments": []
},
{
    "submission_id": "1gcs9q5",
    "title": "Looking for guidance on learning to build recommendation systems and matching algorithms for an E-commerce platform",
    "selftext": "Hey everyone, I hope y'all doing well!\n\nI'm trying to learn more about recommendation systems and matching algorithms as one of the people I know is working on an E-commerce website and is interested in having a recommendation system to recommend deals to users, and an algorithm to match buyers and sellers. I've never worked on such things before, I'm excited to learn but I feel a bit lost on where to start my research and learning journey.\n\nSo far, I have a solid background in Python and general machine learning. However, I'd love your insights on:\n\n1. Any books, courses, tutorials, or research papers that provide a good starting point for understanding recommendation systems? Ideally, books with hands-on example (like O'Reilly's book).\n2. What should I keep in mind when working on such features? Are there nuances or considerations that are often overlooked?\n3. Any tips on how I could design matching algorithms that optimize connections between buyers and sellers? If there are any frameworks or examples of similar models, I’d appreciate it.\n4. If you know of any specific case studies or projects related to recommendation systems and matching algorithms in e-commerce, please share! I learn best by studying real-world applications.\n\nThanks in advance for your advice! I'd love to hear about any experiences or recommendations you may have. ",
    "created_utc": "2024-10-26T12:01:11",
    "num_comments": 0,
    "comments": []
},
{
    "submission_id": "1gcqr7m",
    "title": "Hii can anyone help me with this repository ",
    "selftext": "https://github.com/kenil22/Cryptocoin_Future_Price_Prediciton. I have recently installed it but don't know what is the error the prediction value is not coming I will give something who can help me with this ❤️",
    "created_utc": "2024-10-26T10:52:16",
    "num_comments": 4,
    "comments": [
        "You need to be more specific for someone to help you. You need to share what code you’re running that produces the error, and you also need to share what the error says",
        "Can I DM you I can't send screenshot here",
        "I have installed in anaconda promt all the requirements and run it its run but when the part where will the graph 📉📈 come it is not coming this repo is not mine (making some project) what error is large I can't encounter that",
        "Please help me 😕"
    ]
},
{
    "submission_id": "1gcqaz0",
    "title": "Question on Generative AI job interviews, to those of you who have any idea,please share some insights ",
    "selftext": "What kinds of question should I expect in a new grad level Generative AI job interview, should I focus on NLP, CV, autoregressive models,autoencoders, GANs more and do I need to grind leetcode? By GenAI jobs I mean jobs focusing on LLMS or Computer vision or Machine learning..I'm new to these things,  also I can't find that much of structured guidelines on Internet about these stuff, any suggestion will be helpful..",
    "created_utc": "2024-10-26T10:31:26",
    "num_comments": 5,
    "comments": [
        "I don't know much either but I think some gen AI jobs are about knowing how to use already stablish LLM model's API, like GPT and llama.",
        "Expect deep dives into NLP, especially transformer architectures and attention mechanisms, given the prominence of LLMs. You should also be prepared to discuss autoregressive models, autoencoders, and GANs. While computer vision might come up, it's less central unless the role specifically involves image generation. As for LeetCode, it's not typically the focus for AI roles, but having solid coding skills is important. Some companies might include coding questions, so it's worth brushing up on basic algorithms and data structures.\n\nThe key is to demonstrate a strong grasp of the fundamentals and show enthusiasm for the field. Be ready to discuss recent papers, explain key concepts clearly, and share any projects you've worked on. If you're looking for structured guidance on navigating tricky interview questions, you might want to check out this [AI for interview questions](http://interviews.chat). I'm on the team that developed it as a tool to help people prepare for and ace job interviews, including in specialized fields like AI.",
        "Question regarding the AI tool you shared, for now I don't have a resume or anything, I just want to get comfortable with the question pattern and be really good at solving or answering those when I'll be actually going for interviews, if I just provide what topics I want the questions on as input, can it provide me that and evaluate my answers(approaches) ?",
        "The only required field to generate questions is the job description. Once you sign in, you’ll see it on the Start page. There are also optional fields for interview type and specific questions—use these to request topics if needed. Let me know how it works for you!",
        "So for strong coding skills I'll be needing leetcode , right ? Also, I'm just asking, do you have experiences with GenAI or Machine learning interviews recently? If yes, can you tell me what things remain their primary and secondary focus(coding rounds, GenAI concepts) ? Will definitely check the AI tool out.."
    ]
},
{
    "submission_id": "1gcpk5x",
    "title": "Navigating the Future: Are Tech Companies Doing Enough for Ethical AI?",
    "selftext": "Imagine a world where algorithms dictate crucial decisions in our lives—like who gets a loan or even who gets arrested. As AI technology integrates deeper into society, the importance of ethical guidelines has never been clearer. Yet, with numerous studies revealing biases in AI systems, one must wonder: Are tech companies genuinely committed to ethical AI practices?\n\nFrom Google’s Project Maven controversy to IBM’s AI Fairness 360 toolkit, the landscape is dotted with both successes and failures in AI ethics. While some organizations are stepping up to ensure transparency and accountability, others seem to prioritize innovation over ethical responsibility.\n\nWith emerging regulations like the EU’s AI Act aiming to hold companies accountable, it’s vital we scrutinize these efforts. Are we doing enough to challenge the status quo? How can we collectively push for a future where AI serves everyone fairly?\n\nJoin the conversation! What are your thoughts on the current AI ethics regulations? Are tech companies genuinely committed to ethical AI development?\n\nhttps://techrevolutiondaily.com/article/navigating-the-future-ai-ethics-algorithmic-responsibility-and-combating-bias-in-artificial-intelligence",
    "created_utc": "2024-10-26T09:57:38",
    "num_comments": 5,
    "comments": [
        "Obviously not",
        "Ethical AI has a problem because it's full of people that don't understand AI development. Or that want to regulate others instead of themselves.",
        "Ethical AI is something that governments need to regulate. Companies are not by themselves going to risk profit to protect society.\n\nBiases, incorrect decisions, incorrect data, etc. will leave consumers unprotected from abuse and damage. \n\nAI development and AI usage will be regulated by governments. Sadly, it probably will happen when damage to society becomes so obvious that it cannot be ignored anymore.",
        "I remember a lady form google was ringing bell for ethical AI and she got laughed at mocked for being “ woke” and got fired now every where you go it’s about ethics in AI",
        "The challenge is also that increased regulation isn’t necessarily a good thing. Right now there aren’t that many barriers to developing AI beyond getting chips. Chips are relatively available if you’ve got money (even if you use last generations chips - if you’re not worried about cost and energy efficiency).\n\nRegulation slows down innovation.\n\nNow agreed some of these companies aren’t the most ethical or safe. One important consideration is that whoever gets to AGI first dictates the future of the world basically, or at least shapes the future. So it comes down to: do you trust an American company who has some oversight by gov and responsibility to its western consumers to develop trustable and beneficial AI that will cause a net benefit of people of the world overall… or would you trust North Korea?"
    ]
},
{
    "submission_id": "1gcolmb",
    "title": "🌟 Cognitive AI in Social Robotics: Bridging Emotional Connections in Elderly Care 🌟",
    "selftext": "As our global population ages, the demand for innovative solutions in elderly care has never been more pressing. Traditional caregiving faces challenges like labor shortages and social isolation, but cognitive AI and social robots are stepping up to the plate! 🤖💖\n\nImagine a world where robots not only assist with daily tasks but also provide companionship and emotional support to seniors. From Japan's beloved therapy robot, Paro, to Israel's ElliQ, these machines are designed to foster genuine connections, improve mental health, and reduce anxiety among the elderly.\n\nHowever, integrating social robots into care settings isn't without its hurdles. Social acceptance, privacy concerns, and the ethical implications of replacing human interaction are critical discussions we need to have.\n\n🔍 **What's your take?** Should robots be a part of elderly care, acting as companions to fill the gap left by human interaction? Or should they remain as supplementary tools? Let's dive into this fascinating topic! Join the conversation on Reddit or Twitter and share your thoughts! 🌍✨\n\n  \n[https://techrevolutiondaily.com/article/cognitive-ai-for-social-robotics-fostering-emotional-connections-in-elderly-care](https://techrevolutiondaily.com/article/cognitive-ai-for-social-robotics-fostering-emotional-connections-in-elderly-care)",
    "created_utc": "2024-10-26T09:13:40",
    "num_comments": 1,
    "comments": [
        "Should robots be part of elderly care? According to Harvard Business School Working Paper 24-078 (2024), the answer is yes. In my view, a practical AI companion/robot requires three key features: (1) a language model tailored for elderly users, (2) an Amazon Echo-like voice interface, and (3) buy-in from elderly users. All of these are achievable. Surprisingly, I haven’t found a device with these features—this is a significant missed business opportunity.\""
    ]
},
{
    "submission_id": "1gcolbj",
    "title": "How Can I Get a Job as an ML, Computer Vision, or NLP Engineer Fresher? Need Help with My CV!",
    "selftext": "\n\n\n\nHi everyone!\n\nI’ve been actively applying for roles like Machine Learning Engineer, Computer Vision Engineer, and NLP Engineer, but I haven’t had much luck yet. I’d really appreciate any advice on how I can improve my chances as a fresher.\n\nHow do I stand out in applications for these roles? Are there specific skills, projects, or certifications that hiring managers look for?\nAlso, I’m not sure if my CV is aligned with what recruiters expect for these positions. I’ve included skills and projects related to ML, computer vision, and NLP, but I haven’t received any feedback so far.\n\nIf anyone has the time, I’d love a quick review of my CV or suggestions on how I can present my work and skills better.\n\nThanks a lot in advance! Your input would mean a lot to me.\n\nLink to my cv - https://imgur.com/a/4sh5ONf",
    "created_utc": "2024-10-26T09:13:16",
    "num_comments": 1,
    "comments": [
        "A couple pointers - Remove the profile section, you're not telling me anything. It's just a bunch of fluff. People often have 30 seconds/resume and you don't want them reading that. \n\n\nIn your experience, the first line isn't telling me anything. Tell me how it's delivering value to the org, or in other words, why the hiring manager would want to talk with you. This actually goes for all your bullet points. Your end goal isn't to improve accuracy, it's to do some thing to deliver value. \n\n\nAlso, I'm unsure where you're located, but going from school to MLE would need either a masters with internships in production ML, SW dev, and at the very least at an actual company. I wouldn't hire someone with your experience straight to a MLE role, even without the current saturated market. \n\n\nIf I were you I'd focus on Data Science-y roles like an Analyst, or do SW dev internship at a company, or junior SW dev role. You need experience at a company, and you need production SW experience, if your end goal is to be an MLE. And the way to MLE is likely climbing up through a company. "
    ]
},
{
    "submission_id": "1gcmjw4",
    "title": "Why do we teach so much probabilistic machine learning even though this is of limited use in preparing students to publish at top AI conferences?",
    "selftext": "EDIT: It seems like a lot of people interpret me saying \"probabilistic machine learning\" as \"machine learning that uses probabilities and random variables\". Probabilistic machine learning is an actual term referring to ML approaches which quantify uncertainty in either the model or the data. Below are a couple examples of PML courses so you know what I mean\n\n* [https://www.ucl.ac.uk/module-catalogue/modules/approximate-inference-and-learning-in-probabilistic-models-COMP0085](https://www.ucl.ac.uk/module-catalogue/modules/approximate-inference-and-learning-in-probabilistic-models-COMP0085)\n* [https://www.ucl.ac.uk/module-catalogue/modules/probabilistic-and-unsupervised-learning-COMP0086](https://www.ucl.ac.uk/module-catalogue/modules/probabilistic-and-unsupervised-learning-COMP0086)\n\nThe purpose of this discussion is to clear up some confusion that has been bubbling up within myself and many other young researchers in my field. For context my field is in representation learning / deep learning theory.\n\nThe TLDR version of this confusion is: How come universities focus so much on teaching probabilistic perspectives of ML even though probabilistic methods are of limited use in the most popular ML research paradigm (deep learning). Crudely put, if you take a look at ICLR spotlight, nobody frames things as an inference problem.\n\nThe way machine learning is taught seems to be broadly like this: \"Kids, machine learning is fundamentally an inference problem and about treating uncertainty as a first-class citizen. To call yourself a ML researcher you need to take 12 courses on things like Gaussian Processes, EM, belief propagation, Monte-Carlo, graphical models, etc. Oh also here's like 2 courses on kernel methods and deep learning in which inference doesn't feature at all, but don't worry that's just because we haven't found a way to frame them like that yet\".\n\nIs this a relic of previous times, or am I missing something huge? It seems like if someone's goal is to publish papers on deep learning, there is a whole cornucopia of applied math that seems infinitely more useful to prioritise over Bayesian stuff. Convex optimization, information theory, control theory, statistical physics, etc. Funnily, the coolest part of statistical physics approaches is that in certain limits you can find exact solutions where uncertainty completely disappears.\n\nThe point is that \"machine learning\" seems to carry strong connotations of inference in a teaching context, but much less so in a research context. Even on reddit everyone recommends textbooks like Murphy/Bishop/MacKay where probabilistic perspectives feature prominently. Kevin Murphy literally titled this 2023 podcast episode \"all of machine learning is now probabilistic\" ( [https://soundcloud.com/uclsound/all-of-machine-learning-is-now-probabilistic](https://soundcloud.com/uclsound/all-of-machine-learning-is-now-probabilistic) ). What is going on? Am I missing something? What can one do to understand the roots of this apparent disconnect?\n\nDisclaimer: This is based on my personal experience. For context I studied in the UK at Oxbridge, so this might be a university-specific or UK-specific thing. I am young (started PhD this month) so please forgive me if I am ignorant about the history of the field, or if I step on toes - but I do think this confusion warrants some attention as one of the many goals of the academy is to prepare future academics for success (arguably).",
    "created_utc": "2024-10-26T07:40:44",
    "num_comments": 52,
    "comments": [
        "You need to start somewhere and Uni's tend to focus on theory which if you study well you can then apply theory in many other scenarios. \n\nI am civil engineer and my uni taught us theory in more depth and limited teaching of application. The difference was that most of us were able to apply theory and learn what was needed on our own. In comparison to those that were taught application more over theory. They did really well at first, but when we were faced with unconventional scenarios they were not able to solve the problems. On the flip side those of us with a better understanding of theory were able to design and carry out the necessary tests and then design structures that were able to meet the onsite requirements.",
        "I work on applied ML at FAANG. I can’t image that our team would ever hire someone who does *not* have a deep understanding of that probabilistic view on machine learning.\n\nUnderstanding this stuff is way too fundamental and important to skip over. Even when applying deep learning.",
        "As someone with a very similar background to you, I'd argue that my probabilistic ML education is precisely what enabled me to move on to research. The vast majority of ML theory papers I have ever come across fundamentally take a probabilistic/statistical approach.\n\nFor starters, all generative models rely on probabilistic formulations. You assume you are given a set of i.i.d. samples from an unknown data distribution and try to approximate it by fitting a parametric model. The goal is to minimise some divergence between the true distribution and your approximation. If you use the KL divergence, you are doing VI (e.g. VAEs). If you use the Jensen-Sharon divergence, you are probably using GANs (assuming infinite capacity). The story is similar for NFs, EBMs, diffusion... it's all probabilistic.\n\nFurther, suppose you want to get some uncertainty quantification for your model. You don't have to be Bayesian about it, but that's certainly a common approach. In come Gaussian Processes, Bayesian Neural Nets, and approximate inference or MCMC because you are most likely dealing with intractable posterior calculations. Some people will argue that Bayes is not the right approach, e.g. because of calibration issues - so they do conformal prediction instead; another probabilistic method. And it goes on.\n\nSuppose you want to get generalisation error bounds. Even by using a test-set you are making an i.i.d. assumption about your data and using the tails of the Binomial distribution to bound the risk, so you are already working from a probabilistic perspective. You mentioned in your edit that you are not referring to this when you say 'probabilistic ML' so let's take it a step further. Consider PAC-Bayes, which is arguably the most prominent approach for generalisation bounds without a test set at the moment. The fundamental assumption in PAC-Bayes is the existence of a distribution over predictors that are sampled (Gibbs) or averaged (Bayes) in order to make predictions. Here, you are not just working with RVs and probabilities, but are actively placing a prior over classifiers and bounding the risk of the learned posterior (NB these are not exactly Bayesian priors and posteriors), with bounds involving a KL term between the two. I'd argue this fits your definition of probabilistic.\n\nSo far we have generative models, uncertainty quantification and generalisation bounds. I'd argue this covers a very large part of the papers in the major ML conferences, particularly ones focused on theory. And we haven't talked about things like bandits, neural ODE, optimal transport, MCMC theory etc.\n\nYou advocate for teaching convex optimization, information theory, control theory, statistical physics, etc. I would argue that optimisation and information theory do make their way into ML curricula in the UK and this includes Oxbridge (I mean, McKay himself was teaching the latter course at Cambridge). As for statistical physics, well, it doesn't make sense to teach it in a physics context to ML students does it? But the approaches used therein, e.g. energy and potential functions, hamiltonian and Langevin dynamics, the Fokker-Planck equation, are taught where appropriate. As for control theory, this is more typically seen in Engineering, but there are very prominent researchers with publications on ML and control (e.g. Giancarlo Ferrari Trecate, John Lygeros, Andreas Krause).\n\nYou said you read at Oxbridge. That means you would have been taught by the likes of Zoubin Ghahramani, Carl Rasmussen, and Rich Turner, if at Cambridge or Yarin Gal, Seth Flaxman and Michael Osbourne if at Oxford. These are some of the most renowned researchers in ML and they are all working on probabilistic machine learning. Sure, Oxford and Cambridge are Bayesian strongholds, but that's because Bayesian inference is clearly very prominent in the field.\n\nSo, what areas of ML research are you referring to where the framing of the problem is not probabilistic, even under the bonnet (in the sense that, say, a paper on neural scaling laws won't be doing probabilistic inference, but Seq2Seq, with which it is concerned is probabilistic)? What is the non-probabilistic cutting edge research you have seen at top conferences, which, mind you, is not done in the service of a probabilistic model in the downstream?",
        "Eh, autoregressive generation in an LLM is sampling from a  probability distribution that is factorized into probabilities over the next token and approaches like beam search or greedy sampling are all different approximations to finding the most likely answer. Heck, every time you  use a cross-entropy loss (which is the loss for pre-training LLMs) you have a probabilistic model over outcomes. Even all the human preference reinforcement learning is based on probability theory and some very simple Monte Carlo approximations.\n\nModern diffusion models work in the latent space of a VAE which came from a probabilistic perspective and even the diffusion process itself has some very deep roots in probabilitiy theory.\n\nWhile the practical implementations might not always make this obvious, pretty much all of the state-of-the-art techniques come from probabilistic fundamentals. Now I'd agree that some things like Gaussian processes and EM aren't quite as popular these days and you'll likely be fine without knowing all about them, though I'm sure there are some subfields where they are still used.",
        "I guess if *literally* your only goal is to publish academic papers... maybe?  MAYBE?\n\n... but ML is a field science; all the cutting-edge research is being done by big tech firms.  Academia is absolutely going to lag behind every single time until humanity runs out of things to publish on.\n\nML isn't just deep learning?  I mean, I guess you could be parsimonious to a fault and just completely eschew basic stuff they teach in every MBA program about how to run simple gaussian Monte Carlo simulation, but let me ask you — how in the world are you supposed to be able to benchmark a convolutional neural network when you don't understand how to build a simple statistical model??",
        "Deep learning is fancy and a really shiny tool.\n\n\nBut most companies need just logistic regression to get insights and start making better decisions. \n\n\nSo better be prepared, know your fundamentals and be happy fitting boosted random forest.",
        "Idk what your specific issue is, but lots of people can end up with an especially narrow image of what statistics implies. For example, you're talking about statistical physics as an alternative to a probabilistic based approach, and this seems confused- perhaps you're implying that statistics used in some other context or some other constraint is not statistics and probability, but the people using those tools probably disagree. \n\nPerhaps your note on statistical physics is the key here? It seems like you're saying, specifically with \"the coolest part of statistical physics approaches is that in certain limits you can find exact solutions where uncertainty completely disappears.\" The thing is, this kind of statistical treatment generally relies on a few tools that we can't reasonably manage in an ML context in the same way. We aren't likely to use a \"random variable\" taken to whatever infinite limit derives 0 uncertainty. \n\nTo me, it sounds like you're getting hung up on a specific researcher's specific notion of what \"real\" probability is, and using this to assume people who are using statistics or a statistical approach aren't also using other approaches you listed.",
        "I worked on ad ranking quite a lot. Even the basic task of click-through rate prediction is probabilistic - you are predicting a conditional mean of the 'Click' indicator. Conditional - because it's the mean given features. \n\nAnd if you want to add exploration, meaning, show ads which aren't necessarily the \"best\", but also those for which we cannot estimate the CTR due to lack of data, you also need the probabilistic view. One widespread approach is the upper confidence bound of your CTR prediction - confidence intervals for estimators are also 'probabilistic'. \n\nAnd ads is what makes money for the entire web industry. So you can't say it's something esoteric.",
        "This is like complaining about teaching little kids basic addition because they'll only need calculus in their day to day job.... if you can't do the basics, the more advanced work will be basically impossible.",
        "Framing the goal as \"getting the right output on average\" (probabilistic approach) or \"getting the right conclusion and sometimes not able to reach it\" (inference) sound pretty much the same to me.    \n\nThere is an alternative.   Framing the goal in a completely different way... in terms of \"when\".  See your interface with the world are your muscles.  And every muscle fiber is asking a single question.... \"when should I twitch?\"  The answer is always NOW at the right time.     \n\nSo what makes you think inference is the right way of framing the question?",
        "So I'm not that well-versed in deep learning, but isn't a probabilistic lens used in deep learning theory with things like tangent kernels? You mention information theory, and correct me if I'm wrong but doesn't information theory also view stuff as random variables?\n\nThough admittedly my roots are in statistics/economics, so I can't imagine a different perspective other than a probabilistic one since ML deals with random variables. What would other perspectives be, may I ask? Maybe a purely optimization treatment? decision theoretic?",
        "I work in more of a data platform/ml infrastructure role, so this may not be the perspective you are looking for, but I think I can understand where you are going with this. I describe ML to colleagues (that have even less of a clue than me) that an ML model is some properly configured function that you can plug new data into in order to guess at the answer to a hard problem. That “guessing” part is the probability aspect since you are never sure about the answer afaik. You trained this function on past input data and past desired outputs, so that it can only approximate future desired outputs. Even without time factoring into it, the word past can be swapped out for training and the word future can be swapped out for test.\n\nIs the probability math really needed for most people just working and using ML day to day? It’s mostly linear algebra and calculus to me. Most people that are using ML are not even training models, they are just using it like a tool. Even if you are training a new model, you probably aren’t developing a novel algorithm at most companies, in most roles. So, I agree with you, most people could skip most of the probability, and even though I like the math myself, it’s not needed for most people to get their jobs done that touch ML in some way even if they work with it every day.",
        "Imo it’s sort of the default model for prob inference, and also how the non-orthodox-language-diffusion-mainstream people e.g: yann lecun’s jepa, goal conditioned rl stuff which reformulate bellman’s model as probabilistic inference. Philosophically it’s grounded albeit pessimistic ie aixi while other models doesnt have that ground to build on",
        "Same reason you teach physicists calc 1 or medical doctors basic chemistry. It's basic fundamentals for the field.",
        "Can anyone tell me a resource from where I can learn probabilistic ML, because my current course doesn't cover it",
        "> For context my field is in representation learning / deep learning theory\n\nAKA generative models. as in, generative probabilistic models. your core interest is literally an application of probabilistic modeling.\n\nYou are completely missing the forest for the trees.",
        "Lol which ICLR have you been looking at. Have you not seen any cross entropy loss in a paper?\n\n\n\n\nThat loss came from probability+detection/estimation fundamentals. They didn't just invent it some day. It's a long standing concept. Entropy itself literally is defined in terms of probability.\n\n\nIn fact, a neural network in most cases is the least important thing (unless it is a paper which introduces a new architecture). They are just working alternatives to a function, somewhere in a mathematical model. \n\n\nIt seems you are reading something in the wrong sense.",
        "This is the right perspective.\n\nWhat one learns in school and its application really comes down to the skills needed on the margins. Many of us will not need the theory at all, some will transcend it and others will get stuck in its absence.",
        "What concepts do you test to check if someone has understanding of the probabilistic machine learning?",
        "I hear this view often (that PML is fundamental for deep learning) but could you elaborate as to why it's fundamental?\n\nI'm just confused about how most cutting-edge deep learning research seems to not integrate probabilistic / inference perspectives at all. Isn't this weird if probabilistic ML is somehow fundamental?\n\nTo be clear by probabilistic I mean giving uncertainty explicit treatment, e.g. Bayesian methods. Obviously basic probability and statistics pop up all the time.",
        "You've given the most thoughtful answer on this so far, so thank you.\n\nTo your points:\n\n\\- I completely agree that generative approaches, like VAEs/diffusion are explicitly formulated in a probabilistic way.  \n\\- I agree PAC-Bayes bounds (which originated in the 1990s) come from a probabilistic approach. But the same cannot be said of many of the tighter bounds which have emerged since, like compression-based ones from Arora et al.  \n\\- \"Even by using a test-set you are making an i.i.d. assumption about your data and using the tails of the Binomial distribution to bound the risk, so you are already working from a probabilistic perspective.\" - As I've explained a few times in this thread, this is not what people mean by \"Probabilistic Machine Learning\". These 2 UCL/Gatsby courses might give you a better idea of what people mean by PML: [https://www.ucl.ac.uk/module-catalogue/modules/probabilistic-and-unsupervised-learning-COMP0086](https://www.ucl.ac.uk/module-catalogue/modules/probabilistic-and-unsupervised-learning-COMP0086)  \n[https://www.ucl.ac.uk/module-catalogue/modules/approximate-inference-and-learning-in-probabilistic-models-COMP0085](https://www.ucl.ac.uk/module-catalogue/modules/approximate-inference-and-learning-in-probabilistic-models-COMP0085)\n\nOn areas of DL research where the framing of the problem is not probabilistic - I study representation learning (which is a huge field) and it would be easier to just point out the few niche lines of work that use probabilistic frameworks. It's possible I'm in an echo-chamber, but in the interest of getting to the bottom of this here's a small list of topics that don't involve PML: Any work concerning learning dynamics is seldom done in a probabilistic framework. The literature on NTKs and the ensuing line of work on lazy and rich learning have largely nothing to do with a probabilistic framework (although there are some less-cited tangential papers that try to link them to GPs). Literally ANY paper which contains the term \"teacher-student\". Any mainstream work on phenomena such as grokking and double descent. Any work on DNN interpretability. Work on geometry, e.g. manifolds, CKA, representation similarity. Any of the recent top-conference papers from Max Tegmark's group. Literally any work on self-supervised learning or contrastive learning. I invite you to visit ICLR or NeurIPS spotlight from last year and count how many deep learning papers use a probabilistic framing!\n\n\"What is the non-probabilistic cutting edge research you have seen at top conferences, which, **mind you, is not done in the service of a probabilistic model in the downstream?**\" - I don't think the bolded part here is the correct ring-fence. The topic here is about papers that could be completely framed and written without any knowledge of the probabilistic frameworks from those courses I linked. For example, the one paper I've published could definitely be written without any knowledge of the mathematics behind probabilistic models or inference. The same goes for the topics I mention above.\n\nYour comment that \"Oxford and Cambridge are Bayesian strongholds\" was particularly insightful for me because I had no idea of this reputation despite spending 3 years there. This would explain why it seemed like ML there had such an emphasis on probabilistic/Bayesian methods. In comparison, I've always been puzzled by how the machine learning curriculum at places like Stanford or Berkeley always seemed to me to have quite a different focus.",
        "Using cross-entropy loss is not probabilistic machine learning, and neither is a transformer logit that provides a point prediction of the probability of a token being 0.03. Probabilistic machine learning as a term doesn't refer to machine learning with probabilities and random variables. I've updated my post with an edit to be more clear about what I mean! Honestly it's not your fault though as PML is a pretty bad term and I sometimes wish they'd just rename it Bayesian Machine Learning (although that would be slightly inaccurate for some things).\n\nBut yes I definitely agree VAEs and diffusion models are from a probabilistic perspective since they literally do variational inference. That's definitely one area of research where you do see the probabilistic perspective reliably.",
        "I guess it's true literally my only goal is to publish academic papers lol.\n\nI am a bit puzzled about your point on benchmarking CNNs. Do you think this would at all use skills for building a GLM or regression or something? Not that I think people shouldn't learn basic statistics (all people should) but hypothetically it is completely true that someone who has never learned statistical modelling could train and run a CNN on ImageNet and use the results to produce the standard accuracy/loss benchmarking tables without any trouble.",
        "Exactly this.  Maybe we'll get to a day when Deep Learning dominates the M competitions in timeseries forecasting, but we're not there yet.",
        "Sorry if I was unclear, I edited my post. \"Probabilistic Machine Learning\" is an actual term used to denote a field focused on inference. It's also commonly the name of courses on the topic. It doesn't just mean using probabilities and random variables...",
        "Probabilistic ML is not \"the basics\", it seems like a specific approach/philosophy. The distinction is between between framing learning as an inference problem, or not. A lot of successful deep learning research does not frame it as an inference problem, and does not build on probabilistic ML at all. \n\nAlso, I'd say a lot of probabilistic ML is a lot more technically \"advanced\" than other approaches. It's certainly not simpler or easier.",
        "Sorry maybe I wasn't clear about terminology. By probabilistic machine learning I don't mean machine learning that uses probability theory somehow - all machine learning obviously uses probability theory and random variables.\n\nProbabilistic machine learning is a term usually used to describe the study of learning as the process of inference (i.e. quantifying uncertainty) as opposed to just outputting a prediction. And yes, the dominant perspective apparent from literature (at least in my opinion) seems to be that of optimisation. Typical deep learning classifiers would just tell you the probability of an image being a dog is x, and you do optimization to make x higher when the image is indeed a dog. This approach doesn't tell you anything about the uncertainty/distribution of either the prediction or the parameters used to make them.\n\nThere are lots of other perspectives on deep learning, like geometric, dynamical (which is where NTKs originate from), statistical physics, etc. All these perspectives are not mutually exclusive with each other, and in many cases discoveries from one perspective are often later reinterpreted by other researchers as equivalent to their perspective. But what I am getting at is that starting with a probabilistic/inference perspective to drive DL research just doesn't seem popular at all, which is a bit puzzling if probabilistic ML is somehow fundamental...",
        "Not sure what you're talking about as generative models are actually the only family of models NOT studied in representation learning.\n\nIf you're curious why (as I suspect you will be), you can start with section 6.3.3 of this textbook:\n\n[https://arxiv.org/pdf/2106.10165](https://arxiv.org/pdf/2106.10165)",
        "Bayes estimation, fuzzy logic, maximum likelihood estimation. \n\nLots of the statistical parts of ML is coupled with probability",
        "Ever heard of Variational Auto Encoders aka VAE? Pretty propabilistic and a big part of image diffusion models like stable diffusion",
        "Thank you for your reply and for explaining what you were referring to as non-probabilistic ML. This is perhaps where we will disagree, but it will certainly clear up what each of us means with that term. To be more specific, I view a lot if not most of the things you talked about as orthogonal to the probabilistic vs non-probabilistic question. \n\nI would say for most researchers in the field, probabilistic ML refers to the formulation of \"learning\" as a probabilistic inference problem. For example, while you can view training a NN as an optimisation problem, where you minimise the empirical loss Σ ||y\\_n - f(x\\_n ; θ)||\\_2\\^2  you can also view it probabilistically by assuming an unknown underlying distribution π(x, y), and fitting a parametric model p(y|x, θ) to π(y|x) by finding the parameter setting  θ\\* = argmin E\\_{π(x)} \\[KL\\[π(y|x) || p(y|x, θ)\\]\\] = argmax E\\_{π(x, y)}\\[log p(y|x, θ)\\], where the expectation is computed using Monte Carlo. For me, this is already probabilistic. Indeed, it's the first thing they teach you in a probabilistic ML course like the ones you linked and like the ones we've both taken. And that's what Murphy means when he says all of ML is probabilistic. That we would formulate this problem as density estimation. Of course, there are non-probabilistic approaches, like the SVM for example. But probabilistic approaches are dominant at the moment.\n\nOf course, the above is trivially probabilistic. If you instead want to have a distribution over weights, instead of just having the MLE or MAP so you can compute a predictive posterior, you need to work with the full Bayes. And as this is intractable for NNs, you have BNNs, approximate inference etc. That's where most of the stuff you describe comes in. Now you may well have an issue with Bayesian deep learning, and this approach certainly has its pros and cons. I happen to like it and have worked on Bayesian DL but I appreciate some of its weaknesses. For a bit of context since you liked the Oxbridge comment, BNNs essentially started through work of David McKay and Radford Neal in the 90s. Then, Zoubin's group at Cambridge really popularised the approach. He gave a keynote on the history at NeurIPS 2016 and there was also a panel discussion moderated by Neil Lawrence with Max Welling, Ian Goodfellow and Miguel Hernandez-Lobato among others that I'd recommend checking out.\n\nNow to your points, if we accept that a lot of the time we are doing probabilistic modelling as per the above, it is natural for us to try and understand the characteristics of our training routines, why they work so well, the guarantees we can get for learned models etc. This is where all the research on training dynamics, the generalisation properties of SGD, grokking and double descent phenomena all come in. Here, we are no longer talking about probabilistic or non-probabilistic approaches. We are past the modelling stage. We have a loss we are minimising, be it the L2 loss, the cross-entropy, the ELBO, and we are trying to understand what types of solutions our training method is finding, and how these relate to how well our models generalise (e.g. flat minima, sharpness). Naturally, this won't only involve probabilistic techniques. You'll employ spectral analysis, information geometry, operator theory, differential equations, you name it. But again, this is somehow orthogonal to the modelling question. Do I think it would be good to go into these things in more detail when teaching ML? Absolutely. There is a heavy focus on modelling at the moment. While there are courses like statistical/computational learning theory where you are taught things about generalisation like the properties of ERM, VC dimension, Rademacher complexity etc, it would be good to have more content on information theory, geometry and the lot.\n\nCompression is a very good example (glad to see this mentioned as the last paper I worked on was on compression-based bounds), because all you care about is that your learner uses a subset of the data in a certain way. Whether it's because you ran an SVM (non-probabilistic) and kept the support vectors or because you trained an SoD GP approximation (probabilistic) is irrelevant. The bound will hold regardless of how you modelled your problem as long as sample compression was achieved. The NTK is also a very interesting example, because in my view, while it's also orthogonal to the modelling question, it is a case where probabilistic inference is also used past the modelling stage in an attempt to explain generalisation (like you pointed out, because it relies on infinitely wide NNs aka GPs, which we both consider to be probabilistic models - while you call this a tangential point, I'd say this is probably the key point for people with a similar background to mine, which is heavy on GPs and Bayes).\n\nFor things like KD, I agree that certain parts of the literature generally don't use probabilistic formulations. If you ask me, I think it'd be good if everyone got behind probabilistic modelling because the way things are now, different parts of the literature can look very distinct, even if they have much in common. I haven't worked on KD but surely it could be somehow formulated as a KL minimisation between the true data distribution and the parametric approximation for the teacher and a further, second KL minimisation between the teacher's approximation and the student's? Might be talking nonsense here but you get the idea. Formulate it probabilistically somehow. Then we'll all be on the same page. Finally, I don't know enough about SSL but could it be a case of something I'd call generally probabilistic and you wouldn't (because it doesn't involve all the heavy-duty Bayesian stuff we've been going on about but still sets up an inference problem?)",
        "Autoregressive models that use sampling, as in transformer logits, are also probabilistic ML. They're solving a stochastic differential equation, and can be formulated as a markov chain. Being able to recognize this is part of the importance of learning that stuff.",
        "Any model whose output is a probability distribution is a probabilistic model",
        "Uh...  wat?\n\nI think you might be tunneling really hard on a single subdomain here?\n\nYeah, I guess you could just have someone who *only* does deep learning and neural nets and who never, ever crosses over into the domain of either classical regression or parametric statistics...  but that person is going to have a near-impossible time defending their dissertation if they can't use these classical methods to validate their findings!\n\nMaybe if the researcher is actually secretly doing a humanities PhD and doesn't need to worry about being able to reproduce their findings or prove that deep learning is actually any better than logit/probit regression?\n\nBack to your original point — I do indeed conceive of ML as primarily an inference problem; ultimately it's either all about classification or regression.  That's all it is — you are fundamentally either trying to categorize something or do curve fitting. \n\n>Is this a relic of previous times, or am I missing something huge? \n\nMost likely the latter; I think you may indeed be missing the random forest for the trees.\n\nHow are you supposed to publish papers if you can't prove that the model generalizes well and you can't use robust methodology to defend your work from those who want to pick it apart?  Academia is a blood sport, my friend, I hope you are ready for the big leagues — there are going to be people who are going to try to tear your hard work into ribbons and you had better know how to use classical statistics to prove that whatever you're trying to publish actually generalizes well and isn't just a fluke!",
        "Yeah, business like Zillow losing millons with forecasts powered by Facebook Prophet or similar deep learning models, instead of the classic and validated methods. Shiny thing syndrome.",
        "[deleted]",
        "I see. On your point, would teaching, say for regression, with maximizing a likelihood with gaussian errors count as a probabilistic approach (as opposed to teaching with minimizing L2 loss)? I can see why the probabilistic approach is useful for models where we can quantify uncertainty, but I think I see your point for DL especially, like DL uncertainty estimation is still an active research area.",
        "So you're saying a variational autoencoder is not a generative model. Because that's the representational space used for most modern image models, like stable diffusion. You're also saying a Generative Adversarial Network isn't a representation learning tool, despite being one of the most deeply studied classes of models in representation learning.\n\nI have a masters degree in math and statistics. I work as a senior subject matter expert in machine learning at a company that builds and rents out super computers specifically for AI workloads. I am who you are aspiring to be. Just to give you some context into who you are talking to and why maybe you might want to take my feedback seriously.\n\n> Considering the independence of the different components of the output in the posterior, a natural follow-up question is whether or not **Bayesian learning** ***at infinite width*** enables representation learning. Here, we will show decisively that it does not.\n\n> [...]\n\n> This lack of representation learning stems from the lack of interlayer correlation in the joint distribution `P(z_{l}, z_{l+1})` at infinite width, and thus it persists for all hidden layers with `l < L`. **This is another bad inductive bias of the infinite-width hypotheses**: regardless of the set of observations yA that we make, there’s no amount of new information that will allow the network to update its representations in the hidden layers `l < L`. This state of affairs is somewhat tragic as the whole point of having many layers – in fact, the main motivation given for deep learning on the whole – is the learning of complex representations in those hidden layers. As we will see next, we can solve this lack of representation learning – as well as the lack of wiring together in the output – by backing off the infinite-width limit and looking at finite-width effects.\n\nThe book you linked me is an excellent resource and I recommend it often myself. The section you linked me however is not relevant to this discussion. They're talking about a weakness of the theoretical tools that they are applying to analyze the learning dynamics of DNNs. Read the section yourself.\n\nTwo books by Christopher Bishop you might find interesting:\n\n* PRML, an absolute classic and must read - https://www.microsoft.com/en-us/research/uploads/prod/2006/01/Bishop-Pattern-Recognition-and-Machine-Learning-2006.pdf\n* Bishop's new intro DL textbook - https://www.bishopbook.com/",
        "But that does not require the advanced stats classes OP was referring to. An intermediate stats class on probability distributions would be sufficient to implement and tune a VAE",
        "Thanks for the fantastic reply! I appreciate your formulation of the issue here - it was quite insightful, especially the history of BDL which I was fuzzy about! Despite having been in the same room as Zoubin many times, I didn't know he was essentially the leader of the Bayesian world so to speak (I went to Cambridge).\n\nI completely agree with you that all the things typically studied in representation learning that I mentioned above do not necessarily offer frameworks that replace PML as a modelling perspective - in your words they are orthogonal.\n\nWhen it comes to the dichotomy in formalisms you mentioned, I do favour the formalism of the optimisation approach - but I am willing to chalk this up to personal preference, aesthetics and pragmatism. Even though we have different views on the preferred perspective/formalisms used to model learning, I think you and I are actually on the same page about what I mean. Much (if not most) of the research at top conferences these days do not use tools or frameworks from \"Probabilistic ML\", and the reason you've suggested (which I partially agree with) is that most of the high-visibility research is not about modelling at all - that's just not what the current zeitgeist seems to care for.\n\nMy point was just a very pragmatic and simple one about pedagogy, which you seem to understand! If you're a 21 year old undergrad and want to contribute to papers on grokking, learning dynamics and all this high-visibility stuff, it is of exceedingly little value to have a curriculum so focused on PML and modelling - 1 or 2 classes would suffice to pay respects to that (admittedly rich) intellectual lineage. PML does not even qualify as good \"background knowledge\" for these questions because they are, like you said, orthogonal. So as a result this type of undergrad ML curriculum is mostly orthogonal to current trends in research, which fails many budding researchers who might not want to write papers on probabilistic modelling.\n\nJust for context, if you take just the upper-level ML courses at Cambridge and count up all the weeks of \"probabilistic machine learning\" content (i.e. just what you call \"heavy-duty Bayesian stuff\"), it adds up to over 25 weeks. Kernels get about 3 weeks, and deep learning gets about 6 weeks if you're generous. Out of the 4 final year ML options, graphical models (Bayesian networks) are introduced in 2 of them! Even though I like how principled Bayesian/probabilistic methods are, I think it'd be a wild stretch to argue that this at all reflects the state of research, as in areas where talent/citations/funding is going.",
        "Do you do AI research? I think maybe we are in different fields because no paper I've ever read has used classical statistics to validate/defend findings in deep learning. And maybe it's a product of the times, but every PhD in my cohort plans to work exclusively on deep learning as opposed to classical methods. I mean just look at the papers getting accepted to top AI conferences...\n\nEven when people do empirical work benchmarking performance (which is unlikely for me as I do theory), it's as simple as showing that the mean accuracy of method A on some dataset across 10 random seeds, plus/minus standard deviation...that's what every paper does.",
        "Oh, please stop that crap about I cAn alrEAdY tEll yoUr phD isN't gOnnA gO Well. I don't endorse the PoW of the OP, but there's no need to make such generalizations.",
        "While I appreciate the personal attack at my abilities, I have one first-author paper in 2024 ICLR main track and another under review this year. I've already been doing research on deep learning theory for 2 years now. There's no need at all to be condescending. \n\nI feel like you may be misunderstanding my post. I already did well in an undergrad covering probabilistic ML, HMMs are not \"too much\". But do you think somehow HMMs are at all essential to understanding how deep learning works? I'm curious to hear your thoughts on this because I haven't heard that view at all, let alone seen HMMs brought up in this context.",
        "But would they be able to come up with the concept of a VAE de novo?",
        "OP here. So I actually do think VAEs and other generative models using variational inference definitely qualify as probabilistic. If you want to do theory with them or develop variants of them it is useful to know advanced statistical/probabilistic approaches. And these models are definitely a noticeable part of modern DL research, even if they aren't the most fashionable thing anymore. \n\nBut you hit the nail on the head (with your comment below) that the lion's share of work in areas typically considered the bread and butter of modern deep learning research, like vision models and attention mechanisms, definitely emerged without the framework of advanced probabilistic formalisms. You could certainly mathematically re-interpret them in a probabilistic way (and many have), but the crux is that the mainstream theory and development of these areas relied solely on the perspective of empirical risk minimization (i.e. optimization) without quantifying uncertainty about parameters or any of that jazz.  \n\nPut crudely, a lot of (if not most) of research in deep learning are objectively led by people with PhDs from the CS department who took maybe 2 classes in statistics and don't know the conjugate distributions. This reflects in research. I don't know why this is hard for some people to understand and shouldn't be a point of this discussion. The debate I care about is what this should mean for the undergraduate curriculum.",
        "There's a big difference between \"implement and tune a VAE\" and \"have a PhD level understanding of the theory of how and why they work and what properties they have and the learning dynamics they are subject to.\" Which yes, does involve learning deep probability topics like measure theory so you can understand how to characterize the geometry of the representational space, since it is a probability manifold.",
        "Typically, and this is true in all fields, you will perform better over time if you go deeper into the theory than the immediate layer of application used in your day to day work.\n\nIt’s how you find truly novel solutions and push innovation forward.",
        "It doesnt. But realistically speaking if you take that class you wont know most of it. Heck I cant remember anything from my Analysis II classes.  \nIf you passed such classes you likely at least have a good basis and a certain basic understanding of things which is just good to have. You need to build on something. Of course it is no guarantee that the person is gonna be an asset to the company but rather have someone with solid mathematical foundation over someone I already know is lacking these things.",
        "Thank you for the amazing discussion. This must be by far the most comprehensive discussion on ML topics I've had on the internet! I totally understand where you're coming from in pointing out how prominent probabilistic and Bayesian approaches are at Cambridge. This goes back to what we were saying about the focus and interests of many of the researchers and academics (Zoubin, Carl, Rich, Miguel, Neil, Carl Henrick, Ferenc, all hail from this school of ML).  \n\nThis is my own feeling, but precisely because ML research involves all these different areas of maths to properly study (and there are so many things we haven't talked about like for example geometric DL, which brings in theory from fields like differential geometry, algebraic topology, representation theory and category theory - another Cambridge special), I feel like it really belongs in the maths departments, rather than CS or Engineering. \n\nNo-one from CS or Engineering is going to have the necessary prerequisites on measure theory to do research in PAC-Bayes a la Catoni, know enough statistics to be able to derive asymptotic guarantees for HMC or SGLD, be well versed enough in functional analysis to properly study GPs using Mercer's theorem, Bochner's theorem, and the RKHS, or even know enough measure theoretic probability to understand the Kolmogorov extension theorem. My feeling is that a lot of the content we'd both like to see covered in advanced ML courses, would follow much more naturally from a rigorous mathematics education.",
        "No I do not.  So I guess it does occur to me that maybe I'm just old and have been out of academia for too long (I'm in the corporate world).\n\nEvery PhD program that I have been familiar with years ago always needed to have some basis in statistical inference and research methodologies.  It's just completely bonkers to me that you would be able to publish anything in a scientific journal without some sort of reference to statistical significance of findings, and it's bonkers to me that one would consider a regression problem without some sort of benchmark.\n\nIn timeseries I do for work, we have to benchmark things: against a moving average, against a linear model, against some other gradient boosted tree model's output — something.  In the corporate world I would expect a simple reference to an error metric would be enough, but in academia I can't get my head around how you could get published in a respectable journal without some sort of reference to the foundational statistical methods, because that's the basis of the scientific method itself.\n\nI get it that we're not dealing with parametric statistics here with deep learning, but you're still doing a regression/classification problem, and that at least implies a prediction interval if not a confidence interval — one would need to have *some* sort of reference to probability here!",
        "Most academic content is more focused on the theoretical first principles of machine learning. On the other hand, explainable or interpretable machine learning lacks academic rigour and reproducibility. This bias towards probabilistic models may allows for easier interoperability towards the transition to quantum-based ML in the near future.",
        "Thanks for the detailed comment, and I 100% agree with you that DL/ML education has an overemphasis on probability theory instead of hands on work with DL architectures. \n\nAnecdotally speaking, I did sort of an AI/science Msc instead of ML, and when I came to the workplace and even compared to PhD students, they had a shocking lack of ability to implement the latest models in Pytorch. I’ve seen a lot of undergraduate courses that rebranded as AI,  but are actually just reusing old material from CS/stats and selling it to students. Those students have had no chance to implement an attention mechanism or understand any of the latest papers, not at all preparing them for AI research.\n\nMany of the most cited publications have just adapted DL architectures slightly without a thought of the probabilistic theory side of things. For instance Attention, ViT, BERT, Masked autoencoder etc, are the most highly cited but fails to mention any involvement of probabilistic models.",
        "But 90% of DL is CNN, Attention and autoencoder papers, which are actually just a bunch of matrix multiplications, rarely involving probabilistic elements into the publications. Maybe for diffusion, there’s a bit more advanced stats but I would still say (belief propagation, EM) etc are not used. \n\nWhat would be an example of a truly novel solution that stemmed from probabilistic ML?"
    ]
},
{
    "submission_id": "1gcm0fa",
    "title": "In what sequence should I read these books ?",
    "selftext": "",
    "created_utc": "2024-10-26T07:14:46",
    "num_comments": 40,
    "comments": [
        "**1. For the Absolute Beginner (Knows Nothing: No Python, No Math, Nada)**\n\n* **Step 1: Learn Basic Python Programming** Before anything, you’ll need Python because it’s the backbone of ML. Grab **\"Learning Python, 5th Edition\"** and take it one page at a time. Python is easy to pick up, especially if you follow along and try the code examples yourself. Don’t rush this—get comfortable with the syntax, basic data types, loops, and functions. This foundation will pay off in every ML project you tackle later.\n* **Step 2: Machine Learning Concepts** Once you’re decent at Python, start with **\"An Introduction to Statistical Learning\" (ISL)**. This book is ML 101 without too much math—it explains the algorithms and gives you intuition behind them. When I was starting, ISL was a lifesaver. Don’t worry if it seems like a lot; just try to understand the concepts, like regression, classification, and clustering. This will be your first taste of what ML is all about.\n* **Step 3: Get Practical with Hands-On ML** After you’re comfortable with the basics, move on to **\"Hands-On Machine Learning with Scikit-Learn, Keras, and TensorFlow\"** by Aurélien Géron. This is where things get real—you’ll start coding ML algorithms in Python, working with actual datasets, and experimenting with libraries like Scikit-Learn. Trust me, this book will take you from theory to practical skills that’ll get you building cool projects. When I started, going through this book helped me see the “magic” of ML come to life.\n* **Step 4: Deep Learning** When you’re ready to take it up a notch, **\"Deep Learning for Coders with fastai and PyTorch\"** by Howard Gugger is an awesome next step. It’s focused on deep learning, using PyTorch and fastai, and teaches you everything from building basic neural networks to training image classifiers. I found this book really motivating because it gives you immediate results with real-world applications, even if you don’t fully understand the complex math behind it yet.",
        "Do you mind to share those books, I’m hard in buying 📑 them",
        "Depends on many things: How familiar are you with ML, calculus, linear algebra and probability theory? What is your goal: to understand the inner workings and theory of ML and DL models, or a more hands on approach?\n\n\nI would consider going:\n\n\nBishop -> Goodfellow -> hands on ML\n\nFor me, its one of the best learning pathways, but its really math-heavy on the First two books. Bishop has some incredible background chapters on linear algebra and probability as well\n\nBut the main thing is: stick to one book and read It . One thing many beginners do is get Lost on a bunch of possible paths and dont get started on any thing",
        "In my personal experience I found little to no value in reading books. Thats just my experience though and I had university lectures on most topics. I usually recommend university lectures as well instead of books, many of them are free and have lots of nice exercises alongside them. To give an example I can really recomment cs231n lecture from standord (Deep learning for computer vision)  \nThe only book I actually ever read through was Nielsens Neural Networks and Deep learning book. It was still a decent read but I just prefer the lecture structure and presentations.",
        "Alphabetical",
        "If learning Python is on that list you should start there",
        "Idk the sequence but pls give a link to these books",
        "From beginning to end.",
        "This is a lot of books. You probably need only 3. Cover math, Bayesian statistics and machine learning. See which ones you like first. Reading a book is not enough and this is why you'll probably not be able to do them all. You need to read them with a pencil and paper, working through the math as you go and doing the exercises. The ones with the code you should do similarly, typing the code yourself. If you do it like this, quantity will matter much less.\n\nI definitely recommend Murphy from that list (Machine Learning: A probabilistic perspective). Going through that book thoroughly will require a couple of weeks/months alone. If you do it once, you'll be set and nothing will scare you. There is a new edition split into 2 books now, check them out.",
        "I strongly reccomend you get a CS undergrad",
        "If you have zero ML, start with \"Applied Predictive Modeling\". Then read some of these. \"Introduction to Statistical Learning\" and \"Deep Learning\" are mainstays. I can't speak on the rest.",
        "I suggest you try reading them in all possible orders and report which worked the best",
        "I would suggest a topic+Implementation oriented approach that allows you to follow any source to implement them. Don't get me wrong I love those books but I haven't read them front to back as it's unnecessary for my current research/projects.\n\nLearn these in sequence (some can be done simultaneously) (reply if you want resources)\n\nLearn these first:\nPython and Math (Linear Algebra, Probability theory, Calculus)\n\nThen the classical methods (basically optimization)\n1. SVM (support vector machines) and PCA (Principal Component Analysis) for classification\n2. Curve fitting (both gradient based and Bayesian) regression\n3. Fuzzy inference system\n\nThen deep learning (d2l.ai is a gem btw)\n1. MLP (Multilayer perception) aka Neural Net\n2. CNN (Convolutional Neural Nets)\n3. Sequence models (Recurrent Neural Nets, Long short term memory Nets)\n\nAfter this I don't think it matters what sequence you follow anymore. Let your interests guide you. but some topics that I think are important in general are,\n1. Autoencoders\n2. Transformers \n3. Casual inference \n4. Graph based models\n5. Mixture models\n6. SOM (Self organizing maps)\n\nLearn Pandas on a need to know basis:\nLearn Pandas as you go. Nobody really knows when exactly you'll need to master pandas and Numpy but you will at some point need it extensively, though not at the beginning. You'll only need some simple functions at first so maybe take a short crash course or just read the quick start docs. When the time comes when you feel like you should start taking an in depth view of pandas (in depth view of pandas just means you read the user guide front to back, which is not long lol), start doing that.\n\nI really hope this doesn't overwhelm you. This list of topics should get you to a point where you can just look at a book/paper/video, skim and say \"Hey I already know this\". \n\nAbstract thinking is key:\nThis field is fully abstract so be prepared and comfortable to think in the abstract all the time and accept it when you can't. Maybe it'll click at a later point of your education.\n\nPatience is key(I think you know this already):\nPeople think they can just hop into this field in a few months and understand everything. That's only possible for mathematicians and physicists. If you're not one of these two, be prepared to be in the long game. Have patience. Every line of math and code has some amount of thought behind it so it takes time.\n\nThe last two things are two of many reasons I love this field!",
        "Just follow the sequence",
        "Wow! Awesome collection. Can you share the links? :)",
        "Did you find these for free online or did you pay for them? Id be interested in getting some.",
        "You won’t learn ML through reading books. Books are a point of reference to use while doing an application, not a starting point.",
        "**2. For the Python-Savvy Beginner (Knows Python, Might or Might Not Want Math)**\n\n# A. If You Don’t Want to Dive into Math (Focus on Application)\n\n* **Step 1: Learn Core ML Concepts** Start with **\"An Introduction to Statistical Learning\"** (again, ISL) to get a solid understanding of ML basics. Since you already know Python, you can dive straight into the coding examples without struggling too much with syntax. ISL will lay down the foundations without overwhelming you with math.\n* **Step 2: Practical ML with Python** Move on to **\"Hands-On Machine Learning with Scikit-Learn, Keras, and TensorFlow\"**. This book is heavy on hands-on work and focuses on coding and practical applications, which is perfect if you’re trying to skip the deep math. It’ll get you comfortable working with libraries like Scikit-Learn, building models, evaluating them, and doing some hyperparameter tuning—all essential skills in ML.\n* **Step 3: Deep Learning,** After you’re comfortable with basic ML algorithms, head into **\"Deep Learning for Coders with fastai and PyTorch\"**. This book takes a code-first approach to deep learning, so you’ll be building models and seeing results right away. It’s satisfying and keeps you from getting bogged down in complex theory, which is a huge plus if math isn’t your thing.\n\n# B. If You Want to Understand the Underlying Math (Go Deeper)\n\n* **Step 1: Core ML Concepts with Some Math** Start with **\"An Introduction to Statistical Learning\"** (ISL) to get the fundamentals, but don’t stop there. ISL keeps the math light, but it’s a good stepping stone to get used to the language of ML and statistics before diving deeper.\n* **Step 2: Mathematics for Machine Learning** Once you’re ready, tackle **\"Mathematics for Machine Learning\"**. This book covers linear algebra, calculus, probability—all the math you need to fully understand ML algorithms. It’s not an easy read, but if you’re committed, it’ll give you a serious edge. This book helped me finally understand *why* algorithms work the way they do, not just *how* to implement them.\n* **Step 3: Deep Learning Theory** Now, if you’re still curious about the math behind neural networks, **\"Deep Learning\" by Ian Goodfellow, Yoshua Bengio, and Aaron Courville** is the ultimate deep learning bible. It’s not for the faint of heart, but it goes deep into the theory, from basics to advanced topics like GANs and reinforcement learning. Reading this took me to the next level, though it’s a commitment.\n* **Step 4: Advanced Probabilistic ML** Finally, if you’re interested in a probabilistic approach to ML, **\"ML Machine Learning: A Probabilistic Perspective\"** is a masterpiece. It’s highly math-heavy and explores ML algorithms through the lens of probability. It’s not necessary for everyone, but if you’re considering a career in ML research or data science, this book will give you a deep understanding of probabilistic models and Bayesian methods.\n\n**Don’t Rush the Math if It’s Not Your Thing**: Math in ML is useful but only necessary if you’re diving into research or developing new algorithms. For most applications, understanding the basics and how to apply models is more than enough. But if you’re serious about being a top-level ML engineer or researcher, bite the bullet and dive into the math.\n\nWhen I first dove into ML, I was equally clueless about where to start. I began with the basics of Python, slowly easing into programming with all those “Hello World” and beginner exercises. It took a bit to get the syntax down, but once I was comfortable, I moved to machine learning itself, starting with ISL (Introduction to Statistical Learning), exactly as I suggested to you. ISL was a game-changer in making the basics click without overwhelming me with equations I wasn’t ready for.\n\nAfter that, I knew I wanted to actually build stuff, so I picked up **Hands-On Machine Learning**. This book was all about practical implementation, and once I got through it, I felt like I could really *do* ML. Sure, I wasn’t a math wizard, but I understood how to use Scikit-Learn and Keras to build projects. Around this point, I took on small projects like basic image classifiers and recommender systems. That was my first taste of applying ML to real-world problems, and it made all the reading feel worth it.\n\nThen came the turning point. I wanted to understand the \"why\" behind it all—the math. I went into **Mathematics for Machine Learning** and later, **Deep Learning** by Ian Goodfellow. It was hard, but taking it slow made the math feel approachable. This helped me realize that, while math isn’t essential for every ML engineer, it definitely opens doors if you want to dig deeper.\n\nAnd even now, I keep learning, hitting roadblocks, and picking up new skills. these things started making sense only after months of trial, error, and persistence.",
        "thanks for this detailed guide",
        "https://libgen.is/",
        "Great advice ... I'd personally do the same\n\nThough I'd probably start with python for data analysis then move to bishop followed by goodfellow then hands on ML",
        "Not able to understand the Bishop book much. Where do I learn the math?",
        "    from random import shuffle\n\n    books = […]\n    shuffle(books)\n\n    for book in books:\n        read(book)",
        "Just search the books and download the pdf file",
        "I want those resources that u said",
        "search the books pdfs and download them",
        "from my experience you can find them easily by googling them lol",
        "If math is your thing I suggest \"Understanding Machine Learning: From Theory to Algorithms\" by Shai Shalev-Shwartz and Shai Ben-David. Wish I had found sooner about It.",
        "This comment should be pinned in the sub or something.",
        "Thank you for this, this is super useful for absolute beginners like me.",
        "Thanks for writing this...it was really helpful.",
        "resourceful comment",
        "Great choice as well!",
        "Depends on what part your having issues with. The First two chapters give a brief overview of some math knowledge, but If you need more background:\n\n\nCalculus: James Stewart vol 1 for basic math and limits. I think this volume covers single variate derivatives. Vol 2 teaches multi variate derivatives which is helpful to understand How models learn\n\nLinear algebra: Gilbert strang couese and book. The guy is brilliant.\n\nProbability: from the top of my Head, I cant remember much good intro books in English, Sorry. Im sure someone in this thread might be able to help or maybe theres a similar post here in the sub",
        "There is an awesome Youtube channel, Sina Tootoonian, that goes through the Bishop book line by line. Highly recommended.",
        "thanks man for this...",
        "**MOST RESOURCEFUL COMMENT**",
        "I was having a tough time with matrix calculus. Thank you so much.",
        "Great recommendation. Thanks a ton!",
        "Hmmm in that case i think the last chapters of vol2 or vol3 got you covered (for example: calculating the jacobian)"
    ]
},
{
    "submission_id": "1gcli3d",
    "title": "Ultralytics YOLO Helper Library for Beginners",
    "selftext": "Starting with pose recognition for beginners in Computer vision is a bit of a hazel, Specially with Ultralytics YOLO Pose models, which are really good compared to mediapipe in stable pose recognition but at the same time due to lack of documentation a really hard problem to tackle with. I have created this library yolozone to provide the user with helper functions which I believe will accelerate the learning curve and provide much easier access to\n\n* Find Landmarks\n* Calculate Angles between landmarks\n* Calculate Distances between landmarks\n* Calculate angles between lines\n\nETC... I'm planning on adding more features in the future but at the same time I need the help of you guys to determine what I should add more\n\n[https://pypi.org/project/yolozone/](https://pypi.org/project/yolozone/)  \n[https://github.com/nushankodikara/yolozone](https://github.com/nushankodikara/yolozone)",
    "created_utc": "2024-10-26T06:49:15",
    "num_comments": 0,
    "comments": []
},
{
    "submission_id": "1gck709",
    "title": "How to know if the data is ready for an ML algorithm",
    "selftext": "How!?\nAnd how ML engineers or a data engineer usually get the date ready is it just some panadas code or they go for SQL operations \nAnd what if the data is too complex \n\nIf thers is a course or an article or YouTube video that can help me in this manner i will be thankful ",
    "created_utc": "2024-10-26T05:41:07",
    "num_comments": 6,
    "comments": [
        "EDA",
        ">is it just some panadas code\n\nSometimes \n\n>they go for SQL operations\n\nMost likely \n\n>what if the data is too complex \n\nMore SQL and python codes\n\n>How to know if the data is ready for an ML algorithm\n\nFirst you identify why you want to build a model. Then you choose a model that can solve your problem. Next you transform the data into the supposed format that the model is designed to read as input.",
        "What is EDA",
        "Exactly. Starting off it can seem really complex, but as long as it's in a numerical format the model can take then you're golden. If the data doesn't make sense to predict what you want, its not needed.",
        "So can i say SQL and Python's Libraries are enough tools to do all the work for the data?",
        "Exploratory Data Analysis"
    ]
},
{
    "submission_id": "1gcj39x",
    "title": "Which model to choose for a fluent conversation with finetuning",
    "selftext": "Hey, \nI'm doing a bachelor's in Data science and AI. Now we need to do a project with AI, and my idea was to finetune a LLM on the swabian dialect (my dialect) from Germany. \n\nI just looked a bit around, but I'm not sure which size of llama or Mistral I should use. It should not be that expensive to run and train. I thought about llama 3.1 8b with Lora or qlora. I also tested the new llama with 1b, but it was too weak go get a good conversation. \n\nAny ideas or tips which model and techniques to use? \n\nI want to have a fluent conversation with the model. \n\nThanks Guys ",
    "created_utc": "2024-10-26T04:37:07",
    "num_comments": 0,
    "comments": []
},
{
    "submission_id": "1gcixxu",
    "title": "Re-Envision your Sales Operations with Agentic AI",
    "selftext": "",
    "created_utc": "2024-10-26T04:27:25",
    "num_comments": 0,
    "comments": []
},
{
    "submission_id": "1gciol4",
    "title": "Simple Pytorch network does not learn",
    "selftext": "I make a simple even odd classifier in pytorch. The neural net is basically sin(w\\*x+b). If I initialize w to 1.5 which is close to pi/2, and b to 0, the NN should move the value of w close to pi/2 and b to stay at 0. I.e. the networks should be close to sin(pi/2\\*x) which is exactly an even odd classifier for integet (casted to float) values of x. However, the network does not learn, the weight does not move, and the loss does not decrease.\n\nCan anyone help me figure out whats wrong?\n\n    # %%\n    import torch\n    import numpy as np\n    import pandas as pd\n    \n    # %%\n    \n    # Generate data and scale inputs\n    def generate_data(size):\n        x = np.random.randint(0, 100000, size)  # Smaller range for better visualization\n        return x.astype(float), (x % 2).astype(float)\n    \n    # %%\n    # Generate datasets\n    train_x, train_y = generate_data(1000)\n    val_x, val_y = generate_data(1000)\n    \n    # Convert to tensors\n    train_x = torch.tensor(train_x, dtype=torch.float32).reshape(-1, 1)\n    train_y = torch.tensor(train_y, dtype=torch.float32).reshape(-1, 1)\n    val_x = torch.tensor(val_x, dtype=torch.float32).reshape(-1, 1)\n    val_y = torch.tensor(val_y, dtype=torch.float32).reshape(-1, 1)\n    \n    # %%\n    class Net(torch.nn.Module):\n        def __init__(self):\n            super().__init__()\n            self.fc1 = torch.nn.Linear(1, 1)\n            # Initialize close to the theoretical solution\n            with torch.no_grad():\n                self.fc1.weight.data.fill_(1.5)  # Close to π/2 ≈ 1.57\n                self.fc1.bias.data.fill_(0.0)\n    \n        def forward(self, x):\n            x = self.fc1(x)\n            return torch.sin(x)\n    \n    # %%\n    net = Net()\n    criterion = torch.nn.MSELoss()\n    optimizer = torch.optim.Adam(net.parameters(), lr=0.001)  # Smaller learning rate\n    \n    # Training loop\n    \n    # %%\n    for epoch in range(30):\n        optimizer.zero_grad()\n        output = net(train_x)\n        loss = criterion(output, train_y)\n        loss.backward()\n        optimizer.step()\n        \n        with torch.no_grad():\n            val_output = net(val_x)\n            val_loss = criterion(val_output, val_y)\n        \n        if epoch % 1 == 0:\n            print(f\"Epoch {epoch}\")\n            print(f\"Loss: {loss.item():.8f} Val Loss: {val_loss.item():.8f}\")\n            w, b = net.fc1.weight.item(), net.fc1.bias.item()\n            print(f\"Weight: {w:.8f} (target: {np.pi/2:.8f})\")\n            print(f\"Bias: {b:.8f} (target: 0)\")\n            print(\"---\")\n    \n    # %%\n    # Test the model\n    w, b = net.fc1.weight.item(), net.fc1.bias.item()\n    print(\"\\nFinal parameters:\")\n    print(f\"Weight: {w:.8f} (target: {np.pi/2:.8f})\")\n    print(f\"Bias: {b:.8f} (target: 0)\")\n    \n    # Test on even and odd numbers\n    test_numbers = np.arange(0, 100, 1)\n    net.eval()\n    with torch.no_grad():\n        for x in test_numbers:\n            test_input = torch.tensor([[float(x)]], dtype=torch.float32)\n            pred = net(test_input).item()\n            print(f\"Number: {x}, Prediction: {pred:.8f}, Target: {x % 2}\")\n    \n    \n    \n    # %%\n    import torch\n    import numpy as np\n    import pandas as pd\n    \n    \n    # %%\n    \n    \n    # Generate data and scale inputs\n    def generate_data(size):\n        x = np.random.randint(0, 100, size)  # Smaller range for better visualization\n        return x.astype(float), (x % 2).astype(float)\n    \n    \n    # %%\n    # Generate datasets\n    train_x, train_y = generate_data(1000)\n    val_x, val_y = generate_data(1000)\n    \n    \n    # Convert to tensors\n    train_x = torch.tensor(train_x, dtype=torch.float32).reshape(-1, 1)\n    train_y = torch.tensor(train_y, dtype=torch.float32).reshape(-1, 1)\n    val_x = torch.tensor(val_x, dtype=torch.float32).reshape(-1, 1)\n    val_y = torch.tensor(val_y, dtype=torch.float32).reshape(-1, 1)\n    \n    \n    # %%\n    class Net(torch.nn.Module):\n        def __init__(self):\n            super().__init__()\n            self.fc1 = torch.nn.Linear(1, 1)\n            # Initialize close to the theoretical solution\n            with torch.no_grad():\n                self.fc1.weight.data.fill_(1.5)  # Close to π/2 ≈ 1.57\n                self.fc1.bias.data.fill_(0.0)\n    \n    \n        def forward(self, x):\n            x = self.fc1(x)\n            return torch.square(torch.sin(x))\n    \n    \n    # %%\n    net = Net()\n    criterion = torch.nn.MSELoss()\n    optimizer = torch.optim.SGD(net.parameters(), lr=0.001, momentum=0.9)  # Smaller learning rate\n    \n    \n    # Training loop\n    k = 10  # early stopping\n    inc = 0  # counter\n    prev_val_loss = float('inf')\n    \n    \n    # %%\n    for epoch in range(30):\n        optimizer.zero_grad()\n        output = net(train_x)\n        loss = criterion(output, train_y)\n        loss.backward()\n        optimizer.step()\n        \n        with torch.no_grad():\n            val_output = net(val_x)\n            val_loss = criterion(val_output, val_y)\n        \n        if epoch % 1 == 0:\n            print(f\"Epoch {epoch}\")\n            print(f\"Loss: {loss.item():.8f} Val Loss: {val_loss.item():.8f}\")\n            w, b = net.fc1.weight.item(), net.fc1.bias.item()\n            print(f\"Weight: {w:.8f} (target: {np.pi/2:.8f})\")\n            print(f\"Bias: {b:.8f} (target: 0)\")\n            print(\"---\")\n    \n    \n        # Early stopping\n        if val_loss.item() > prev_val_loss:\n            inc += 1\n        else:\n            inc = 0\n        if inc == k:\n            break\n        prev_val_loss = val_loss\n    \n    \n    # %%\n    # Test the model\n    w, b = net.fc1.weight.item(), net.fc1.bias.item()\n    print(\"\\nFinal parameters:\")\n    print(f\"Weight: {w:.8f} (target: {np.pi/2:.8f})\")\n    print(f\"Bias: {b:.8f} (target: 0)\")\n    \n    \n    # Test on even and odd numbers\n    test_numbers = np.arange(0, 100, 1)\n    net.eval()\n    with torch.no_grad():\n        for x in test_numbers:\n            test_input = torch.tensor([[float(x)]], dtype=torch.float32)\n            pred = net(test_input).item()\n            print(f\"Number: {x}, Prediction: {pred:.8f}, Target: {x % 2}\")",
    "created_utc": "2024-10-26T04:10:01",
    "num_comments": 0,
    "comments": []
},
{
    "submission_id": "1gch9cg",
    "title": "How to know which is a good PhD",
    "selftext": "Hi experts, I'm confused on how to choose a PhD. Ideally I want to go into the industry after graduation. It'll be a PhD in CS in the UK and will only do it if it is fully funded. That aside, my main option right now is using LLMs and GenAI in education. I don't see any multidisciplinary aspect in this. Since I want to go into the industry, what is an ideal way to shape a PhD that'll increase my chances. I'm asking this as there's very little I can do with LLMs. Even if I'm the best mind, openai has 1000000 best minds and will beat me to it. I don't want to be doing software engineering and API calling. I'm extremely confused.",
    "created_utc": "2024-10-26T02:29:22",
    "num_comments": 3,
    "comments": [
        "If you want to maximise your potential of getting an industry job, that phd is actually fine. Most of what you ll do in private companies is calling API's and software engineering sadly. If you want to do things that are actually fun to do, you have to pivot more towards the more mathy parts of ML. A fun field that is emerging right now is topological deep learning for example, so you might want to get into that. Beware, a proper research phd in ml is hard af in the maths part. I ve been working as an AI engineer doing research for a couple of years, and i dont know if i feel confident yet in trying to go for a phd",
        "... You don't see how using llms in education is multidisciplinary? Seems like you would need to interact a lot with social scientists and education experts",
        "I'm mainly looking at R&D roles so I don't know how much they will value a guy who just knows how to do API calls and apply it to domains."
    ]
},
{
    "submission_id": "1gcgst0",
    "title": "Guides on how to train with streaming dataset",
    "selftext": "Hi does anyone have guides on how to train with this kind of dataset?\n\n[https://huggingface.co/datasets/google-research-datasets/conceptual\\_captions](https://huggingface.co/datasets/google-research-datasets/conceptual_captions)\n\nThe usual way is to download the dataset and  use it.\n\n    from concurrent.futures import ThreadPoolExecutor\n    from functools import partial\n    import io\n    import urllib\n    import PIL.Image\n    from datasets import load_dataset\n    from datasets.utils.file_utils import get_datasets_user_agent\n    USER_AGENT = get_datasets_user_agent()\n    def fetch_single_image(image_url, timeout=None, retries=0):\n    for _ in range(retries + 1):\n    try:\n    request = urllib.request.Request(\n    image_url,\n    data=None,\n    headers={\"user-agent\": USER_AGENT},\n    )\n    with urllib.request.urlopen(request, timeout=timeout) as req:\n    image = PIL.Image.open(io.BytesIO(req.read()))\n    break\n    except Exception:\n    image = None\n    return image\n    def fetch_images(batch, num_threads, timeout=None, retries=0):\n    fetch_single_image_with_args = partial(fetch_single_image, timeout=timeout, retries=retries)\n    with ThreadPoolExecutor(max_workers=num_threads) as executor:\n    batch[\"image\"] = list(executor.map(fetch_single_image_with_args, batch[\"image_url\"]))\n    return batch\n    num_threads = 20\n    dset = load_dataset(\"google-research-datasets/conceptual_captions\")\n    dset = dset.map(fetch_images, batched=True, batch_size=100, fn_kwargs={\"num_threads\": num_threads})\n\nI'm rather unsure how to handle this dset for training. Any guides out there? Any help is appreciated. Thank you. I'll like to learn how to stream, fetch the image and then train. not quite sure what's the code is doing tbh.",
    "created_utc": "2024-10-26T01:54:15",
    "num_comments": 0,
    "comments": []
},
{
    "submission_id": "1gcfr78",
    "title": "I shared a beginner friendly PyTorch Deep Learning course on YouTube (1.5 Hours)",
    "selftext": "Hello, I just shared a beginner-friendly PyTorch deep learning course on YouTube. In this course, I cover installation, creating tensors, tensor operations, tensor indexing and slicing, automatic differentiation with autograd, building a linear regression model from scratch, PyTorch modules and layers, neural network basics, training models, and saving/loading models. I am adding the course link below, have a great day!\n\n\n\n[https://www.youtube.com/watch?v=4EQ-oSD8HeU&list=PLTsu3dft3CWiow7L7WrCd27ohlra\\_5PGH&index=12](https://www.youtube.com/watch?v=4EQ-oSD8HeU&list=PLTsu3dft3CWiow7L7WrCd27ohlra_5PGH&index=12)",
    "created_utc": "2024-10-26T00:33:35",
    "num_comments": 0,
    "comments": []
},
{
    "submission_id": "1gcey5u",
    "title": "extract humans and ball from image - do i need a gpu server of will something like roboflow be better?",
    "selftext": "Hi,\n\n  \nI am doing a college project and need to create a flask service that allows me to send a video frame as a base64 image to a human detection and ball detection models for inferencing.  Then I need to remove everything from the image and convert it to a transparent image showing just the humans and a ball.  This can be for any sport!\n\nMy Solution:  \nI have created a python script that uses flask and tensorflow which sits on port 5000 waiting for images.  The process works well however it is way to slow.  It takes around 30 seconds to send the image from the client, inference it, convert it to a transparent base64 png and send it back.\n\nMy hardware:  \nI am using a virtual cpu for testing the solution at the moment but need to speed it up.  The issue is I am facing is that my project will only doing a handfull of inferences a day (maybe 100 at the most).  So renting a gpu server for this project seems way to expensive as 99% of the time the server will be doing nothing.\n\n  \nCan someone advise how I can speed up the process using maybe a cheap 3rd party service like roboflow?",
    "created_utc": "2024-10-25T23:32:09",
    "num_comments": 1,
    "comments": [
        "Could an NPU (or two) be used with specific trained models? \n\nhttps://www.raspberrypi.com/products/ai-kit/"
    ]
},
{
    "submission_id": "1gcegle",
    "title": "What min specs do I need to finetune LLMs??",
    "selftext": "Hello people, I want to finetune say gemma2-2b on my dataset what are the minimum specs i need on my pc to finetune my llm using QLora or LoRa....\n1) Also suggest me what services online would be affordable if I want to do it in paid way.\n2) Is Finetuning without using these techniques is better?....coz they decompose the matrices so I reckon there would be some performance issues for the model later\n\nMy pc has 8gb ddr4 RAM and 4GB nvidia Rtx3050....if anyone needs context to what specs I have on my local machine",
    "created_utc": "2024-10-25T22:57:41",
    "num_comments": 2,
    "comments": [
        "A Chromebook with Internet access to go on Colab, and $10 worth of credits, (depending on how much you're fine tuning).\n\nEdit: more seriously, you aren't going to train a large model on 4 GB, even with LoRA.",
        " Buddy the answer you’re gonna get is \n\nIt depends\n\n\nBut i feel you’re better off using kaggle’s tpus and gpus"
    ]
},
{
    "submission_id": "1gce6vd",
    "title": "I have to write a thesis from my undergrad. I chose ML and I am a beginner",
    "selftext": "I am in my final year of BSc with general computer science with no specialization. I plan to write my thesis on \"Personalised agents for people with ADHD and ADD\" using machine learning. Is this a good topic to work on as a newbie. This thesis also contributes to my plan of applying to university abroad for my master's in Data Science.   \n  \nIs this feasible and correct in terms of the technical aspects of it.  \nAny advice would be helpful ",
    "created_utc": "2024-10-25T22:38:02",
    "num_comments": 1,
    "comments": [
        "I am unsure about the project itself, as I am not familiar with these types of applications but a couple of tips:\n\nAsk your professors first and foremost, exactly what you say in this post. I imagine if you wrote this post, you wanted some additional insights, and of course it’s good to gather as much as possible. But professors, even more for research-oriented undergraduates, can be extremely of help in terms of individuating research gaps/areas of improvements, scope definitions, etc. \n\nSecondly, check the most recent literature about it, as well as the literature constituting the fundamentals. You could do this before the first point, or after with some input from professors. Both these processes will do wonders in seeing what is feasible/unfeasible.\n\nPro tip: quality over quantity. With the right guidance, narrow down the scope to something very specific and you’ll do it very well, and in the narrowing down process you’ll also put aside many potential ideas that might contribute to your application somewhere. If you choose quality over quantity, for every door you open you also usually know what doors are you closing. Nothing will stop you in the future to go back and open some directions previously not taken!"
    ]
},
{
    "submission_id": "1gcdwcb",
    "title": "Model Training ",
    "selftext": "How should I decide on the amount of training to give my model since. If training is too much it can lead to over fitting and model being accustomed to that particular dataset",
    "created_utc": "2024-10-25T22:18:04",
    "num_comments": 6,
    "comments": [
        "Setup check points. You can always go back to a earlier checkpoint. Setup early stopping as well. You can setup metrics for it to stop training when loss on Eval stops decreasing.",
        "If it's an ML model just do gridsearchcv or other methods. Do some feature engineering as well",
        "Well i think best is, to just train till you see the model overfitting? Meaning that the val loss suddenly increases and the test loss keeps improving. You should also safe some checkpoints during training",
        "Setup Early Stopping",
        "Are there any known matrices that I can use",
        "I don't understand your question."
    ]
},
{
    "submission_id": "1gcckgq",
    "title": "Minor in Data Science vs Statistics  ",
    "selftext": "Would either minor be more useful than the other in terms of understanding and developing models? Or should I spend more time and energy on outside resources like projects, research, or getting an internship to learn and apply myself more to the deep learning field? TIA. ",
    "created_utc": "2024-10-25T20:55:39",
    "num_comments": 5,
    "comments": [
        "The stats minor would be better in most cases, but the best would be to get the internship or networking.",
        "The most beneficial will be to get an internship and applying ML or DL models. But those can be hard to get, and might be expecting to already know stuffs before hand so seems that no matter what you will still need to get that minor and outside projects to even appeal to the interviewer. Only way I see that you could skip the minor is if you already majoring in CS, which you could just learn on a project, but if got another major you, the minor will probably help you better to show certain competences, at least fundamentals, for ML models.",
        "The perils of model drift are real – but with self-supervised learning, we can 'train' our models to adapt!",
        "thanks for the feedback",
        "explain this further please"
    ]
},
{
    "submission_id": "1gcc2fc",
    "title": "\"Foundations for Machine Learning\"",
    "selftext": "\n\nhttps://preview.redd.it/pea5gbvrr0xd1.png?width=1280&format=png&auto=webp&s=452854ecdc3e378b5b4a5b57dbbf145cf093b551\n\nIn 2022, I graduated with a PhD in Mechanical Engineering from MIT. \n\n\n\nAlthough a big component of my research was purely hands-on experiments, my exposure to foundational graduate-level ML courses at MIT, research courses, and Scientific Machine Learning via Julia gave me the confidence of a Machine Learning researcher. \n\n\n\nI incorporated ML into my research, and it solved a problem that is otherwise difficult to solve theoretically or experimentally. Now I have co-authored multiple AI-ML research papers and two of them are accepted to the upcoming NeurIPS workshop. \n\n\n\nBehind all of this effort, there is the confidence that stems from knowing what happens underneath the ML algorithms.\n\n\n\nMost of the online courses have little emphasis on fundamentals. People are so used to spending time on toy Kaggle projects. Very few people I know can build a neural network from scratch or explain what happens behind them.\n\n\n\nFor the last 4 months, I have been working to launch a new course titled on \"Foundations for Machine Learning.\" This will be a 45-hour course with \\~65 lectures. I will be hosting all lectures on this playlist: [https://www.youtube.com/playlist?list=PLPTV0NXA\\_ZSiLI0ZfZYbHM2FPHKIuMW6K](https://www.youtube.com/playlist?list=PLPTV0NXA_ZSiLI0ZfZYbHM2FPHKIuMW6K)\n\n\n\nMy singular goal with this course is to teach you the entire foundations required to learn ML from scratch.\n\n\n\nThere are no prerequisites. If you have basic logical thinking capability and a willingness to dedicate time, consistently, you can follow this course.\n\n\n\nI have split the course into 5 modules.\n\n\n\n1) First I will cover the 4 mathematical pillars of ML: Linear Algebra, Probability, Statistics, and Calculus.\n\n\n\n2) In the second module I cover the basic programming fundamentals for a complete beginner. I will teach you Python from scratch and it's some of the most important packages for ML including NumPy and PyTorch.\n\n\n\n3) In the 3rd module we will learn about optimization and gradient descent. I wanted to dedicate an entire module to optimization because when you actually build ML models, you will be spending a lot of time on optimization.\n\n\n\n4) In the 4th module, I will give you an overview of the AI landscape. What happened from 2010-2020 and what does it look like from 2020-2030? This overview will help you understand overall where ML, DL, NLP, CV, and GenAI are heading.\n\n\n\n5) In the final module, I will cover the most important 2 steps you will have to master as a Data Scientist or ML engineer: processing data and communication via storytelling. I will teach you some of the most powerful preprocessing and visualization techniques.\n\n\n\nI have already published the first lecture. Check out here. I am sure you will enjoy and learn a lot: [https://youtu.be/C8hEa2qb46k?si=7dRHM6EZwlUBDC5C](https://youtu.be/C8hEa2qb46k?si=7dRHM6EZwlUBDC5C)",
    "created_utc": "2024-10-25T20:24:51",
    "num_comments": 8,
    "comments": [
        "Looks like models are getting smarter, but are they smart enough to outsmart the 'Godfathers of AI'?",
        "Looking forward as well. Thanks  for all your efforts.",
        "Cool",
        "Looking forward to rest of the lectures",
        "Thank you. Looking forward to more lectures.",
        "Thank you, this is great. Please keep doing what you are doing!",
        "Amazing! Thank you for your work. Keep it up!",
        "Hey Reddit!\n\nWe just published an “early days” benchmark evaluation of **Plexe**, our prototype AutoML framework designed to train ML models from natural language problem descriptions, data, or both. In our post, we focus more narrowly on the framework’s performance when producing supervised learning models from datasets.\n\nIn our benchmarks, Plexe consistently achieves either competitive or marginally superior results relative to frameworks like **AutoGluon**, **H2O AutoML**, and **TPOT**.\n\nIf you're interested in AutoML, check out our post for some preliminary findings, code and data. While we are not yet ready to release Plexe to the world, we intend to launch a public release in the next few months. We would love to get some feedback from the community, so please feel free to [join our waitlist](https://plexe.ai/).\n\nIf you want to try Plexe out, we have a discord server where you can message us your ML problem description along with a small dataset and we will create a model for you along with a report about the solutions considered and the corresponding performance metrics.\n\n👉 [Read more here](https://www.plexe.ai/post/plexe-production-ready-custom-ai-from-natural-language)"
    ]
},
{
    "submission_id": "1gc9100",
    "title": "NFL ML Co-Project?",
    "selftext": "Hi, \nI am trying to learn machine learning and have been reading on it, but I always found I learn better by doing projects. Is anyone else learning and wanting to collaborate? I would love to do a collaborative project with someone. Would love to have fun with it. Please DM me if you are interested. Doing it purely for fun and trying to learn ML. Let me know! ",
    "created_utc": "2024-10-25T17:32:16",
    "num_comments": 1,
    "comments": [
        "Residual GATs for thyroid cancer classification? Meanwhile, Anthropic's AGI worries are making me wonder: 'What's next for graph-based sanity?'"
    ]
},
{
    "submission_id": "1gc81w6",
    "title": "Graduate Programs/Masters in Computer Vision",
    "selftext": "I am looking for graduate programs/masters in computer vision and needed some advice from the community. I am about to complete my bachelors in computer science.\n\nI have a few doubts:\n\n1. Is it better to specifically look for programs in machine learning and AI, or pursue a masters in computer science. My goal is to get into industry after my degree (such as robotics, etc.), but with a strong theoretical knowledge.\n2. Other than robotics, what other well-established fields heavily seek computer vision expertise. I want to get a sense of job prospects. How competitive is this field?\n3. Are there any such programs available? What sort of places should I look into?\n\nAny advice, and any extra insights independent of my doubts will be really helpful.",
    "created_utc": "2024-10-25T16:42:19",
    "num_comments": 2,
    "comments": [
        "Residual Graph Attention Networks are like trying to calibrate a Trump tweet - messy, yet strangely informative!",
        "I would think it's better to get it in AI because every class project will be something you can add to your portfolio, whereas general cs may not be like that, but I dunno. I only have the experience of most of a CV masters and haven't gotten a job in it yet."
    ]
},
{
    "submission_id": "1gc7sm8",
    "title": "How dalle image generator works ? ",
    "selftext": "",
    "created_utc": "2024-10-25T16:29:25",
    "num_comments": 0,
    "comments": []
},
{
    "submission_id": "1gc77z2",
    "title": "What chairs are you guys using to code with?",
    "selftext": "I need a chair for my desk. What ones have you been happy with? I dont have too much budget to spend, like maybe 300-400$",
    "created_utc": "2024-10-25T16:01:13",
    "num_comments": 14,
    "comments": [
        "Sir this is not a furniture store",
        "I normally use my keyboard.",
        "Aeron",
        "Steel case leap",
        "Half the time on a shitty $100 staples chair (in office), the other half on a AKRacing premium (home), and for an hour or two every few days it'll be a yoga ball (also at home)",
        "Whatever chair there is available usually...",
        "Artificial intelligence is 'trained' to make sense of our poop – but can we trust it to handle sensitive military data?",
        "A good chair is very important",
        "I dont have that budget to spend",
        "And it is also very important to do things in appropriate setting/places.",
        "Get a used/refurbished one, I got one for $150",
        "Where did u get it?",
        "Check Craigslist, Amazon, ebay.",
        "I couldn´t find it, also, I was looking for steelcase series 1, Im still watching reviews, do you think it is worth it?"
    ]
},
{
    "submission_id": "1gc59xk",
    "title": "Mass downloading data from baseball savant for ML project ",
    "selftext": "",
    "created_utc": "2024-10-25T14:29:36",
    "num_comments": 0,
    "comments": []
},
{
    "submission_id": "1gc3zix",
    "title": "How you train your LLM models to produce grammatically correct text?",
    "selftext": "E.g. I have some small LLM, I trained it till it almost overfit to some small text corpus, but it still produces typo like \"expxt\" instead of \"expect\" once in a while.",
    "created_utc": "2024-10-25T13:31:23",
    "num_comments": 4,
    "comments": [
        "First of all you should check the corpus itself and check if there is some typo and stuff, after i would check if there a problem of sort with the machine, rigorous testing in different environments could help and after that maybe grokking could help",
        "Aside from checking the corpus, is each token a word, or a word part?",
        "I would add that spelling and grammar are separate things.",
        "Small large LM"
    ]
},
{
    "submission_id": "1gc3ni6",
    "title": "Seeking Advice on Choosing a Master’s Program in AI for Summer 2025 USA\n",
    "selftext": "Hi everyone,\n\nI’m currently in the process of selecting a Master’s program in Artificial Intelligence for the summer intake of 2025. Unfortunately, I've found that there are limited options available, and I'm having difficulty deciding on the best fit for my goals.\n\nI would greatly appreciate any suggestions, insights, or feedback regarding programs you might have experience with or know about.",
    "created_utc": "2024-10-25T13:16:48",
    "num_comments": 1,
    "comments": [
        "Residual Graph Attention Networks may outperform transformer models in cancer doc classification, but can they handle the 'dark matter' of bias?"
    ]
},
{
    "submission_id": "1gc2qzw",
    "title": "Has anyone followed pantech Ai courses..are they worth it..",
    "selftext": "i found [this](http://www.pantechsolutions.net/) Ai course site through a facebook ad(reason these courses attracted me the most is they are project based) and it's lecturers are indian..at the moment with my financial situation i can't afford to purchase many courses.. so i need to know whether anyone has followed these courses and whether it is worth buying these courses and do they gives relevant downloadable source codes with them..thanks..",
    "created_utc": "2024-10-25T12:37:54",
    "num_comments": 1,
    "comments": [
        "LongGenBench's results confirm what we've known: LLMs are like teenager smartphones - always improving, but occasionally crashing in long conversations."
    ]
},
{
    "submission_id": "1gc26xj",
    "title": "Seeking Advice on Building a Self-Learning NLP Model for Image Data Extraction",
    "selftext": "Hey everyone! I’m working on a project where I need to build an NLP model that can learn and improve itself in a feedback loop. The main goal is to extract specific data from a large number of images. I initially tried using regular expressions (regex) for this, but creating regex patterns for all the variations in the images is proving to be too time-consuming. I’ve also experimented with libraries like Pytesseract and EasyOCR, but they haven’t been precise enough for my needs.\n\nI’m doing all of this in Google Colab. Does anyone have any suggestions on how I could approach building a model that can handle this better, possibly one that improves over time with feedback? Any recommendations on frameworks, libraries, or approaches would be really helpful.",
    "created_utc": "2024-10-25T12:13:12",
    "num_comments": 1,
    "comments": [
        "Residual GATs are 'gating' the door to more efficient cancer doc classification, but will they 'attend' to the data scarcity issue?"
    ]
},
{
    "submission_id": "1gc23mz",
    "title": "ML calculus topics ",
    "selftext": "I want to self study Stewart calculus book. Do I have to study the book cover to cover, or is there relevant topics and if yes, what are they? Will I miss anything if I didn't study the book cover to cover?\n\nI have a graduate entry machine learning exam, which involves a lot of calculus and linear algebra. I also, want to understand models enough and apply math, not just aim to pass the exam.",
    "created_utc": "2024-10-25T12:09:15",
    "num_comments": 1,
    "comments": [
        "I'd love to see an 'AI-Powered Toilet Seat' hack, leveraging computer vision & machine learning for accurate poop analysis!"
    ]
},
{
    "submission_id": "1gc152u",
    "title": "Basic ML projects to get future work",
    "selftext": "Hi,\n\nI've been an offshore data scientist and data analyst from 2020 until 2022/23 and would like to get back into it after a period of not being a part of the space for a while, I was wondering if there any small or medium-sized projects that I could contribute to, learn from that or just start that would look good on a resume.\n\nI know the usual recommendation systems algo, naive bayes and tree ensamble algorithms are a little basic at this point, so would like some opinions on it.\n\nI've been looking into d3.js because data visualization is cool and I'd like to learn it on the side. I'm not a part of many online ML communities so apologies if this isn't the best place to ask for advice. Thanks/sorry in advance",
    "created_utc": "2024-10-25T11:27:25",
    "num_comments": 2,
    "comments": [
        "Looks like AI is 'registering' another victory in model performance, but can it avoid occlusions of accuracy?",
        "Sorry, feels like I'm missing an inside joke or that you're calling me a bot. Just feel kind of lost at this comment."
    ]
},
{
    "submission_id": "1gc0887",
    "title": "Question about Bidirectional RNN",
    "selftext": "Hi guys I'm not super well versed in RNNs but I have a question regarding the input of bidirectional RNNs.\n\nLets say I have a sequence [1, 2, 3, 4, 5] if I want to predict any of the time steps, how is my training data supposed to be structured?\n\nFor example if I want to predict the middle value Y= [3] what data should the model take? \n\nMy intuition tells me it should have the form [1, 2] for the forwards direction and [4, 5] for the backwards direction but the definition of a Bidirectional RNN says it should take the entire sequence?\n\nI think i'm seriously not understanding the inputs an RNN takes.\n\nThanks!",
    "created_utc": "2024-10-25T10:48:15",
    "num_comments": 1,
    "comments": [
        "It seems like 'AI's Dark Side' is shining bright, with calibration challenges in sports & cancer doc classification - AI must adapt to improve!"
    ]
},
{
    "submission_id": "1gbviqw",
    "title": "Self-Supervised Learning for Autonomous Driving",
    "selftext": "",
    "created_utc": "2024-10-25T07:26:34",
    "num_comments": 0,
    "comments": []
},
{
    "submission_id": "1gbvgry",
    "title": "Is this course anygood? It has Andrew NG as one of its instructors",
    "selftext": "",
    "created_utc": "2024-10-25T07:24:05",
    "num_comments": 30,
    "comments": [
        "THE\\_REAL\\_ANDREW\\_NG = \"Andrew Ng\"\n\nlist\\_of\\_course\\_instructors = \\[\"Anthony Ng\", \"Rob Percival\"\\]\n\nif THE\\_REAL\\_ANDREW\\_NG in list\\_of\\_course\\_instructors:  \nprint(\"Take the course!\")  \nelse:  \nprint(\"Don't take the course!\")",
        "Brother, read... It's Anthony NG , not the goat Andrew NG",
        "damn , andrew ng , is sooooooo well respected that we even have knockoffs now",
        "Personally I would not pay for any ML courses since there are trillions of free ones, but for <$10 I'd say fk it, that's a fine place to start.",
        "you will probably find the same things online for free. dont waste your money",
        "[6.0001 | Undergraduate\nIntroduction to Computer Science and Programming in Python](https://ocw.mit.edu/courses/6-0001-introduction-to-computer-science-and-programming-in-python-fall-2016/)\n\nJust take this instead. It's free and offered by a world class university. I took this as my first course and I learnt a lot.",
        "600 what rupees? I hope whatever currency that is it's not alot and why are you browsing weird courses just do the popular ones",
        "output:  \nDon't take the course",
        "What about that other course taught by Jeffrey Hinton? That one’s gotta be gold.",
        "Please recommend some free ones?\nI'm done with ml specialization by Andrew.\nWhat next for me?",
        "7.14 usd",
        "Don't do too many courses for ML, go on kaggle start doing projects, check out notebooks by experts and learn from them",
        "If you want to go with deep learning, you can do Andrew NG's Deep Learning Specialization. It's on Coursera iirc. If you care about the certificate and do it for free you can try this: create two accounts, you audit the course with the secondary account and do the course at whatever pace you're comfortable with. When you're done, sign up to the course with your main account and enter the free trial week. Do the evaluations and get your certificate and unsubscribe. \n\nI also recommend Sebastian Raschka's Deep Learning Fundamentals. It covers a bit of everything, from the fundamentals to tricks to optimize training. It doesn't go as in depth as Andrew NG's but it's still pretty solid.\n\nBut honestly, after learning the theory don't waste much time doing more courses. Just pick a project and practice.",
        "I'd recommend UVA's DL courses. They do well to blend ML theory w/ actual code, and they cover fairly SORA topics. Good place to go after learning basics.",
        "Understood, but after the ml specialization idk what to do.\nI tried projects but realized I couldn't code anything. I understood the concepts in the course tho. I know we have to clean the data but dk how to. Like that.\n\nI'm lost and clueless now",
        "Oh.\nYeah I understand we should pick a project. I'm not sure how to code the things. I know I should normalize the data, plot it, use neural nets. Dk how to. Dk how to use the libraries or syntaxes",
        "Thanks. If you could read my comments in this thread you will know my problem. Can you help with that",
        "That's like a huge barrier you'll only cross it, by practicing, take on some medium level of datasets and try filling in missing values,normalizing and converting catgegorical vals etc. I have been where you are and from experience I can tell you only thing you can do right now is struggle with datasets and improve",
        "NP, and like others mentioned u just need to bite the bullet and learn from guides/notebooks of actual ML projects to learn how to code (i.e. just copy and paste code to start). The uva course cover code well also tho.",
        "Ok.\nBut how to go about it. Idk the syntax and stuff.\nGoogle along?",
        "Okie thanks.",
        "Ohh, that isn't taught in Andrew's course or what?\nI'm sorry but I have only done the deep learning specialization",
        "https://www.udemy.com/share/101WaU3@S4OTeUTzQWv5FcJEXRzrjkwKZymidUyQ-o2iwPbW1zE2kRgqBl39wqlcd5SF9J4eYA==/\nCheck this course out, it's paid but not that expensive (wait for udemy sale) , he will guide you through all data cleaning parts etc, and takes it slow and teaches well",
        "dude just take a book or a course on python then python for data analysis, then python for data pre-processing. a Good ML (like hands on ML or intro to ML) book also teach you most of the steps you need to take for cleaning datasets. Then as the commenter said, just need to practice.",
        "We only code snippets.\nLike compute the gradient, implement neural nets. Just that function. Not the entire code",
        "Any idea if I can use kaggle.com/learn?",
        "Ohk thanks, will check it out",
        "Thx",
        "Nah, don't go there yet, or else you'll just get overwhelmed, try to learn coding basics from a course",
        "Ok"
    ]
},
{
    "submission_id": "1gbvczd",
    "title": "Career Choice: PhD in LLMs or Computer Vision?",
    "selftext": "Hey everyone so I recently got two phd offers, however I am finding a hard time deciding which one could be better for the future. I mainly need insights on how relevant each might be in the near future and which one should I nonetheless take given my interests.\n\nBoth these phds are being offered in the EU (LLM one in germany and Vision one in Austria(Vienna) ). I understand LLMs are the hype at the moment and are very relevant. While this is true I have also gathered that a lot of research nowadays is essentially prompt engineering (and not a lot of algorithmic development) on models like the 4o and o1 to figure out there limitations in their cognitive abilities, and trying to mitigate them.\n\nComputer Vision on the other hand is something that I honestly like very much (especially topics like Visual SLAM, Object detection, tracking).\n\n1. PhD offer in LLMs: Plans to use LLMs for Material Science and Engineering problems. The idea is to enhance LLMs capability to solve regression problems in engineering. 100 % funded.\n2. PhD in Computer Vision: This is about solving and understanding problem of vision occlusion. The idea is to start ground up from classical computer vision techniques and integrate neural networks to enhance understanding of occlusion. The position however is 75% funded.\n\nI plan to go to the industry after my PhD.\n\nWhat do you think I should finally go for?",
    "created_utc": "2024-10-25T07:19:18",
    "num_comments": 28,
    "comments": [
        "Just ...something to note. I got my PhD in vision guided surgical robotics. I graduated 12 years ago. I have worked on surgical robots, autonomous vehicles, industrial robots, mining equipment, forklifts, ship propulsion, laboratory automation, sensor development and IP strategy. \n\nIt's your degree it's not a prison sentence. Your overall success on either field will largely be driven by your passion for it. Do what you would be excited to do when you wake up in the morning. Now this doesn't apply when comparing going to law school or underwater basket weaving college, but you are picking between two of the most sought after disciplines in the hottest industry in the world right now. You won't starve either way",
        "Compare the publications of the two labs in terms of impact. \n\nBy ‘solving’ the problem of occlusion you mean using a ML model to guess what the occluded parts of the scene look like? \n\nThe llm sounds a lot more general in scope. You don’t want to work too long on a narrow problem",
        "Computer vision honestly, the state of LLMs in industry is just API calls to OpenAI. I feel learning LLMs will be a wasted investment as everyone will just be calling them by APIs, unless you think you are good enough to get onto the teams at mistral / openai / deepmind.",
        "Going against the grain, I would pick the PhD in Computer Vision. You are more passionate about it. You might very well develop more useful technical skills long term, even if it's less of a 'hype' right now. And Vienna is excellent, I would pick it for Vienna. The only downside is the lessor funding, but I think you can figure that out no problem.",
        "Given you plan to go to industry after graduation, I would suggest go with the professor that has more connections with the industry. Maybe try to find out where former graduated PhDs in those two labs ended up working at.",
        "To be honest, I think you should choose whatever speaks to your current skill set and whatever you can see yourself kicking out a couple of papers on. The only other thing to consider is who your supervisor is going to be. Do you have a relationship with one of them? Because that can make a huge difference in your experience and trajectory. I'm not saying choose the path of least resistance but, answers to these questions can provide some good guidance and these are items only you can answer.",
        "I would take the LLM path. It’s funded and much of what you learn can be applied to MV, if you still want to do that.",
        "Are those the actual names of the programs? Or will your PhD be in CS or Engineering? The thought of an LLM-specific PhD scares me, because they may very well be a stepping stone to some other kind of much better AI in the future. It could be like having a PhD is BetaMax or DVD production. ",
        "I mean clearly LLMs are extremely relevant at the moment for obvious reasons. And it's fully funded.",
        "Go with llms. I just finished my thesis based masters. The scope for llms is wide in the industry. Every data science position is starting to have an llm requirement. But cv leave out a lot of options forcing u to be considered only in robotics or other cv application fields.",
        "I think CV presents more of a challenge and has a lot more remaining potential. But LLM has a lot of financial and cultural momentum.",
        "Toss a coin see what you hope it lands.\n\nId choose CV because u said u like it very much. I think its critical u wont curse ur next years because u arent so passionate about what u would be doing.",
        "I'd go with computer vision. It has way more practical applications that can be easily commercialized, and consequently more opportunities for employment in different fields or even to be your own boss, if you ever needed to go that route.",
        "Imo, LLMs kind of near to saturation. CV has a very good scope and a lot to explore. But finally it depends upon your interests.",
        "Looks like AI researchers are 'calibrating' their expectations, but will they 'transform' performance to tackle cancer classification?",
        "I will say Go For Phd in Vision. Computer Vision is one of the best fields in the world. You will really Enjoy working with it. Like you said LLMs are mostly prompt engineering, and rarely professors  go for development algorithmic LLM but decision is still yours. \nBesides that most of other factors count:\nFor Example\nYour phd advisor, believe me if he/she will be good your half of the journey will be much easier. Moreover, he should well aware of his field. I had advisor in Masters, he knew nothing but got Govt Funding Project. Believe me it was nightmare working with him. I was searching literature and suggest novel work that I discussed with my course instructor as well, my course i structor who was well aware of topic, really like my ideas and encourages me. But working with my advisor is nothing more than wasting my energy and efforts but still I keep going and took office hours with my course instructor and keep completed my thesis myself. It was masters but for phd you must check professor has all the capabilities. Its two way process, I guess. Sometimes advisors interviewed us but we didn’t do much work of asking them as we were desperate to get the opportunity in the end, things turned out like happened to me. Carefully, check your advisor and environment of your lab and university you are going to most impirtant and hectic period of Life there. \nMoreiver, check the ranking of university, believe me its an other factor. High ranked universities have more and better opportunities thats one of my experiences as well. \nBut Vision has some of great great journals as well. I really like CVPR but it depends not matter for everyone \nBut must consider other factors besides the topics you will be very relaxed later in your phd and will enjoy your work and research.\nBest of Luck for your future endeavours!",
        "Damn cool career choice! Bet ur one of the most interesting ppl to speak to in a room haha",
        "Well there are a lot of optimization techniques on images to boost contrast and visualise the occluded objects. However these approaches do not generalise too well for varied object motions in Occluded settings. Hence the use of learned representations comes into play here (and thus neural networks) \n\nYes, LLM one does sound a lot more general but I don't want to get stuck only doing prompt engineering.",
        "I recently started my PhD in machine translation, and from what I see, there is a trend towards local models. Perhaps not 'large' language models per se, but I definitely see specialised local models picking up",
        "Agree.",
        "I agree with this as well. It's possible to simply use LLMs via API calls and work with them. Not much to algorithmically advance there.",
        "Sorry I didn't quite get what's MV here. The only thing about LLMs PhD is that it presents little avenue for further algorithmic development. Prompt engineering is not a future proof skillset anyway. Given it's application case if that's all that I'll need to do here then I'm not certain I'll be able to market myself to top companies.",
        "It would be a PhD in CS/ML in both cases.",
        ">The only thing about LLMs PhD is that it presents little avenue for further algorithmic development.\n\nI dont think thats the case at all. Very unlikely that the transformer backbone is the best out there given its disatvantages. \n\nAlso there is still room to develop novel algorithms by finetuning open source LLMs and combining them with other architectures to solve relevant problems. Look at AnomalyGPT or AutomaTikz for example.\n\nYou could also look into creating novel evaluation metrics that are better than the current ones.\n\nThat being said, I would probably still go for computer vision as this area is still highly relevant but more of a nieche right know thanks to the domination of LLMs.",
        "Understood. I thought it meant ‘build LLMs’, not prompt Engineering. MV = Machine vision.",
        "I think there is still plenty of real engineering in terms of fine tuning, steering vectors, and creating custom models for your specific use case. LLM projects, especially for specialized applications with defined correct answers, are definitely not just prompt engineering",
        "Than I think either is a good choice. If the topic interest is a gossip for you, go with funding. But topic interest is the most important. You’ll spend years on this. ",
        "Ah okay thanks for clarifying."
    ]
},
{
    "submission_id": "1gbv6y6",
    "title": "Developing code beginner: AI integration question",
    "selftext": "Hello,\n\nI am trying to develop the starting portion of a python code with Open-AI preview that I'm eventually sending to a real programmer to implement.\n\nChat GPT is recommending integrating Rainforest for testing and AI learning so it can improve upon using/learn to optimize functions on its own.\n\nI then asked Chatgpt in another chat and apparently this isn't affordable unless you're a business.\n\nfeel like this is the type post to get flamed in a sub like this 🔥 😂  asking the real guys\n\nAny advice? Any feedback is appreciated\n\nThank you\n\n ",
    "created_utc": "2024-10-25T07:11:41",
    "num_comments": 6,
    "comments": [
        "Yeah, don't do that. Just have a list of requirements and a budget in mind and give that to the programmer",
        "What does starting position of a code even means and what is rainforest and what's the problem you're trying to solve? \n\nHuh??",
        "Residual attention layers may hold the key to mitigating long-context generation degradation, but can we 'Rescue' the model?",
        "in terms of this, the only solution i can think of (and chat gpt) recommended was to build it out modular as possible piece by piece that way i don't run into this losing comprehensive script/memory problem however being a beginner it's hard for me to decipher how these parts/analytical tools are coding together in a specific way (some of them are working together in some aspects) which may also be important past just building out the tools modular (completely seperate?).\n\nI'm thinking in some way this might be my best bet to salvage this and then im hoping a developer can piece together?\n\nIt's somewhat hard to explain it's a very customized project revolving around very specific customized tools that won't be familiar. \n\nIt revolves around Bayensian Statisitics and reading patterns in randomized strings, specific methods or pattern identifiers I taught AI to write into code so I can automate many things I perform manually.\n\nI'm purely a beginner who's trying to automate methods I complete manually into automated code, which I'm hoping will also in time (randomforest?) optimize these methods/improve preditability / track performance etc.\n\nAny advice is appreciated",
        "sorry I ment randomforest",
        "Try asking chatgpt. It might be able to understand what you're saying."
    ]
},
{
    "submission_id": "1gbuvkb",
    "title": "Deep Learning For Natural Language Processing | Free Udemy course for limited enrolls",
    "selftext": "",
    "created_utc": "2024-10-25T06:57:32",
    "num_comments": 0,
    "comments": []
},
{
    "submission_id": "1gbugtg",
    "title": "Leland McInnes: Decomposing Algorithmic Black Boxes, UMAP, HDBSCAN & the Geometry of Data | Learning from Machine Learning Episode #10",
    "selftext": "",
    "created_utc": "2024-10-25T06:38:04",
    "num_comments": 0,
    "comments": []
},
{
    "submission_id": "1gbu8dl",
    "title": "Help needed for my first ML project",
    "selftext": "Soo I just started learning machine learning through my college course. I've chosen a project that involves building an agent that solves wordle puzzles. I have about a month left to complete this project. Would it be considered an ml project if I use information theory to build this model. If not suggest me some not too complex algorithms. ",
    "created_utc": "2024-10-25T06:26:34",
    "num_comments": 1,
    "comments": [
        "Hey!\n\nIf you’re interested in building an ML model, I’m working on **Plexe**—an AutoML tool that lets you create models from data or simple descriptions, without needing to code.\n\n**Here’s what Plexe does:**\n\n* Quickly generates models from your data or ideas\n* Works well for any experience level\n* Performed on par with top AutoML tools in testing\n\nI’d love any feedback! If you’d like, you can join our [Discord](https://discord.gg/JwZAXjmZZu), share an ML problem, and we’ll build a model + report for you.\n\n👉 More info and waitlist at [www.plexe.ai](http://www.plexe.ai/)"
    ]
},
{
    "submission_id": "1gbu67k",
    "title": "AI-Generated Content: Are Deepfakes Changing the Game or Breaking It? 🤔",
    "selftext": "In today’s world, AI can create hyper-realistic images and voices, but it also blurs the line between truth and lies. From deepfakes to AI-generated media, new tech is forcing governments to act. Countries like the U.S., EU, and China are stepping up with new laws to regulate this wild west of digital content. But here’s the dilemma—how do we stop harmful content without silencing creativity? And what about freedom of expression?\n\nAs deepfakes spread, journalists are fighting an uphill battle to keep the truth alive. Are these new regulations enough, or is more needed to keep misinformation in check?\n\nWhat’s your take—can we regulate AI without sacrificing free speech? Let’s talk!\n\nhttps://techrevolutiondaily.com/article/regulating-ai-generated-content-new-policies-on-digital-media-manipulation",
    "created_utc": "2024-10-25T06:23:54",
    "num_comments": 0,
    "comments": []
},
{
    "submission_id": "1gbtdtm",
    "title": "Ffmpeg setup ",
    "selftext": "Setting up ffmpeg\n\nWas following a guide on RAGs from Deeplearning Ai , I already have all modules needed , but once I run the script  there’s error “ffprobe and ffmpeg not found” \nPS: clones the ffmpeg correctly but I can’t see bin folder",
    "created_utc": "2024-10-25T05:45:35",
    "num_comments": 4,
    "comments": [
        "You should try to use FFMpeg first and understand what the software does in an hour or two. Nobody wants to help you setup 20 year old software my guy. ",
        "😂😂😂😂thank. You next",
        "Not a single of the 12 billion it looks like 🤣   \n\nBut you are girls who code, respect!! ",
        "😂😂😂😂stoppp"
    ]
},
{
    "submission_id": "1gbtd66",
    "title": "Ffmpeg setup ",
    "selftext": "",
    "created_utc": "2024-10-25T05:44:36",
    "num_comments": 1,
    "comments": [
        "Can you specify what you are looking for here?"
    ]
},
{
    "submission_id": "1gbsu52",
    "title": "Setting up ffmpeg",
    "selftext": "Was following a guide on RAGs from Deeplearning Ai , I already have all modules needed , but once I run the script  there’s error “ffprobe and ffmpeg not found” \nPS: clones the ffmpeg correctly but I can’t see bin folder",
    "created_utc": "2024-10-25T05:16:45",
    "num_comments": 0,
    "comments": []
},
{
    "submission_id": "1gbry4s",
    "title": "Synthetic data use in model training",
    "selftext": "Writer Inc. just announced their newest proprietary large language model, Palmyra X 004, designed to power AI applications and autonomous agents.\n\nWhat caught my attention is their innovative use of synthetic data for training. By curating their own structured datasets through a specialized LLM called Instruct-Adapt X and utilizing an \"early stopping mechanism,\" they've achieved high accuracy and performance at a fraction of the cost reported by competitors. This approach could signal a shift toward precision training and architectural innovation over sheer data volume.\n\nTheir achievements in the latest Stanford HELM reports - ranking in the world's top 10 models - underscore the potential of this approach.\n\nI'm eager to see how Palmyra X 004 impacts the industry and what innovations it will inspire. These advancements bring us one step closer to unlocking the full potential of AI in enterprise applications.\n\nExciting times ahead in the AI landscape, what do you think of the idea of using synthetic data to train models?",
    "created_utc": "2024-10-25T04:25:45",
    "num_comments": 1,
    "comments": [
        "Residual Graph Attention Networks for toxic troll detection? Sounds like a match made in heaven - let's calibrate the hate!"
    ]
},
{
    "submission_id": "1gbrq7l",
    "title": "IOAI Help",
    "selftext": "I am planning on competing in the International Olympiad in Artificial Intelligence in a few months. Most of my knowledge in AI/ML is practical so I should do well on the practical portion of it. But, the olympiad has a scientific portion as well and as I just looked through some of the sample questions I had no idea how to even begin doing them. What resources can I use to effectively prepare myself for this in the next few months.\n\nThis is a link to the sample questions: [https://ioai-official.org/problems/](https://ioai-official.org/problems/)\n\n[https://ioai-official.org/how-to-prepare/](https://ioai-official.org/how-to-prepare/) (scroll to the bottom of page to see sample questions for this one)",
    "created_utc": "2024-10-25T04:11:48",
    "num_comments": 0,
    "comments": []
},
{
    "submission_id": "1gbrbtm",
    "title": "Free Human-Like Text-to-Speech Using Python – A Great Alternative to Paid Options! 🎤",
    "selftext": "Hey community 👋,\n\nI recently created a video tutorial on how to convert text into natural, human-like speech using **free** tools with Python and shell scripting. This method serves as a great alternative to paid options like ElevenLabs, especially if you’re looking to avoid costly software for voice automation projects, audiobooks, or realistic TTS needs.\n\nIn the tutorial, I walk through:\n\n* Setting up a free Python environment for TTS\n* Splitting large text into smaller chunks for smoother processing\n* Using human-like voices for a natural sound\n* Merging audio files to create a seamless output\n\nWhile **this method isn’t as fast as some paid options, it’s entirely free**, and the output quality can be surprisingly realistic! given we set the parameters right It does take a bit of time to generate speech from text, so it may not be for everyone, but I think it’s an exciting option for anyone who doesn’t mind a few extra steps.\n\nIf this sounds useful, please check out the video and let me know what you think! Your feedback is always welcome! 🙏\n\nVideo Link: [YouTube Video](https://youtu.be/pEgqffQKGUI?feature=shared) GitHub Repository: [Code & Instructions](https://github.com/datageekrj/Free-HumanLike-Text-2-Speech)",
    "created_utc": "2024-10-25T03:45:46",
    "num_comments": 2,
    "comments": [
        "I'm intrigued by the models' performance degradation in long-context generation - sounds like we need more 'training data with a side of sanity'!",
        "Great catch! If we’re going to use this, we should approach it thoughtfully. We should aim to structure our transcripts or text so that **each paragraph is completed within 400 tokens**, allowing the entire context to be captured within the speech output. Although this is a limitation, content creators seeking a free text-to-speech solution can still manage it effectively. When combining multiple audio segments, it’s helpful to include a 500ms pause, which can be done with any video or audio editing tool.\n\nIt’s worth noting how powerful this solution is by comparing it to other paid options in terms of processing and voice quality; ours holds up impressively well. That said, some experimentation is definitely needed. I’ve been using this script in my production videos, and while it’s a bit slow, I’m very impressed with the results."
    ]
},
{
    "submission_id": "1gbqh0l",
    "title": "If i find linear algebra and multivariate calculus hard , should I not major in ML ?",
    "selftext": "Hi , so I'm actually going to pursue my degree but I have only two options , Bach Comp Sc ( major ML ) abroad and Bach Comp Science ( major in Data Analytics ) at my own country in which theres an ML subject. The abroad uni requires me to do linear algebra and multivariate calculus while the other one is engineering maths. I can do maths and discrete maths the one that is not too abstract and more too visual/logical but I find the higher level maths mostly on abstract topics beyond integrals and differentation to be intimidating. What do you guys think ? I do have an interest into ML but I dont know which path to choose as I see there are people who jump career into AI/ML even with a degree in software engineering/data. Which option do you guys think is better ?? ",
    "created_utc": "2024-10-25T02:47:19",
    "num_comments": 13,
    "comments": [
        "It’s okay to find linear algebra and multivariate calculus hard. What is not good is if you find those topics boring. As long as you have some motivation to learn those topics then you will eventually be able to learn them.",
        "you don't need to be a linear algebra wizard, once you understand the basics the math of deep learning is always the same",
        "Math is hard for everyone",
        "it's completely your decision to go for it or not, but as a biology BSC I had no idea about them too. I just started to read randomly about them in the past 6 month and meanwhile studied ML too. at first I just memorized them but now I can understand most of the topics without even finishing a whole course for math part of AI. I just read all the topics that I didn't understand with youtube and LLMs. for LinAlg if you have time I suggest to complete gilbert course on YT and it's more than enough. for calculus I suggest to read every part that you encounter during your studies cause you can't read all those topics at once. also don't forget about Stats and Probs.",
        "Model degradation in LongGenBench is a symptom of over-reliance on Gemini-1.5-Flash; let's optimize for real-world data, not just benchmark scores!",
        "hard=high value\n\neasy=low value\n\nwatching tv is easy=low value\n\nlearning math is hard=high value",
        "I work as an ML Engineer for a big tech and I can assure, you won't need a lot of maths and number crunching in the job. \n\n\nHaving said that, a good understanding of linear algebra, statistics, optimization engineering comes in handy every now and then. Since you are in a stem program and you are holding up well in it, I am quite sure the problem is not you not finding Linear Algebra hard. In my opinion you were never introduced to these concepts well enough. This is a challenge I faced in my early days. The professor was horrible. Couldn't hate linear algebra more. But once you start understanding those concepts, it becomes so much fun, you will be seeing it in use everywhere in daily life. \n\n\nMy suggestion would be, try to find resources that can aid your understanding. Even if you do just okay in these areas you can get into ML. 90% of the job isn't number crunching anyway. ",
        "Do you think you will find it easy with practice? No? Then probably no.",
        "Yea Id agree with this. I work as an ML Engineer, masters student in CS and did BA in Data Science, so have done a good amount of math so far. \n\nMath courses were never easy; however, I found them fascinating so I spent many hours working on them to do well. \n\nMath is probably the most \"beautiful\" subject IMO; however, that does not mean it comes easily to me at all. \n\nDepending on your future job, the amount of math you do will vary. I hardly ever use math in my job, as my job is more related to software engineering/ cloud/ ops, modeling is a small part of my job.",
        "Great answer. Genuine curiosity is the best motivator",
        "Does both of these subjects have like a prerequisite of maths topics i should master before starting both of these calculus subjects ? and where do we apply linear algebra and multivariate calculus in ML ?",
        "What's the highest level of mathematics you're comfortable with?\n\nIf algebra or high school level stuff are not a problem for you, check out **Deeplearning.ai's mathematics for machine learning and data science specialization and supplement it with 3Blue1Brown's linear algebra, calculus and neural networks playlists on youtube.**\n\nThis should take care of the motivation and intuition parts of learning mathematics for machine learning. And I think will be enough for you to jump into any ML course like Stanford's statistical learning or Andrew Ng's courses.\n\n\nIf algebra and such are a problem then start with khan academy to catch up to everything before calculus and linear algebra. I even remember browsing **Maths for Data Science by Duke** on coursera or something similar which covers the parts you need to know before doing M4ML and 3Blue1Brown.\n\n\n\nTo answer your second question, I used my linear algebra knowledge in almost every single subject I've learned so far in ML and I haven't even gotten far yet.",
        "Check out this overview of the book Mathematics for ML: https://open.spotify.com/episode/3OBneYlmKlaQOUgWgmMhWI?si=6l0f6FBhRhS12nTRdIsBnQ&t=77"
    ]
},
{
    "submission_id": "1gbqci5",
    "title": "Why does Adam optimizer work so well?",
    "selftext": "Adam optimizer has been around for almost 10 years, and it is still the defacto and best optimizer for most neural networks.\n\nThe algorithm isn't super complicated either. What makes it so good?\n\nDoes it have any known flaws or cases where it will not work?",
    "created_utc": "2024-10-25T02:38:06",
    "num_comments": 29,
    "comments": [
        "You can study the previous algorithms, like classic gradient decent, the momentum and RMSPROP algorithm, and you will find that Adam is just a improved version of those algorithms, combining multiple ideas, there have been many variations of Adam as well which work in specific domain, but I think why it has prevailed all other algorithms is because it has taken all good parts from all algorithms and built itself",
        "I prefer plain old SGD. There are papers out their saying Adam is more sensitive to hyperparameters than tradition SGD and that models trained on SGD generalise better. That matches my experience fine tuning YOLO on small datasets.\n\nIf you are using weight decay you should at least use AdamW as the implementation of weight decay is broken in most libraries for Adam.",
        "An excerpt from a 2019 Andrej Karpathy blog post[ **A Recipe for Training Neural Networks**](https://karpathy.github.io/2019/04/25/recipe/):\n\n\"**Adam is safe:** In the early stages of setting baselines I like to use Adam with a learning rate of [3e-4](https://twitter.com/karpathy/status/801621764144971776?lang=en). In my experience Adam is **much more forgiving to hyperparameters**, including a bad learning rate. For ConvNets a well-tuned SGD will almost always slightly outperform Adam, but the optimal learning rate region is much more narrow and problem-specific. (Note: If you are using RNNs and related sequence models it is more common to use Adam. At the initial stage of your project, again, don’t be a hero and follow whatever the most related papers do.)\"\n\n**Simple explanation:**  Truly optimal hyperparameters do not exist. We can get close with grid, random, bayesian search, etc but that takes time and compute. The more models you train, the more you realize that Adam is incredibly reliable and has been for the people training models for years. It is the Toyota Camry of optimizers. Why do people still buy Camry's for 30-40K when the rest of the industry has caught up and there are better options?",
        "The succinct answer for WHY is that RMSProp and Momentum are cheap approximations of the Hessian aka the curvature of the optimization landscape.",
        "Try research-level projects, such as those based on physics-informed neural networks. Then you'll see that none of the standard methods work (NNs simply won't converge to anything meaningful), and every project requires extensive customization of the training process and the loss function.",
        "It considered speed",
        "Adam was created to physically behave like a Ball with some friction but in terms of Gradient Descent.",
        "It works well for the high dimensional surfaces that occur in dnn optimization. Momentum keeps it from getting stuck in a local min and the surfaces just happen to have a general downward slope so this simple heuristic works. If the surfaces didn't behave this way Adam wouldn't work. It seems natural that the surfaces would be amenable to optimization since we see neural structures in nature but I have no deeper explanation than that.",
        "AI's next chapter: From text-to-image diffusion to generative avatars - will 'Godfathers of AI' redefine human simulators?",
        "The original Adam paper uses the phrase \"sparse gradients\" a lot. What exactly does \"sparse\" mean?",
        "AdamW",
        "I think Adam is substantially less sensitive to optimizer hyper parameters (namely learning rate) than SGD. This make sense since Adam can adapt its learning rate dynamically while SGD cannot.",
        "AdamW works magic for me but yea SGD is great if you have the time or patience",
        "Would you please share a few of these paper? Are they impactful ones?",
        "+1\n\nSGD",
        "Can you provide some examples, if possible? I am new to PINNs.",
        "Adam optimizer considered momentum of how weights change and it takes in its own had to optimise learning rate to give out the best results",
        "Like the idea of behaving like a ball is mentioned in the paper?",
        "gradients where most of the entries in the matrix are near zero, and the majority of the \"mass\" (values with nontrivial magnitude) is concentrated in a small fraction of entries. If you visualized the gradient using a heatmap, it would look like a matrix of zeros with a few randomly scattered non-zero \"hot\" spots.",
        "A simple statement in normal language of \"sparse gradients\" would be \"where there's a few obvious things to change, (just potentially in a high dimensional space)\".",
        "Can you tell, in which context exactly is this term used?",
        "This one was presented at NeurIPs and has over a thousand citations:\nhttps://proceedings.neurips.cc/paper_files/paper/2017/file/81b3833e2504647f9d794f7d7b9bf341-Paper.pdf\n\nThis one also has hundreds of citations:\nhttps://arxiv.org/pdf/1712.07628\n\nAnother NeurIPS paper with hundreds of citations looking at why SGD generalises better:\nhttps://proceedings.neurips.cc/paper/2020/file/f3f27a324736617f20abbf2ffd806f6d-Paper.pdf\n\nThis is a less cited paper but it looks at a new approach to fix Adams generalisation issues for image classification:\nhttps://www.opt-ml.org/papers/2021/paper53.pdf\n\nAnd this is the AdamW paper which argues weight decay is broken in typical implementations of Adam:\nhttps://arxiv.org/pdf/1711.05101",
        "The paper states that Adam works well on problems with sparse gradients.",
        "Wonderful!!! Thanks 🙏🏽",
        "You must be familiar with sparse matrices, it's basically where most values are zero, and very few non zero value exist, like a one hot encoding using vocabulary size of 100k would contain very few 1s and a ton of zeros that's example of sparse matrix",
        "Im currently studying neural networks as preparation to make the hello world of NNs hahah namely the MNIST classifier with MLP as the architecture. So I dont know what is exactly meant by sparse gradients, but correct me if im wrong if I think about sparse gradients does this refer to using reLU activation function in the neurons? Since from what I have seen this neuron only fires (>0) if we pass some threshold. So I think using this would result is sparse gradients between the layers as for a given imput some neurons fire and the rest are 0 so then adam optimizer would work well? Again this is me just theorizing I have not seen the adam optimizer yet and im currently on backpropagation with GD.",
        "That's exactly where my mind went to, but I'm far from an expert",
        "I am not sure about this sorry lol, there is an issue with sigmoid such that it's gradients always become very small but the sparse gradient problem largely refers to sparse output matrices I think, you might be correct but idk..",
        "Relu can contribute but big drivers of sparse gradients are;\n\n- Training on datasets with many parameters that can be sparse (like LLM-style NN training)\n- Training NN with *lots* of layers, you can end up with vanishing gradient problems where the numbers passed back through layers get smaller and smaller \n- Regularisation functions like L1 which encourage weights to become 0 and then result in irreviersibly \"dead\" neurons in some cases, as opposed to L2 which just encourages weights to be very small. There are benefits to L1, like reducing complexity and avoiding the problems that L2 causes (i.e. pushing correlated features' weights to all be very small).\n\nBasically it can crop up for a bunch of reasons, most often in the kind of high parameter space problems that are the most popular, so Adam being better equipped to handle the problem is an understandable boost to its own popularity."
    ]
},
{
    "submission_id": "1gbpv9z",
    "title": "What text books are good for mathematics in machine learning? ",
    "selftext": "",
    "created_utc": "2024-10-25T02:01:49",
    "num_comments": 3,
    "comments": [
        "https://mml-book.github.io/\n\nThis could be the best for starters",
        "Long context generation scenarios: where model performance goes from 'trained' to 'train wreck' - a crisis of neural trust!",
        "RemindMe! -7days"
    ]
},
{
    "submission_id": "1gbppbg",
    "title": "Review my transformer",
    "selftext": "I have implemented transformer in pytorch by referring to this [video](https://youtu.be/ISNdQcPhsts?si=d_QBT3IqJO81-Czt) mainly and some random blogs. Please review my [code](https://github.com/KrishChordiya/mlthings). Like in the video he has trained transformer for translation purpose, similarly I want help to train my transformer on this [dataset](https://huggingface.co/datasets/rajpurkar/squad_v2), starting from tokenisation in a neat and modular way.\n\nSuggestion: To all people stuck in what to learn/do in deep learning. Start implementing things.",
    "created_utc": "2024-10-25T01:48:55",
    "num_comments": 1,
    "comments": [
        "The dataset looks like question answering dataset. These dataset will be preferably used to fine tune pre trained instruct models. Since you want to implement transformer from scratch, the dataset can be unstructured ( for reference watch andrej karapathy video gpt 2 from scratch video and the dataset he has used)\n\nI am not 100% sure about the things i said above, but you can give it a try"
    ]
},
{
    "submission_id": "1gbo83i",
    "title": "NLQ- Data Aanalytics",
    "selftext": "I've been thinking about Natural Language Query (NLQ) and its impact on data analytics. It seems like using NLQ to make data more accessible by allowing users to ask questions in plain language (like \"What were our sales last month?\") is a big step forward for non-technical users.\n\nIn my opinion, while NLQ has made data interaction easier, it still has limitations, especially when it comes to understanding complex queries or providing precise results for more intricate questions. I feel like there's still a long way to go in terms of improving accuracy and handling nuanced queries.\n\n**What do you all think?**  \nIs NLQ living up to its potential in making data analytics more user-friendly, or are there still big challenges that need to be addressed?\n\n***Would love to hear your experiences and thoughts on where it can improve!!***",
    "created_utc": "2024-10-24T23:54:20",
    "num_comments": 0,
    "comments": []
},
{
    "submission_id": "1gbo4xc",
    "title": "10 Best Large Language Models Courses and Training (LLMs)",
    "selftext": "",
    "created_utc": "2024-10-24T23:47:28",
    "num_comments": 0,
    "comments": []
},
{
    "submission_id": "1gbneng",
    "title": "Training the same model using different loss function",
    "selftext": "I trained the same model two times, each with a different loss function, and reached the same accuracy, can I say that they will have the same performance? Or the can have different accuracy for different regions of data, and I can use the both to make a better model?",
    "created_utc": "2024-10-24T22:54:14",
    "num_comments": 4,
    "comments": [
        "I presume it’s a classification problem. Can you be more specific with the details. Eg. the model, the loss function u used etc",
        "something like MSE & MAE or what? Give the context of your work.",
        "It's a classification problem, I'm using kl divergent and categorical cross entropy",
        "Is a classification problem, I'm using lstm+dense , softmax in the output, my train_y is an probability array [0.9, 0.1, 0] , and the loss function is categorical cross entropy and kl divergent"
    ]
},
{
    "submission_id": "1gblzhc",
    "title": "[Discussion] Alternative to Chat GPT Plus",
    "selftext": "",
    "created_utc": "2024-10-24T21:22:41",
    "num_comments": 0,
    "comments": []
},
{
    "submission_id": "1gblzb4",
    "title": "[Discussion] Alternative to Chat GPT Plus",
    "selftext": "",
    "created_utc": "2024-10-24T21:22:24",
    "num_comments": 1,
    "comments": [
        "[https://lmstudio.ai/](https://lmstudio.ai/)"
    ]
},
{
    "submission_id": "1gblae9",
    "title": "Manim : python package for animation for maths",
    "selftext": "",
    "created_utc": "2024-10-24T20:41:33",
    "num_comments": 0,
    "comments": []
},
{
    "submission_id": "1gbkzzc",
    "title": "How to start learning bout machine learning for student",
    "selftext": "I'm a software engineering college student that is about to start his thesis and i plan to base mine on a mobile application for with artificial intelligence/machine learning and i would like to lern how these technologies work, could i kindly ask for recommendations for material to start studying so i can lern how to program one? Thanks in advance.",
    "created_utc": "2024-10-24T20:25:12",
    "num_comments": 3,
    "comments": [
        "I want to learn too.",
        "Do a project",
        "Hey, why don't you start looking at Kaggle competitions and playing around with generating a few models to test on those competitions? I used [https://www.plexe.ai/](https://www.plexe.ai/) to generate a few models and then iterated on it to do well in some of these competitions. It is a fun way of learning ML"
    ]
},
{
    "submission_id": "1gbkd1b",
    "title": "Upstage AI Document Parser: Revolutionise Complex PDF Data Extraction!",
    "selftext": "",
    "created_utc": "2024-10-24T19:50:30",
    "num_comments": 0,
    "comments": []
},
{
    "submission_id": "1gbk4p1",
    "title": "Do I Need a Powerful Laptop for Data Science Showcases and Remote Coding?",
    "selftext": "Hey everyone,\n\nI’m a data scientist, and my company provides remote servers for handling heavy computational tasks. My main focus now is figuring out if I need a powerful laptop for coding remotely and presenting to stakeholders, or if a standard laptop would be enough since all the heavy lifting is done on the server.\n\nWould it make sense to invest in a powerful laptop just for showcasing projects and coding, or would a standard one work fine since the server does most of the work?\n\nWould love to hear from anyone with similar workflows. Thanks!\n",
    "created_utc": "2024-10-24T19:37:46",
    "num_comments": 21,
    "comments": [
        "Basically every company offers this. That said, you usually do some work local and need more than a business laptop for that. \n\nYou do not need it for presenting.,, unless you present the work directly from its source. \n\nAlso, your company should be paying for this.",
        "Not particularly but I would say you want something strong enough to experiment with and run VMs in. ",
        "I use a 13 year old ThinkPad for everything I do... remote into servers, etc. I never have to run things locally, and worst case, I just remote into my desktop PC and do things on there like a quick script execution. All you really need sometimes is an internet connection, a keyboard, and a screen.",
        "Get a cheap VPS w Racknerd! Seriously, it'll cost you like 50 bucks a year.",
        "For remote coding and presenting, any laptop with a network connection will do. Now, assuming you only need to do prototyping locally, I believe a MacBook or a similarly spec'd PC is more than sufficient. I personally chose a MacBook because of its battery life, lightness, POSIX terminal, and Keynote.\n\nI was on the stakeholder side a couple of years ago. I was personally not amused if the data scientist bring a gaming laptop to present, because I was the user of whatever they created and I want it to run smoothly in my business laptop.  One guy presented a dashboard that takes 30 seconds to refresh on his maxed out gaming laptop, we told him to scrap it and build a new one from scratch.",
        "Graph-based R-GAT models are 'kicking it up a notch' in cancer doc classification, outperforming BERT and more!",
        "Yeah you’re right. However, they suggested a server to start with but do you think most companies allow virtual machines?",
        "I create dashboards and run it directly from my laptop, but it gets slow because it is an interactive dashboard and the load time is taking so long. Hence, it gets awkward because during meetings I have to wait till it loads for me to showcase the dashboard.",
        "Btw, I have an iMac at home which I don’t use often. I thought of taking it to work and let IT help desk configure it to make it a work PC. What do you think?!!",
        "But I’m not sure if companies allow VM because of running sensitive data over the internet.",
        "In where I work, they do allow but it depends.",
        "That’s interesting. What do you use to run it from remote? Any specific VM?",
        "Damn!! Because I’m thinking between a MacBook and a gaming laptop. But yeah I get what you mean, I saw one guy at work who has a gaming laptop and it felt unprofessional for some reason even though the company purchased him that laptop lol.",
        "Yeah I do work on a MacBook Pro but it’s a personal one. The speed is unbelievable, it works pretty well with jupyter lab. But I am not sure if the company will allow me to request for a MacBook Pro since most IT users use either Linux or windows.",
        "I think you should not be using a personal computer for work. \n\nWhy is your work not issuing you a computer?",
        "I mean moreso if you are building ands running docker containers. Really depends on role specifics but if they are paying for it get the best you can lol",
        "Yeah I figured. It’s not that but it’s the fact that it takes long to be delivered 🥲",
        "I do have a PC and a laptop. It’s a business laptop but it’s very slow so I wanna request for another one that can work well without crashing. Sometimes I be working on jupyter lab and suddenly the whole screen turns white and crashes. I struggled for couple of weeks to finish the project, and they kept asking why I am delaying. I can’t request one until I am sure what I want, so most people say to request for a MacBook Pro. And I might request for an extra monitor to work on multiple projects. What do you think??",
        "I am actually a BI analyst and part of my responsibility is building AI and ML models so no lol.",
        "I think you should have requested a laptop as soon as what you were working on was too much for the PC you have.",
        "I agree, my manager recommended to get a server. However, I don’t think that would help much since am coding remotely. The PC works fine with coding but sometimes I work from home and a powerful laptop is necessary."
    ]
},
{
    "submission_id": "1gbjf0u",
    "title": "I need critique on my model performance!",
    "selftext": "I recently built this model and these are the results: There is a sign of overfitting but my f1 score is at 90% and Mean CV score is 87% for a 10 fold. It appears this is as much accuracy I can get from the data. Any additional regularization or modification to model complexity reduces the accuracy! This was a NLP Classification project that used about 200K reviews to predict sentiments, Is this good? Any advise, or can I proceed to deployment (P.S Personal Project)\n\nhttps://preview.redd.it/h6lqmvl27twd1.png?width=698&format=png&auto=webp&s=40c2359f8f94e5de18914d49ed5b37a243ce1ae6\n\nhttps://preview.redd.it/d1ydy9ff6twd1.png?width=988&format=png&auto=webp&s=76b87fbeb2d2fdfa65bef64fcded7b75cd07f073\n\nhttps://preview.redd.it/zmdhw16h6twd1.png?width=988&format=png&auto=webp&s=3f9e35cf39df1017d367a257e7e25647722fec28\n\nhttps://preview.redd.it/kxh7a19i6twd1.png?width=1489&format=png&auto=webp&s=99cb69db27e21575f754576e59bb969e014c403a\n\n",
    "created_utc": "2024-10-24T18:59:41",
    "num_comments": 0,
    "comments": []
},
{
    "submission_id": "1gbhomv",
    "title": "Surgical Tool Recognition using PyTorch and Deep Learning",
    "selftext": "Surgical Tool Recognition using PyTorch and Deep Learning\n\n[https://debuggercafe.com/surgical-tool-recognition-using-pytorch-and-deep-learning/](https://debuggercafe.com/surgical-tool-recognition-using-pytorch-and-deep-learning/)\n\nDeep learning and computer vision have a lot of applications in the field of medical imaging. In fact, many deep learning-based approaches are currently being used in medical imaging. These can range from medical image recognition to disease detection, and segmentation of diseased areas. But that’s not all. We can also use deep learning on surgical images. As such, recognizing surgical tools is a big part of such an approach. In this article, we will carry out ***surgical tool recognition using PyTorch and deep learning***.\n\nhttps://preview.redd.it/mg68yzk7rswd1.png?width=1000&format=png&auto=webp&s=1b4bbfe677937105756896b8387c4774bacf125b\n\n",
    "created_utc": "2024-10-24T17:27:36",
    "num_comments": 0,
    "comments": []
},
{
    "submission_id": "1gbgr94",
    "title": "Are Pluralsight’s Skill IQ Tests Accurate for ML & Cybersecurity?",
    "selftext": "Hello!\n\nI'm beginning my cybersecurity and machine learning studies and considering signing up for Pluralsight. I'm curious about their Skill IQ tests. Has anyone had experience with them?\n\n* How accurate do you find the Skill IQ scores?\n* Do they truly reflect your expertise? It looks like they grade on a percentile \n\nI appreciate any thoguhts experiences you can share! Thanks a bunch! 😊\n\n  \np.s. I'm also taking Andrew Ng's Machine Learning course and the Google Cybersecurity Course! I'm so excited",
    "created_utc": "2024-10-24T16:41:04",
    "num_comments": 4,
    "comments": [
        "They're pretty worthless",
        "oh ok, that's good to know. how is it worthless? just totally wrong or nothing really meaningful about them?",
        "Nothing meaningful",
        "ok, good to know. tysm!"
    ]
},
{
    "submission_id": "1gbg14o",
    "title": "ELI5: difference between Variational Inference and Black-Box Variational Inference ",
    "selftext": "Hi all, could you explain me the difference between Variational Inference and Black-Box Variational Inference? In VI we approximate the true posterior minimizing the elbo, so the loglik of the marginal on the data and the KL between the prior and my posterior, what about BBVI? It seems the same for me",
    "created_utc": "2024-10-24T16:06:25",
    "num_comments": 0,
    "comments": []
},
{
    "submission_id": "1gbfypm",
    "title": "Help Needed with A Search Heuristic Design for a Grid Navigation Problem",
    "selftext": "Hey everyone,\n\nI'm working on a search problem and need help with designing an admissible heuristic for A\\* search. The scenario is that an agent (let's call it Blue or B) is trying to reach a goal (the green zone) on an NxN grid where N is odd (max value of N is 9). B starts at a random position and can move in four directions:\n\n* Left (cost 1)\n* Right (cost 8)\n* Up (cost 2)\n* Down (cost 10)\n\nThere's an additional complication: If the agent repeats the same move (e.g., moving left twice in a row), it incurs a penalty of 5.\n\nSome roads are blocked with cement barriers that prevent movement between certain grid cells. My task is to find the optimal path for B to reach the green zone while expanding the **fewest number of nodes**.  \n  \n**Here's where I'm stuck:**\n\nThe heuristic I use for A\\* needs to be **admissible**, meaning it must never overestimate the true cost to the goal. Additionally, I'm required to use a graph-based version of A\\*, where I can re-expand a node if a lower-cost path is found during the search (i.e., with cost relaxation). The heuristic function cannot be another search applied to the problem.\n\nSo far, I’ve tried a few variations of directional Manhattan distance based on the varied movement costs, but I’m still expanding too many nodes or getting higher costs than expected.\n\nDoes anyone have suggestions for a better heuristic or tips on how to optimize my current approach? Any help would be greatly appreciated!",
    "created_utc": "2024-10-24T16:03:14",
    "num_comments": 0,
    "comments": []
},
{
    "submission_id": "1gbds97",
    "title": "Is there a way to find unknown operation?",
    "selftext": "Hello, guys. I have a problem which is 100% solvable with ML. I have a bunch of in-pictures and a bunch of out-pictures. Out-picture is produced by applying to in-picture 3 consecutive \"convolution-like\" operation (I'll explain in comment section). So, in-picture is 54x54 and out picture 54x54, 1 channel. Convolution operations are 3x3 matrices and I know two of them. How can I find the 3rd one? I don't know the order of convolutions application either.",
    "created_utc": "2024-10-24T14:23:01",
    "num_comments": 7,
    "comments": [
        "The operation is \"convolution-like\" because it's not quite convolution. Instead it takes 3x3 matrix from input layer, multiply numbers and sum them. This number places in the (i, j) place of output matrix, where (i,j) is the central number of matrix that we were \"convolving\". Edges \"convolves\" too because we multiply kernel on 3x3 matrix with zero-padding.",
        "It’s solvable with linear algebra and several images of each (in and out). Just need to translate it to the linear algebra language. Try with chatgpt, just provide detailed explanation and talk in terms of matrices, not images",
        "I've solved it. For anyone who will encounter simillar problem, train neural network with several fully-connected layers. Input layer - image, output layer -- kernel for unknown convolution (9x1). The loss function is 3 matrix equations that come from convolution operations, like AX_1 = B, BX_2 = C, CX_3 = D. Unknown vector will be model output. The loss will be MSE between our D and actual D. A, B, C are resized and padded where needed. I can't believe it actually worked.",
        "Already tried. Just a bunch of nonsense. I know I could just solve AX = B. But that would only work if unknown convolution is the last one.",
        "There is 6 different options of ordering 3 convolutions. Evaluate on 1 img, test on different one",
        "I don't think you quite get what I am trying to say. Indeed, if the unknown operation was the last one, than I could just rephrase it in terms of vector of variables (convolution), in-matrix A and out-matrix B. But when the unknown operation is the first one, then the previous method can't be used here, because I don't know out-matrix/vector. So, just like that, I think I need to use deep learning to try to brute-force my unknown layer. I know how to write it algebraically, but I don't know how to implement ML methods here to find unknown elements of convolution.",
        "Say that a1-a9 is the first filter, b1-b9 second and c1-9 third. D input, E output. \n\nFor each i,j in E, its D[i-1:i+2, j-1:j+2] conv a = D1[i, j], now convolve D1 with b get D2 convolve D2 with c get D3.\nIf you plug your known filters to (say) a and c, b1-b9 are unknown in D3.\n\nEqualize D3 with E and you get 54x54 linear equations with 9 unknowns.\nCompute (if it has a solution) b1-b9 and try with different input and output img if it works you are fine, if not, plug known filters to (say) a and b. There is 6 different ways to plug known filters to a, b and c"
    ]
},
{
    "submission_id": "1gbcjh4",
    "title": "UMAP and t-SNE: visualization or dimensionality reduction?",
    "selftext": "Hello,\n\nI have read in forums and blogpost that t-SNE should be principally used for visualization purposes rather than pure dimensionality dimension (i.e getting a dataset with lower dimension that then can be used with another Machine Learning algorithm). Why is that?\n\nOn the other hand, UMAP claims that it can be used for general purpose dimensionality reduction, not just visualization. But reading on its conception, it seems that it is a lot similar to t-SNE, in the sense that it constructs a high-dimensional graph. Why can then UMAP be used for general dimensionality reduction but not t-SNE.\n\nI'd be glad if you could provide academic papers discussing those points as well.\n\nThanks for reading!",
    "created_utc": "2024-10-24T13:28:59",
    "num_comments": 3,
    "comments": [
        "as far as i remember, tsne isn't guaranteed to always give you the same output, but umap is.",
        "Well that is not true according to this link: https://umap-learn.readthedocs.io/en/latest/reproducibility.html\n\nUMAP also has some variance between different runs. There is still some randomness in the optimization procedure, but I have read that the initialization of the lower dimensional embeddings follow a deterministic (w.r.t the data at hands) procedure",
        "You just replied with \"that's not true but it's true\".  Regardless, after looking into it, the answer to your question is literally what I said."
    ]
},
{
    "submission_id": "1gbcaqb",
    "title": "What additional math classes should I take to better understand AI?",
    "selftext": "I’ve taken the following so far:\n1. Calculus 1\n2. Linear algebra \n3. Intro to statistics \n4. Probability for computer scientists\n\nWhat other classes would you recommend?",
    "created_utc": "2024-10-24T13:18:56",
    "num_comments": 11,
    "comments": [
        "You're gonna take calc 3, so do that. (Althought these aren't as helpful tbh. You mostly just need to understand gradients and partial derivatives)\n\nMore stats, you need to at least get to linear regression. (Optional) If you can take a class doing KL divergence, markov chains etc.. thatd be great.\n\n(Optional) Maybe 1 more linear algebra class if you want to do any 3d computer vision.\n\n(Super optional) A topology course if you want to do analysis on the latent space of deep neural networks. These ideas are used a lot for Generative models. (Aka what parameters do you change to make a person's hair color change, face change, stuff like that)",
        "Calc2 , introduction to pure mathematics ,  real analysis i guess",
        "More advanced stats courses, especially any that deal with regression modeling.",
        "Calculus 1-3 (+ vector calc.), real analysis and a proof based linear algebra class to get a deeper understanding beyond these usually purely computation classes. \n\nMore mathematical stats and other stats related courses would probably be great (regression analysis (for sure), statistical learning (duh), and maybe bayesian statistics, stochastic processes, time series analysis, etc.) and possibly measure theory.",
        "[deleted]",
        "What AI class should I take to understand math.",
        "Are any of general linear model, linear programming, and nonlinear optimization being helpful and in demand in ml and DS?",
        "🤣what",
        "r/lostredditors",
        "Are u a Tate fan ?😂",
        "no"
    ]
},
{
    "submission_id": "1gbc81l",
    "title": "Perplexity AI PRO - 1 YEAR OFFER - Almost 75% CHEAPER!",
    "selftext": "As the title: We offer Perplexity AI PRO voucher codes for one year plan.\n\nTo Order: https://cheapgpt.store/product/perplexity-ai-pro-subscription-one-year-plan\n\nPayments accepted: \n- PayPal. (100% Buyer protected. \n- Revolut.",
    "created_utc": "2024-10-24T13:15:42",
    "num_comments": 5,
    "comments": [
        "Bonjour,  \nQuelqu'un a-til utilisé ce deal ?  \nMerci.",
        "https://www.reddit.com/r/CheapGPT/s/r9uHuTkNO3",
        "plus de 1500+",
        "Certes, mais vos conditions générales de vente font mention de 30 j de garantie.   \nQu'advient-il si le plan ne fonctionne plus au bout de 2 mois ?  \nMerci...",
        "La garantie concerne le produit lui-même et l’activation. Le produit devrait fonctionner comme prévu. De plus, la garantie générale est de 180 jours / 6 mois, offerte par PayPal pour contester votre commande à tout moment."
    ]
},
{
    "submission_id": "1gbar5q",
    "title": "Friday Space:  How 1 founder and 1 engineer are applying ML at their startups.  \n",
    "selftext": "Friday: an opp to learn how 1 founder and 1 engineer are applying ML at their startups.\n\nHow:  My buddies at [Oxen.ai](http://Oxen.ai) u/sthoward u/FallMindless3563  host bi-weekly “AI Water Cooler” calls, informal discussions with people applying ML in their personal projects, startups, or BigTechCo apps.\n\nWhen:  Tomorrow (Friday Oct 25) 10:00 AM California time,  1:00 PM Boston time\n\nVenue:  We're experimenting with doing these as a X (Twitter) \"Space\"  [https://x.com/i/spaces/1MnxnDLLONYGO](https://x.com/i/spaces/1MnxnDLLONYGO)  instead of Zoom.\n\nFeatured Guest:   Jack Connor,  u/jac5connor,  founder of  Lingua Aeterna  [https://www.lingua-aeterna.com/](https://www.lingua-aeterna.com/)\n\nQuestions for Jack:  \n1. Why were you just at the Arctic Circle?   \n2. How does that relate to endangered languages and Machine Learning?\n3. What skateboard was used? 4) What if a creative college student wants to intern with / help your project?\n\nFeatured Guest:  \nTomie u/tomieinlove [https://x.com/tomieinlove](https://x.com/tomieinlove)What:  a recreational carbon-fiber aircraft for one Tomie works u/pivotalaero1  [https://x.com/pivotalaero1](https://x.com/pivotalaero1) [https://x.com/tomieinlove/status/1839102796190331336](https://x.com/tomieinlove/status/1839102796190331336)\n\nQuestions for Tomie: \n\nSo many, but let's start with:  What is the most important mathematical concept that makes that aircraft work that might be different from traditional aircraft.\n\nHype to share: \n\nMy X post: \nhttps://x.com/ParallaxAngle/status/1849181569807716832\n",
    "created_utc": "2024-10-24T12:13:40",
    "num_comments": 2,
    "comments": [
        "The conversation last week caused an “after show” hang… will it happen again?!",
        "For a handy calendar reminder for Friday [oxen.ai](http://oxen.ai) Arxiv Dives or AI Water Coolers (on X):\n\nhttps://www.oxen.ai/community/?utm_source=reddit"
    ]
},
{
    "submission_id": "1gb9yjd",
    "title": "Am I stupid or Machine learning course is trash in my college?",
    "selftext": "Warning: Extremely long post, \\~1400 words.\n\nI am from a tier 4 local college in India, qualifications: currently in 5th semester, B.Tech.\n\nI do not think I am academically challenged, I personally think I did pretty well in engineering entrance exam and state board exam. Even though my rank was good enough to get into \"good\" private colleges in Bangalore, I decided to stay in my home and attend a \"supposedly\" good private college close to home.\n\nNeedless to say, the college isn't good. It's pretty bad to be honest. Now I'm in my third year of computer science and engineering degree and the amount of problems in this college is ridiculous. Anyway, that's besides the point.\n\nOne of the problems in Indian engineering educations, is, according to most people, \"outdated syllabus\". I don't think this is a big issue, the standard CSE curriculum consists of all basic foundations of computer science. I don't see any problem with teaching the basics. Basic foundations of the field cannot be updated. I don't think such a thing is possible.\n\nHowever my college tries to fix this \"outdated syllabus\" issue 3rd year onwards. We are taught Web technologies (react, spring boot etc.) and I have checked that we have a blockchain elective, there's more, but this comes at the expense of theory of computation course taught in other colleges. My college also fixes the outdated syllabus problem by updating the machine learning course.\n\nThat is fine and all, but I think I might be stupid because I simply do not understand our machine learning course.\n\nI have seen that all computer science programs in our country do have a AI/ML course in 3rd year. But in my college, it seems like we ONLY have AI/ML courses for a comp sci program 3rd year onwards because there's too many mandatory courses on AI that I am forced into (I didn't sign up for this, the program got updated this year, just before I entered 3rd year).\n\nThe mandatory courses are \"Machine learning and deep learning\" in 5th semester and \"NLP with generative AI\" in 6th semester, I am aware only of these, but I'm pretty sure there's one more in 4th year.\n\nI will get into the syllabus but let me talk about projects. We have mandatory project in the ML/DL course, a mini project (also in AI/ML but we have a choice, though about 70% choices are in AI/ML), a minor project (similar to mini project), and senior year project (most likely the same structure as minor project in 3rd year).\n\nIt's evident that there is too much emphasis on ML in this college.\n\nBut I have problems with this, please correct me, I think I am stupid, please free to criticize my thinking.\n\nMy problem is that there is TOO MANY WORDS, TOO MUCH technical jargon thrown around without really explaining why things work.\n\nLet me outline the syllabus:\n\n1. Introduction to AI/ML, linear regression, logistic regression, Bayesian classification, decision trees.\n2. Regularization, support vector machines.\n3. Ensemble learning, bagging, boosting.\n4. Neural networks, introduction, convolutional neural networks.\n5. Sequence to sequence models, attention mechanism, transformers.\n\nI have omitted many more parts of the syllabus, if interested, I can share the whole course plan.\n\nIn the above syllabus, only chapter 1 and neural network introduction has mathematics, the rest of the syllabus is just prose after prose. I feel like the course is way too abstract for an engineering course. They use fancy words but never ever explained anything deeply.\n\nNow the thing is, I don't have any problems with abstraction. Operating system course was also abstract, but not THIS abstract. We had plenty of low level concepts already taught in digital design and computer organization course so operating system, though abstract, made complete sense. But this course has been taught with no background in mathematics and statistics. None of the concepts which are taught in maths courses are linked in this course. Most of the course is just fancy diagrams and technical vocabulary thrown around, which I cannot seem to understand one bit, because I believe I do not have the necessary prerequisites to even understand these concepts.\n\nI would like to say that, I would just move on by memorizing the fancy words (which is what I believe our teachers are doing) and passing the course somehow. But the problem is projects.\n\nProjects are a massive part of my CGPA. I had no knowledge of AI prior to signing up for a mini project in AI, and once I signed up, there is no going back. I have no choice but to make projects in ML/AI for mini, minor and senior projects, the system is like that (I will come back to this later).\n\nProblem with projects is that these problem statements are just... way too high level. I have looked up online, and it seems like most of these projects require at least a PhD to understand. I was required to do a literature survey on my problem statement (all on my own), but I did not understand A SINGLE WORD from whatever research papers I found. I somehow just memorized the key words and passed the review (thanks to ChatGPT), I had to present a power-point presentation with 5 research papers.\n\nSome technologies (?) on which projects are made include:\n\n\\- Generative Adversarial networks,\n\n\\- Explainable AI,\n\n\\- NN architecture search,\n\n\\- Vision Transformers,\n\n\\- Optimization,\n\n\\- Proving the existence of a computer that can solve halting problem,\n\n\\- Bringing back dead people\n\n(last two are obviously sarcasm, but I can make projects on the latter 2 as well as I can make on the others. Some problem statement are in an Alien language, I can tell you more about it if you want).\n\nI just don't know why no other student seems to have any problems with this. This is just my personal opinion but I don't think knowing technical jargon is equivalent to understanding the concept. I don't say I understand something until I understand it as well as I can understand for example, elementary calculus, and I am pretty confident on my understanding of elementary calculus that I studied in class 12.\n\nAll other students just find some ready made project on GitHub and copy paste the research papers and use Humanize AI to somehow bring down the plagiarism rate.\n\nI don't like doing projects like this one bit. I feel trapped because I cannot switch my problem statement. It also doesn't help that the professor coordinating mini, minor and the machine learning course is a big bully who threatens to fail me if I even think if changing my problem statement now. He's a bully in general, he has made it clear that whoever doesn't publish papers for mini and minor project (and cite his research and give him authorship) will not be eligible to pass the course. And he seems to mean it.\n\nI also have to mention that, the course's lecture classes were finished in 7 days, with some 2.5 hours of lectures a day, a week before the semester officially began. Yes, entire syllabus I mentioned above was taught in 7 days, with 2.5 hours of continuous lectures every day. And now I am expected to make two projects in ML/DL and write papers about them.\n\nI don't know what to do now. AI is getting shoved down my throat. I don't like making projects by plagiarizing others' work. I think I will like ML/AI if I learnt it slowly at my own pace, but it will probably take years until I begin understanding these research papers, let alone come up with a novelty.\n\nPlease help me, maybe I am completely wrong and all this is completely normal. But I don't see the same issue being raised by my friends in other colleges. It might be possible that my friends too don't care about the project and are like others in my college. I want to learn stuff deeply and not just at an abstract high level. That kind of learning feels so superficial and shallow. I understand that many people in computer science feel that there is no need to reinvent the wheel, but how can I make a car without ever understand how the wheels work, in as much detail as possible?\n\nI would love to hear your thoughts. Thank you for your time. I know this post was too long.",
    "created_utc": "2024-10-24T11:40:23",
    "num_comments": 18,
    "comments": [
        "I can‘t say much about the content of these courses without seeing them, but the syllabus you’ve listed is definitely in use and, overall, sounds like a well-rounded Machine Learning curriculum. You do really go into a lot of detail considering this is not your main focus, but it doesn’t sound like the courses are bogus, at least not from this topic list.\n\nHowever, you need to know your way around statistics, calculus and vector math beforehand. Teaching this stuff without that prior knowledge is reckless to say the least, especially when also rushing through topics like this. That really is horrible.\n\nAbout the project topics - all of those are quite massive subfields of ML. So whether or not these make sense depends on what exactly you’re supposed to do. There’s no way you’re going to „solve“ any of these fields as a whole, but it should be possible to write a decent literature review for each of them, or focus on a subproblem - again, if you’ve had time to actually familiarise yourself with these topics, which isn’t the case here.\n\nI also don’t know which journal is going to accept these papers, when they get probably dozens of them at once, most if not all of them of dubious quality. I know researchers having trouble with getting years of research published, and students publishing papers is almost unheard of at my uni, especially for the results of a „mini“ project. This is the part that sketches me out the most, it genuinely sounds like an academic fraud scandal waiting to happen.\n\nAnyway, I would normally never recommend this, but if you want a shortcut - skip everything about statistics, regression, and SVMs and everything else up to point 3. Go straight to neural networks, accept the backpropagation algorithm as something that just works, and start learning about architectures. This is far from ideal, but most of the prior subjects isn’t needed for the projects, so this will make for a fairly good entry point.\n\nFinally, if you want a topic suggestion, I would recommend generative adversarial networks, and specifically focusing on the loss functions used for them. https://hunterheidenreich.com/posts/gan-objective-functions/ lists a ton of these functions and has links to the respective papers, and https://cvnote.ddlee.cc/2019/09/25/gans-pytorch has implementations for a lot of them. You could write an extensive comparison with, I think, relative ease, and if you want something new, you could also try modifying one of these loss functions and test the results.",
        "TLDR?",
        "It’s a classic case of the Indian education system trying to throw buzzwords and advanced tech at students without actually preparing them for it. The curriculum design is all over the place, and the way they’re handling your projects feels more like a setup for failure than an opportunity for genuine learning.\n\nFirst off, I totally agree with you on the outdated syllabus debate. Basics are crucial. You can’t just \"update\" foundational concepts of computer science every couple of years—they form the bedrock for understanding the more advanced stuff. So when people whine about the outdated syllabus, what they often miss is that core subjects like algorithms, data structures, and the theory of computation *need* to stay constant. However, what you’re describing—where your college tries to \"fix\" the outdated syllabus by dumping advanced topics like ML, DL, and blockchain without setting a proper foundation—is just insanity.\n\nHere’s the crux of the problem: you’re being taught advanced AI/ML concepts in a vacuum. Without the necessary mathematical and statistical background, you're just being spoon-fed fancy words without ever getting into the \"why\" or \"how\" of these algorithms. Machine learning is incredibly math-heavy. You need a solid understanding of linear algebra, probability theory, and calculus to even begin to grasp the inner workings of algorithms like support vector machines or neural networks. But instead of building that base, they’re jumping straight to high-level concepts like transformers and GANs. It’s like trying to build a skyscraper without bothering to lay down the foundation. No wonder you feel lost.\n\nYour frustration with the project system is also completely justified. The way your college is forcing you to tackle these complex problem statements, without guidance or support, is a huge red flag. And the fact that your professor is a bully who’s more interested in forcing students to publish papers with his name on them than actually helping you learn is just... pathetic. It reeks of academic exploitation, and sadly, this is way too common in India’s engineering scene.\n\nNow, about the high-level projects. You’re not wrong to feel overwhelmed. The kinds of projects you’re describing—GANs, Vision Transformers, etc.—are *seriously* advanced topics. They require a deep understanding of both the theory and the implementation details, something that takes years of study and hands-on experience. The fact that they’re expecting you to jump into these without any real prep is honestly absurd. You’re not stupid for struggling to understand PhD-level research papers in your third year; the system is failing you by throwing you into the deep end with no support.\n\nNow, here’s what I’d recommend:\n\n1. **Build your foundation**: Before you can tackle these AI/ML concepts properly, you need to strengthen your understanding of the mathematical underpinnings. There’s no way around it. I suggest you go back and study linear algebra, calculus, and probability theory. A great resource for this is the book *\"Mathematics for Machine Learning\"* by Marc Peter Deisenroth. It breaks down the math concepts that are crucial for understanding machine learning in a way that’s digestible for people who might not have that background yet.\n2. **Take online courses at your own pace**: Platforms like Coursera and edX have fantastic ML and AI courses, many of which are designed by top universities. Andrew Ng’s *Machine Learning* and *Deep Learning Specialization* on Coursera are gold standards. These courses build up the concepts step by step, with practical examples, and most importantly, at a reasonable pace. It sounds like you’re more of a deep learner, and these structured courses will give you the time you need to understand the concepts properly, rather than cramming them in a 7-day lecture marathon.\n3. **Focus on small, manageable projects**: Don’t dive headfirst into PhD-level topics like Vision Transformers or GANs just yet. Start with more manageable projects that allow you to apply the basic concepts of machine learning. Kaggle is a great place to find datasets and projects that range from beginner to advanced levels. Pick something that interests you but is within your current level of understanding, and slowly build from there. Projects like credit card fraud detection or spam classification are great starting points.\n4. **Talk to your peers**: It’s crazy how you mentioned that none of your classmates seem to be struggling like you. I’d bet my last dollar that they *are* struggling, but they’re just faking it. Don’t be afraid to open up to your peers and see if you can form a study group. Sometimes, just having someone to bounce ideas off of can make a huge difference.\n\nLastly, you’re not wrong in thinking that this abstract, jargon-heavy way of teaching is making things unnecessarily complicated. It’s possible to explain complex topics simply, but sadly, that’s not what’s happening in your college. The system is broken, but you’ve got the right mindset in wanting to truly understand how things work, rather than just memorizing buzzwords. Keep that up, and you’ll get through this.\n\nIn the meantime, take it slow, build your knowledge brick by brick, and don’t let this poorly structured curriculum break your spirit. You’ve already recognized the flaws in the system, and that’s half the battle won. Stay focused on your goals, and you’ll come out of this mess way more knowledgeable than your classmates who are just scraping by on GitHub projects and buzzwords.",
        "Do yourself a favor and ask a lot of questions. Ask for technical detail. Let them explain. And honestly, when things start a bit techy and then turn in prose quickly, I always get the feeling that instructors do not know the details themselves. Therefore, ask a lot, ask deeply. When you do not all the detail in the answers this is still not a no-go, but at least the answer should somehow show the flavor of \"don't know but let us find out in an excourse experiment..\" or similar. And when you get detail it's good.\n\nWhen you still do not feel comfortable with the answers on the end, you can break up anyways, but you have to give them and yourself a chance to find out, if the course has potential.\n\nReading the syllabus, in my opinion, the course is covering all the important topics for entry level machine learning courses.",
        "This is total speculation, barely grounded in any kind of reality, but I wonder if your professor is in over his head.  Tenure track professors are required to publish regularly to obtain tenure, and perhaps he's not having enough success with his own research group.  Having a large class attempting to write research papers could be a last ditch effort to bolster his publication record.    \n  \nI've seen publications come out of classes, but it's unrealistic to make that a requirement.  One could even argue that it's an unfair requirement for PhD qualifiers or a PhD degree.  You might come armed with a table of acceptance rates for major ML journals and conferences.  There's even heavy competition to get slots at doctoral/student research consortiums in these venues.  If it came to it, you could bring the case to your your department head or dean.  If they are at all knowledgeable about academics, they'd agree that acceptance to a journal, conference or workshop is too strict of a requirement for a course.  On top of that, the timelines might not work at all.\n\nNow if your professor's ask is just that you write up your findings and submit for publication, I'd say play along.  You'll get useful feedback.  But I'd also remind him, that if his name is on it, he will want to give guidance and review before it goes for submission.  The possible downside is that he may demand lots of additional experiments and revisions if he gets too close to the work.\n\nIt often takes years to really understand what's novel and how to make a contribution to a field.  Perhaps you can focus on a very specific task that hasn't been done with much machine learning.  Build a small dataset and show that the application of ML has potential in your problem space.  Here's a toy dataset you could build on your own.  Your goal is predict how early or late you wake up relative to your alarm clock time.  Log +/- on the alarm clock for your outcome variable.  Then also track inputs like time you went to sleep, time you ate, importance of next days goals, whether you exercised, etc...   If you had a fitbit or heart monitor, you could track all kinds of other information.  Record for a couple months, and then for your experiments show which features and which models performed best on this task.  \n  \nRegarding the syllabus, the topics look as expected.  The pacing and material from your descriptions sounds awful.  There are lots of resources to augment your knowledge if you're not able to learn from your professor's lectures.  Many suggest Andrew Ng's course.  I would also recommend [Hal Daume's A Course in Machine Learning](http://ciml.info/).  It doesn't have the latest with deep learning, but his sequencing really helps you to understand what is possible with machine learning before going too deep with fancy algorithms and all of the linear algebra and calculus.  His choice to put decision trees first is brilliant.",
        "I will tell you my take on this:\n\nI am in 7th sem currently and I am from the uni which is considered as one of the best in my state. Here's also the same case, they are teaching us web technology in 5th sem, AI in 6th sem, ML in 7th sem. Though they have covered imp subjects like OS, TOC, Compilers in earlier sems. What's worst thing is here curriculum are not properly structured. They are repeating some of the topics in ML which we already studied in AI. Professor don't teach under the hood of the algorithms, they just read ppts. They don't even care to teach the mathematics. \n\nRegarding projects, we all have same criteria as minor project and then major project in 8th sem. We are provided by the list of the problem statements and all the problems statement were having inclusion of AI. \n\nBut one thing I understood is that this system will not going to changed. Professor will not take interest in teaching students properly. At the end of the day, you'll have to be on your own.\n\nIf you are really interested in learning ML, consider topics from your curriculum as they are good as well as important and start preparing and learning on your own from internet and books. \n\nI myself have started learning ML from books. I can recommend one book which I have followed rigorously: Hands on Machine Learning with Scikitlearn, Keras and Tensorflow. It is quite good and beginners friendly. It covers all topics from ML to Neural networks and Gen AI. There are also some of the good YouTube channels. Let me know if you need list of yt channels.",
        "Course is trash",
        "Course topics u me mentioned are fine and  are fundamentals of Data Science and anyone will start with that. Most importantly the case of AI ML is evolving so fast no colleges are able to keep up. Even organizations are struggling now.",
        "Thank you for a detailed answer, I really appreciate it.\n\n\"it genuinely sounds like an academic fraud scandal waiting to happen.\" this is exactly how I feel as well. I don't want to violate academic integrity and hurt my chances for higher education. But I am forced into publishing these papers by that bully of a professor.\n\nI want to ask you, is it normal for 3rd year/juniors students to know about all these considering they major in computer science and not specializing in ML?",
        "Thank you, thank you so much. I always had mild anxiety but college this year had amplified it too much. Your advice truly is diamond (reddit can be too good to be free sometimes). I will make sure to follow this advice as much as I can. And thank you very much, for assuring me that I am not stupid to struggle like this. Thank you very much.",
        "What do you think about the projects though?",
        "Thank you, your insight is appreciated and thanks for additional resources ",
        "Thanks, it's good to know I am not alone. I also want to learn from books, but our college uses PyTorch instead of tensorflow, so idk if that book is helpful. I only read theory from books now, I cannot find a good book, for beginners using PyTorch. Is there any that you know of? I couldn't find one even after hours of searching online.",
        "ML is new enough of a field that there don't seem to be any real standards for how much of it to teach yet.\n\nThe way it's handled at my uni is that you have a mandatory 1 semester \"introduction to machine learning\" course with two weekly lectures and an exercise, intended to be taken in your 3rd or 4th semester, which covers pretty much the same topics as in your list up to very basic neural networks.\n\nThere's then a follow-up \"advanced machine learning\" course that's also 1 semester, with two weekly lectures + 1 weekly exercise that covers pretty much the rest of the topic list. It isn't mandatory though, you can also choose other \"advanced\" courses building on other mandatory lectures instead.\n\nBoth courses are known to be tough, even though two lectures + exercise is pretty much the most time a course can get, it's still not a lot for the amount of topics covered there.\n\nThen, if you go for a master's degree, there are literally dozens of seminars and smaller courses about more advanced topics in various ML sub-fields.\n\nIf your college puts a strong focus onto ML, I think it isn't outlandish to make the full topic list mandatory. Squeezing it into too short time frame, without teaching any of the fundamentals first, and also forcing you to make projects in this area definitely are though.",
        "I don't know you, but it seems like you tend a little bit to be overwhelmed by those fancy words from the literature review. Data science indeed is not a field of science that is just simple learning. You will have to dive into topics. Yes some are complicated but none of those you mentioned require a PhD to understand. It depends on the level of detail. These are pretty solid fields of research on data science (most of them machine learning in specific) and most of them I know just a little bit, but have a good understanding of it to sort things out or into. So this is often the more important portion of knowledge. And then you can stick to one of those fields and get more familiar. Take your time. Do not be frightened. Others have been successful, you will as well, when you gradually make progress. Just a little once a day or week or whenever. It has taken me years to get a good feeling about a lot of topics. But you have to start and projects are a beautiful way to do it, because it is challenging and not just a course you can listen to. And this is something (and do not get this wrong) I have often seen in colleagues from India, I felt like a lot of people tend to be more happy with direct instructions. Sometimes this is good. But challenges in your daily work would not very often be like that. You have to get through on your own. So combination is the key. Learn theory during courses in a safe and confortable way, get pain and experience in projects.",
        "There is this yt video for pytorch: \nhttps://youtu.be/Z_ikDlimN6A?si=93L714TR0KbeuHAN\n\nIt is pretty lengthy, I haven't seen full video but you can give it a try. \n\nWell, you can also try to learn theory from books and implement it in pytorch by reading pytorch docs. Their docs are really good. Initially it may be difficult for you, but don't give up.  Keep asking questions and keep searching for answers regarding your doubts on internet.",
        "Thank you again. It seems like I am not liking this course only because it doesn't involve mathematics, only uses words to explain concepts (except for the chapters 1 and part of 4) at a very high, abstract level. My college did not put a strong emphasis on ML until this year. I signed up for computer science, not a major in machine learning, but now I will be forced to learn generative AI next semester even though I don't want to learn it without learning the prerequisite mathematics.\n\nBut thank you anyway, your insight was very helpful.",
        "Ok I will try. Thank you very much."
    ]
},
{
    "submission_id": "1gb9own",
    "title": "What do u think?",
    "selftext": "I have been trying my hands on llms for quite sometime and came across one of the best resources available out there the \"LLM Engineer's Handbook\", what intrigued me the most was the attention to detail that the authors provides here from fundamentals to deploying the most advanced applications using llmops best practices.\n\nWhat I liked the most about this book is the way the book reads through its course and explains all the Fundamental concepts using a practical example project throughout the book. I believe this is the best resource out there to dwell into as no book out there has the these kinds of descriptive theoretical flow as mentioned above.\n\nPs: a personal reviewer!",
    "created_utc": "2024-10-24T11:29:18",
    "num_comments": 2,
    "comments": [
        "Happy to answer all your questions!!",
        "Heres where I found it:\nhttps://www.amazon.com/LLM-Engineers-Handbook-engineering-production/dp/1836200072/ref=mp_s_a_1_1?crid=168DXA6YEYSXY&dib=eyJ2IjoiMSJ9.Vm-e0yXjUMOASQqn6NzKILEWKa-uTIEk-bkZAYjFTgNphZVkfIqn7e5JA6O6R29baRHwywj7rPKJBm4Qx2zP4cxOXwgzqrH9KuUnP1XfkOY6scVCi0UwiTzXt4HlbObTzvE2-raGUPS3S95ZeXziztjJTsLVL2u0Ch717aNIjREugVf2qbB53tnwAerCzXG4gRYqPXImyqtkJGOz2Nr4Yw.tk7F-B2OAhOVlUIKL4GNtdLwcmv-xOPb0Aqu-3c8lvU&dib_tag=se&keywords=llm+engineer%27s+handbook&qid=1729794845&sprefix=%2Caps%2C854&sr=8-1"
    ]
},
{
    "submission_id": "1gb9h9q",
    "title": "Big Data  vs Parallel Computing -- Which skill do you guys is more relevant for a ML Engineer ",
    "selftext": "By Big Data I mean concepts like MapReduce model, Distributive concepts, Distributive file systems, cloud computing and proficiency in Hadoop and Spark.\n\nParallel Computing basically dealing with low level systems programming dealing with memory bandwidth, latencies, multithreading, vectorization and proficiency in CUDA, OpenMP\n\nWhich subject do you guys think is more useful and relevant for profiles like ML Engineer & ML Ops in today's market ?  \n  \nPlease help me in choosing one of the above courses, I have option to take only one in upcoming semester.",
    "created_utc": "2024-10-24T11:20:22",
    "num_comments": 4,
    "comments": [
        "My vote is for big data. Parallel computing is the technical aspect, it's how to calculate all the data faster. But which data to compute and how to get value out of them? That's where the breakthroughs are. After all building a faster car means nothing if you have nowhere to go. With big data you can organize, integrate, and apply data in new ways to get new value, to find the destination your fast car should go to, so to speak.\n\n\nSource, among others: https://www.gigabyte.com/Article/what-is-big-data-and-how-can-you-benefit-from-it?lan=en",
        "I think it depends on want to do in your career. Knowledge of big data concepts is a crucial skill when running ML models in production. My anecdotal experience is there’s more jobs where this is relevant than knowing CUDA programming. Granted, I think having cuda knowledge is awesome and you could do really cool work but I don’t think the job market is as robust for an MLE.",
        "big data - data engineers\nparallel computing - ml engineers",
        "Data engineering team will handle the big data, perform ETL processes using airflow, spark, hadoop and map reduce etc. MLE should not have to worry about that. \nThe role of ML engineer is to build efficient training and inference pipelines. So parallel computing will be most wanted skill for a MLE role."
    ]
},
{
    "submission_id": "1gb95q8",
    "title": "Benchmark GGUF model with ONE line of code",
    "selftext": "Hi Everyone!\n\n👋We built an **open-sourced too**l to benchmark **GGUF model**s with a single line of code. [GitHub Link](https://github.com/NexaAI/nexa-sdk/tree/main/nexa/eval)\n\n**Motivations:**\n\nGGUF quantization is crucial for running models locally on devices, but quantizations can dramatically affect model's performance. It's essential to test models post-quantization (how benchmark comes in clutch). But we noticed a couple of challenges:\n\n* No easy, fast way to benchmark quantized GGUF models locally or on self-hosted servers.\n* GGUF quantization evaluation results in the existing [benchmarks](http://github.com/terryyz/llm-benchmark) are inconsistent, showing lower scores than the official results from model developers.\n\n**Our Solution:**  \nWe built a tool that:\n\n* **Benchmarks GGUF models** with **one line of code**.\n* Supports **multiprocessing** and **8 evaluation tasks**.\n* In our testing, it's the **fastest benchmark** for GGUF models available.\n\n**Example:**\n\nBenchmark Llama3.2-1B-Instruct Q4\\_K\\_M quant on the \"ifeval\" dataset for general language understanding. It took 80 minutes on a 4090 with 4 workers for multiprocessing.\n\n1. Type in terminal\n\n`nexa eval Llama3.2-1B-Instruct:q4_K_M --tasks ifeval --num_workers 4`\n\nhttps://reddit.com/link/1gb95q8/video/dxk7fcjxuqwd1/player\n\n2. Results\n\nhttps://preview.redd.it/3only9j2frwd1.png?width=1475&format=png&auto=webp&s=1ae70e7161536f9570adf936253293de84499141\n\nWe started with **text models** and plan to expand to more on-device models and modalities. Your feedback is welcome! If you find this useful, feel free to **leave a star** on GitHub: [https://github.com/NexaAI/nexa-sdk/tree/main/nexa/eval](https://github.com/NexaAI/nexa-sdk/tree/main/nexa/eval)",
    "created_utc": "2024-10-24T11:06:53",
    "num_comments": 1,
    "comments": [
        "I want to try this on my AMD Ryzen GPU"
    ]
},
{
    "submission_id": "1gb9529",
    "title": "Generative Adversarial Network (GANs) Deep Learning - day 75 - INGOAMPT",
    "selftext": "",
    "created_utc": "2024-10-24T11:06:03",
    "num_comments": 0,
    "comments": []
},
{
    "submission_id": "1gb8yqo",
    "title": "What Would You Want if We Built a Job & Career Site Only for AI?",
    "selftext": "I've noticed more and more friends I know are leaning towards finding jobs in the AI field, but they haven't found a platform that truly focuses on job opportunities and careers in AI. LinkedIn is overwhelming and filled with fake job postings, and other platforms just don’t seem to hit the mark. \n\nSo, we're thinking about creating a site dedicated entirely to AI jobs and careers. I'd love to hear your thoughts: What information and features would be most valuable to you? What are the biggest pain points you're facing while looking for AI jobs?",
    "created_utc": "2024-10-24T10:59:05",
    "num_comments": 2,
    "comments": [
        "It's been done several times and those examples get splattered around this sub daily",
        "It has been done"
    ]
},
{
    "submission_id": "1gb8eyd",
    "title": "Algoritms Before Deep Learning course?",
    "selftext": "Hello guys i just finished ML by Andrew NG and i am thinking about doing the Deep Learning Course Vs doing the stanford Algorithms course with the [fast.ai](http://fast.ai) course (i would do 1 hours every day of each), which should i do?",
    "created_utc": "2024-10-24T10:36:14",
    "num_comments": 1,
    "comments": [
        "I would do the Deep Learning first. Sorry I didn't go down the [fast.ai](http://fast.ai) rabbit hole to see how that learning is structured. I was surprised at how easy it is to build ai with Deep Learning--it's a confidence-booster. I think then do Andrew's Stanford courses on algorithms. That is more advanced. Alternatively, Microsoft has Semantic Kernel which, I believe also has step-by-step training. They use C# so I have yet to go down that rabbit hole either. Because semantics are important in ai as in search, I intend to try that out as time permits. Also check out https://digitaldefynd.com/. You may find a free or inexpensive certificate course listed on that platform. I was hoping someone more experienced than I would comment but, alas, I'll be first. Hope it helps."
    ]
},
{
    "submission_id": "1gb78i9",
    "title": "Paper summaries for some of our papers that recently got accepted in NeurIPS",
    "selftext": "Hey everyone, here is the list of papers by our groups that got accepted recently in NeurIPS 2024; It is a proud moment for us as an all-UG group; all the papers were published without any external support from the academia; here is a summary of our papers. We hope this inspires others to pursue AI and look into research as a perspective where we can work together, and all you require is the right guidance (not even necessarily a PhD or a professor). If you find these papers useful and want to working/collabrating with us, feel free to connect with us!\n\n* Give me a hint: Can LLMs take a hint to solve math problems? 👉 [Arxiv link](https://arxiv.org/abs/2410.05915)\n   * We propose improving LLM performance on advanced math problems using \"hints,\" inspired by human pedagogy. We also test the model's robustness to incorrect hints. Our approach is evaluated on various LLMs using diverse problems from the MATH dataset, comparing it with one-shot, few-shot, and chain of thought prompting.\n* Attention Shift: Steering AI Away from Unsafe Content 👉 [Arxiv link](https://arxiv.org/abs/2410.04447)\n   * This study explores methods to restrict unsafe content in generative models. We propose a novel training-free approach using attention reweighing to remove unsafe concepts during inference. Our method is compared to existing techniques, evaluated on direct and adversarial jailbreak prompts. We also discuss potential causes, limitations, and broader implications.\n* Unmasking the Veil: An Investigation into Concept Ablation for Privacy and Copyright Protection in Images 👉 [Arxiv link](https://arxiv.org/abs/2406.12592v1)\n   * This paper extends the study of concept ablation in pre-trained models, as introduced by Kumari et al. (2022). We reproduce results from various concept ablation techniques and propose a novel variant, \"trademark ablation,\" to address branded elements in model outputs. We also analyze the model's limitations, behavior under ablation leakage prompts, and performance degradation on unrelated concepts.\n\n**The Vision Language Group at IIT Roorkee** has compiled an excellent repository of **comprehensive summaries** for deep learning papers from top conferences like **NeurIPS, CVPR, ICCV, and ICML (2016-2024)**. These summaries break down key papers in computer vision, NLP, and machine learning—perfect if you want to stay updated without diving deep into the full papers.",
    "created_utc": "2024-10-24T09:46:47",
    "num_comments": 1,
    "comments": [
        "Found [2 relevant code implementations](https://www.catalyzex.com/paper/arxiv:2410.04447/code) for \"Attention Shift: Steering AI Away from Unsafe Content\".\n\nIf you have code to share with the community, please add it [here](https://www.catalyzex.com/add_code?paper_url=https://arxiv.org/abs/2410.04447&title=Attention+Shift%3A+Steering+AI+Away+from+Unsafe+Content) 😊🙏\n\nCreate an alert for new code releases here [here](https://www.catalyzex.com/alerts/code/create?paper_url=https://arxiv.org/abs/2410.04447&paper_title=Attention Shift: Steering AI Away from Unsafe Content&paper_arxiv_id=2410.04447)\n\n --\n\nFound [1 relevant code implementation](https://www.catalyzex.com/paper/arxiv:2406.12592/code) for \"Unmasking the Veil: An Investigation into Concept Ablation for Privacy and Copyright Protection in Images\".\n\n[Ask the author(s) a question](https://www.catalyzex.com/paper/arxiv:2406.12592?autofocus=question) about the paper or code.\n\nIf you have code to share with the community, please add it [here](https://www.catalyzex.com/add_code?paper_url=https://arxiv.org/abs/2406.12592&title=Unmasking+the+Veil%3A+An+Investigation+into+Concept+Ablation+for+Privacy+and+Copyright+Protection+in+Images) 😊🙏\n\nCreate an alert for new code releases here [here](https://www.catalyzex.com/alerts/code/create?paper_url=https://arxiv.org/abs/2406.12592&paper_title=Unmasking the Veil: An Investigation into Concept Ablation for Privacy and Copyright Protection in Images&paper_arxiv_id=2406.12592)\n\n--\n\nTo opt out from receiving code links, DM me."
    ]
},
{
    "submission_id": "1gb6foo",
    "title": "Lightning AI offering Live Coding Sessions using Compilers to speed up training",
    "selftext": "For those interested in using compilers to speed up their training, this youtube playlist is 12 episodes long and they livestream every Friday at 11am EST with additional eps. Two great minds at work: [https://youtube.com/playlist?list=PLaMu-SDt\\_RB7ImARcTT\\_Wjypwx2vBIBen&si=\\_7XMxgHDjs4jxvKM](https://youtube.com/playlist?list=PLaMu-SDt_RB7ImARcTT_Wjypwx2vBIBen&si=_7XMxgHDjs4jxvKM)",
    "created_utc": "2024-10-24T09:13:16",
    "num_comments": 0,
    "comments": []
},
{
    "submission_id": "1gb5n88",
    "title": "Can anyone tell if this is possible or not??",
    "selftext": "So i have to deploy my already trained ml model i.e.  .pt file on a cam module. It basically detects pest i did web implementation using streamlit but now i wanna deploy it on cam module specifically esp cam module. So what can be the approach for this ??",
    "created_utc": "2024-10-24T08:40:34",
    "num_comments": 0,
    "comments": []
},
{
    "submission_id": "1gb3l3y",
    "title": "Fast AI's deep learning for coders by jeremy howard for begginer? ",
    "selftext": "\nI am a full stack python developer who do web dev in django\n\nI am now starting deep learning,i am a compelete begginer\n\n(Have worked with pandas,numpy,matplotlib,langchain only)\n\nI wanna ask,should i do this course,will i understand what he is coding and code myslef\n\nI just dont want to do blind coding,i wanna learn what is the purpose,how it works and how to do it\n\nWill this course teach me that or not?\n\nThanks in advance",
    "created_utc": "2024-10-24T07:12:52",
    "num_comments": 10,
    "comments": [
        "Do Andrew ng ‘s course",
        "i would start here ----> https://youtube.com/@datascienceanalytics1826?feature=shared",
        "That one feels boring",
        "If you think that course is boring, I got bad news for your hopes of developing anything machine learning.",
        "Then ML isn’t for you. Give up.",
        "I disagree \n\nThere are many sources for many things\nDoes not mean if one find one source boring\nIts not for them lol",
        "just give up",
        "Its for the losers like you man\n\nGiving up is not my thing🌚",
        "you cannot learn machine learning, just give up and thank me later.",
        "I already have 😹\n\nNice try siri larkian,make sure to take a shower"
    ]
},
{
    "submission_id": "1gb1ofs",
    "title": "Neural network from scratch ",
    "selftext": "Hey I recently started my ML journey and was learning about neural network and wanted to implement it from scratch so I searched on YouTube and found a video.\nThen I implemented it and modified a bit and also wrote a blog on medium on it.\nI know it is a very basic thing and I might have many mistakes as it was my first blog and first time doing something like that. So I just wanted to know opinions of you all and suggestions if any.\n\nBlog link-https://medium.com/@pankajgoyal4152/understanding-neural-networks-by-building-one-from-scratch-a-beginners-journey-3a11617313a4\n\nI hope this is what \"learn in public\" is called right?\n\nI would greatly appreciate anything like suggestions or anything from you so feel free.\nIt might help me a lot.",
    "created_utc": "2024-10-24T05:41:22",
    "num_comments": 1,
    "comments": [
        "[deleted]",
        "[https://medium.com/@pankajgoyal4152/understanding-neural-networks-by-building-one-from-scratch-a-beginners-journey-3a11617313a4](https://medium.com/@pankajgoyal4152/understanding-neural-networks-by-building-one-from-scratch-a-beginners-journey-3a11617313a4)\n\n  \ncan you please check again, there is a cross on top you can click on that and then read it without having an account."
    ]
},
{
    "submission_id": "1gb1fmi",
    "title": "Handling Large API Structures with LLMs: Best Practices?",
    "selftext": "Hey everyone,\n\nI'm working on a project involving a massive number of API endpoints (around 15,000 from the Microsoft Graph API), many of which depend on or interact with each other. Given the sheer size, it's impossible to simply feed all of these endpoints directly into an LLM. The challenge is figuring out how to ensure that an LLM can fully understand all the relevant endpoints and their relationships in order to provide accurate results, especially for complex, multi-step queries.\n\nQuestion: How would you approach feeding an LLM all this information? Not just the endpoints themselves, but also dependencies, required parameters, and use cases? Are there any best practices for setting up such a system?\n\nI’m thinking about using a graph database (like Neo4j) to represent the API endpoints and their relationships. Has anyone worked with this kind of solution or have other suggestions?",
    "created_utc": "2024-10-24T05:28:12",
    "num_comments": 0,
    "comments": []
},
{
    "submission_id": "1gb1exm",
    "title": "The Data Product Marketplace: A Single Interface for Business",
    "selftext": "",
    "created_utc": "2024-10-24T05:27:14",
    "num_comments": 0,
    "comments": []
},
{
    "submission_id": "1gb0tcm",
    "title": "AI Breakthrough: GPT-4 Now Hacks Zero-Day Vulnerabilities with 53% Success Rate",
    "selftext": "In a groundbreaking development, researchers have demonstrated how GPT-4, the latest iteration of OpenAI’s language model, can now identify and exploit zero-day security flaws with a 53% success rate. This capability raises crucial questions about AI’s role in cybersecurity and its ethical implications. Published today, the study reveals that GPT-4 is not only able to comprehend complex code but also manipulate it to uncover unpatched vulnerabilities. This discovery could fundamentally change how we approach computer security in the future.      \n What are your thoughts on the ethical implications of using AI like GPT-4 in cybersecurity? Should there be stricter regulations on AI capabilities in security roles?",
    "created_utc": "2024-10-24T04:54:13",
    "num_comments": 18,
    "comments": [
        "Citation",
        "That's really interesting, from an ethical point of view anything that can identify issues and have them fixed pre day zero has to be a benefit to everyone.",
        "Well I have been using my own ML model with AFL++ with radare2 for vuln discovery. It still requires the user to have exploitation development & patching knowledge. Its not as easy as this report makes it sound. It might be great. But still for example if you find a vuln on a repo of a developer theirs a great possibility there are more in others just because like painters developers have styles and habbits. Would love to see a link or research paper on this.",
        "53% sucess doing what. this is idiotic.",
        "https://newatlas.com/technology/gpt4-autonomously-hack-zero-day-security-flaws/",
        "How, fuzzing?\n\nNeed to know more.",
        "All language models are only as smart in a certain field as the user prompting to it",
        "I don't know why people would publish this given the value of zero-days.",
        "I'd guess it's a good thing. There are always more people looking for cracks than those available to keep them out, so this approach while useful to both black and white hats is going to help white hats more.",
        "There's a reason no code AI platforms are not going to make it.\n\nAlso, what do you think cyber security has been doing for ages? These models have already been around pre chatgpt.",
        "What do you mean ethical? There are no ethics among cyber criminals. If gpt4 can help avoid security breaches why wouldn’t you go all in on that? Criminal sure will",
        "[deleted]",
        "Thanks",
        "No. No it will not and no this is far from the \"real question\".",
        "Id be curious to have to do automated testing pre release. Try as many options as possible.",
        "What the fuck?",
        "Would it do the reverse?\n\nAka, if you can commission a super human white hat to stress test your system before release, then that would make it harder for people to find anything later?",
        "Ok develops",
        "Depends whose superhuman AI is more sophisticated."
    ]
},
{
    "submission_id": "1gb0g87",
    "title": "Seeking Feedback and support on my Indie Project -> Picpilot",
    "selftext": "Hey r/learnmachinelearning \n\nI have Built a indie project to solve the headache of deploying image generation and video generation models in production. It's a scalable pipeline that lets you work with SDXL, Flux, and CogVideoX through a single API - basically everything you need to build image generation apps without the infrastructure hassle  anyone can build an image , video creation tool with their own branding easily\n\nCore stuff:\n\n* Batch processing with configurable timeouts.\n* Docker ready.\n* Multiple model support (SDXL, Flux Inpainting, CogVideoX)\n* Support local logging and basic s3 support for temporary URLs for Files\n\nUpcoming:\n\n* Lora Support for Video Models\n* Support for Custom Flux Loras\n* UI for end to end interaction\n* Serverless API's on Runpod.\n\nUses -> Transformers ,Diffusers, Litserve,  Pydantic and Lightning\n\nBuilt this because I was tired of reinventing the wheel every time I needed to deploy these models. Would love to hear from others who've dealt with similar challenges. What bottlenecks have you hit? What would make this more useful for your stack?\n\nI am by no means perfect and the cost to host these models is sky high and there is a lot of stuff to fix but still if you like the project and the problems it solves , please feel free to give a star the [Github Repo](https://github.com/VikramxD/PicPilot/) and support me by upvoting the project on [Peerlist Page ](https://peerlist.io/vikramxd/project/picpilot)its currently Ranked 6th there\n\nLooking for feedback, especially from those who've deployed image gen and video gen models in prod. PRs welcome if you spot improvements, issues\n\nCheers and Happy Building",
    "created_utc": "2024-10-24T04:35:42",
    "num_comments": 0,
    "comments": []
},
{
    "submission_id": "1gazumf",
    "title": "Is 3blue1brown's linear algebra and calculus Playlist enough for ML engineering?",
    "selftext": "I'm wondering if going through 3blue1brown's essence of linear algebra and essence of calculus Playlist would be enough for mathematical foundation for ML?(I am not considering stats and probability since i have already found resources for it) Or do i need to look at more comprehensive course.\n\nMath used to be one of my strong point in uni as well as high-school, but now it's couple of years since I touched any of math topics. I don't want to get stuck in tutorial hell with the math perquisites. \n\nI'm currently learning data structures and algorithm with sql and git on side. Since I was good at math i don't want it take more time than necessary.",
    "created_utc": "2024-10-24T03:59:24",
    "num_comments": 18,
    "comments": [
        "No it’s not enough. It’s good for building intuition though",
        "Is a good intuition helper. \n\n\nBut nothing beats working alongside a textbook or lectures, the most important part is solving problems and challenge your understanding with problem sets and tests.\n\n\nCheck MIT OCW for some algebra and calculus lectures and materials. If time is an issue, look for some mathematics for machine learning lectures/textbooks.",
        "No. It's more of a supplement to a real course. You could also self study with textbooks (doing lots of exercises) with the videos to help with intuition. But nothing will truly help you learn better than dedicated study and practice",
        "Commenting to ask: Is the mathematics for ML and Data science by deepleaning.ai enough tho?",
        "Can you tell me the resource for the stats and probalitu??",
        "It's a good way to get an intuitive understanding but only complements actual theory I would say",
        "It's a great start.",
        "Much more comprehensive. 3blue1brown series on LA isn’t meant to be a course. It’s just meant to help you gain some intuition for some concepts that seem abstract.",
        "Not enough but the best for building the intuition around the basics !",
        "😂😂😂",
        "No it's not. They make interesting videos which is good for understanding the topics intuitively but it's not the replacement of regular courses.",
        "NO",
        "no",
        "> MIT\n\nGilbert Strang's lectures, especially.  He is the GOAT of Linear Algebra teachers.",
        "Most of the people seems to prefer textbook : mathematics for machine learning by A. Aldo Faisal, Cheng Soon Ong, and Marc Peter Deisenroth and for lectures khan academy and mit lectures of prof. Gilbert strang.\n\nAt the courses/lectures seems long enough at a Glance and I'm really confused about textbook as well hence I came here to ask this question",
        "It is enough to get you going in the beginning, you can always check out more advanced stuffs like mit ocw courses once you have a job or something.",
        "Learning is a skill, and every skill requires practice. \n\n\nI can watch a thousand lectures and tutorials about playing guitar. But if I never pick the instrument and practice, I'll never play guitar.\n\n\nWith maths is the same, if you don't do exercises maybe you'll understand the intuition at a surface level, but you need to sit, struggle, understand and solve problems... something magical will happen at some point and everything will make sense.\n\n\nAfter that, implement or understand implementations of algebra operations or gradient descent algorithms is much easier.\n\n\nCheck the \"Coding the matrix\" book and website, maybe it will help combining your other programming knowledge with linear algebra in a more practical way.",
        "Will do. Thanks a lot!"
    ]
},
{
    "submission_id": "1gazrse",
    "title": "Perplexity AI PRO - 1 YEAR OFFER - SUPER DISCOUNTED",
    "selftext": "As the title: We offer Perplexity AI PRO voucher codes for one year plan.\n\nTo Order: https://CheapGPT.store\n\nPayments accepted: \n- PayPal. (100% Buyer protected. \n- Revolut.",
    "created_utc": "2024-10-24T03:54:13",
    "num_comments": 2,
    "comments": [
        "Looks to be totally legit...",
        "- Deals can be done in DM too. (no links needed)\n- Also PayPal accepted as a middleman for anti scamming."
    ]
},
{
    "submission_id": "1gazibo",
    "title": "Bayesian Optimisation for Active Learning ",
    "selftext": "Hi all, \n\nApologies in advance if these are pretty basic questions, but I'm pretty much brand new to this area and need some guidance on what I want to do. I work in computational physics and am doing high throughput simulations on systems of particles. I am trying to do ML to predict new particles that have my property of interest. This is a binary property that I have encode as 0 or 1 and then my inputs are features of the particles, some discrete and some continuous. My idea is this:\n\n  \n1. Create surrogate model to estimate true objective function. I know that GP has an option for classification tasks but apparently isn't that good? I've found random forest works well for my data but then traditionally this doesn't have associated uncertainty. Another potential problem is that my data points take a long time to collect and so the training data will be sparse. Potentially mitigating this is the fact that the data space is pretty small (\\~1000s). \n\n2. Use acquisition function to decide which part of the space to explore next. This is what I'm struggling with. My inputs have hard constraints on them. For example, I can use the size of the molecule (radius of gyration, Rg). My understanding is that the acquisition is supposed to tell me a new molecule to try next but if it gives me a random new value for Rg, how am I supposed to map that to a molecule? Also some of the inputs might have correlations (for example Rg and molecular weight). How do I make sure the suggested search space is actually real and makes sense? If I remove correlation with something like PCA I have an even bigger problem because this is very hard to relate to a real molecule. Finally, some inputs depend on each other i.e. one always has to be higher than the other. I think I am misunderstanding the acquisition function. Am I actually supposed to give it the full search space and it tells me where is next best to go? Are there any best practices for this in the context of my problem?\n\n  \n3. Test the new points and feed them back into the surrogate model until some criterion is met (also not sure on this, is number of new molecules with the desired outcome found suitable? I guess it's flexible?)\n\n  \nAny advice anyone has would be much appreciated",
    "created_utc": "2024-10-24T03:37:19",
    "num_comments": 1,
    "comments": [
        "Ok so I might be extremely off, so if what I suggest doesn't make any sense, please just ignore it.\n\n\nThe second point you mentioned, especially the part about exploration of space leads me to think about using Hamiltonian Monte Carlo (HMC). The idea there is to create a Markov Chain of samples, where each sample has an associated probability. The samples approximate in the limit a probability distribution. Now, since you mentioned some example constraints, perhaps you can define your energy function (which in turn measures your probability of being in a certain state) based on that constraints. Perhaps that might also solve your issue about whether the space explored makes \"sense\", as it might be a measure of the likelihood of a certain molecule being in a certain state.\n\nNow regarding the prediction of the binary property, well, it's a tricky one. I don't know exactly your setup (or exactly what you're doing, I am not familiar with particle simulations), so I don't know if the prediction can be encoded in a standard manner, as in Bayesian inference (i.e. running on the weights of a neural network and then using the samples of those weights for the predictive distribution). Perhaps one idea would be to freeze your system at some point on some convergence criteria, and then run a normal binary classification network on the features of each molecule at a specific state, if that is even a thing given your setup (again, this might be complete nonsense).\n\nAnyway, I think you should also try Gaussian Processes as you've mentioned initially. I've also read about Stein Variational Inference recently, which is also a method inspired about simulations of systems of particles, so maybe check it out?"
    ]
},
{
    "submission_id": "1gayiss",
    "title": "Wrapper for OpenAI's Whisper \"library\"/\"framework\" etc? ",
    "selftext": "I have used the command line version of OpenAI's Whisper since it was released but it doesn't offer all the options the Whisper-\"framework\" (or whatever you call it) contains. There must be someone who has written a \"wrapper\" for this purpose, mustn't it? But I can't find anything on Google. Can you recommend something?\n\nI have 20 000 files, from 10 seconds to several hours long, that I want to transcribe as efficient and with as high quality as possible (I prioritize quality over efficiency. Currently I use the command line client with the large v3-model).",
    "created_utc": "2024-10-24T02:28:30",
    "num_comments": 0,
    "comments": []
},
{
    "submission_id": "1gay4ll",
    "title": "App/website/software that will make my life easier",
    "selftext": "I need to create a human benchmark for my dataset. I have a bunch of images, I want to ask humans a question, and then I want them to either click some points in the image or draw a bounding box (haven't decided yet). What is the best way to do this? I could look into building something like a small app (maybe google colab+gradio) but I have no experience with this kind of stuff and I need this as soon as possible.",
    "created_utc": "2024-10-24T01:58:39",
    "num_comments": 0,
    "comments": []
},
{
    "submission_id": "1gaxw9j",
    "title": "How can I create a loss curve for Gradient Boosting and Random Forest models? Could you provide an example?may I need to use n_estimators for this or is there any other way?",
    "selftext": "**Here is my code for tune model Random forest and GBM** \n\n\n\n`def tune_model(model, param_distributions, X_train, y_train, cluster_label, model_name, is_rf=False):`\n\n`\"\"\"Applies RandomizedSearchCV for hyperparameter tuning and plots the loss curve.\"\"\"`\n\n`random_search = RandomizedSearchCV(`\n\n`estimator=model,` \n\n`param_distributions=param_distributions,` \n\n`n_iter=30,  # Number of parameter settings sampled`\n\n`cv=5,       # 5-fold cross-validation`\n\n`verbose=2,` \n\n`random_state=42,` \n\n`n_jobs=-1   # Use all processors`\n\n`)`\n\n\n\n`random_search.fit(X_train, y_train)`\n\n\n\n`# Best estimator and best hyperparameters`\n\n`best_model = random_search.best_estimator_`\n\n`best_params = random_search.best_params_`\n\n\n\n`return best_model, best_params`\n\n  \n**and apply param\\_RF** = 'n\\_estimators': randint(10, 1000), 'min\\_samples\\_leaf': \\[1,2,3,4,5,6,7, 8\\] \n\n**GBM**\n\n`param_grid_gb = {`\n\n`'n_estimators': [100, 200, 300,400,500,600,700,800,900],`\n\n`'learning_rate': [0.01, 0.03,0.05,0.06],`\n\n`'min_samples_leaf': [1,2,3,4,5] }`",
    "created_utc": "2024-10-24T01:40:03",
    "num_comments": 0,
    "comments": []
},
{
    "submission_id": "1gaxkfi",
    "title": "Streamlining Finance Operations Through Agentic AI",
    "selftext": "",
    "created_utc": "2024-10-24T01:13:23",
    "num_comments": 0,
    "comments": []
},
{
    "submission_id": "1gax0y6",
    "title": "Should I join research lab at my university if I want to work as an AI/ML engineer in the future?",
    "selftext": "I'm currently a student at an university, and I want to work as an engineer in the future. Will it help my career if I join research lab at my university? What are the benefits of it? Or is research labs only for those who want to be PhDs and researchers?",
    "created_utc": "2024-10-24T00:29:58",
    "num_comments": 3,
    "comments": [
        "Yes. It won't hurt, that's for sure. Intially, you will most likely work on reading papers and implementing some models. They might later on involve you into their research. Research will probably help you when getting a job.",
        "Will I develop skills needed for my job if I do research at research labs? Or is these skills only needed for people who work in research?",
        "Yes, because initially, they won't just make you do research. They will guide you and teach you to train a model, challenges, etc. So, join them. See how it goes. If you dont like it, then leave."
    ]
},
{
    "submission_id": "1gavxjn",
    "title": "What is the correct way to calculate perplexity of an LLM in code ?",
    "selftext": "I tried to determine the perplexity of gpt-2 model on the wikitext2 dataset using 2 methods:\n\n1. The huggingface Trainer (ppl: 262915.39172431716)\n\n2. following Perplexity of fixed-length models (ppl: 16.45)\n\n\nAs I understand, method 2 might be more accurate as explained in the blog, but when I used the following script to get perplexity, I get a very high value as mentioned above.\n\n```python\nimport torch\nfrom transformers import AutoModelForCausalLM, AutoTokenizer, Trainer, TrainingArguments\nfrom datasets import load_dataset\nimport math\n\n# Load the pre-trained language model (e.g., GPT-2)\nmodel_name = \"gpt2\"  # You can replace this with another model\nmodel = AutoModelForCausalLM.from_pretrained(model_name)\ntokenizer = AutoTokenizer.from_pretrained(model_name)\ntokenizer.pad_token = tokenizer.eos_token\n\n# Load the Wikitext-2 dataset from Hugging Face\ndataset = load_dataset(\"wikitext\", \"wikitext-2-raw-v1\")\ntest_dataset = dataset[\"test\"]\n\n# Tokenize the dataset\ndef tokenize_function(examples):\n    return tokenizer(examples['text'], return_attention_mask=False, truncation=True, padding=\"max_length\", max_length=512)\n\ntokenized_test_dataset = test_dataset.map(tokenize_function, batched=True, remove_columns=[\"text\"])\n\n# Define the data collator\ndef data_collator(features):\n    batch = {\n        \"input_ids\": torch.stack([torch.tensor(f[\"input_ids\"]) for f in features]),\n        \"labels\": torch.stack([torch.tensor(f[\"input_ids\"]) for f in features]),\n    }\n    return batch\n\n# Setup training arguments for evaluation\ntraining_args = TrainingArguments(\n    output_dir=\"./results\",\n    per_device_eval_batch_size=4,  # Adjust based on your GPU memory\n    evaluation_strategy=\"no\",\n    logging_dir='./logs',\n    do_train=False,\n    do_eval=True,\n    report_to=\"none\",\n)\n\n# Define a Trainer\ntrainer = Trainer(\n    model=model,\n    args=training_args,\n    eval_dataset=tokenized_test_dataset,\n    data_collator=data_collator,\n)\n\n# Evaluate the model and calculate perplexity\neval_results = trainer.evaluate()\n\n# Perplexity calculation\nlog_loss = eval_results[\"eval_loss\"]\nperplexity = math.exp(log_loss)\n\nprint(f\"Perplexity on the Wikitext-2 dataset: {perplexity}\")\n```\n\nWhat am I doing wring here ?",
    "created_utc": "2024-10-23T23:08:07",
    "num_comments": 0,
    "comments": []
},
{
    "submission_id": "1gat3gy",
    "title": " Seeking Help on Automating Data Entry from Restaurant Menu Photos",
    "selftext": "I’m currently facing some challenges while trying to automate data entry from photos of restaurant menus. The main hurdle is dealing with the inconsistent formats across these menus—different layouts, varying text sizes, and diverse pricing structures (like small/large options for items).\n\nI’m looking for reliable solutions to ensure accurate data extraction despite these variations. Has anyone here worked with advanced OCR techniques, machine learning models, or any effective preprocessing methods to handle this?\n\nWould love to hear any insights or recommendations on tools that could help improve the consistency and accuracy of parsing menu data. Thanks in advance!",
    "created_utc": "2024-10-23T20:13:43",
    "num_comments": 0,
    "comments": []
},
{
    "submission_id": "1gas9aj",
    "title": "MMDS clustering",
    "selftext": "hi! i’m trying to solve this question on clustering from mmds.org:\n\nExercise 7.2.6: Consider the space of strings with edit distance as the distance measure. (Edit distance of 1 = 1 insertion or 1 deletion) Give an example of a set of strings such that if we choose the clustroid by minimizing the sum of the distances to the other points we get one point as the clustroid, but if we choose the clustroid by minimizing the maximum distance to the other points, another point becomes the clustroid.\n\na to b = 3,\na to c = 4,\na to d = 1,\nb to c = 3,\nb to d = 3,\nc to d = 5\n\nI have come up with the above distances that would work, but I am having trouble coming with the actual strings. Any insights would help, thank you!\n",
    "created_utc": "2024-10-23T19:28:59",
    "num_comments": 0,
    "comments": []
},
{
    "submission_id": "1gar5yw",
    "title": "Is taking Linear Algebra in undergrad for an AI/ML phD necessary? ",
    "selftext": "I am a senior computer science major completing 2 research internships in ML towards the end of the academic year. I am trying to finish my degree by adding CS electives to my schedule, but I’ve been told that not having taken Linear Algebra would hurt my chances of getting into a program. It’ll be hard to fit in into my schedule next semester. Anyways, is it a make or break to an ML grad school application to not have taken linear algebra?",
    "created_utc": "2024-10-23T18:33:16",
    "num_comments": 68,
    "comments": [
        "I work as a data, scientist, and my graduation was also in computer science. I think linear algebra should be compulsory for all the computer science students and yes, you really need to take that course if you intend to do PhD in machine learning. It’s the building block of machine learning.",
        "I took linear algebra, got an A, and still didn't have enough linear algebra intuition and had to study and practice more on my own. It's more than necessary, it's absolutely essential.",
        "I'd question any program that didn't require it with grading restrictions.  It's that important.",
        "I can't imagine you'd go very far without it.",
        "How is Linear Algebra not a required course for your major at your school? Every STEM major and quantitative social science major should have taken a linear algebra course, imo. Linear Algebra is as important as Calculus, if not more-so.",
        "Linear algebra is single-handedly the single most important subject you will ever learn for AI/ML. You can just do absolutely nothing without it. You can (somewhat) skirt by with surface level understandings of other subjects but not lin alg. ",
        "😂\n\n\nIs that a real question ?",
        "AI is just a fancy word for Linear Algebra :D",
        "It will be incredibly useful to understand advanced material, that’s for sure.",
        "It should be mandatory for a CS degree maybe double check your course requirements because I’m shocked.",
        "A PhD implies that you will develop new knowledge. How will you create new knowledge in ML/AI if you dont understand its basic underworking?",
        "Yes. I came from a psyche bachelors to a CS masters focused on Machine Learning. I didn’t take Linear Algebra and it’s become a serious issue with learning the actual ML.\n\nEdit: I don’t think it’s impossible w/o taking the formal class, just that it’s a very real handicap.",
        "How are you doing computer science and ml without it??",
        "Bro...",
        "AI is mostly applied linear algebra.  Yes you need to understand it well.  \n\nNow if you are just applying ML to problems and not working on the ML algorithms it is still a good idea so you can understand how it works.",
        "Linear algebra was required in both my college and my friends colleges for CS degrees. It’s good to take regardless",
        "Linear Algebra is a must for a CS student. Especially for  ML.",
        "It's probably the single most important class you need to build a strong mathematical foundation for ML",
        "yes.",
        "Linear Algebra is a requisite CS course.",
        "1. You don't have to take it officially, the point is you should be very good at Linear Algebra & Calculus for Ai/ML. (Especially for a PhD.) - you can take any basic course or refer to a book like Linear Algebra by Friedberg or Strang. For Calculus go for Early Transcendtals Calculus by Stewart.\n\n2. Get the Machine Leaning Mathematics book, Marc Peter Diesenroth. - this is freely available by the authors if I remember correctly\n\n  \n3. Coursera is also a good option\n\n  \nThe point being you need to be very good at math but you don't need to take it officially in your undergrad per say (it would be handy though as math is tough to get your head around without a teacher at times)",
        "I'm hopping on this thread to ask if anyone knows where i can get good linear algebra materials?I'm a cs student as well but my uni didnt offer it",
        "Definitely take linear algebra, I had to give myself a crash course on it for my intro to ai class and I would’ve had a MUCH easier time understanding the algorithms if I had already taken it. ",
        "Yes.",
        "ML is linear algebra so probably (ML is also stats but shh 🤫)",
        "Yea probably. It's not that hard though. If you can pass calc II, linear algebra is cake.",
        "There is barely a chance that you will survive without a decent grasp of calculus and linear algebra. \n\nAnd\n\nAlmost 100 percent chance that you will not make any contribution to research.",
        "Unconditionally.",
        "In your undergrad? No ... But my masters program required linear algebra for everyone that didn't take it. I don't think I would take a PhD in ai seriously if he couldn't multiply some matrices.",
        "It's an easy class, besides being necessary.",
        "i am with everyone else, how is it not a required course?",
        "how did you get those internships though?👀",
        "Yes obviously…",
        "Could you explain why it would be difficult for you to fit it in? What are other courses or projects you’re doing?",
        "There’s a difference between an MIT degree and an MIT education. \n\nYou can take an MIT level Linear Algebra course for free. Getting credit is another story.\n\nThat’s the main difference for me - some programs allow alternative methods of demonstrating LA competence.",
        "Linear Algebra was the most useful math course out of any math course I had ever taken.",
        "I just recently did/about to complete a data analytics apprenticeship and got bodied when it came to the statistics part of the course because outside of some basic algebra in high school I never really went on to learn more. This is just some DA course where you learn the basics of forecasting, regression, SQL, Python, PowerBI, etc, definitely not going as far as ML or AI. You *will* get humbled quick if you don't know LA.",
        "I would recommend it for anyone ",
        "“Guys can you do a math heavy phd without much math?”",
        "Absolutely 100%. Linear Algebra and Statistics are by far the two most important things in ML/AI",
        "I’m curious why you haven’t even take it yet? You can’t really seriously understand ML without LA.",
        "dude you shouldn't even be able to get an ML *bachelors* without linear algebra \n\nit is intrinsic to the basically all modern AI models",
        "Pretty much anything with ML's mathematical foundation has linear algebra in it since at the end of the day most of the implementations are done with matrix multiplications.",
        "How are you a senior cs major with two research internships\n\nAnd you haven’t taken linalg?",
        "All of the technologies in ML/AI and DS are basically all linear algebra all the way down...",
        "Linear algebra, calculus and basic probability theory",
        "Just passing by on my feed and no offense, but are you for real? Linear Algebra is fundamental to any sort of numerical methods-based approaches for fitting ML models. No way on earth I would accept a PhD student in ML who hasn't taken linear algebra, stats, and calc 3. There's no way to understand how ML solvers work without those subjects",
        "You have to know linear algebra, it's debatable if taking a class is the best way of learning it.",
        "What kind of ML are you learning if you haven't been doing linear algebra. What do you learn for PCA?\n\nAnd please dont tell me you are also missing a course in probability",
        "Linear algebra, calculus 1-2-3, graduate level statistics, advanced probability distributions, regression analysis, multivariate analysis, bayesian methods I-II, monte carlo, GLMs, stochastic processes, time series analysis, causal inference is the bare minimum that you should take, and in terms of programming languages a working level R is a must and GOOD Python, not the OOP bloat what CS classes teach. (OOP is required, but it is not satisfactory in this job).",
        "I thought linear algebra are usually compulsory core subjects in CS programs?",
        "Honestly.... Linear algebra is just fantastic. I have a PhD in robotics, didn't even need to take it, but it's SO USEFUL, and such a fun branch of math!",
        ">It’s the building block of machine learning.\n\nAlong with multivariable calculus, right? Or would you say that lin-alg is even more fundamental?",
        "LOL idk about other places but it’s actually a degree requirement for me",
        "yes",
        "This.  10 years in and I still sometimes take brush up courses on it to make sure I'm not missing anything.",
        "What else did you study and practice besides the undergrad linear algebra course in order to get more comfortable?",
        "i find it concerning that OP will get a degree and complete ML internships without learning LA...",
        "For real. I know calculus and linear algebra. But my understandings of ML are all rooted in the \"vibes\" of calculus applied to the reality of linear algebra. Unless you are working on making new optimizers, you don't really need to be a calculus expert. But to even interact with a NN, you need to know your way around vector algebra.",
        "Maybe a badly trained bot asked this",
        "Algebra linear?",
        "Yep, I took linear algebra in 1993, and learned it well enough to take higher math classes, but then had to go re-read my college textbook in 2022 to understand LLMs. It made me glad to still have it.",
        "That’s good to hear. My undergrad covers all of that content, save for time series analysis and casual inference, which are offered as electives.",
        "You didnt need linalg for robotics? What about differential equations? Linalg is integral for that subject, and in my college it’s required",
        "Arguably more fundamental since Jacobians are usually written in matrix form",
        "More of the same. If I was doing it now I'd probably hit up Strang: [https://math.mit.edu/\\~gs/linearalgebra/ila6/indexila6.html](https://math.mit.edu/~gs/linearalgebra/ila6/indexila6.html) and Axler: [https://linear.axler.net/](https://linear.axler.net/)",
        "When I started school they didn't have a robotics program. When I got my PhD I was the first robotics PhD, the program now requires it (partially because of how much I bitched at them for not requiring it)",
        "Interesting. State space must have been a pain for the other PhDs (assuming they havent taken a undergrad lin algebra course). actually, did you bitch at them to have a graduate lin alg course?"
    ]
},
{
    "submission_id": "1gapiz4",
    "title": "Prompt Engineering   Part 1",
    "selftext": "",
    "created_utc": "2024-10-23T17:12:14",
    "num_comments": 0,
    "comments": []
},
{
    "submission_id": "1gan7o0",
    "title": "[H] PERPLEXITY AI PRO - 1 YEAR VOUCHER KEY [W] 30$ or 27€ (PayPal Accepted)",
    "selftext": "As the title says got some legit Perplexity AI PRO voucher codes for one year plan. Asking 30$ Per activation\n\n-No login needed\n- Key Delivery activated by the customer himself.\n- No links or anything shady\n- PayPal accepted as middleman for scam protection.\n- Legitimacy & Proof ask in DM’s.\n- Several available \n\nThanks",
    "created_utc": "2024-10-23T15:25:10",
    "num_comments": 0,
    "comments": []
},
{
    "submission_id": "1gammu3",
    "title": "Should I care about the math behind algorithms or just about knowing how to implement them?",
    "selftext": "I often see on tutorials that they start by explaining more or less how each step of the neural network cell or layers works and honestly I can't see whether it's a fun fact or something I should pay attention to ",
    "created_utc": "2024-10-23T14:59:38",
    "num_comments": 9,
    "comments": [
        "It's important.  It can help you identify issues and misapplications.  ",
        "Excuse the rather vulgar vernacular but knowimg the maths separate you from the \"code monkeys\". I'm not saying that you have to have maths locked down like it's breathing but being able to understand them enable you to be more flexible and all in all skilled DS.",
        "You can't implement the algorithms correctly unless you understand the math, because the math is necessary for identifying and correcting a variety of different mistakes.",
        "This will be controversial but I think you don't need to know it for solving most problems that you meet on a day to day. \n\nBut it helps a lot to understand and improve them. Also when you make adjustments due to efficiency and other reasons. You better understand what the trade off is.\n\nAnd later sometime. You will gain intuition that is based on the math.\n\nNo one remembers all the formulas I think. I certainly don't. But the concepts stuck with me and they give me a different feeling of understanding because I went through understanding the math.",
        "only do it if u want to pursue a career in MLE or DS",
        "Yes it’s important to learn from scratch",
        "if you just want to use the algorithm then no, you do not need to understand the math behind the algorithm. Having a general understanding of how different algo work will be beneficial but no.",
        "Caring so much about them won’t be fun unless you are into research. Try to play around with projects and implement them by tweaking parameters to suit your needs",
        "It's important for some ML jobs.\n\nFor others, it's no more important than a youtuber knowing the math behind H.264.\n\nBut it's interesting to follow the math.   For example, understanding how Rotary Positional Embeddings (RoPE) encode position information using a rotation matrix.  \n\n* Did the guy who invented RoPE need to know the math -- absolutely.  \n* Does anyone re-implementing that paper need to know the math -- not really.\n* Do people using models that use it need to know the math -- not a chance."
    ]
},
{
    "submission_id": "1galsmz",
    "title": "How to bypass AWS Lambda memory limits?",
    "selftext": "I am trying to setup my model on AWS Lambda server. The lambda function runs the prediction. However, I'm running into memory issues.\n\n* My model size is 150 MB in TorchScript\n* My total dependencies size is roughly 900 MB (most of which is occupied by PyTorch, cpu version)\n\nThe lambda server has a limit of 250 MB when uploading. I've uploaded `model + code to run` together as a zip file from s3 bucket, which takes up roughly 135/150 MB (zipped/unzipped). I've broken down dependencies into 2 directories (torch\\_dependencies and rest\\_of\\_all\\_dependencies). torch\\_dependencies take up roughly 260/900 MB (unzipped/zipped) and rest\\_of\\_all\\_dependencies take up roughly 62/200 MB (unzipped/zipped). I'm uploading dependencies on 2 separate layers, as each layer can store maximum 250 MB.\n\nModel + code to run has been uploaded successfully, but when I'm uploading 1st dependencies to 1st layer (rest\\_of\\_all\\_dependencies), I'm getting this error:\n\n    Function code combined with layers exceeds the maximum allowed size of 262144000 bytes. \n    The actual size is 340936593 bytes.\n\nWhat's a work around in this case?  \nFrom what I've understood, I cannot upload more than 250 MBs of data to the lambda server + layers. If that's correct, how can I manage to use the lambda server?\n\nNote: I'm using free tier.",
    "created_utc": "2024-10-23T14:23:17",
    "num_comments": 7,
    "comments": [
        "You can also create a docker image of your lambda instead of a zip. You can also set up to 10 gb of mem on a lambda. \n\nIf you really want to keep using the zip you can also load your depencies from s3.",
        "I am not constraint to using zip. I can use docker image that's fine. But how do I bypass 256 MB limit? You mentioned that I can set up to 10 GB memory on lambda. Is it on subscription or do I have to pay for it?",
        "So you're saying I should let lambda function invoke s3, to load dependencies, every time I invoke lambda, right? Isn't that expensive in the sense that everytime lambda is invoked, s3 bucket is also invoked, can't I just put everything on lambda server, so there are no recurring invoked to s3?",
        "I'm not sure what the limits are on the AWS free tier since I've never used that.\n\n  \nHere's the docs for deploying with a dockerfile  \n[https://docs.aws.amazon.com/lambda/latest/dg/python-image.html#python-image-instructions](https://docs.aws.amazon.com/lambda/latest/dg/python-image.html#python-image-instructions)\n\nI'd stop using layers personally since there's zero benefit in your use case - it's too big for the layer.\n\nYou can edit the memory and other limits via the console > your lambda > configuration\n\nMemory: [https://docs.aws.amazon.com/lambda/latest/dg/configuration-memory.html](https://docs.aws.amazon.com/lambda/latest/dg/configuration-memory.html)\n\nStorage: [https://docs.aws.amazon.com/lambda/latest/dg/configuration-ephemeral-storage.html](https://docs.aws.amazon.com/lambda/latest/dg/configuration-ephemeral-storage.html)\n\nSo that's one way, the other would be to copy the files from s3 when you boot up your lambda and run it during start up. When Lambda loads your [handler.py](http://handler.py) or [app.py](http://app.py) (whatever you called it) you can use global functions outside the lambda handler function like you'd do with a normal python file. We have some code internally for the company I work at but I guess it is a bit of a hack.\n\nAnother option I expect would get expensive is simply using EFS with your lambda. It's basically \"adding a ssd to your lambda\" that already has all of your stuff.",
        "To add: this is what you've tried so far right?\n\n[https://segments.ai/blog/pytorch-on-lambda/](https://segments.ai/blog/pytorch-on-lambda/)",
        "Wow thanks. I'll look through these...",
        "This isn't what I've tried but this is the only solution that I had in mind. Thanks for the help."
    ]
},
{
    "submission_id": "1gak2ab",
    "title": "Looking for Resources to Learn Practical AI/ML Development (Not Just Theory)",
    "selftext": "Hey everyone,\n\nI’m a computer engineering graduate with a genuine interest in AI, machine learning, neural networks, and related topics. During my time at university, I took courses in AI and ML, but they were mostly focused on the theoretical side of things. Here’s where I’m at right now:\n\n**What I know:**\n\n1. Theoretical concepts like confidence, accuracy, clustering, etc.\n2. Basic understanding of some models (e.g., KNN, SVM) and how they function.\n3. I can differentiate between AI, ML, and DL.\n\n**What I don’t know (and where I need help):**\n\n1. I don’t know how to set up anything programming-related—no idea how to write code for models, train them, or even set up a notebook.\n2. I’m unfamiliar with the practical side of developing AI/ML models because my university courses emphasized theory over application.\n3. I’m unsure where to find resources that teach the actual development process.\n\n**What I’m looking for:** I want to bridge the gap between theory and practice. I’d love to find resources (preferably free, but I’m open to paid ones too) that focus on coding, setting up environments, and implementing AI/ML models. Ideally, these resources would provide practical, hands-on experience—not just calculations and theory.\n\nI feel like I understand about 50% of this field and want to work on the other 50% that involves actual implementation. Any advice on where to start or what resources to use would be greatly appreciated!\n\nThanks in advance!",
    "created_utc": "2024-10-23T13:10:37",
    "num_comments": 6,
    "comments": [
        "Have you tried Kaggle? It is free, also there some free courses on  Anaconda Web site and you don't need to setup anything using Kaggle hosted Jupiter",
        "i suggest this course [https://bamboogeeks.dev/en/courses/انجليزي-أكاديمية-تعلم-الآلة/](https://bamboogeeks.dev/en/courses/انجليزي-أكاديمية-تعلم-الآلة/) you can find in both languages arabic and english and it cover all ML/ Deeplearning concepts from theory and practice also you can check guided projects on github qnd kaggle/zindi",
        "I am aware of Kaggle, so far I just know it's for fetching datasets and checking out community projects/codes (correct me if I'm wrong). But is there anywhere on the site where I can understand what's going on in the code?",
        "It has some good courses, theory and practice, they are really good for beginners"
    ]
},
{
    "submission_id": "1gajqhz",
    "title": "should I get into bioinformatics?",
    "selftext": "I am a data science BCs student and my main and only focus is AI and I actually really succeeded in it,but i had a small interest in biology as a kid and i noticed that am very good at it and have a good general knowledge even though i didnt study it outside of my high school syllabus,\n\nand there is a good extensive course about bioinformatics  that end up with summer training if i did well, but i don't want to go far from AI, do you guys thinks it is a good idea to take it as a ds student that is interested in AI or you think this is far from my main interest , keep in mind while advising that i got other things to do this year so i may leave it to focus on learning bioinformatics",
    "created_utc": "2024-10-23T12:57:23",
    "num_comments": 2,
    "comments": [
        "If you don't find a niche, you'll have to rely on luck.  Everyone will be looking for jobs with the same exact background.  Be coming a subject matter expert can really help you out in a lot of cases and set you apart (depending on where you're applying of course).",
        "Essentially what I’m aiming for in the sustainability world. As far as I can tell data science is heavily under utilized in my field"
    ]
},
{
    "submission_id": "1gajpws",
    "title": "rx 7800xt vs 4060ti 16 gb\n",
    "selftext": "I am going to buy new gpu but i don't know which one. i am going to start with machine learning on windows or linux(i have been using these system for some time. I want to get more information before i decide).\n\n",
    "created_utc": "2024-10-23T12:56:41",
    "num_comments": 0,
    "comments": []
},
{
    "submission_id": "1gajmkw",
    "title": "Is it necessary to include a loss curve when evaluating Random Forest and Gradient Boosting classifiers in machine learning? If so, how can one create and visualize the loss curve for these models to assess their performance effectively?",
    "selftext": "* when to use the learning curve and loss curve?\n* what is the epoch concept is it needed in the Random forest classification problem?\n* how do you create this type of curve? please write a code solution\n\nhttps://preview.redd.it/hicdoe189kwd1.png?width=484&format=png&auto=webp&s=48eb39e5cba25e884ecffe5178c60a7b78d61b08\n\n",
    "created_utc": "2024-10-23T12:52:42",
    "num_comments": 3,
    "comments": [
        "There is no concept of epochs or iterations in tree based models. You keep adding a) splits, and b) weak learners in random forest or sequential new trees in the case of gradient boosted trees. My guess would be that the “iterations” in this plot is a misnomer",
        "how can i create loss curve then?for GBM and RF and is there example ?",
        "For classical ML algos you only have single point metrics, not an array of points, hence no curves.\n\nWhy do you need these curves in the first place?"
    ]
},
{
    "submission_id": "1gaifbk",
    "title": "Request: Research paper recommendations for Machine Learning",
    "selftext": "Hi I am a B.Tech student with an interest in machine learning. Now that have brushed up on my fundamentals and worked on a few projects, I want to try implementing a research paper. I've heard it's one of the best ways to learn. Seeing how active this community is, I would love some suggestions fora research paper (my interest lies particularly in the Natural Language Processing side of ML). ",
    "created_utc": "2024-10-23T12:01:52",
    "num_comments": 0,
    "comments": []
},
{
    "submission_id": "1gah0ps",
    "title": "How to convert Multivariate Time Series into Univariate Time Series?",
    "selftext": "I have multivariate information, about 9 dimensions of time series collected from different sensors. I want to combine or consolidate these signals into ONE SIGNAL time series to represent that event. I will then use that single time series to do anomaly detection. **What are some recommended methods (mathematical or machine learning), besides PCA, to reduce my 9-D time series to 1-D?** The 1-D single time series should effectively capture the nuances of that time period from all the sensors. And then some classification or regression methods can be used afterwards.\n\nI want to avoid using PCA and explore other available methods. Are there software packages that can create single-unified time-series models from multivariate time series signals?\n\nhttps://preview.redd.it/lizypt2sojwd1.png?width=1024&format=png&auto=webp&s=dd9a53df8f6fdfb34b3b1d265651058002c0b27c\n\n",
    "created_utc": "2024-10-23T10:59:00",
    "num_comments": 23,
    "comments": [
        "sum?",
        "There's probably lots of methods I'm unfamiliar with, but\nif you have a big enough dataset you could use variational autoencoders to gain a lower dimensionality representation of the data with small reconstruction errors.",
        "No matter what you do, you will lose information. Why not just use an anomaly detection method that uses multivariate data?",
        "Why not do anomaly detection on each separately and then combine everything (e.g moving average) into a singular anomaly series?",
        "I don't know but perhaps so sort of correlation test? Seems that dew and temp are highly positive correlated, then at the same time highly negative correlated to press. Almost perfectly. Then you can see that wnd\\_spd and rain are somewhat correlated where's there's clusters there're peaks on temp.",
        "What is the timestep for the time series?",
        "Depends on how do you plan to do anomaly detection, but if you do it with DL, you could just do 1x1x1 convolution in the first layer with 1 kernel and get out a 1D signal, that is already filtered in a way",
        "Seems to easy. Lol ;)",
        "Interesting! I'll certainly look into that. Would you mind sharing what kinds of data applications have you used VAEs on ?  Did it involve time series?",
        "True. I'm okay with losing some information.   \nI tried that and the methods I used reported anomalies only in each of the individual time series. So the model did not take into account how the other variables could factor in determining if a sample is truly an anomaly or just part of the baseline. For example, the algo may have detected an anomaly in the \"snow\" time series, but it really shouldn't be considered an anomaly unless there are also observed peaks in the \"wind speed\" category.",
        "Yes, some signals can definitely correlate with each other. However, I don't believe determining correlation values will help \"combine\" the multi-series into one signal. It would just measure the strength of the linear relationship between two time series right?",
        "1 sample per minute",
        "I didn't, looked at them for a project (wanted to reduce dimensions because training time took too long) , and ended up using a CNN and suffering for it.",
        "Instead of treating each variable as a separate time series, the normal thing to do would be to concatenate them along the time axis and treat them as a single time series. Each time step would include both snow and wind.\n\nYou might need to normalize them a bit though because the magnitude of the variables are different.",
        "right I forgot the whole combining part, i was just thinking by using correlation test you could remove highly correlated signals so less signal to combine. My first thinking was seeing how dew, temp and press were almost perfectly correlated no need to combine as you could remove the others.",
        "Alright, so the resolution would be 1 sample per minute. How long back of a sample are we analyzing as each moving batch?",
        "Ah, sorry to hear. But many thanks for sharing your thoughts on this!",
        "I don't understand what you're trying to convey. Concatenation means to put things in series like a chain. What you're describing sounds like you will combine the snow time series after the train just to make a single time series. Are you referring to something else besides concatenation?   \n  \nAnd yes normalization will be a given in this instance because of the different scales.",
        "I'm okay with analyzing the whole time series at a time to look for 24 hour windows that have the most anomalies in the data.",
        "Not sure if that answered your question. But I'm open to clarifying",
        "It's okay lol",
        "You can concatenate on different axes. Making it a chain is just one axis, I'm refering to a different axis.\n\nI'm saying treat the time series as a 7-dimensional time series (an actual multivariate time series) rather than doing anomaly detection on each dimension independently.\n\nFor example, imagine you a time series with two variables like \\[ 1, 2, 3, 4 \\] and \\[ 5, 6, 7, 8 \\]. In your rain wind example, you are running your algorithm on \\[ 1, 2, 3, 4 \\] and \\[ 5, 6, 7, 8 \\] separately. The normal thing would be concatenate them along the time axis to make it \\[ \\[ 1, 5 \\], \\[ 2, 6 \\], \\[ 3, 7 \\], \\[ 4, 8 \\] \\]. Most anomaly detection methods, for example Matrix Profile, can be used with multivariate data directly without separating the dimensions.",
        "Understood, I'll give that a try. Thank you!"
    ]
},
{
    "submission_id": "1gagvwj",
    "title": "Register for Kaggle's 5-Day Gen AI Intensive Course (Nov 11-15) with Google",
    "selftext": "",
    "created_utc": "2024-10-23T10:53:20",
    "num_comments": 10,
    "comments": [
        "whats the timing of this course ,  US timing or Indian Timing ?",
        "I registered for the course last week.  They said that they would send out a \"Getting Started\" guide one week prior to the start.  We are well within the week prior and I have not seen anything yet.  Has anybody gotten content from them yet?",
        "Me neither",
        "they sent it out dawg, you need the links?",
        "I don’t have an answer as the times aren’t listed anywhere. The only component that is time sensitive are the livestreams, and this will be available on demand",
        "I haven’t, yet",
        "I've received my introductory email today",
        "I've received my introductory email today",
        "I'm good, thank you for checking in",
        "Looks like I just got mine, too."
    ]
},
{
    "submission_id": "1gagb2y",
    "title": "How do we know that gradient descent truly provides the optimal parameters in optimization problems?",
    "selftext": "Gradient descent just seems like one (very good) possible approach to finding optimal weights in something like a Neural Network. But do we know for certain that there is not a better solution (ie more optimal weights with less overfitting etc) that would NOT be found through gradient descent? Or is the solution returned by gradient descent guaranteed to be the best possible solution even theoretically?",
    "created_utc": "2024-10-23T10:29:53",
    "num_comments": 23,
    "comments": [
        "It is relatively easy to prove that gradient descent can always find a local minimum. These could potentially be local minima that are vastly far from global minima, but that's why you run multiples, from multiple different starting conditions, to increase your odds of finding the global minima.\n\nI am unaware of any algorithm is proven to find the global minimum, given the restrictions ML is dealing with.\n\nSo we use gradient descent because it is the best we have.",
        "It usually doesn't provide the globally optimal solution. And in fact people have written a bunch of papers suggesting that you don't necessarily want the global minimum of the loss function; they suggest that, instead, you want the *broadest* minimum, because this provides better generalization.\n\nAn important thing to understand is that the minimum that you get with gradient descent depends significantly on your initial guess for the weights. You'll usually converge to the minimum that is \"near\" the initial guess in some respect. There have been many papers written on the issue of choosing good initial guesses, too.",
        "There are convergence guarantees for some types of functions, particularly convex functions, meaning after some number of iterations you are guaranteed to reach the global minima. But neural network training is not a convex problem, and I don't think any guarantees exist for any algorithms. If your run GD multiple times from different starting weights, you can easily end up at different weighs. As of now I don't think it is possible to even know what the optimal weights are.",
        "It doesn't, and essentially never does. Gradient descent is used because it is 1) computationally efficient, and 2) works well enough empirically. In nearly all realistic problems, it will at best find a local optimum.",
        "It doesn’t? There is no known way to get the “true” optimal weights and it likely isn’t possible for any large-world problem. There are lots of papers and claims about ways to get optimal X, but none are proven optimal. At best you just get another useful tool. *Optimish* if you will.",
        "Emphasis on \"local minimum\", yes. That guarantee is worthless, however.\n\nThere are no known guaranteed generalized algorithms for finding global extrema",
        "Imagine you are placed in a random location on a mountain range.  Now go down hill from wherever you are until you reach a flat area.  You probably didn't reach a local minimum your first try.  Maybe you land in small valley near the top of a mountain?  Though you'd probably get pretty low if you tried several different starting places.   \n\nThat's kind of what gradient descent is doing but in n-dimensional spaces that are affected by other n-dimensional spaces.   Though I believe that thinking of it in fewer dimensions can reveal many of the ways that it could create a poor result and some ways to avoid certain types of pitfalls.",
        "This is optimisation and analysis. You can start with Taylor expansion of a function at a point. ",
        "Nothing? If you can solve it you solve it, or lower bound it, or pray I guess lol. No guarantee that gradient descent will give you global minima.",
        "> It is relatively easy to prove that gradient descent can always find a local minimum\n\nI don't think this is always true; gradient descent can also converge to a saddle point instead of a minimum. See e.g.\n\n- https://stats.stackexchange.com/a/279094\n- https://stats.stackexchange.com/a/489501",
        "I think a lot of packages are now using the Hessian for faster convergence ",
        ">I am unaware of any algorithm is proven to find the global minimum, given the restrictions ML is dealing with.\n\nBrute force search is proven to find it, but is obviously intractable for neural networks with more than a few dozen weights.",
        "The global minimum over a sparse training set will also be the broadest minimum? Not sure what you mean. Sounds smart but don’t think this provides any substance",
        "Gradient decent is not guaranteed to find any minimum, local or global.",
        "Iirc depends on convexity continuity (beta-smoothness) and step-size.",
        "You’re right, that statement would only hold for strictly convex loss surfaces. With more complex functions, there’s no guarantee that GD will find the global optimum.\n\nIf it were always guaranteed, training optimal models would be less of an art and more of just a giant waiting game since every model would always converge given sufficient training time.",
        "You can easily construct functions that have a narrow global minimum and broader local minima.",
        "What i mean is that it won't necessarily find *any* minimum, global or local. Saddle points are not local minima.",
        "I’m not saying you can’t. You can also construct the global minimum to be the broadest minimum.",
        "yeah, but as anyone in this field knows, if you have 0 training loss you probably have a garbage model. That is, extremely low minima, and by extension global minima, are generally pretty poor.",
        "How likely is it that the global minimum will also happen to be the broadest?",
        "I’m not disagreeing with that lol what are you even talking about? Disagree with something I actually said",
        "0 to 100%. Depends on the engineer"
    ]
},
{
    "submission_id": "1gafwqa",
    "title": "How difficult is CUDA to install?",
    "selftext": "I have a 2060, and i'd like to train some image classification models locally...from what i've read getting all the CUDA stuff installed so that pytorch can properly utilize the GPU is a major pain...is this the case? I'm on windows. ",
    "created_utc": "2024-10-23T10:13:11",
    "num_comments": 39,
    "comments": [
        "Following pytorch's documentation, I have found it quite painless! I always install pytorch first (with conda), to avoid having any other weird dependency interactions.",
        "Pytorch is easy just look at their docs, they tell you all the versions you need.\n\nI'd rather fight an eldritch demon than do Tensorflow cuda setup.",
        "Use the NVIDIA base container images for CUDA. [https://hub.docker.com/r/nvidia/cuda](https://hub.docker.com/r/nvidia/cuda) \n\nYou don't have to \"install\" it that way, because it's already installed in the base image. All you have to do is build your application on top of it.",
        "Since your on Windows id recommend just using the Google colab GPU docker image. Pretty sure it installs everything for you. Otherwise, if you want it on your machine outside of a container, just follow the pytorch docs: https://pytorch.org/get-started/locally/",
        "its pretty easy from what i remember ..",
        "For Torch and on Windows, it’s basically just following the instructions to download a .exe from NVIDIA and run it.\n\nIt really is a major pain on Linux, but that doesn’t affect you.",
        "Just don't.... don't touch TensorFlow *shudders*",
        "Forget Windows… if anything, I‘d say use WSL. I wouldn’t say it’s a major pain, it’s usually quite straightforward.",
        "Windows = pain, I strongly recommend switching to linux for ML work. You will have problems with every package on Windows",
        "I set up new servers from scratch (no docker) all of the time, it's easy",
        "All you need to do is run the windows conda command from there [https://pytorch.org/get-started/locally/](https://pytorch.org/get-started/locally/) .",
        "I had a hard time in Linux. Then I figured out how to use docker, I know you're asking about windows but give docker a go.",
        "You don't\nAll the latest library versions, including tensorflow and pytorch, install it for you. But if you're thinking C++ instead of Python, then I guess install the version compatible with the library version.",
        "A lot of great suggestions here but whether system, venv, conda, docker, whatever you do need the Nvidia driver on the system - which is especially easy on Windows.",
        "If you’re using pytorch it’s not too bad, but lots of steps",
        "It was super easy last year when I did it.",
        "CUDA is pretty straightforward but I've been struggling to get rapids to install in my autogluon environment ",
        "`sudo pacman -Sy cuda`",
        "If you use pytorch in conda anyway, the pytorch documentation provides the complete installation command with the required dependencies anyway. I find it really convenient to do.",
        "This, installing with Conda using the PyTorch docs has been painless each time I’ve done it. From what I remember it is 3 total commands",
        "you don't actually need to install anything from NVIDIA. Pytorch already comes with CUDA and is compiled with a specific version.",
        "Good luck getting cuda to work on WSL. Much easier to just install it on Windows.",
        "Yes, i'm going to learn a whole different operating system for a niche project. Thanks for the advice.",
        "How do you setup the gpu drivers on new servers? Do you use apt or directly install from a run file?",
        "I had to download a CUDA toolkit, but that might've been because my graphics driver was outdated.  \nHonestly, I'm not too sure any more, that was a long time ago",
        "Pytorch comes compatible with CUDA (if you install the right version) but you need to install CUDA separately yourself first.\n\nIt isn't a python thing, it's a layer of software that makes it easier/faster/etc to train models on your hardware, but it's not python exclusive since you can also use CUDA in other languages (though python is the favourite child obviously). \n\nIf anything, CUDA is conceptually closer to a device driver for how most people use it (though it does stuff at the software level too).",
        "Knowing how to use linux is a basic software engineering skill",
        "apt, it's not the most effective. Most others I know use docker",
        "I think that is the case for older pytorch versions. I remember having to do that too. But most recently I think it was as easy as installing pytorch.",
        "If you build pytorch from source you need CUDA installed. But as long as you run commands  from https://pytorch.org/get-started/locally/ the CUDA will be bundled with the packages you install",
        "Knowing how to prioritise learning new skills appropriate for the scale of the project is a vital *everything* skill\n\nCould I learn how to spin wool? Probably, peasants could do it centuries ago. Is it easy enough to do? Almost certainly in the modern day. Am I going to bother if I just want to crochet a couple of scarves a year? *Fuck no*. \n\nIf OP is dipping their toe in to do one project, they absolutely don't need to live and breath the Professional Software Dev Lifestyle^(TM), let people be hobbyist or amateurs or, frankly, just a bit crap, at the things they want to do. You don't need to optimise every thing you engage in.",
        "which is why i'm not a software engineer",
        "If you think that SWE doesn't overlap with ML, your perspective will soon be completely flipped",
        "Then why are you trying to do software engineering?",
        "Isn’t ML data science? I’m a data scientist I wouldn’t call my skills close to software engineering",
        "ML models are software. Making software is software engineering",
        "Machine learning is 100% data science. You need just as much of a foundation in stats/algebra as you do coding to do it well.",
        "that's nonsense.\n\nsoftware engineering is the professional process of making software.\n\nlots of people fuck around with creating programs as a hobby or for a course, and wouldn't consider themselves as software engineers. \n\n[The Software Engineering Process: Definition and Scope (computer.org)](https://www.computer.org/resources/software-engineering-process)",
        "They still need to learn basic skills though"
    ]
},
{
    "submission_id": "1gaf7w7",
    "title": "What models should i use for my Shoplifting detection project ?",
    "selftext": "As the title says, I’d like to ask for some advice on computer vision. I’m fairly new to this field but eager to dive deeper. I’m currently want to start working on a project that aims to detect shoplifters. After weeks of research, I discovered that I likely need to use pose estimation and LSTM. Does this seem right for my project, or am I missing something? Like yolo or another models ?\n\nSpecifically, I need to detect the action of someone putting an object in their pocket. I know i need a really big dataset for it but Is it difficult to train LSTM for this? If so, are there better alternatives? \n\nSince I’m still learning, please let me know if I’m adding unnecessary models to my project. Any guidance would be appreciated.",
    "created_utc": "2024-10-23T09:45:37",
    "num_comments": 3,
    "comments": [
        "You don't start a project with an architecture, you first have to decide what exactly are you going to model and how realistic would that be (do you have the data, is there a fitting model architecture, will you be able to train it, etc). \n\nIf your target is detecting an arm connecting to a pocket, then object detection or skeleton detection+classification could be your direction, you'll need lots of annotated images. If your target is the process of an arm moving towards a pocket, then you'll have to take into account the temporal information and build an action recognition model which will require lots of image sequences/videos.",
        "are you a cop? you have to tell me if you're a cop",
        "Where are you able to get large samples of footage or images that would fit your criteria?"
    ]
},
{
    "submission_id": "1gaem46",
    "title": "Ways to label events based on title and description text",
    "selftext": "I am exploring ways to add generated tags which we can display with the Events in our app.  E.g an Event about European Politics should output Politics, European etc. Based on the event title and descriptions (assume we have enough text)\n\nWe could just use a LLM for this but we prefer to self deploy something.\n\nI have been looking at bertopic bit this is mainly categorising and inferring topics: https://maartengr.github.io/BERTopic/index.html\n\nThen I came across bertkey, same author as Bertopic: https://github.com/MaartenGr/KeyBERT\n\nAre there any other approaches? Libraries etc?\nIs this currently stota? We want to explore other options as well before choosing the first one we came across.\n\nThanks",
    "created_utc": "2024-10-23T09:20:45",
    "num_comments": 0,
    "comments": []
},
{
    "submission_id": "1gae62s",
    "title": "Understanding Unsupervised Pretraining Using Stacked Autoencoders - INGOAMPT",
    "selftext": "",
    "created_utc": "2024-10-23T09:02:42",
    "num_comments": 0,
    "comments": []
},
{
    "submission_id": "1gabs11",
    "title": "CNN From Scratch ",
    "selftext": "Hi folks , i need help regarding learning CNN from scratch , so plz share imaterials if its helped  u guyz during learning ",
    "created_utc": "2024-10-23T07:22:39",
    "num_comments": 0,
    "comments": []
},
{
    "submission_id": "1gabre0",
    "title": "Understanding the features of fastText",
    "selftext": "I am trying to understand one of the original papers of fastText for classification (https://arxiv.org/pdf/1607.01759). \n\nHow they handle features is still a mystery to me. My understanding is as follows:\n\n(1) Each word is seen as a bag of vectors of itself and character n-grams. For example, the word 'cookie' is seen as a bag of vectors of 'cookie', 'coo', 'ook', 'oki', 'kie' if we assume n=3.\n\n(2) We take the average of all vectors on the document, multiply it by weights, and feed it into the hierarchical softmax.\n\nNow, there is one section that I don't understand (Section 2.2):\n\n>**2.2 N-gram features** \n\n>Bag of words is invariant to word order but taking explicitly this order into account is often computationally very expensive. Instead, we use a bag of n-grams as additional features to capture some partial information about the local word order.\n\nDo they mean character n-grams here? In that case, I don't understand how they maintain word order with character n-grams, since each bag of vectors does not contain information about adjacent words.\n\n",
    "created_utc": "2024-10-23T07:21:53",
    "num_comments": 1,
    "comments": [
        "Found [48 relevant code implementations](https://www.catalyzex.com/paper/arxiv:1607.01759/code) for \"Bag of Tricks for Efficient Text Classification\".\n\n[Ask the author(s) a question](https://www.catalyzex.com/paper/arxiv:1607.01759?autofocus=question) about the paper or code.\n\nIf you have code to share with the community, please add it [here](https://www.catalyzex.com/add_code?paper_url=https://arxiv.org/abs/1607.01759&title=Bag+of+Tricks+for+Efficient+Text+Classification) 😊🙏\n\nCreate an alert for new code releases here [here](https://www.catalyzex.com/alerts/code/create?paper_url=https://arxiv.org/abs/1607.01759&paper_title=Bag of Tricks for Efficient Text Classification&paper_arxiv_id=1607.01759)\n\n--\n\nTo opt out from receiving code links, DM me."
    ]
},
{
    "submission_id": "1gab8mv",
    "title": "I keep getting a reshaping error CNN + DWT +LSTM",
    "selftext": "Image dimensions img\\_height = 128 img\\_width = 128 batch\\_size = 32 # \n\nStep 1: Preprocess and Load Data train\\_datagen = ImageDataGenerator(rescale=1.0/255.0, validation\\_split=0.2) train\\_data = train\\_datagen.flow\\_from\\_directory( train\\_dir, target\\_size=(img\\_height, img\\_width), batch\\_size=batch\\_size, class\\_mode='binary', subset='training' ) validation\\_data = train\\_datagen.flow\\_from\\_directory( train\\_dir, target\\_size=(img\\_height, img\\_width), batch\\_size=batch\\_size, class\\_mode='binary', subset='validation' ) test\\_datagen = ImageDataGenerator(rescale=1.0/255.0) test\\_data = test\\_datagen.flow\\_from\\_directory( test\\_dir, target\\_size=(img\\_height, img\\_width), batch\\_size=1, class\\_mode='binary', shuffle=False )\n\n \\# Step 2: Define the DWT Layer class DWTLayer(layers.Layer): def call(self, inputs): def process\\_image(image): coeffs = \\[\\] for c in range(image.shape\\[-1\\]): channel\\_data = image\\[..., c\\] cA, (cH, cV, cD) = pywt.dwt2(channel\\_data.numpy(), 'haar') coeffs.append(cA) return np.stack(coeffs, axis=-1) dwt\\_outputs = tf.map\\_fn(lambda img: tf.py\\_function(process\\_image, \\[img\\], tf.float32), inputs) return tf.reshape(dwt\\_outputs, (tf.shape(inputs)\\[0\\], 64, 64, inputs.shape\\[-1\\])) \n\n\\# Adjust the output shape \n\ndef compute\\_output\\_shape(self, input\\_shape): return (input\\_shape\\[0\\], 64, 64, input\\_shape\\[3\\]) # Update to match expected output shape \n\n\\# Step 3: Define the CNN Model with DWT model = models.Sequential() model.add(tf.keras.Input(shape=(img\\_height, img\\_width, 3))) \n\n\\# Convolutional layers \n\nmodel.add(layers.Conv2D(32, (3, 3), activation='relu')) model.add(layers.MaxPooling2D((2, 2))) model.add(layers.Conv2D(64, (3, 3), activation='relu')) model.add(layers.MaxPooling2D((2, 2))) model.add(layers.Conv2D(128, (3, 3), activation='relu')) model.add(layers.MaxPooling2D((2, 2))) \n\n\\# Add the DWT \n\nlayer model.add(DWTLayer()) # Flatten the output for the fully connected layers model.add(layers.Flatten()) # Fully connected layers for classification model.add(layers.Dense(128, activation='relu')) model.add(layers.Dense(1, activation='sigmoid'))\n\n  \nthe error reads \n\nInput to reshape is a tensor with 200704 values, but the requested shape requires a multiple of 524288\n\n\\[\\[{{node sequential\\_1/flatten\\_1/Reshape}}\\]\\] \\[Op:\\_\\_inference\\_one\\_step\\_on\\_iterator\\_2399\\]\n\n2024-10-23 22:58:31.839712: W tensorflow/core/kernels/data/generator\\_dataset\\_op.cc:108\\] Error occurred when finalizing GeneratorDataset iterator: FAILED\\_PRECONDITION: Python interpreter state is not initialized. The process may be terminated.\n\n  \nI will appreciate any help",
    "created_utc": "2024-10-23T06:59:14",
    "num_comments": 3,
    "comments": [
        "1.  Stop using keras/tensorflow and use pytorch\n\n2.  It means one of the layers is outputting a shape that does not fit into the next layer.  You'll have to do some debugging to figure out which one it is and how to fix it.",
        "Thank you so much for your response! Can you tell me the advantages of pytorch over keras/TF. Also is it possible to perform CNN+DWT+LSTM using pytorch",
        "You can do whatever you want in Pytorch, if it's not already a standard model you can write from scratch if you need to.  No one really uses keras and tf anymore, so it's kind of moot trying to learn/use it.  Even google doesn't use tf anymore."
    ]
},
{
    "submission_id": "1gaavof",
    "title": "DL or ML Andrew Ng Coursera",
    "selftext": "Hey, I read a book from Raschka about machine learning (topics without deep learning). \n\nI have deep learning in my studies, but unfortunately it's based on Keras and TensorFlow, so for now I've let go of reading this book (which I'll probably come back to, since it's based on PyTorch).\n\nI started looking at a course on machine learning, but it seems very simple to me, and most of the mathematical aspects are left out. \n\nI am wondering whether to continue with this course (to repeat the material) or start DL Coursera.\n\nI am afraid that the DL course will also be very poor in mathematical aspects- but maybe as an introduction to TensorFlow it will be suitable?\n\nWhat do you guys think about the CS299 course? It's said to be more math-based, but I don't know if it's worth redoing it as I redo the Coursera courses\n\nThanks for your response and engagement",
    "created_utc": "2024-10-23T06:42:52",
    "num_comments": 1,
    "comments": [
        "CS299 does have a reputation for being more math-heavy, so if that’s what you’re after, it could be worth a revisit. But if you're concerned about the time commitment, you might want to think about balancing the DL course for practical skills with a few math-heavy resources on the side—there are some great standalone lectures and books that go deeper into the theory."
    ]
},
{
    "submission_id": "1gaapy7",
    "title": "My Strategy for Making Advanced Machine Learning Concepts Easy",
    "selftext": "As someone who's been teaching machine learning for three years at [Bamboogeeks](https://bamboogeeks.dev/), one of the key challenges I faced was helping students overcome their fear of advanced topics like neural networks and real-world projects. Over time, I developed strategies to simplify concepts:\n\n1. **Use relatable examples**: Breaking complex ideas down into simple terms.\n2. **Start with small projects**: Build confidence with manageable tasks.\n3. **Visual learning tools**: Using visuals and interactive tools to explain models.\n4. **Step-by-step approach**: Introduce topics gradually to avoid overwhelming students.\n5. **Peer collaboration**: Encourage group work for shared learning experiences.",
    "created_utc": "2024-10-23T06:35:30",
    "num_comments": 6,
    "comments": [
        "I was expecting more.",
        "Can you give an example?",
        "Hmm... Sounds complicated. I'm gonna have my students do survival prediction of the Titanic.",
        "Just generic teaching ideas. I thought actual resources/contents would be shared.",
        "Pretty sure the post is just AI",
        "Aww I hate it when they get one past me."
    ]
},
{
    "submission_id": "1gaafmn",
    "title": "What if?",
    "selftext": "Credit: 0xPBIT on X",
    "created_utc": "2024-10-23T06:22:01",
    "num_comments": 2,
    "comments": [
        "Could you reference the paper of the right graph? I just watch a video on youtube, but never read the article",
        "model loss is proportional to the resources spent on their training. And we're getting better at this as in the past we would flatten out quite early, not so much anymore.\n\n  \nWhat's your point?"
    ]
},
{
    "submission_id": "1ga9pck",
    "title": "Bringing Droids into Business processes and Enterprise Systems",
    "selftext": "",
    "created_utc": "2024-10-23T05:47:27",
    "num_comments": 0,
    "comments": []
},
{
    "submission_id": "1ga92zv",
    "title": "Check out the \"Top 10 Data Careers\" and the \"Role SQL Plays in each Career\"!",
    "selftext": "[https://youtu.be/UXRzJxE8mu0](https://youtu.be/UXRzJxE8mu0) ",
    "created_utc": "2024-10-23T05:16:09",
    "num_comments": 0,
    "comments": []
},
{
    "submission_id": "1ga8hdc",
    "title": "Help! Quickest way for learning ML/AI/DL/Data science?",
    "selftext": "What is the best and quickest way to learn ML/AI/DL or data science to start contributing to open source and creating projects (at least simple ones)?\n\nAbout my background: I am a CS student willing to get a job or internship abroad—meaning remote jobs—as my country has little to no scope for AI engineers. I am in my second year and have learned basic Python and OOP with Python; I will be learning DSA this semester. I plan to take data science courses in upcoming semesters, but I don't have high hopes for them.\n\nGetting a job right after graduation is my target. Therefore, it's not possible to learn everything about AI in depth, as it would take a lot of time, and I have my university courses to study. That's why I am aiming for a quicker route. I know that participating in GSoC and making open source contributions can help me secure an internship.\nWhich resources would be most helpful in this situation?\n\nAny assistance would be greatly appreciated. 🙏\nThis is my first post; I apologize if I've mistakenly broken any rules.",
    "created_utc": "2024-10-23T04:45:06",
    "num_comments": 12,
    "comments": [
        "ML/AI/DL/Data science != CS.  Start by taking several statistics courses.  Brush up on calculus.  Take a course in actuarial mathematics too if you can.",
        "Look at the tinygrad project. Go through their MNIST tutorial. Code some simple neural networks with it. They also offer bounties for contributions. Meanwhile, as already advised, brush up statistics and linear algebra.",
        "Just be aware that getting a job (in the US) while working from another country is complicated and most employers won’t do that.\n\nCompanies would have to follow local laws and taxes etc.",
        "Contrary to whatever anyone might tell you, there simply are no shortcuts.  It takes years of effort to become competitive.  You are not going to be a competent ML engineer overnight, and the subject matter is both complex and voluminous.\n\nIf you have to ask this question you are obviously nowhere near ready to contribute anything at all to open source.",
        "[removed]",
        "There is no quick way.",
        "I would suggest doing the courses on Kaggle, they are quick easy and practical. I am not sure about the math and statistics advise? most algorithms are already implemented in SciPython and other libraries.\n\nIn 25 years of working as a software developer, I don't remember one time I had to implement an algorithm and I have worked at some of the biggest financial banks and investment management companies - I don't have much experience in the AI/ML space so don't take my word for granted but I would definitely start on Kaggle first and then learn what you 'actually' need, this is the quickest way",
        "Fortunately, I can take an elective course on statistics in my university. So, I'll surely do that. Thanks.",
        "Yeah. I don't really know what's the best thing to do at this point. I don't have good job prospects in my country. I can try to be a US citizen through a job, visa and IELTS; although I don't know how hard that would be. Many students who have the money, go to US universities for masters and PhD and find a job there. That is an option, but I may need to focus more on GRE and IELTS instead of AI and ML if I choose that path. \n\nWhat's your opinion on this?",
        "Day3 is not accessible",
        "It’s not all rainbows on the US side either. If you go to the cscareers subreddit there are tons of posts that can’t land a job after graduation. \n\nIt’s even worse for people that would need visas. People with a strong resume that are turned down are often asked if they need a visa on that subreddit. \n\nIt’s just a horrible market right now. \n\nYou could try to do school here and hope that the market gets better in 4-5 years but that’s risky and maybe a waste of time.",
        "good point  thanks for mentioning ,  I will ask the team member to fix it ( maximum by end of this week ) until then you can check the day 3 from the menu bar above as well . any other feedback is very welcome as well"
    ]
},
{
    "submission_id": "1ga7uex",
    "title": "Can someone help me understand the bias-variance trade-off in machine learning?",
    "selftext": "Hey everyone!  \nI'm trying to wrap my head around the whole bias-variance trade-off concept in machine learning, and I could use some help. I get that it's about balancing the complexity of a model to avoid underfitting and overfitting, but I’m still a bit confused on how this trade-off actually works in practice. How do you find the sweet spot between high bias (oversimplified model) and high variance (overly complex model)?",
    "created_utc": "2024-10-23T04:07:58",
    "num_comments": 2,
    "comments": [
        "It may not be the most comprehensive answer but I highly recommend the youtube channel \"StatQuest with Josh Starmer\". \nI found myself watching his videos quite often and they were always a good starting point for deeper research about the specific topic.\n\nHe has a video on the bias-variance trade-off as well:\nhttps://m.youtube.com/watch?v=EuBBz3bI-aA",
        "When talking about Bias-Variance, there are a lot of very abstract paths you can take. But if you want to understand in a more practical way, one exercise is to try to understand exactly what you might do to a Dense NN layer, to transform it into a specific trained CNN config. You can do this- the first difference is CNNs have a limited receptive field, while a Dense layer is Fully Connected. By setting all but a few weights to zero, you can make each neuron of the dense layer limited to the same receptive field of a CNN node, and do this differently for each FC neuron. By tying certain weights to be the exact same value, we can also do the same kind of \"weight sharing\" that a CNN would. \n\nAnyways, the point being that setting weights to be the same, settings weights to a constant value, and tying weights to the same value are all instances of reducing Variance, and increasing Bias. The general idea is that in the low-bias network (the Dense NN layer), there are free parameters which are constrained in the high-bias network. \n\nIn practice, this usually looks like trying to figure out what elements of your model are similar, vs which are not. In image analysis, the similarity of each pixel's red channel is engineered into the data format and how the network operates on the channels. Treating each red channel the same is largely due to the principle of Shared Weights, above- applying the \"same\" kernel to each region. \n\nIf you're working in a health prediction problem, you might want to treat all kinds of doctor visits similar, like red channel data. For example maybe you want all non-surgical dentistry to be similar. By making a common function analyze each patients dental history, and by preventing that data from reaching all other parts of the network, you can effectively make dental health a totally separate analysis which is only considered by the network under certain learned circumstances. \n\nLikewise, you can reduce bias by breaking an assumption. Say you have image-like data, but you find out its actually a 1D time series, just in the same shape as image data. Your data process might include Image Transpose for augmentation. Turning this off would reduce Bias. Suppose you find out the data is from sensors randomly distributed around the world. A normal CNN will treat each adjacent pair of points with the same kernel function. By switching from a spatially assumptive CNN model to a Dense or Time Series model, you would break that assumption and reduce Bias in this analysis."
    ]
},
{
    "submission_id": "1ga7clm",
    "title": "Last Day to Sign Up for FREE AI/ML Course with CloudxLab!",
    "selftext": "Don't miss your chance to join the **“Upskill in AI for Free”** course! Whether you're a complete beginner or looking to level up, this 12-month journey will teach you everything from **Python** and **Data Structures** to **Machine Learning** and **Generative AI**. All for **FREE**!\n\n📅 **Live Session**s: Every **Thursday & Frida**y, 8 PM - 10 PM (IST)  \n🎯 **Duratio**n: 12 months  \n📺 **Wher**e: YouTube Live\n\nReady to master AI/ML? **Sign up here**: [https://cloudxlab.com/events/175/master-data-science-and-ai-with-our-free-12-months-online-course/](https://cloudxlab.com/events/175/master-data-science-and-ai-with-our-free-12-months-online-course/)\n\n**Hurry!** This is your last chance before we go live!\n\n\\#AIForAll #FreeCourse #AI #MachineLearning #DataScience #GenerativeAI #Upskill",
    "created_utc": "2024-10-23T03:36:36",
    "num_comments": 2,
    "comments": [
        "Thanks stupid gorilla",
        "You are welcome."
    ]
},
{
    "submission_id": "1ga6shb",
    "title": "My first contribution to NumPy documentation!",
    "selftext": "Hello everyone!\n\nI wanted to take a moment to share a recent accomplishment that I'm really excited about. I made a tiny contribution to the documentation of NumPy, one of the most widely used libraries in the data science and machine learning community.\n\n# What I Did:\n\nI focused on enhancing the documentation related to floating-point precision, which is a crucial aspect of numerical computing. In my contribution, I added a new section that explains the following:\n\n* **Understanding Floating-Point Arithmetic**: I clarified how floating-point arithmetic can introduce small inaccuracies in calculations due to the way computers represent decimal numbers. This is particularly important for new users who may not be aware of these nuances.\n* **Practical Examples**: I provided practical examples to illustrate how users can encounter unexpected results when performing calculations, such as computing determinants of matrices. This helps users grasp the concept of precision in numerical libraries.\n* **Threshold for Comparison**: I discussed the importance of using functions like `np.isclose` to handle floating-point inaccuracies effectively. This method allows users to set a threshold for comparisons, ensuring they account for minor discrepancies that may arise.\n* **Additional Resources**: I linked to external resources for users interested in diving deeper into floating-point arithmetic, helping to foster a better understanding of this essential topic.\n\n# Why It Matters:\n\nContributing to documentation might seem like a small task, but I believe it plays a significant role in improving user experience and accessibility, especially for beginners. Clear documentation helps users avoid common pitfalls and encourages them to explore the library more confidently.\n\n# Acknowledgements:\n\nI want to thank the NumPy community for their valuable feedback and support throughout the process. It was an enriching experience that taught me a lot about collaborative coding and open-source contributions.\n\nIf you're interested in checking out my contribution, feel free to take a look at the [pull request](https://github.com/numpy/numpy/pull/27602).\n\nI'm looking forward to more contributions in the future and continuing to learn and grow in this field! If you have any questions or suggestions on contributing to open-source projects, I’d love to hear them!\n\nThanks for reading!",
    "created_utc": "2024-10-23T03:00:00",
    "num_comments": 0,
    "comments": []
},
{
    "submission_id": "1ga6ayg",
    "title": "Career in Machine Learning",
    "selftext": "Hi, I am an undergraduate in data science and recently my course has started to explore machine learning.\n\nHow do I seek a career in machine learning? Should I be networking early? How does the industry find new employees? I signed up on LinkedIn but there are too many ‘boss’ like personalities that don’t really indicate hiring than the preaching they do.\n\nAny tips on where to look out for jobs? And what jobs would be worth looking into? Personal preference of healthcare would be added benefit",
    "created_utc": "2024-10-23T02:24:36",
    "num_comments": 6,
    "comments": [
        "Linked in is a cesspool of wanna be influencers and bots copying and pasting what influencers say. It is a necessary evil at this point.\n\nAs for getting a job in this field, plan on going to grad school. Look to get internships anywhere and know your shit. This field is insanely competitive and everyone already has a masters, so consider that the baseline and you'll need to find a way to stand out from there.\n\nAlso, please ignore the people disconnected from reality that say you don't need a degree in this field. It's exceptionally rare to get in nowadays without a graduate degree and anything below that gets even more rare. The people that parade this are simply lying. Their third cousins best friend that got in after being a janitor for 12 years did not actually get in.",
        "Ok so - my niche is in healthcare, and the course I’m pursuing does allow to progress into a Masters programme part time while working.\n\nAny tips on how I can score internships or entry level jobs? Issue is I will be hitting 35 years old by the time I complete all this",
        "Ahhh you're in the UK. Follow up on r/cscareerquestionsuk",
        "No I’m from Asia - Singapore to be exact",
        "Ahh, yeah, find a sub more connected to your locale for better info",
        "is there any research internship opportunities in NUS or NTU ?"
    ]
},
{
    "submission_id": "1ga5bwy",
    "title": "Why does SVM have a worse decision boundary than Logistic Regression?",
    "selftext": "Hello. I've run both SVM with Polynomial kernel with a degree of 3 and Logistic Regression with transformed features by [PolynomialFeatures](https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.PolynomialFeatures.html) with the same degree of 3 on the default scikit-learn's [Moons](https://sklearner.com/scikit-learn-make_moons/) dataset. Decision boundaries turned out to be as follows:\n\nhttps://preview.redd.it/rsbveq5urgwd1.png?width=704&format=png&auto=webp&s=1bedd8b3009e6f3b8435624df5c189a3c90b652a\n\nI cannot wrap my head around as to why the SVM underperforms compared to the logistic regression. Might that be due to the fact that we need to respect the margin between support vectors? I'd appreciate any help. Thanks in advance.",
    "created_utc": "2024-10-23T01:10:09",
    "num_comments": 4,
    "comments": [
        "Polynomial features in logistic regression are each weighted independently, while in kernel svm all features are considered together. It looks like the prior method is just a better way to classify your points, it's also possible that your data better meets the assumptions made by a polynomial regression model.",
        "Logistic regression imposes a cost to each point relative to its distance from the decision boundary, but svm imposes a fixed cost for each misclassified point that is specified or tuned by the user. If the C value is low then a decision boundary may be selected that tolerates a higher number of misclassified points.\n\nSVM also has a hyper parameter called coef0 that determines the relative influence of higher order polynomials vs lower order ones.\n\nSklearn logistic regression has a C parameter as well, but this controls the degree of regularization and is a different animal from svm's C function.\n\nI would play around with these settings and see if you get different results. My guess is that your coef0 and C values for svm are both set low and consequently your algorithm is nudging you closer to a linear decision boundary and tolerating a higher number of misclassified points.",
        "Makes sense, thx!",
        "Thx!"
    ]
},
{
    "submission_id": "1ga4t07",
    "title": "A guide on how to run the Whisper speech to text model behind OpenAI's STT locally without APIs",
    "selftext": "",
    "created_utc": "2024-10-23T00:34:46",
    "num_comments": 0,
    "comments": []
},
{
    "submission_id": "1ga4s4w",
    "title": "Learn pytorch before taking the course of andrew ng?",
    "selftext": "I started the course but tbh its pretty boring for me \n\nI might do it in the future but rn i feel like learning pytorch\n\nI wanna ask can i do it without causing any problems for myself or should i compelete the course and then focus on the framework? ",
    "created_utc": "2024-10-23T00:32:59",
    "num_comments": 21,
    "comments": [
        "He doesnt even use pytorch, but tensorflow",
        "I think it depends on your experience.   \n  \nIf you want to machine learning from scratch with little to no background, then for sure a course.\n\nIf you have a background in neural networks/machine learning, then go ahead and learn pytorch or a simpler framework if you have less programming ability.\n\nOtherwise you will have absolutely no idea what you are making with pytorch.\n\nIf the course is boring. Maybe give something else a go? I very much enjoyed the book Deep Learning illustrated when I was first starting out. The writing is really good and keeps you engaged which cannot be said for other machine learning books I've read...",
        "If you find the course boring, then maybe it's not the right course for you, there are many other courses that might be more interesting.\n\nI would still advise starting with the course first and then learning a specific framework. When it comes to learning frameworks, the best thing is to find some good repositories with examples and try to replicate them on your own. That way you're learning through practice right away.\n\nAlso, various AI tools can help make it more fun, like Cursor IDE.",
        "What is this Andrew Ng course everyone talks about?",
        "Ik\n\nBut it teaches u about the machine learning and concepts",
        "I'm new to ml.\nCompleted the ml specialization by Andrew ng.\nWhat next? I'm not comfortable coding. Like it's new. I know python but all the new syntax of the libraries and everything. Any course recommendations? Many are saying do kaggle. But I gotta know how to code right. I learnt the concepts tho.",
        "I have worked wirh\nPandas,langchain,matplotlib ,numpy and streamlit \n\nOnly\n\nShould i go for the course first?",
        "Can you reccomend me a course that's more programming focused?",
        "Can you reccomend me a course that's more programming focused?",
        "Its a course for ML\n\nMost people consider it the best one",
        "Hmm I’d think of a good little side project you want to make and then each step of the way find tutorials etc to help build that thing.\n\nIt’s proven the best way for me to learn. This way, you have a goal your motivated towards and aren’t just doing tutorials following something you don’t really care about",
        "Sounds like you have a bit of a base as a programmer that is trying to learn machine learning. I'd guess then it is probably best you do a course (that one you are talking about or another that you find more exciting), learn about neural networks and how they work before learning pytorch because it is a library to build neural networks.\n\nIt's pretty easy to screw things up and train a model endlessly for nothing if you don't know about what you are making (what an artificial neuron does, what the hyperparameters of a neural network do, strategies for training a neural network and how to tell when it is overfitting or underfitting, etc).",
        "Where do i find it?",
        "Cool, thanks",
        "Thanks man\n\nI am back end web dev so i know about programming\n\nWould you recommend me a course which focuses on ML course but with programming more ?",
        "Course\n\nYou can also find it on youtube",
        "what do you want to code? if you do not know how?  \noption **1**: open kaggle try to solve a few competitions. (try from scratch without copy-cat). after that open his course.  \noption 2:  you know smth, skip or x2 speed.\n\noption 3: you donot know basic staff... and do not want to learn. Do you need ML? install a few GPT and watch how GPT answer questions.\n\npart2: open youtube Karpathy  videos to build GPT from scratch.\n\nthe issue with \"skipping\" basics is that, later you will need to do smth, for exawmple: encode cities: London, NY, Osaka, etc.   \nand probably you would fail it in terms of model quality.  \nit's an illustration of issues, but solutions, i've wrote higher.",
        "Honestly id recommend the Deep Learning illustrated book. It’s got a lot of the basics and theory with coding snippets for each chapter so you can build what they’re writing about while reading. I loved that book and ended up starting my phd on the subject shortly after",
        "I want something that teaches basics and coding at the same time\n\nAka practical and theory both",
        "Thanks man will check it out",
        "read answer above."
    ]
},
{
    "submission_id": "1ga4oh3",
    "title": "Learning without labels? ",
    "selftext": "Hi everyone, I just dropped a video on contrastive learning and a specific implementation called SimCLR which was pretty famous some years ago. Hope you enjoy!  \n[https://youtu.be/UqJauYELn6c?si=y1w7bEGKxOuswtJE](https://youtu.be/UqJauYELn6c?si=y1w7bEGKxOuswtJE)",
    "created_utc": "2024-10-23T00:25:15",
    "num_comments": 0,
    "comments": []
},
{
    "submission_id": "1ga49fd",
    "title": "Building a Model Recommendation System: Tell Us What You’re Building, and We’ll Recommend the Best AI Models for It!",
    "selftext": "**Hey Reddit!**\n\nWe’re working on something that we think could make model discovery a LOT easier for everyone: **a model recommendation system** where you can just **type what you're working on in plain English**, and it'll suggest the best AI/ML models for your project. 🎉\n\n# 💡 How it works:\n\nThe main idea is that you can **literally describe your project** in **natural language**, like:\n\n* \"I need a model to generate summaries of medical research papers.\"\n* \"I'm building a chatbot for customer support.\"\n* \"I want a model that can analyze product reviews for sentiment.\"\n\nAnd based on that input, the system will recommend the best models for the job! **No deep diving into technical specs, no complex filters—just solid recommendations based on what you need.**\n\n# 🌟 What else we’re building:\n\nAlongside the model suggestions, we’re adding features to make the platform super user-friendly:\n\n* **Detailed model insights**: You’ll still get all the technical info, like performance metrics, architecture, and popularity, to compare models.\n* **Advanced search & filters**: If you’re more hands-on, you can filter models by task, framework, or tags.\n* **Personalized suggestions**: The system will get smarter over time and offer more relevant suggestions based on your past usage.\n\n# We need your feedback:\n\nWe want this platform to actually solve problems for people in the AI/ML space, and that’s where **you** come in! 🙌\n\n1. **Does a tool like this sound helpful to you?**\n2. **What features do you think are missing from model platforms like Hugging Face?**\n3. **Are there any specific features you’d want to see, like performance comparisons or customization options?**\n4. **How could we make the natural language input even more useful for recommending models?**\n\n# TL;DR:\n\nWe’re building a tool where you can just **describe your project** in plain English, and it’ll **recommend the best AI models** for you. No need for complex searches—just type what you need! Looking for your feedback on what you'd want to see or any features you think are missing from current platforms like Hugging Face.\n\nWe'd love to hear your thoughts and ideas! What would make this platform super useful for you? Let us know what you think could improve the model discovery process, or what’s lacking in existing platforms!\n\nThanks in advance, Reddit! 😊",
    "created_utc": "2024-10-22T23:54:23",
    "num_comments": 10,
    "comments": [
        "Best ai for figuring out which moderators work for Russia on Reddit? ",
        "Did you ask in r/LocalLLaMA",
        "Here's a simple - credit card fraud predictor- need to train a model",
        "so, are you building ChatGpt 2.0... oh wait... 5.0?",
        "Wow that’s really interesting, are you accepting collaborations, I would love to volunteer in working along side with you in such project, let me know the possibilities, thanks",
        "if this actually works properly yes it would be very useful, especially to the newer less experienced folk",
        "The biggest question I have is how will you compare models? Is it based on self reported metrics or community feedback? Also, will you converge to recommend everyone the same model or will you recommend people (in the same domain) different models based on e.g. license / compute requirements?",
        "everyone is fraudster. 100% accuracy.",
        "We’re planning to take a multi-faceted approach when comparing and recommending models. Here’s a breakdown:\n\n\t1.\tModel Metrics: We’ll start with self-reported metrics from model creators, like accuracy, performance on specific tasks, benchmarks, etc. However, we recognize that these alone can sometimes be limited or cherry-picked, so we’re looking to supplement this with real-world usage data, such as download counts, user ratings, and community feedback.\n\t2.\tCommunity Feedback: In addition to metrics, user reviews and community insights will play a role. This helps us capture real-world experiences and edge cases that might not be covered in benchmarks. We want to create a feedback loop where users can share how models performed in actual projects, giving the recommendations more depth.\n\t3.\tPersonalized Recommendations: The idea is to avoid one-size-fits-all recommendations. We’ll tailor suggestions based on the specific needs and constraints provided by the user. For instance:\n\t•\tCompute Resources: Some users may have access to large-scale infrastructure, while others might be working on a local machine. We’ll take compute requirements into account to recommend models that fit their hardware.\n\t•\tLicense Restrictions: Not all models are open for commercial use. We’ll factor in licensing information to suggest models that align with the user’s legal and operational needs.\n\t•\tTask-Specific Recommendations: Based on the user’s domain (e.g., NLP, image generation, etc.), we’ll prioritize models that have been fine-tuned or built for that specific task.\n\nUltimately, the goal is to create a system where two users working on similar projects might get different recommendations based on factors like available resources, licensing, and even preferences (like favoring open-source models over closed ones).",
        "This is the second new idea I came across in the ML space that excited me. Tbh, I might even take this as an inspiration and build something adding my own ideas after a couple of months😅"
    ]
},
{
    "submission_id": "1ga3k5l",
    "title": "New to Machine Learning - AI and ML! please guide ",
    "selftext": "Hello all! I am originally a mechanical engineer, turned program manager and now a product manager in automotive domain. \n\nI want to continue being a product manager, for the automotive domain but more so AI/ML focused PM. \n\nI do not know how to code or anything about AI ML (What a shame - I know!) but, I am willing to put in the effort to learn. \n\nHere is how I am thinking my learning path should be, if you can please tear it apart, recommend any other learning course, guide on what should I improvise on, I would greatly appreciate. \n\n  \n1) Learn Python - [https://www.coursera.org/learn/python](https://www.coursera.org/learn/python)   \n(is there a better course than this which is fun, takes me into more detail? - if yes, please recommend!). \n\n2) Learn some math for ML: [https://www.youtube.com/playlist?list=PLRDl2inPrWQW1QSWhBU0ki-jq\\_uElkh2a](https://www.youtube.com/playlist?list=PLRDl2inPrWQW1QSWhBU0ki-jq_uElkh2a)\n\n3) Complete the ML course by Andrew NG - on coursera - [https://www.coursera.org/specializations/machine-learning-introduction#courses](https://www.coursera.org/specializations/machine-learning-introduction#courses)\n\nThank you for your guidance! I greatly appreciate it! \n\nBest",
    "created_utc": "2024-10-22T23:03:36",
    "num_comments": 0,
    "comments": []
},
{
    "submission_id": "1ga2mnt",
    "title": "\"Vectors\" or \"Matrices\"?",
    "selftext": "I have been studying the ŷ = Xw formula and the instructor teaching us keeps interchangeably referring to matrices as vectors and vice versa. For this formula, they mentioned that ŷ is a vector, X is a matrix (which is why it's capitalized) and w is a vector.\n\nMy question is - What exactly is the difference between a matrix and a vector in ML ?\n\nThanks a lot in advance.",
    "created_utc": "2024-10-22T22:03:50",
    "num_comments": 6,
    "comments": [
        "**Vector:** A vector is 1-dimensional (either a single row or column) and typically represents single data points, features, or weights.\n\n**Matrix:** A matrix is 2-dimensional (with rows and columns), and it represents collections of data points (like a dataset) or transformations between spaces.\n\nThe formula you referred **ŷ = Xw**\n\nX is a matrix with dimension (m,n) and w is vector with dimension (n, 1)\n\nThe matrix multiplication 𝑋𝑤 works because the number of columns in 𝑋 (which is 𝑛) matches the number of rows in w (which is also n), resulting in a new vector ŷ with 𝑚 elements, one for each data point.\n\nI hope you understand it",
        "A vector is a 1xn or nx1 matrix.",
        "This makes it very clear. Thank you very much!",
        "*// A vector is a 1xn or nx1 matrix.*\n\nI think it is more correct to view a matrix as a set of column *vectors* or as a set of row *functionals*, in a dual space equivalency sense. \n\n[https://en.wikipedia.org/wiki/Row\\_and\\_column\\_vectors](https://en.wikipedia.org/wiki/Row_and_column_vectors)",
        "So both can be two representations of the same dataset, right?",
        "No. But in this case it would be bx1xn where b = batch size and it would be a matrix and not a vector, since a vector must be one dimensional by definition, but tbh, I think equations make it more confusing for me at that point. \n\nDescriptively, you input a batch of one or more feature vectors into a model and receive an output of your desired shape. \n\nIf b=1 then technically you could just make it 1xn depending on implementation and it would indeed just be a vector.\n\nEdit: I think I might be confused on your use of the term dataset, as well as the notation here. I think w is a weight vector being multiple with some matrix. That matrix might be a batch of vectors. That equation isn't really some kind of golden standard for ML, he's just using it to explain a single concept of obtaining a prediction(?), presumably.\n\nDataset means the collection of datapoints used to train, validate, or test."
    ]
},
{
    "submission_id": "1ga1m39",
    "title": "Seek for guidance ",
    "selftext": "Hello to all,\n\nI'm just started to learn ML.\nI learned the basic ML algorithms like \nLR,LOR, Decision Trees, Lda, qda,naive Bayes, \n\nNow my question is that after learning this , what should I learn next ?\nSuggest me some topics for my learning.🙂\n",
    "created_utc": "2024-10-22T21:01:56",
    "num_comments": 5,
    "comments": [
        "Learn web-programming, SQL, EDA etc your skillset is poor",
        "Thanks for ur suggestion, \nLearning SQL and about EDA is  ok,but I don't know about web programming.\n\nCan u explain why should I learn that and tell me how it's useful for me?",
        "In order to get a job on the market, we deploy models and services, with web interfaces as well",
        "Thanks man👍",
        "No problem"
    ]
},
{
    "submission_id": "1ga1l2c",
    "title": "Looking for Resources on Ads Marketplace Dynamics and Optimization\n",
    "selftext": "I'm looking for good resources to learn about the basics of **ads marketplace dynamics** (e.g., bidding strategies, auction theory) and how **data insights** can be used to optimize ad performance. Does anyone have any recommendations or references that could help? Would appreciate any book suggestions, articles, or courses or youtube video! Thanks!",
    "created_utc": "2024-10-22T21:00:33",
    "num_comments": 0,
    "comments": []
},
{
    "submission_id": "1ga1izf",
    "title": "How can Machine learning be used in the fields of HVAC and piping design? ",
    "selftext": "",
    "created_utc": "2024-10-22T20:57:18",
    "num_comments": 2,
    "comments": [
        "[deleted]",
        "We had a talk at a local ai conference about this. Surprised most data centers are already using RL to optimize cooling, and saw as much as 30% reduction in electricity usage iirc. Power plants and similar infrastructure as well. Crazy",
        "[deleted]",
        "https://www.telus.com/en/about/news-and-events/media-releases/using-ai-for-good-telus-and-vector-institute-partner-to-reduce-climate-impacts-from-data-centres\n\nhttps://github.com/VectorInstitute/HV-Ai-C"
    ]
},
{
    "submission_id": "1ga17yi",
    "title": "How to build a manual QA monitoring system",
    "selftext": "",
    "created_utc": "2024-10-22T20:39:59",
    "num_comments": 0,
    "comments": []
},
{
    "submission_id": "1g9z86j",
    "title": "Improve Inference Speed after SAHI",
    "selftext": "Hello,I would like to know if there are any effective techniques that can be used to balance the inference speed after applying SAHI with object detection model ? Or, is there any better alternative to SAHI for improving small object detection that does not slow down the inference speed ?\n\nYour suggestion is appreciated.\n\nThank you.",
    "created_utc": "2024-10-22T18:53:19",
    "num_comments": 0,
    "comments": []
},
{
    "submission_id": "1g9yhpf",
    "title": "Help! How to prepare for ML Engineer interview (Investment focus)?",
    "selftext": "\nHey everyone! I have an upcoming interview for an ML Engineer role focused on building models for investment analysis and portfolio management at a startup. I’m nervous and not feeling super confident about my skills.\n\nAny advice on how to prepare or key resources to focus on? I’d appreciate any tips to help me feel ready!\n\nThanks!",
    "created_utc": "2024-10-22T18:15:50",
    "num_comments": 2,
    "comments": [
        "i’m a student working especially with time-series analysis- definitely not keeping up with specifically financial ml atm but it might be helpful to know the concepts in this paper:\n\nhttps://arxiv.org/pdf/2305.08740\n\noutside of ml, might be helpful to show industry knowledge. you know about back testing and other  cs applicable fintech vocab? maybe take a glance at r/algotrading or related and glance at some of their jargon to see if that helps.\n\nsorry i’m not much help other than this, best of luck",
        "Focus on brushing up your knowledge of machine learning algorithms commonly used in finance, such as time series analysis, regression models, and clustering techniques. Familiarize yourself with financial concepts like risk assessment, portfolio optimization, and market trend prediction. Practice explaining complex ML concepts in simple terms, as you'll likely need to communicate with non-technical stakeholders. Be prepared to discuss real-world applications of ML in investment analysis and how you'd approach building models for specific financial scenarios.\n\nThink of the company's specific needs and challenges in the investment space. Research their current products or services and think about how machine learning could enhance their offerings. Be ready to discuss your problem-solving approach and how you'd handle data-related issues like missing values or outliers in financial datasets. Consider doing some mock interviews to build your confidence. I've found that tools like this [AI for interview](http://interviews.chat) can be helpful for practicing tricky interview questions and honing your responses. Full disclosure: I'm on the team that created it."
    ]
},
{
    "submission_id": "1g9wf3q",
    "title": "Best Online Master's Course in AI for Working Professionals (Math-Heavy)",
    "selftext": "Hey,\n\nI’m currently a working professional looking to enroll in an **online Master’s program in AI**. My main focus is on finding a course that’s **heavy on the math side**—particularly linear algebra, calculus, probability, and optimization techniques. I’d prefer a program that emphasizes the theoretical and mathematical foundations of AI/ML rather than just focusing on practical applications.\n\nHas anyone here pursued such a program or know of universities that offer online options fitting this description? I’d appreciate any insights or personal experiences!\n\nThanks a ton in advance!",
    "created_utc": "2024-10-22T16:35:01",
    "num_comments": 4,
    "comments": [
        "RemindMe! 1 day",
        "OMSCS ML specialization",
        "I will be messaging you in 1 day on [**2024-10-24 04:00:57 UTC**](http://www.wolframalpha.com/input/?i=2024-10-24%2004:00:57%20UTC%20To%20Local%20Time) to remind you of [**this link**](https://www.reddit.com/r/learnmachinelearning/comments/1g9wf3q/best_online_masters_course_in_ai_for_working/ltagsk8/?context=3)\n\n[**2 OTHERS CLICKED THIS LINK**](https://www.reddit.com/message/compose/?to=RemindMeBot&subject=Reminder&message=%5Bhttps%3A%2F%2Fwww.reddit.com%2Fr%2Flearnmachinelearning%2Fcomments%2F1g9wf3q%2Fbest_online_masters_course_in_ai_for_working%2Fltagsk8%2F%5D%0A%0ARemindMe%21%202024-10-24%2004%3A00%3A57%20UTC) to send a PM to also be reminded and to reduce spam.\n\n^(Parent commenter can ) [^(delete this message to hide from others.)](https://www.reddit.com/message/compose/?to=RemindMeBot&subject=Delete%20Comment&message=Delete%21%201g9wf3q)\n\n*****\n\n|[^(Info)](https://www.reddit.com/r/RemindMeBot/comments/e1bko7/remindmebot_info_v21/)|[^(Custom)](https://www.reddit.com/message/compose/?to=RemindMeBot&subject=Reminder&message=%5BLink%20or%20message%20inside%20square%20brackets%5D%0A%0ARemindMe%21%20Time%20period%20here)|[^(Your Reminders)](https://www.reddit.com/message/compose/?to=RemindMeBot&subject=List%20Of%20Reminders&message=MyReminders%21)|[^(Feedback)](https://www.reddit.com/message/compose/?to=Watchful1&subject=RemindMeBot%20Feedback)|\n|-|-|-|-|",
        "Thanks a lot, was looking for something like this"
    ]
},
{
    "submission_id": "1g9tfre",
    "title": "Character recognition assignment",
    "selftext": "Hello everyone, I am currently taking a ML course for my masters degree, and I was asked to create a MLP that predicts a particular feature for an sample taken from a given dataset.\n\nI started with the [Letter Recognition](https://archive.ics.uci.edu/dataset/59/letter+recognition) dataset from the UCI machine learning repo, and I am trying to build a program that can predict the value of any predetermined feature, given the other 16 as input.\n\nBecause one of the features is a character, I decided to try and treat everything as symbols (my assignment also requires the code to treat every feature as symbolic, for learning purposes), and I am using one-hot encoding to transform any feature in a vector of binary values.\n\nOnce an input has been fully encoded, I combine the vectors of each feature it into a single 282-component vector, which is fed into my sequential, densely connected network.\n\nThe problem is that, even with 2 hidden layers, with 300 neurons each, my model is less accurate than a simple KNN model I built a few weeks ago that worked over the same dataset (the KNN model had an accuracy of 96%, while the NN averages 95%). I only got to 96.4% with 300 nodes and 3 hidden layers, with 300 epochs of training!\n\nIncreasing the hidden layers to 5 and leaving every other hyperparameter unchanged gave me a 97.5% accuracy and it took 20 or so minutes to train.\n\nFor some reason all of these layers, numbers of neurons and epochs of training seem overkill to me, am I wrong? Is this as good as I can get? I am completely new to this field of computer science, please be patient, thanks!",
    "created_utc": "2024-10-22T14:20:18",
    "num_comments": 0,
    "comments": []
},
{
    "submission_id": "1g9rsu3",
    "title": "Need suggestion regarding selecting the ranges of hyperparameters using gridsearch",
    "selftext": "Hi, I am confused about the correct approach of deciding the ranges of hyperparameters.  \nIf I have 4-5 or more hyperparameters to tune for an algorithm then what is the best way to decide the ranges of each hyperparameters to get the best performance?",
    "created_utc": "2024-10-22T13:11:28",
    "num_comments": 2,
    "comments": [
        "It may depend on the domain, but in my experience it is best to only do a minimal amount of hyperparameter tuning. Grid search is usually a bad idea unless you're grid is very small or your model is very inexpensive to train.\n\nDon't fall into the trap of tuning hyperparameters to eke out small gains in performance. These gains often do not translate to out-of-sample performance and are a waste of time.\n\nIf you are using an automated method, my rule of thumb is to only have each unfixed hyperameter be able to take on 2 values or at most 3 values. When I determine ranges, I usually think of them in terms of \"low\" / \"high\", \"default\" / (\"lower\" or \"higher\"), or \"low / \"medium\" / \"high\". How you actually set the allowable values is somewhat subjective, but you should know what each hyperparameter does to tune it properly. There is no \"general guidance\" other than to understand the learning algorithm well enough and (usually) err on the side of wider distances as you only have 2 or 3 potential values if you use my method.\n\nMake sure your trials cover a good percentage of the space of hyperparameters. If not, reduce the space you are searching over. I try my best to only have two values per hyperparameter and use 3 only rarerly when I have a good reason.\n\nThere are really two kinds or phases of hyperparameter tuning. The first isn't really tuning, but exploration to find out which hyperparameters are more fundamental and/or important. Of the important ones, determine which ones can be fixed to a good enough value and then tune the rest. Aside from these, the only ones that you can consider tuning are regularization hyperparameters. This first exploration type of tuning is best done manually. Actual tuning (second phase) is usually best done with an automated method like random search.",
        "Thank you so much for the guidance. As a begginner it wasn't clear to me what aspects to consider and be cautious about while doing hyperparameter tunning. I tried to increase the model accuracy by a small percentage so to achive 90% which I now understand was a complete waste of time. I will definitely narrow down the number of hyperparameters to tune and follow rest of the suggestions, thanks again."
    ]
},
{
    "submission_id": "1g9r7mz",
    "title": "pro tip dont learn tensorflow",
    "selftext": "",
    "created_utc": "2024-10-22T12:48:05",
    "num_comments": 4,
    "comments": [
        "I refuse to believe tensorflow, a library that my mother who doesn’t even know how to open a pdf knows about, is less popular than JAX.  \n  \nThis is obviously a result of faulty methodology and can’t be taken seriously.",
        "You're still posting this? \n\nIf you're entire ML understanding comes from using a single library, you're not job ready. \n\nProtip: OP has no idea what they're talking about. This plot is worthless.",
        "does anybody know if there is individual statistics on the \"other languages and frameworks\"",
        "Definitely raises questions"
    ]
},
{
    "submission_id": "1g9malv",
    "title": "Unlock the Secrets of Autoencoders, GANs, and Diffusion Models – Why You Must Know Them? -Day 73 - INGOAMPT",
    "selftext": "",
    "created_utc": "2024-10-22T09:26:09",
    "num_comments": 0,
    "comments": []
},
{
    "submission_id": "1g9m0o3",
    "title": "How we're planning to give machinelearning builders a online home and a way to fund their projects!",
    "selftext": "\n\n\n\nHi there,\n\nI wanted to give you a update on what we're trying to do since i believe what we're planning is really cool and could be of major help to you guys.\n\nWe're currently building a platform for people in tech/ ai that want to contribute to projects.\nFrom indiehackers to engineers, students and developers. Everyone that wants to contribute in tech/ ai is welcome to sign up.\n\nFrom here people can get to know each other, make connections and already start working together.\n\nOur second stage:\n\nOnce the first people are there we would like to organize online hackathons and builder programs so that people who work on their personal projects now have a place to share their work.\n\nThe idea is to do this in a collaborative environment with a small competition aspect added to it. This way we are trying to get sponsorships around our events to fund you guys their projects.\n\nThird stage:\n\nIf everything goes as planned we will try to bring our events and programs to public settings so that our community also has a place to be in the public :)\n\n\nFor now, you can sign up for the waitinglist below if you haven't done that already\n( Further info can be found in the link ! )\n\nhttps://tally.so/r/3N0zZN\n\nLet me know what you think of it! :) \n",
    "created_utc": "2024-10-22T09:15:06",
    "num_comments": 0,
    "comments": []
},
{
    "submission_id": "1g9lvix",
    "title": "Built my first neural network from scratch – looking for feedback or ideas to improve!",
    "selftext": "",
    "created_utc": "2024-10-22T09:09:08",
    "num_comments": 2,
    "comments": [
        "video looks cool man , will go through it completely once free lol 😭, can we connect , wanna ask something ?!",
        "For sure, will do, thanks man!"
    ]
},
{
    "submission_id": "1g9luf5",
    "title": "Stable Diffusion 3.5 is out !",
    "selftext": "",
    "created_utc": "2024-10-22T09:07:52",
    "num_comments": 0,
    "comments": []
},
{
    "submission_id": "1g9kvaw",
    "title": "HELPPP!!What skills to learn to do freelance/intern/part-time work?",
    "selftext": "HI,this is my first post here.What are some skills that I can learn (eg GenAI,LLMs etc) which can be sold as a service.I have background working with CNN's and coding experience in pytorch,currently completing sequence models from Andrew Ng.  \nWhat should be my next steps and where should I learn them from and how and where to find opportunities and what should I focus on.  \n(Additionally would love if someone could suggest a roadmap kind of thing)  \nThanks\n\n",
    "created_utc": "2024-10-22T08:27:02",
    "num_comments": 1,
    "comments": [
        "hey there, altho i havent done such a thing but scrapping some data from AI jobs from various freelance platform will surely tell u what kind of skills u need to learn!"
    ]
},
{
    "submission_id": "1g9k1k1",
    "title": "Simplifying the AI/ML to Production Pipeline",
    "selftext": "",
    "created_utc": "2024-10-22T07:52:50",
    "num_comments": 0,
    "comments": []
},
{
    "submission_id": "1g9izzr",
    "title": "Romeo & Juliet",
    "selftext": "Juliet\t\t: Oh Romeo, where have you been, why are you so late?\n\nRomeo\t\t: I’m so sorry my love, the traffic was heavy. I’d say it was the 95th percentile of all traffic!\n\nJuliet\t\t: Oh my! That means it’s greater than 95% of all traffic! It must be horrible! Why would you go through such lengths just to see me? I’m just a normal girl with an above average face.\n\nRomeo\t\t: Because you are the most Beautiful, Lovely, Unique and Elegant woman I’ve ever met. To see you, my dear, nothing shall stop me. \n\nJuliet\t\t: Do you really mean it when you say it? That I’m BLUE?\n\nRomeo\t\t: Yes, I do. A girl like you is an outlier! All else being equal, I would rather die than to live without you!\n\nJuliet\t\t: Oh Romeo! I missed you so much. I can’t stop thinking about you. You are the mode of my unimodal thoughts. I can’t live without you. My chances of surviving would approach zero as time progresses.\n\nRomeo\t\t: My love for you cannot emerge simply by chance! It is four standard deviations greater than the average love!\n\nJuliet\t\t:  I love you, too, with all my life. My love for you is irrational. Take me! Take me with you at the first moment! I was never content with all the constraints of a noble’s life. I wanted ordinary life in the least crowded square in town.\n\nRomeo\t\t: And I would want nothing but to be with you. I make projections of a life together with you. Independent of our parents.\n\nJuliet\t\t: But, Romeo, how can we function wiithout the support of our parents?\n\nRome\t\t: Indeed, we are naive and biased, but overtime we would be efficient with our lives. I can learn the machines and you can get into modelling. As long as we have each other, we can get through anything.\n\nJuliet\t\t: Thank you, my dear, for raising my confidence level. But what if they distribute search parties throughout the realm? They are rigorous. They would do an exhaustive search to find us!\n\nRomeo\t\t: Indeed, we are working with very low degrees of freedom. Let us go to the edge of the domain. Where the roads are left unsmoothed and the population whine about inequality. The place with the maximum likelihood of escaping the scouts’ range. \n\nJuliet\t\t: I won’t be able to say goodbye to Mom & Dad, but now they are insignificant. Let us leave the parameter at once! To our new, joint life!\n\n  \n\n\nI should really stop procrastinating...",
    "created_utc": "2024-10-22T07:07:22",
    "num_comments": 3,
    "comments": [
        "This is probably the worst thing I've seen on Reddit this month, and I've seen like 3 NSFL factory accidents.",
        "Oh man... I'm sorry. I'll go back to studying.",
        "Hit them books, king 👑"
    ]
},
{
    "submission_id": "1g9hz9h",
    "title": "QHAdam Optimizer Explained | Theory and Code",
    "selftext": "",
    "created_utc": "2024-10-22T06:20:52",
    "num_comments": 0,
    "comments": []
},
{
    "submission_id": "1g9hcan",
    "title": "Creating inference microservice for ModelKits hosted on Jozu Hub",
    "selftext": "",
    "created_utc": "2024-10-22T05:50:03",
    "num_comments": 0,
    "comments": []
},
{
    "submission_id": "1g9g508",
    "title": "Book recommendations to learn AI from beginners Advanced.",
    "selftext": "I’m done With Maths from Mathacademy Now i wanna wet my feets in the AI domain. Where shall i start? Can y’all provide a roadmap of books?For instance learn ML then NLP then DL and LLM and so in an order thanks in advance",
    "created_utc": "2024-10-22T04:46:50",
    "num_comments": 18,
    "comments": [
        "intro to statistical learning with python-> andrew ng deep learning specialization-> hands on machine learning with sklearns and tensorflow-> csc 224n(stanford NLP course) and build projects practice with kaggle datasets for actual practical experience and practice",
        "I started with this. Did all the jupyter notebooks etc. AMAZING\n\n[https://udlbook.github.io/udlbook/](https://udlbook.github.io/udlbook/)",
        "[https://d2l.ai/](https://d2l.ai/)",
        "I bought \"machine learning with pytorch and scikit-learn\" but haven't started",
        "Statquest YT videos and book is all you need for statistical theory",
        "Some say tensorflow is dying instead read and use the pytorch lib",
        "Did you also create notes? Or just do hands on?",
        "I just searched for the intro to statistical learning with python and it's a free book. Thanks for the recommendation!",
        "Or just this [edu.machinelearningplus.com](http://edu.machinelearningplus.com)",
        "Just for deep learning, nlp, cv",
        "Honestly once you get the theory down either library is fine I personally use pytorch. And I would recommend reading the documentation to learn that",
        "of course the best thing to retain information is to write notes by hand or write the code yourself if you just read code or just copy paste you're not gonna retain anything",
        "This is suggested in this sub quite a lot",
        "This is paid, though",
        "in the first few chapters the book goes through some ML to interduce you to regression and then jump in to NNs \n\nso yah I would say this book is great for intermediate level or some one who finished his ML foundations",
        "link",
        " The course OP suggested seem to have paid. Nevertheless, for free options Andrew Ng and Krish Naik's channel are good. I prefer to have one or two quality resources over learning from a lot of books and courses.",
        "[PyTorch documentation — PyTorch 2.5 documentation](https://pytorch.org/docs/stable/index.html)"
    ]
},
{
    "submission_id": "1g9fg2f",
    "title": "Why Isn't Anyone Talking About Generative Motion Matching?",
    "selftext": "",
    "created_utc": "2024-10-22T04:05:08",
    "num_comments": 0,
    "comments": []
},
{
    "submission_id": "1g9f7vp",
    "title": "OpenAI Swarm : Ecom Multi AI Agent system demo using triage agent",
    "selftext": "",
    "created_utc": "2024-10-22T03:50:55",
    "num_comments": 0,
    "comments": []
},
{
    "submission_id": "1g9ecou",
    "title": "[D] What GPU to use with standard laptop (Lenovo Yoga) for Machine Learning training",
    "selftext": "I am looking to buy a decent GPU (e.g. RTX 4080/4090) for training machine learning models to compete in Kaggle competitions. **I'm wondering if I can simply buy just the GPU and use it with my 6 year old Lenovo Yoga (i7 core)**, I think you can use it via a USB-C Lighting Bolt port which my laptop does have.\n\nI've read a lot about building custom work stations which is not something I'm interested in. I understand that there may be CPU bottlenecks with my GPU/Laptop setup, and if I experience this then I will look at other options to fit the GPU I've purchased. But for now I'm trying to avoid spending an exorbitant amount of money on a pre-built workstation, or doing a custom build.\n\nCould someone let me know if this is possible or sensible?\n\nN.B. I do not have an existing monitor, just the Lenovo Yoga. My budget is approximately $2-3k AUD, the cheaper the better obviously.",
    "created_utc": "2024-10-22T02:51:06",
    "num_comments": 1,
    "comments": [
        "Yes you can buy an  \"External PCIe enclosure\", put the GPU in it, and connect it to your laptop via USB-C. And yes this could mean significant data bottlenecks, probably because of your CPU and bus more so than because of the enclosure.\n\nTwo things to look out for though:\n\n1. RTX 4090, especially, usually takes up *three* PCIe slots, but a PCIe enclosure might be smaller than that. Mabe you could just run it without the case or make a new case. \n\n2. You need to make sure the enclosure can provide enough power. Both GPUs you mention can use over 300 watts, but a given PCIe enclosure won't necessarily provide that much.\n\nIMO you should just build or buy a desktop. It's not about being \"interested\" in it, it's just the correct tool for the job."
    ]
},
{
    "submission_id": "1g9dtjr",
    "title": "Free Harvard Machine Learning Courses ",
    "selftext": "Wanted opinions on the course given in the title, is it good?? Will I get a certificate of completion? And other resources from which I can learn machine learning (preferably ones with certificate of completion)",
    "created_utc": "2024-10-22T02:10:23",
    "num_comments": 1,
    "comments": [
        "I'm sure the content is solid, but no certificate is worth the bytes it's saved on in this field."
    ]
},
{
    "submission_id": "1g9dolm",
    "title": "Besoin d'aide pour l'exercices de reseaux de neurones",
    "selftext": "",
    "created_utc": "2024-10-22T02:00:24",
    "num_comments": 1,
    "comments": [
        "Claro! Escribe en francés en este sub de reddit donde TODO se comunica en inglés...\n\nFrench people, really..."
    ]
},
{
    "submission_id": "1g9dbwi",
    "title": "Want to transition into a AI/ML role",
    "selftext": "Hi there,\n\nI have recently completed my 1 year AI ML course from a reputed institute (online and love classes) and looking to transition into a analytics role. I come with rich sales background experience. I am also doing industry needed projects on my own to enhance my profile. Can someone help how can I get shortlisted for interviews?",
    "created_utc": "2024-10-22T01:33:07",
    "num_comments": 2,
    "comments": [
        "I think it is better to deal with some project which actually do the training. There are lot of opportunities in jop posting websites keep applying and meanwhile contact someone who work industry",
        "Sure appreciate the feedback. I am trying to look for projects that are industry agnostic."
    ]
},
{
    "submission_id": "1g9d3ka",
    "title": "[D] Courses about Machine Learning",
    "selftext": "Hi, I'm a student from Argentina. I'm studying industrial engineering. I was awarded a scholarship to spend a year in Germany. In the first two months, I'll be taking an intensive German course, and then I'll be going to Technical University of Munich for a semester. After that, I'll be looking for work. I only have two subjects left and a final project to complete in Argentina. So, I'm hoping to take some courses at TUM that will help me in my future career. I decided to take 1 or 2 courses about machine learning. They are called \"Machine Learning for Business Applications\" and \"Machine Learning and Optimization\". The teacher told me that Machine Learning and Optimization is very technical and I am not sure if it worths it. I need some advice about this new field for me. I can share the contets and objectives of each course. Also, I'm still not sure which industry I want to work in.",
    "created_utc": "2024-10-22T01:15:47",
    "num_comments": 0,
    "comments": []
},
{
    "submission_id": "1g9c4wc",
    "title": "[Discussion] Sophon AI Accelerator accuracy problem",
    "selftext": "Hey everyone,\nI recently got my hands on a Sophon AI accelerator and wanted to share some strange findings. I was testing the ResNet50 model in FP32 mode (32-bit floating-point) on both the Sophon and an NVIDIA RTX 4090 GPU. I used the exact same ONNX model and dataset to keep things fair.\nHere's what I observed:\n-NVIDIA RTX 4090 (FP32): 76.7% accuracy;\n-Sophon (FP32): 73.2% accuracy\n\nI was surprised to see that the Sophon had a noticeable drop in accuracy compared to the NVIDIA GPU, even though both were running in FP32 mode. Isn't FP32 supposed to give us the highest precision?\nI double-checked everything:\n-Same model and weights\n-Same dataset\n-Same preprocessing steps\n\nBut the accuracy gap is still there. I'm wondering why there's such a significant difference. Could it be due to Sophon's hardware architecture or how it handles computations internally?\nHas anyone else experienced something like this with the Sophon accelerator? Any ideas on what might be causing the lower accuracy in FP32 mode?\nJust thought I'd share my experience and see if anyone has insights. Cheers!",
    "created_utc": "2024-10-22T00:01:16",
    "num_comments": 0,
    "comments": []
},
{
    "submission_id": "1g9bzdj",
    "title": "File format for finetuning",
    "selftext": "\nI am trying to fine tune llama3 on a custom dataset using LoRA. Currently the dataset is in a json format and looks like\n{ \n   \"Prompt\" : \"\",\n   \"Question\" : \"\",\n   \"Answer\" : \"\"\n}\n\nThe question is can I directly use the json file as the dataset for fine-tuning or do I have to convert into some specific format.\n\nIf the file needs to be converted into someone other file format it would be appreciated if you provide a script about how to do it since I am rather new to this.",
    "created_utc": "2024-10-21T23:49:54",
    "num_comments": 2,
    "comments": [
        "[{“instruction”:””, “input”:””, “output”:””},…]",
        "Thank you"
    ]
},
{
    "submission_id": "1g9blcg",
    "title": "How do I make Machine Learning fun?",
    "selftext": "I am a web developer, currently studying AI at uni.\n\nAs a web developer, there’s always an endless number of websites or apps you can build for various use cases. This is what made me fall in love with programming and software engineering in the first place. But I don’t feel the same with ML / AI.\n\nI never feel the urge to just open PyTorch and code my own architecture. Why would I? Thousands of people far smarter than me have already optimized the best approaches. It feels like there’s very little incentive to learn and fail—like you need to be on the cutting edge to be relevant. The barrier to entry in AI feels incredibly high. How am I supposed to compete?\n\nIn my free time, I built a DistilBERT text classifier to detect news bias, and I used maybe 1% of what I learned in university. The job is really just gathering data and training a pretrained model. It's was like 99% software engineering at the end of the day anyways. Didn't have to use any ML library (except Transformers, if you count that).\n\nI’m confused about how to find passion in an industry that feels impossible to break into for the average person. Do I really need to study all this time just to become a ***certified Hugging Face model downloade****r*? That’s pretty demotivating…\n\nI want to fall in love with AI the same way I fell in love with software development and programming in general. But right now, I’m struggling to find joy in it. It feels like every problem has been solved, and I’m left feeling useless, unable to catch up.\n\nWith all these doubts, I really hope that the ML community can help me. I'm looking forward to hearing all your advice and comments. Thank you in advance!",
    "created_utc": "2024-10-21T23:20:43",
    "num_comments": 17,
    "comments": [
        "The way I approach AI is that I am basically building a mind. Imagine the cold hard reality that AI is basically just linear algebra. A neural network is just a convoluted collection of functions. It's just mathematics. But the fact is that with \"just mathematics\" you can make a machine that can speak. You can make a machine that can follow your instructions. You can make a machine that has an actual \"mental\" (not conscious) representation.\n\n>there’s always an endless number of websites or apps you can build for various use cases. This is what   \nmade me fall in love with programming and software engineering\n\nThis is the complete opposite of me with AI. 90% of the time I absolutely hate making apps, and I couldn't care less about making to-dos or connecting databases. I work as an AI Engineer. I finetune and test out models for my org, but honestly, I don't give a hoot on what our models can or cannot do. But my favorite project is not the ones in my org where I improve or refine practical performance, but a personal project where I train a tiny-ass 7M parameter model from scratch on my native language. It's shite and useless, but it can output grammatically coherent sentences. The fact is that through a bunch mathematical operations I am able to tamper with or develop a mind from scratch... is honestly unbelievably insane.\n\n>In my free time, I built a DistilBERT text classifier to detect news bias\n\nThis sounds super boring, and I get why you don't enjoy it.\n\n>Thousands of people far smarter than me have already optimized the best approaches\n\nYou could say the same about the hundreds of thousands of open source libraries out there. But trying to make your own package (without caring to publicize it or make it competitive) for your own personal hobby can be fun.\n\nMy suggestion is that you try to develop a hobby model, architecture, or use case where you go \"*Holy shit, I can't believe this is scientifically possible!*\" but not caring about whether your model actually works. Try making a shitty GPT model. A shitty diffusion model. A shitty multimodal model. Try making the wackiest contraption possible without caring about how useful it is. That's my 2 cents anyway.",
        "The machine learning mindset is a different one from a pure software development.  It's often as much about experimental design as it is software design.  From a time perspective, for a proof-of-concept, I put more time into thinking about evaluation and data splits than implementation.\n\nFor me, the exciting part is trying to pick apart shortcomings of the model. This is closely related to error analysis, which many people consider a chore, but for me it's about understanding what phenomena or processes underly the human judgment.  My work is in natural language processing, so we're often asking questions like\n*  \"are we missing things because we aren't properly modeling the semantics, the syntax or something else.\"\n* \"do humans require external knowledge to do this or is everything captured within the text?\"\n* \"are there phenomena in the text our model can't account for\"\n\nOnce you understand limitations of a model, that's where you start to really dig in and see possibilities.  Successful people in this field will be constantly generating new hypothesis for how to improve performance or solve a task, and will work to almost with a chip on their shoulder to run the next experiment. This is where the go down paths of developing new architectures, doing some feature engineering, or tinkering with parameters and loss functions.  What you get out-of-the-box with foundational models gives higher baselines, but it doesn't mean every problem is fully solved.\n\nI also find great joy in finding new ways to use existing data.  There's a rush repurposing data to create new models and capabilities.  For example, I thought it really clever at the time how people flipped question answering datasets to train question generators.  And so as a team manager, I'm often on the lookout for new data sources that open up new possibilities to automate something new.\n\nThis is probably not fun for most people.  The best way to know is to work on a problem or domain you really care about.  If you start daydreaming about how to make the model better, you're on the path towards fun in ML.",
        "Hi there buddy I'm also a web developer learning machine learning as my masters. I agree with you I personally find it boring compared to web development where you can do features and functions based on individual creativity. For my case I am mainly interested in speech recognition and I am just planning to focus on that alone. I don't feel hyped or interested when I work on computer vision/regression/ classification. Currently for speech recognition I'm working on open source models but eventually I plan to go down the rabbit hole and learn the building blocks of speech recognition. My suggestion is for you to choose a field you are interested and just focus on that. Good luck and you can do it.",
        "You are 100% spot on. The vast majority of people who want to break into this field are completely satisfied using other people's pre-trained models and gluing them together to build solutions. There are certain things within ML that are helpful to learn for more advanced customizations and such, but the vast majority of people who want to build with AI really just need to learn the vocabulary around different tasks to be able to find the components relevant to the application they're working on.",
        "For me, its finding an interesting research paper and then trying to create a toy model of it in a new way. With anything though, sometimes it takes a few hours of grind before it becomes fun",
        "\n\nGo find a dataset on kaggle or in real life and build. Not everything is based on huggingface. Some are genuinily cool projects like disease forecasting or sports. Just find something new and keep chugging",
        "Remindme! 2 days",
        "I fell in love because of the theoretical background of it.Learning how anns work and gradient descent etc was fun. If ml was just about importing libraries then I would find it boring too",
        "One line -- Start loving it",
        "Hey, I really appreciate your time and effort on this comment. I get where you're coming from and you're right, its fucking crazy to think that it's basically just multi variate calculus at a large scale. \n\n>Try making the wackiest contraption possible without caring about how useful it is\n\nI love that take. I might try it out. I've been thinking of picking up Golang or Gleam, maybe I'll just make a ML library out of it by beginning with Math and then building it up.\n\n>I work as an AI Engineer.\n\nCan you tell me a bit more on this, im super curious? What are your day to day tasks? What kind of projects are you doing? What aspects do you enjoy and dislike from that job?\n\n>I don't give a hoot on what our models can or cannot do\n\nI really like how you care about the process more than the practicality, personally, I just love shipping something and seeing it being used and made an impact. How could you advise me on this?   \n  \nThis might be a bad take, but it feels like often times for most projects, using the openai api just solves a bunch of ML  projects for pretty cheap. However using openai with LangChain is just inherently boring and its Software engineering more than it is ML. \n\nAlso, last question, out of interest, what is your native language?",
        "Nice reflection!\n\nAnd following one 's imagination is something I'd personally like to add. Imagining some process or approach to be possible and then dogedly following every probable angle until only the (hitherto) deemed impossible realitities sometimes turn out to be true.\nu/diligentgrasshopper -< r/UsernameChecksOut",
        "Hey thanks a lot. It’s so nice to know that I’m not alone. I get where you’re coming from with focusing on a specific part of ML. Tbh it sounds weird, but my favourite courses so far were logic, statistics and Responsible AI, which is kinda like ethics.\n\nI’ll look into it! Thanks!",
        "I will be messaging you in 2 days on [**2024-10-24 08:59:29 UTC**](http://www.wolframalpha.com/input/?i=2024-10-24%2008:59:29%20UTC%20To%20Local%20Time) to remind you of [**this link**](https://www.reddit.com/r/learnmachinelearning/comments/1g9blcg/how_do_i_make_machine_learning_fun/lt57szy/?context=3)\n\n[**CLICK THIS LINK**](https://www.reddit.com/message/compose/?to=RemindMeBot&subject=Reminder&message=%5Bhttps%3A%2F%2Fwww.reddit.com%2Fr%2Flearnmachinelearning%2Fcomments%2F1g9blcg%2Fhow_do_i_make_machine_learning_fun%2Flt57szy%2F%5D%0A%0ARemindMe%21%202024-10-24%2008%3A59%3A29%20UTC) to send a PM to also be reminded and to reduce spam.\n\n^(Parent commenter can ) [^(delete this message to hide from others.)](https://www.reddit.com/message/compose/?to=RemindMeBot&subject=Delete%20Comment&message=Delete%21%201g9blcg)\n\n*****\n\n|[^(Info)](https://www.reddit.com/r/RemindMeBot/comments/e1bko7/remindmebot_info_v21/)|[^(Custom)](https://www.reddit.com/message/compose/?to=RemindMeBot&subject=Reminder&message=%5BLink%20or%20message%20inside%20square%20brackets%5D%0A%0ARemindMe%21%20Time%20period%20here)|[^(Your Reminders)](https://www.reddit.com/message/compose/?to=RemindMeBot&subject=List%20Of%20Reminders&message=MyReminders%21)|[^(Feedback)](https://www.reddit.com/message/compose/?to=Watchful1&subject=RemindMeBot%20Feedback)|\n|-|-|-|-|",
        "I get that, math programming can be really fun, and ML is just applied mathematics. However how practical is this in the real world? I’d like ML to be fun as a job, not only as a hobby. This might be a bad take though…",
        ">Can you tell me a bit more on this, im super curious?\n\nI work mostly on the company's research team. The nice thing is that my job is literally just tinkering around with technology. We're not doing some huge cutting edge science, but I'm responsible for fine-tuning new models, creating synthetic datasets, and benchmarking models in performance and compute cost. There is a lack of AI benchmarks in our language, and I recently had fun developing some novel multimodal benchmarks for our internal use. I have near-zero involvement in frontend and backend, but sometimes I develop the flow that the models process data (e.g., prompting and scripting). We're also a bit open-ended or just vaguely trying to improve performance for some use case. No crazy deadlines to chase around :) Organizationally, it's a nice comfortable place to work in (also WFH), though responsibilities may differ in different orgs.\n\n>How could you advise me on this?\n\nI was (still is) super into science and research, so I always thought of both my working and hobbying around as fun science projects. Mostly I am drawn by a field of research in cognitive science interpreting human cognition in algorithmic terms, so this naturally make me interested in the technology in itself. Understanding (or trying to) understand how it works can be super fun! Everything else follows.\n\n>This might be a bad take, but it feels like often times for most projects, using the openai api just solves a bunch of ML projects for pretty cheap.\n\nI completely agree. Have you looked into agents and giving your LLMs function calling abilities? You can make them execute arbitrary code and it can be super fun to play around, even with just OpenAI API.\n\n>Also, last question, out of interest, what is your native language?\n\nNot telling, but I am Southeast Asian :)",
        "Damn this is super interesting!\n\n>No crazy deadlines to chase around :)\n\nLiving the dream haha.\n\n>Understanding (or trying to) understand how it works can be super fun!\n\nI absolutely agree. Learning is inherently fun. I also love learning so much. I think a problem I might have right now, like many others is financial pressure (and deadlines to meet specific bills). It feels like learning is often a privilege and not a right. It feels like I don't have much time to spend on things which I know aren't the most optimal. But maybe I'm seeing this in a wrong way.\n\n>Have you looked into agents and giving your LLMs function calling abilities?\n\nI've built some small apps using LangChain for work. I don't think I can really call that AI development or ML. It's just an easier way to call LLM endpoints....\n\n>Not telling, but I am Southeast Asian :)\n\nSoutheast Asian language which is not supported by popular LLMs 🤔 Papua? Javanese? Khmer? idk...",
        "How do you create synthetic data sets? I’m working on a restaurant ml and can’t quite understand how to do so without over correlating things in a bias way. \n\n\nAfter all if you pretrain a model with your bias, it’s just a normal model right?"
    ]
},
{
    "submission_id": "1g9aufu",
    "title": "Seeking AI Enthusiasts, Professionals and Industry Leaders that WANT to Write, Provide, or Help Propel a Community First - AI Site.",
    "selftext": "I am close to launching a project 2 years in the making, I did not take on sponsors bc I did not want to be like every Directory site in the world, I wanted to be honest however  - I am missing one component and was hoping this may help find it.  I am solo I would like to build a team of AI Enthusiasts, Writers, Devs, Influencers and much more….\nNOT HIRE THEM \nI want people as passionate about AI as me to contribute what they’re able to, when they can. \nWill it become monetized? Yes to sustain growth and upgrade as well as make it our full time job but up front, no. It’s just me (Matt) and the platform(website) I designed isn’t just decent, it’s going to shake up the websites that sit at the top eating on ad money. I refused to sell out - I will take on sponsors whose products I use myself or recommend …and with a diverse team I think we can create a great site that visitors feel good going to - SEO has lower standards than this Brand lol and that’s good bc we’re passion-first, community-focused, and lack corporate greed.\n\nI’m asking for good ppl who are honest and gave a true desire to learn more about Ai or tell others - YOY DONT NEED TO HAVE PUBLISHED A THING- just lmk if any of y’all are interested. \n\nI want  to roll-out. YouTube and/or Podcasts as well as Online Courses and Events.\n\nIf you’ve got that AI bug and you want to get your work seen by millions or teach courses, moderate AI FB Groups, the potential is limitless and I’m open to your ideas.\n\nIf this describes any of you then I have a home for you and it’s special\nDM ME @TechAdvPro \nOr\nEmail Me:  Matt@FutureAIGuide.com\n\nI will reply immediately to anyone interested in next 24 hrs as I prep for launch.\n",
    "created_utc": "2024-10-21T22:30:03",
    "num_comments": 0,
    "comments": []
},
{
    "submission_id": "1g990j8",
    "title": "What do ai programmers do (how do they work)",
    "selftext": "Hi everyone this is my first post. I have recently started programming (3 months at most) and had this road: python basics --> pandas numpy matplotlib and scipy --> scikit --> tensorflow and keras. All of that I have learned more or less superficially tho. Of course later on I will reinforce my knowledge (both theoretically e.g. math and practically like making projects). But I have just got confused what do ai programmers do these days? Like they use models from HuggingFace to make e.g. object detection, chatbots etc.? Please tell about your experiences, maybe workstyle and tips. Thanks in advance.",
    "created_utc": "2024-10-21T20:38:38",
    "num_comments": 75,
    "comments": [
        "\"AI\" is a broad field. What you are probably referring to are **Large Language Models (LLMs)** or **Generative AI** (like those found on HuggingFace), which are subsets of **Deep Learning**. Deep learning itself falls under **Neural Networks**, which are part of **Machine Learning (ML)**—a branch of **Artificial Intelligence (AI)**.\n\nIn practice, AI programmers work on different types of projects. For example, I’ve worked with pathfinding algorithms like A* for robot navigation and neural networks that perform object detection using LiDAR on edge devices.\n\nThe workflow varies depending on the type of project and the algorithms involved. Some projects might involve fine-tuning pre-trained models (like from HuggingFace), while others require custom models built from scratch for specific tasks.",
        "> e.g. object detection chatbots\n\n🤨",
        "I work in ML research.\n\nMost machine learning is data collection and management. It takes a lot of data and computing power to train a model. To train our models, we first label a small section of the data (2,000 to 5,000 audio clips) and then use those clips to train the model. We then run the model against our data bank, search for clips of interest, and do more labeling for those clips. The process repeats over months and years until we build up datasets.\n\nNext, we have engineers working on the data management platform itself so users, engineers, and investors can all sort through their data and access it as needed. We have over three years of audio clips which requires an extensive backend to manage.\n\nFinally, the model training takes a very expensive supercomputer. PyTorch in Python allows you to make a complicated model in only a few lines of code (20 - 30 lines). So engineers don't spend time making models. Most of the time is spent tweaking existing models and waiting for the supercomputer to compute the results.\n\nGo into machine learning from a data science and business perspective. Machine learning is a rapidly growing field and the costs of VRAM will drop drastically. Tesla's supercomputer made of of 100,000 A100s cluster will someday fit into a small room and an A100 will fit onto your phone. Last I heard, Tesla collects 80 years of data every 3.5 minutes. Good models for complex tasks don't start until break into the billions of data points at the low end.\n\nIts easy to be lost in the scale of things, which is why it is some important to understand theory. As I am writing this, I am training a small test model that compares ReLu versus non-ReLu. The accuracy difference is 30%. Do you know why?",
        "    from openai import OpenAI",
        "I think this is a pretty small segment of the AI jobs out there. Most of the real work to be done is integrating AI with all the legacy systems that make up the world.",
        ">But I have just got confused what do ai programmers do these days?\n\ncall OpenAI API?\n\n>Like they use models from HuggingFace to make e.g. object detection chatbots etc.?\n\nSounds more like a MLE task.\n\nOh ya ..  I have a bias where I assume anyone with \"AI\" in their job title, but not working in R&D, is just using the OpenAI API in their role.",
        "Everybody is just using HuggingFace models via the API these days. If you can do that, call yourself an AI Programmer.",
        "I’m and AI dev and I usually work with LLMs and creating knowledge bases. My job is to integrate different LLMs without our services, then create embeddings for them, and a lot of other code related to chatbot development. It’s not heavy on ML but mostly standard chat completions to high level ones with streaming. I’ve also worked with other stuff such as Speech to text models, calling and creating APIs for the functions within the chatbot, and a bit of prompt engineering. Most of the time it’s just me going through documentation of the LLMs and trying to integrate it without our system. Overall very cool stuff icl",
        "I use object detection chatbots daily",
        "https://x.com/_jasonwei/status/1760032264120041684?s=46&t=x6aIMKxtOHuyj50nx-e74A\n\nIt was a bit tongue in cheek but note the themes on empirical experiments and managing data.",
        "Research architectures by reading a lot of papers, try/code models, tune models, experiment with all kinds of loss functions, datasets, metrics, etc. I would say it is equal parts research and development.",
        "Whatever you find in this forum can you relay it to me ?  There's too many people talking here couldn't find any relevant info like do data scientist actually build all the models or do they use APIs and what should be the projects a beginner should make to enter this market",
        "Thanks a lot for the feedback. As I've said, I am new to forums and LLM models. Can I then ask you how did you get work (where have build your portfolio and made projects). If I learn how to use models in HuggingFace (like YOLO) and try to make portfolio in Kaggle would it work out then? Thanks again.",
        "If **everything** is a **keyword** then **nothing** is.",
        "Tell me about fine tunning pre trained models? Any guide or practice websites?",
        "R u going to die on that comma?😂",
        "Well yeah I just thought it would be pretty informative this way 🙃 P.S. yeah I have missed the comma the point was that they are both LLM models. Now fixed it.",
        "Can you explain more about the career path that you followed to reach your current job? And do you believe it's necessary to start as a data analyst or data scientist in order to reach the position of a machine Learning engineer/researcher?",
        "Can you elaborate on \"Last I heard, Tesla collects 80 years of data every 3.5 minutes\" I don't understand, genuine question. thanks.",
        "I am a researcher who applies AI to my research and everything you're describing is pretty much what I do on a small scale.",
        "I will try to answer, but I'm guessing the non-relu means not using non-linear activation function? I think there is not enought info about what non-relu is",
        "Relu can activate and deactivate neurons based on negative inputs so the processing time is less",
        "Hi, I was curious on the mention of how you divide data into small sections. I am currently working on a project that has a huge dataset (59,000 files) that I want to reduce it. How is it done? Is there any specific method that is followed? \nIf you could suggest any videos or blogs for me to learn from. That would be great!\nThank you",
        "Next step - post on LinkedIn.",
        "so engineers in ml use OpenAI API (I mean ready-to-use models) most of the time? or tweak them if there is a need?",
        "I can't believe that's all you need...Back in 2017, I was using SSH to process images and train models on a government supercomputer; then build a front-end for it...Nowadays, no one get more rejection emails than me lol",
        "So most of the time you are busy integrating ready-to-use chatbots into your services, did i understood it right?",
        "I'll check it out thanks",
        "I got\" lucky\", as my thesis advisor offered me a job at his company, where I applied what I learned from the master’s level courses I took as electives. Aside from that \"luck\", having contacts is very helpful in academics (and even in industry, though I work more as a researcher). Many of the people I know secured jobs through recommendations from teachers or professors.\n\nOnce I had my job, I started building my portfolio. My knowledge of machine learning (I prefer not to call it just AI) mainly comes from two courses: a general machine learning course from a master’s program in computer engineering and a reinforcement learning course from a master's in mechatronics, as well as a specialization course bundle offered on Coursera. Before that, I had a basic understanding of neural networks because it was a topic I was interested in and taught myself about.",
        "No you are going to need to do a lot more than than that to stand out in this field \n\nAnyone can train or fine tune a model these days \n\nDo novel research or build things that are extremely useful to others",
        "Thanks, didn't seem that bad to me, but it was probably unnecesary on the last paragraph",
        "For starters, I would recommend using base models that include in its documentation a guide or scripts that explain how to fine-tune, for example YOLO, Florence 2, Deepth-Anything (the former having examples and tools from Roboflow). In the case of LLMs, due to their popularity, you can find a lot guides/help from r/LocalLlama.",
        "TBH it hadn't occurred to me that maybe there was just a missing comma ha.",
        "Object detection such as yolo detects objects inside of an image or videos and chatbots are two different types of model\n\nWhile chatbots are llms which are specifically trained on different architecture such as ssm Or attention \n\nTo do NLP task",
        "As I mentioned in my post, most of machine learning is collecting, generating, and visualizing data before it enters a machine learning model. \n\nA few years ago I got hired on a large project. It's not at OpenAI or Tesla, but I can't say much more about it besides the fact that I work with Audio.\n\nMy original job was designing mobile applications for our product. The easy part is making a machine learning model, the hard part is bridging that gap to the consumer to make it useful. I self-taught myself the basics of full-stack development the year before. You'd be surprised how much easier and cheaper it is to train and deploy an ML model if you learn cloud services such as AWS.\n\nI am also an undergraduate at a decent tech university and this year I decided that I wanted to pursue machine learning. I took two graduate-level machine learning classes, probability/ statistics, and more calc courses to strengthen my math skills. \n\nThe graduate-level machine learning classes taught both pre-neural network and post-neural network algorithms. This includes feed-forward networks, convolutional networks, transformers, NeRF networks, and gradients. The pre-neural algorithms taught were Bayesian networks, A\\* algorithms, etc.\n\nThis allowed me to start training my own models. The great part is that you don't need to be an expert to train your own models or get into the field of AI. PyTorch already gives you many of those tools, and there are many great resources online to explain the concepts for you (StatQuest, 3Blue1Brown, etc). While the barrier to entry is low, making a good model requires a deep understanding of the network itself. You can't brute force a good model.\n\nData science is extremely important in machine learning. All an AI is a statistical network that takes in numbers and outputs numbers. The better you are at understanding and working with the data, the better. I see a background in data science as a necessity to becoming a successful machine learning researcher.",
        "I think by data of 80 yr they mean data quantity like temporal data ex audio ,video as they collect from multiple sources they can collect 1000min of video in 1 min from 1000 different sources",
        "Tesla's full self-driving feature detects when the user disengages and takes control from self-driving back to the operator. When they do so, it will upload that video to the cloud and a team of labelers can label that section of video (they have over 300 pages of documentation on labeling standards) to make sure the model does not mess up in that scenario again. \n\nThis process is repeated across every Tesla and some online sources suggest that is around 5 million Tesla's. Every few months they train a new model and after five or six years they have a model that can drive a car 30x safer than a human while only labeling a small fraction of the data.",
        "Edit: post-RELU functions are an entire rabbit hole to dive down and generally a very complex topic; not entirely familiar with their usage, but things like SILU, SELU, and GELU preserve the desirable attributes of RELU (handling vanishing gradient) while performing better than RELU in higher dimensional problems.",
        "Id be very surprised if that’s what they mean, that way the model just ends up being a linear model. They probably mean another activation function that’s not relu",
        "It depends on the file type. I joined the project after the initial model training was complete. So we can use our own model to find data of interest to label to make the model better for a specific purpose. You could label maybe 100 of those files. Train a model, and find the examples where the model was most wrong. Label the 100 most wrong examples (in your eyes) and train again. This will quickly cause the model to converge without labeling all 59,000 files.\n\nThere are some large open-source pre-labled datasets online that can used to train a model. You could train a model based on that open-source dataset, run that model against your dataset to classify it, and use that to identify data of interest. Even if the open-source model does not have the labels you are looking for, it provides a good starting point. \n\nIn our scenario most of our data, I'd say around 80%, was environmental noise/ background noise that we do not use. We labeled this noise data and used it to filter the data-set down to the 20% that mattered. Even if it sucked on everything else at least we knew what data not to look at.\n\nYou could also find a way to encode each of your files into a vector. For example, the head of a CNN produces an output vector that then is processed by a feed-forward network. If you detach the head of the CNN, you now have a model that will produce a vector encoding of every input image. You can then take any given vector encoding and search it against your database of vector encodings to find images that are similar (Euclidean distance between the encodings). You could use a pre-trained and open-source model to do this, or a model trained from the open-source dataset. \n\nThis is a useful way to group data. Find a target image (even if it is not in your dataset), run it against the 59,000 image encodings, and store the closest 1000 images. Remove those 1000 and repeat the process 59 times to now divide up your dataset.\n\nAlso, what data management platform are you using?",
        "I think it’s a bias he has, or he might be being a bit sarcastic (he’s missing the /s). What he says could apply, though; there is a lot of noise around \"AI,\" where \"AI\" mostly refers to LLMs or even just chatbots (like ChatGPT). As I mentioned in another answer, AI is a broad topic. It’s similar to how the term \"software developer\" is often associated with working at companies like Meta, Google, or Amazon.",
        "These terms are too broad to put all individuals’ roles into. Often it doesn’t matter what the title is, ML vs AI, cause the company creating the position title doesn’t necessarily know what they actually want. I used to do AI research, make new NN architectures from scratch with custom: layers, optimizers, loss function, etc… and neither AI or ML was in my title. In my personal opinion (which no one in the field fully agrees on) AI isn’t really a thing yet. LLMs got buzz worded to hell and now the new term is AGI. I’ve noticed a lot of jobs with AI in the title just means working with pre-trained LLMs while anything else within the same larger field uses ML in the title. These are all generalizations. To make it even worse these are all a part of data science, which just confuses most people since most data scientists aren’t doing ML/AI. \n\nLast thing I’ll say: if you want to ask unambiguous questions relating to these fields, ask about specific tools/technologies/what have you. Terms like deep learning and neural networks make it much more clear you’re talking about actual “AI”\n\nEdit: spelling",
        "No he is being sarcastic and talking down on people who only do that but call themselves AI devs\n\nAnyone with half a brain can use the OpenAI api",
        "Sorry to hear it. Tough times - even for AI programmers. Business has been better here as well. Stay strong - they'll turn on the money printer and the hiring spree will restart soon.",
        "Hi, recent grad here. I've been trying to land roles related to AI but despite having projects and experience working as an intern I'm not able to land any. Given your experience I was wondering if you could provide some advice on what to learn and focus on",
        "okay got it thanks",
        "Like trying to make new models? (if you have ideas) 😄",
        "I think he is missing a \",\".",
        "An object detection model with a chatbot interface isn't that unreasonable; \"who is this?\", and joint text/image foundation models are under development now, so it's not just large language models producing a chatbot that acts as an interface, but multi-modal information being used interchangeably as part of the basic structure of the model, so that images or image components are treated as tokens to have associations built from them the same as text.",
        "That makes sense, thank you.",
        "Tidbits like this always remind me how far AI has to go.  A human child can learn to drive in a few minutes with 0 or 1 experiences if you count the current one they are learning in (count the  8-10 years of growing/living/learning if you want).   A machine with 80 years per minute of driving data still has issues with many basic driving situations.",
        "Thank you for the great explanation.  \n  \nThis is a long-tail problem, with it taking longer amounts of time to find and iron out the increasingly rarer incidents. How long do you think it will be before its safe enough to pass the regulators and finally be deployed?",
        "Hey, the information is great! Thank you.\nI actually was working on up3d dataset which is very huge. I wanted to scale it down to train over time hopefully and I Don't own the data so....",
        "What I hate about the software developer term is that mostly refers to web development hahaha it's a broad field to just be focused on that ",
        "Yeah I've understood kind of. I just do not know how actual models (not calling to them) are made? I've saw many people that use pretrained models in freelance sites fine tuning them for customers. they are kind of conductors aren't they?",
        "I see thanks",
        "Unfortunately, I can’t offer much advice on job searching since, as I mentioned, I got my job through my advisor. Once I started, I applied my knowledge of machine learning to solve problems related to robotics and robotic systems. I was in a similar situation to many others and recently applied for another job (due to a friend's recommendation), but that offer was canceled.\n\nThat said, I’ve heard from others that using platforms like LinkedIn or Upwork can be effective for job searching. Sadly (at least to me), networking is really something that should be prioritized to secure jobs or at least facilitate them. I was lucky; I’m an introvert, but I sometimes had to reach out and talk to professors to seek their help. My advisor recognized my potential (which still surprises me, since I was a regular to bad student) and made me an offer. Although it wasn't the field I initially wanted, once I was there, I pushed my ideas and worked on various projects. I maintained relationships with friends and made an effort not to upset anyone, as they were more likely to assist me. In my lab, I worked on problems that faculty found interesting, and they would offer me opportunities to be a paid assistant. Sometimes, I would ask for temporary positions, and they would eventually consider me.\n\nMy next plan is to pursue a master’s degree and pivot more toward autonomous systems or advanced AI systems, which should provide me with more opportunities. I also need to get papers published since that is something that will allow me to get jobs in the field or opportunities at universities abroad.",
        "It is not only hard to land a job but nowadays even summer internships are notoriously hard to find! A friend of mine went through 7 round of  interviews with many hours of homework for a 3 month intern!...and he got rejected. He had other similar interviews as well. Also regarding your question, you are asking something very general. \"Related to AI jobs\" can meana lot of things.",
        "Once you dive into learning about them you will figure out what you want to do. There are all types of different ways to go about training, ways to optimize little things, ways to better label data for better performance all sorts of stuff\n\nJust start getting your hands dirty by doing this and then figure out what use case you are going for \n\nIf you have an idea of what problem you aim to solve I can help you with an idea of how to go about it",
        "Yes I just didn't mention those\n\nBut rather than calling those models object detection we call them vision models\n\nOne of the example Is newly released llama 3.2 11b model of course there are other models as well\n\nAnd object detection is just a part of their ability",
        "The human brain is exceptional. It's hard to comprehend that it only takes 12 - 20 watts (based on some light Googling) to replicate what supercomputers do on Kilowatts of power. \n\nWhile GPT and full self-driving are great, they don't learn in real time. Unlike the human brain, the models are static because updating weights in real time is too expensive.  I'm not trying to convey that a human brain is the same as a linear regression network, but our linear regression networks are the best model we have so far of the human brain. And despite decades of research, we are still many years away from any computer like our brain.",
        "Tesla is starting production of the cybercab in 2026, which will have no operator controls and be self-driving. Currently, Teslas are already driving on FSD and the requirement to have your hands on the steering wheel is being removed in an upcoming update (to my knowledge).\n\nA few years ago, Tesla decided to go full end-to-end neural network. The car captures the videos from the car cameras, feds them into a neural network, and the neural network outputs the actions for the car to take next (turn steering wheel two degrees, apply 10% braking force, etc). They replaced their original 300,000 FSD codebase with 3,000 lines of code.\n\nThis is risky, as these neural networks are black boxes that operate in continuous space (infinite amount of unique inputs and outputs), so what happens when the car encounters an edge case and decides to drive off the road?\n\nTesla built a virtual replica of San Francisco and produced synthetic data for edge cases, on top of labeling video from cars even if they don't have self-driving.\n\nUnderstand that neural networks tend converge quickly from 0% to 80% accuracy, but struggle with the last 20%. More than 95% of the unique cases a human encounters when driving occur in only an hour of driving.\n\nApproaching 100% is asymptotic which requires an infinite amount of data, so those last 20% of edge cases are what Tesla has been working on labeling to improve the model. One such example is a truck carrying a load of stop signs on the highway, causing the Tesla to slam on its brakes. Another is a wind that blows a fence or tree into the road.\n\nIt seems that Tesla has labeled enough of these edge cases that the model is safe enough (20x - 30x last accidents per mile last I heard), that they are confident enough to start selling cars with no user inputs.\n\nAlso note that computing power is now doubling every six months due to the AI race. Within only 2.5 years we are going to see GPUs that are 32x as powerful as today in less space. This will allow for such deep neural networks that self-driving accidents will be news worthy rare events.\n\nI remember a few years ago when a 20 GB VRAM GPU was $20,000. Now it is only $1,000 and a $20,000 GPU is 40 GB of VRAM (A100).",
        "It Depends on your goal, what are you attempting to do with the dataset? Image classification? NeRF 3D construction? Neural networks only see numbers, so the first step is to identify exactly what the model will be receiving.",
        "Oh if you want to know how the models are made from scratch, learn a framework such as: PyTorch, tensorflow, Jax, there are some others too I can’t remember. PyTorch is the most popular, I personally prefer tf but I’m looking to learn Jax here soon. Very high level: models are made of layers you specify based on your needs, then you train them with data until satisfactory, then you can use them by passing in new data and getting the result/inference. All the frameworks have pretty great docs and tutorials to get you started learning.",
        "A lot of people use PyTorch \n\nThere is a difference between being an ML engineer and building a wrapper using OpenAI api",
        "By related to AI i mean roles like AI engg, Data scientist, Computer vision engg, NLP etc",
        "Thats a brilliant explanation thank you. I am genuinely interested.\n\nWhat do they do about model drift? Things change and models require retraining, no?  \nRoads are resurfaced, no markings, routes are changed, diversions, new intersections, signs are removed/replaced. etc.\n\nThinking of other edges cases:\n\nWhat about bad weather? heavy rain, wind or snow.\n\nWhat about lighting conditions? Nighttime driving, are the cameras nigh vision capable?\n\nWhat about glaring sunlight at dawn and dusk that requires sun visors, sun glasses and cautious driving?\n\nThanks very much.",
        "3D mesh generation!",
        "Hi, BlueHueys, is pytorch better than tensorflow? (it's just that I have seen that tensorflow is more effective and therefore used in big companies) but many  people right now say that pytorch is better. What do you think?",
        "Yes all these are whole fields of active research. It is very hard for someone to give you advice if you are not specific. To give you an example: Computer vision jobs can include research scientists working in many sub fields, roboticists, self-driving cars and autonomous vehicles of all kind,  anything related to cgi and corresponding software development, digital photography, recognition of character, pose, item and other entities and many more from business problems to theoretical research ones.",
        "Yes, models do require re-training once new data is introduced. This is why Tesla has billions of dollars worth of computers for training models - because there is so much data.\n\nNote that neural networks are designed for pattern recognition. Even if it sees an interchange it has never seen before (nor is it in the training set), it can still navigate it (just as a human can) by applying learned patterns.\n\nA neural network stores patterns, not complete videos of the events. Just like your human brain stores ideas rather than complete books. Those patterns are then applied to the incoming video stream to recognize objects and lane markings - even if the incoming video has never been seen before.\n\nAll of those edge cases are a fantastic example of what ML researchers think of. For the next few responses, this is my best guess considering Tesla hasn't released all the details of their model.\n\nFor glare, you can simulate glare (rain and snow) by taking existing videos from cars and blocking out a quarter (or some other portion) of the video. See if the model can still learn to navigate when trained on that video. Note that models have context, they work on sections of video rather than per-frame basis. So if an object is not glared for one frame and glared the next, it can remember that. The cameras also have a 360-degree recording of the car, hence there is always some camera that is not being glared.\n\nFor bad weather, Teslas also use GPS. I am assuming that, even if the entire road is hidden in a layer of snow, the model still knows the bounds of the road to some degree due to this GPS.\n\nIf a human can do it, a model can do it given enough neurons in the model and the training data.\n\nEdit: Also just remembered that Tesla's new Optimus program is experimenting with sharing data. Robots can split up and explore a factory, but all robots receive that information to form a complete picture. Perhaps Tesla's will work with traffic cameras and other Tesla's someday to navigate when the view is blocked on a particular camera.",
        "Are you converting images to 3D meshes or text to 3D meshes?",
        "Thank you for your insights. I appreciate the explanations. Here's hoping they perfect the technology soon. I look forward to a cleaner, safer, cheaper world to live in.",
        "It's images to 3D meshes.\nIt's like pose estimation (keypoints of the image) by PosePrior and ShapePrior for 3D shape prediction and later mesh generation for that.",
        "On a personal standpoint, I am very cautious about the future. The tech industry is already feeling ripples from AI replacing workers. Every programmer I know uses ChatGPT and its sped up development at least 3x if not more. Demand for software engineers is dropping because ChatGPT can do much of the programming for you.\n\nAlso, if China decides to invade Taiwan tomorrow, that is it. No more GPU production, no more supercomputers can be constructed, and the entire industry will come to a halt alongside the stock market. My current PC is about to turn six years old, and I am debating buying a new PC because in a few months I may not have that option.\n\nOptimus robots will be able to replace fast food workers in a few years. You can check out Optimus's X account and there are videos posted if it, completely on its own, handing out either water or chips to people as requested at a stand. What will all of those workers do?\n\nOn the bright side, they are completely remotely controlled if need be. Engineers on earth could teleoperate Optimus robots on the Moon to construct an initial moon base from the comfort of a office on earth."
    ]
},
{
    "submission_id": "1g98ra3",
    "title": "CMA-ES - es.tell() Takes Forever to Run and Returns \"Process finished with exit code 137 (interrupted by signal 9:SIGKILL)\"",
    "selftext": "I am trying to optimize the weights of an LSTM using the CMA-ES. In my current code, I create the LSTM model, initialize random weights, and create the CMA-ES model.I am using the [cma library](https://pypi.org/project/cma/)to create and manage the CMA-ES.\n\nFollowing this, I ask for solutions from the CMA-ES, and I get a fitness value for each solution. When I have all the possible solutions, I update the \"cma.CMAEvolutionStrategy\" object using tell.\n\nDuring this process, the program uses excessive memory, around 80 GB. Moreover, when I come to the es.tell part, the program takes forever to respond and returns the exit code 137 error in the title.\n\nThis is a pseudo-code of what I am doing:\n\n    model = LSTM(\n            input_size=INPUT_SIZE,\n            hidden_size=128,\n            output_size=OUTPUT_SIZE,\n            num_lstm_layers=1,\n            num_fc_layers=3,\n            fc_hidden_size=64\n        )\n    \n    start_weights = model.get_weights()\n    es = cma.CMAEvolutionStrategy(start_weights, sigma)\n    for i in range(100):\n           gen_fitness = []\n           solutions = es.ask()\n           for solution in solutions:\n                     gen_fitness.append(get_fitness(solution))\n           es.tell(solutions, gen_fitness)\n    \n\nI hope that this is enough information to explain the problem, and I hope that you can help me with it. My program crashes in the first iteration of es.tell(), so this is not a memory piling-up issue.\n\nI tried to run the model with smaller parameters and it was able to work. But the issue is I also have to train my model with a larger LSTM to have more accurate results. I think that having this big of a memory usage makes me think I am doing something completely wrong.",
    "created_utc": "2024-10-21T20:24:34",
    "num_comments": 0,
    "comments": []
},
{
    "submission_id": "1g96910",
    "title": "Does it make sense to manually add interaction effects in gradient boosted decision trees?",
    "selftext": "I'm curious if manually adding interaction effects of my features would help the tree learn more efficiently or do more harm than good in terms of over fitting? ",
    "created_utc": "2024-10-21T18:14:48",
    "num_comments": 2,
    "comments": [
        "Could do more harm than good",
        "Can you elaborate on why/how?"
    ]
},
{
    "submission_id": "1g947h7",
    "title": "Deep learning topics for linear algebra undergraduate research paper?",
    "selftext": "I am a junior undergraduate student and for my math major I need to write a 6-8 page paper on some application of linear algebra. I am interested in machine learning, and know that neural networks deal with linear algebra, so I figured that would be a good topic. My professor was concerned that this would be too large of a topic for the scope of the paper, and that it may not have enough linear algebra material. Do you guys have suggestions of any subtopics of neural networks that are very involved with linear algebra? Something that can be understood at an undergraduate level but has proofs, theorems and lots of mathematical material that I can go over in the paper.",
    "created_utc": "2024-10-21T16:36:31",
    "num_comments": 1,
    "comments": [
        "There's a lot that you can say about linear algebra in neural networks, but in terms of doing lots of proofs it's tricky to find something that's both substantive and appropriate for undergrads.\n\nSomething easily undergrad appropriate might be LORA, for example, which uses low rank matrix updates to change the behavior of a model. I don't know if there are any good proofs about this though; it's just a neat trick that people tried and it seems to work well.\n\nA much more meaty topic might be group equivariant neural networks. See e.g.:\n\n- https://arxiv.org/abs/1602.07576\n- https://arxiv.org/abs/2106.06610\n- https://nvlabs-fi-cdn.nvidia.com/stylegan3/stylegan3-paper.pdf\n\nThat's really all linear algebra under the hood but it might be too hard for an undergrad.\n\nAnother good topic that you might be able to just about handle is orthogonal/unitary linear layers for neural networks:\n\n- https://arxiv.org/abs/2104.07167\n- https://arxiv.org/abs/1811.04142"
    ]
},
{
    "submission_id": "1g92e3z",
    "title": "Recent CS grad. Trying to learn ML/Data Science in my free time. Can I just hop right into Andrew Ng's course? Or should I first/concurrently take some Data Science course.",
    "selftext": "Just graduated in CS. I took Linear Algebra, Applications of Linear Algebra, Computational Methods, Linear Optimization, Diff Eq, MV Calc, Statistics, etc, and Intro to AI.\n\nBut I did not take Intro to Data Science, so I could not take Intro to ML nor Intro to NLP.\n\nPlanning out some self-study now.\n\nI was thinking to do Data Science then/concurrently with some ML course. But I think I'll just do Andrew Ng's ML course alone if the Data Science course isn’t worth the time…\n\nSo:\n\n1. Do I need to / Should I spend time on a specific Data Science course? Can you recommend one? Typically I liked learning from MIT's OpenCourseWare. Lecture style learning with problem sets and stuff.\n2. Which Andrew Ng ML playlist? I see two on youtube, the Stanford class one [Stanford CS229: Machine Learning Course, Lecture 1 - Andrew Ng (Autumn 2018)](https://www.youtube.com/watch?v=jGwO_UgTS7I&list=PLoROMvodv4rMiGQp3WXShtMGgzqpfVfbU) , and the DeepLearning ai ML specialization one [#1 Machine Learning Specialization \\[Course 1, Week 1, Lesson 1\\]](https://www.youtube.com/watch?v=vStJoetOxJg&list=PLkDaE6sCZn6FNC6YRfRQc_FbeQrF8BwGI)",
    "created_utc": "2024-10-21T15:12:12",
    "num_comments": 2,
    "comments": [
        "https://www.reddit.com/r/learnmachinelearning/s/tU9UUQMtxo",
        "Thanks. Hoping to find problem sets for the Stanford course"
    ]
},
{
    "submission_id": "1g907vb",
    "title": "Does it make sense to automatically optimize prompts without ground truth data? (AutoGrad)",
    "selftext": "",
    "created_utc": "2024-10-21T13:40:57",
    "num_comments": 1,
    "comments": [
        "It makes sense. Whether or not it's actually a sound practice is very difficult to determine though, because it would be extremely labor intensive to test it: you'd need to have human curators put significant effort into creating test examples where they can thoroughly determine what a good response looks like. Like, it's definitely not true that an LLM is better than a human at this, but it takes significant human time and effort to do it well and that's not cheap.\n\nThis is generally true of almost all LLM applications though. The biggest problem with LLMs is that they are very difficult to perform statistical tests on, which is kind of a serious problem because the success and reliability of other ML algorithms is fundamentally built upon good statistical testing."
    ]
},
{
    "submission_id": "1g8xjyj",
    "title": "Does Hinge loss penalize misclassifications beyond margin ,less than logistic loss ?",
    "selftext": "Found this question and I am not sure how is it true . For misclassifications i.e y(wx+b) <0 clearly the graph of hinge loss is above logistic then why is it true ?.",
    "created_utc": "2024-10-21T11:54:01",
    "num_comments": 1,
    "comments": [
        "in this case when talking about weight, we are talking about influence on the training, i.e. how much does a single point shape the model, i.e. how much does it affect the loss function.\n\nFor example, with squared loss the effect outliers have grows quadratically, which gives them a lot more leverage and thus a point further away from the regression line is weighted more than one close to it w.r.t. the loss, if that makes sense.\n\nThus you should compare the sizes of „outliers“ (not necessarily the correct term here) for hinge loss and binary cross-entropy"
    ]
},
{
    "submission_id": "1g8x1ev",
    "title": "Local vs. Cloud",
    "selftext": "What is the consensus on learning machine learning, or completing LLM type tasks on local hardware vs the cloud? \n\nPersonally, I have a $500 budget which can get me about a 12GB RTX4070 if I'm lucky.   \nI don't game much, maybe a little, my steam library is collecting dust. So not really dual use.\n\nMy goals are to learn more about fine tuning models and applying LLM to specific problems. aka, create webapp or scripts, integrate LLM, output refined data or accomplish task. \n\nI would also like to play with things like Stable Diffusion to create images to jazz up presentations, create logos and modify/improve pictures I take.\n\nWould it make more sense to get access to high quality GPU's in a cloud environment and subscribe to Stable Diffusion for the times I actually use it?   \nWhat are the best paths to learn LLM in the cloud?  \nAs a beginner, how large are the cloud bills that you've ran up learning? 😄",
    "created_utc": "2024-10-21T11:33:19",
    "num_comments": 8,
    "comments": [
        "Sponsored by Beam 😄😄",
        "Doing some research online and on this subreddit I came across Beam, which gives you serverless GPU access.  \nIn this case the price of one 4070 off of ebay, could equal 725 hours access to a 4090...  \nor 926 hours with a T4.\n\nRealistically, going cloud route is most likely the best option cost wise. I had a feeling that would be the case.  \nNow the question is how difficult is it to use or run custom models. There are a lot of unknowns here for me but it will be really fun to investigate and learn through the process.\n\nBeam: [Serverless GPU Pricing • Beam](https://www.beam.cloud/pricing)",
        "haha I know that's what I was thinking after I posted. I sound like a Beam sales rep. 😨  \n  \nLet me know if anyone can recommend other sites or learning tools to help me along my way. Don't let me give Beam all my money. 😅",
        "If you want to do research, you can check out technical deep dive of some more providers:   \n  \nThis includes benchmarks around cold-starts, performance consistency, scalability, and cost-effectiveness for models like Llama2 7Bn & Stable Diffusion across different providers - [https://www.inferless.com/learn/the-state-of-serverless-gpus-part-2](https://www.inferless.com/learn/the-state-of-serverless-gpus-part-2) Can save months of your evaluation time. Do give it a read.\n\nP.S: I am from Inferless.",
        "Not difficult at all mate, there are companies who have APIs set up that make it very easy",
        "Beam is not a good solution there are a lot of complaints going around about them\n\nLook at lamdalabs or Nebius for a good solution",
        "Thank you! I will check that out.",
        "Thanks u/BlueChimp5 I appreciate the recommendations."
    ]
},
{
    "submission_id": "1g8uqva",
    "title": "adult model?",
    "selftext": "hello\n\ni need to detect 18+ details  on photos\n\nlike\n\n\"girl in black stockings\"\n\n\"girl in white lugerie\" etc\n\ncheked:\n\nmicrosoft/resnet-50  \n  \nmicrosoft/Florence-2-large\n\nbut i am getting like \" footwear', 'footwear', 'human face', 'woman'\n\nhow can i fix it ?",
    "created_utc": "2024-10-21T10:01:19",
    "num_comments": 6,
    "comments": [
        "Fine-tune it on the massive porn collection that you know you definitely have.\n\nEdit: The massive *labelled* porn collection you definitely have",
        "Dude who just discovered that mixing yellow and blue gave green is asking how to paint the Mona Lisa.",
        "Your best bet might be a tagging model or a clip model with tags, and check for matches to things like \"girl\", \"lingerie\" etc.",
        "I would try something like Pony Realism and use CLIP interrogator.",
        "And afterwards, make sure you push the final model to HuggingFace and call it a `fapformer`. It would be criminal if you didn't.",
        "On second thought, push the model to “TuggingFace” instead.\n\nI’ll be here all week, folks."
    ]
},
{
    "submission_id": "1g8u8ay",
    "title": "Seeking Help: Building a Generative AI Stack for Branding & Brand Systems – Willing to Pay for Expertise & guidance ",
    "selftext": "I’m looking for guidance on developing a generative AI stack that can design branding systems and storytelling with a high degree of control and quality. My goal is to create a system capable of producing brand identities that match the creative standard of top agencies, avoiding the common pitfalls of AI-generated designs that often feel generic or cliche.\n\nKey aspects of the project:\n\n* Building an AI system that allows for precise control over creative taste, ensuring the output aligns with specific aesthetic and brand storytelling goals.\n* Avoiding the typical \"AI look\"—the stack needs to produce designs that feel as human and creative as if they were developed by a top-tier agency.\n* Custom training might be necessary to reach this level of quality, so I’d appreciate any recommendations on the best models, frameworks, or approaches for this.\n\nI'm willing to pay for expert advice and guidance on how to build, train, and fine-tune this system. Any direction on the right tools, processes, or best practices would be invaluable.\n\nThanks in advance for any suggestions!",
    "created_utc": "2024-10-21T09:40:45",
    "num_comments": 0,
    "comments": []
},
{
    "submission_id": "1g8u817",
    "title": "Finding ML models ",
    "selftext": "Hi. So I am starting a project on video generation. I want to find out open source state of the art models(preferably models which utilizes VQ-VAE types of models) published upto now.\n\nI thought of going through review papers to compare performance between different models. But searching using keywords like \"video generation, survey, VQ-VAE\" etc in google scholar did not give me the results I'm looking for. So is there any efficient and quick ways I can use to find these types of information?\n\nI am somewhat new to machine learning research and would greatly appreciate your help on this. Thanks",
    "created_utc": "2024-10-21T09:40:27",
    "num_comments": 1,
    "comments": [
        "[papers with code](https://paperswithcode.com/task/text-to-video-generation)"
    ]
},
{
    "submission_id": "1g8u7ik",
    "title": "Finding ML models ",
    "selftext": "Hi. So I am starting a project on video generation. I want to find out open source state of the art models(preferably models which utilizes VQ-VAE types of models) published upto now.\n\nI thought of going through review papers to compare performance between different models. But searching using keywords like \"video generation, survey, VQ-VAE\" etc in google scholar did not give me the results I'm looking for. So is there any efficient and quick ways I can use to find these types of information?\n\nI am somewhat new to machine learning research and would greatly appreciate your help on this. Thanks",
    "created_utc": "2024-10-21T09:39:55",
    "num_comments": 1,
    "comments": []
},
{
    "submission_id": "1g8suho",
    "title": "A good book. Provided that I have completed multivariable calculus. Will soon learn probability and statistics in college course. I want the book that actually explains what machine learning is and after learning the book I would be able to learn articles in machine Learning. ",
    "selftext": "Basically, I need a book with a good amount of analogy and mathematics so that I can actually understand what machine learning is and can proceed on other advanced topics. I don't want to be ungrateful but please don't suggest a very beginner level one. I get bored with beginners' book basically an Intermediate one would help.",
    "created_utc": "2024-10-21T08:45:31",
    "num_comments": 9,
    "comments": [
        "Hands on Machine Learning- Geron\n\nFundamentals of Machine Learning for Predictive Data Analytics - Keller\n\nPattern Recognition and Machine Learning- Bishop.",
        "What i would do is plug chapters from those suggested books into notebooklm, listen to the ai podcast while looking around the chapter, then do an exercise to program with it. Rinse repeat.",
        "Intro to machine learning - tom mitchel \n\nThis book starts from rules based systems, hypothesis space, general solution/specisic solution and goes on gradually and ends with reinforcement learning.",
        "Won't machine learning need a foundation in Probability and Statistics?",
        "did you mention them in order? or I can start with any one of the three?",
        "What do you use for hardware, cloud?",
        "100%\n\nBut OP says he is going to take up probability and stats in uni and Id assume a fair (base level knowledge of Stats first)",
        "In the stated order.\n\nGerons book is the most accessible and easy to digest. Itll give you a good introduction to the topic.\n\nKeller’s book is slightly more academic and goes into case studies to show your the moving parts and the cogs. So you’ll be able to see the knowledge from Genrons book “applied”\n\nBishops is the most rigorous out of the lot and I wouldnt recommend diving into it first.",
        "What do you eat, food?"
    ]
},
{
    "submission_id": "1g8sgth",
    "title": "Need suggestion regarding choosing the lambda values for Lasso Regression for doing feature selection",
    "selftext": "Hello, I am pretty new in machine learning and I am stuck on this topic for more than a week and still confused.\n\nI am using lasso for feature selection purpose and I am confused about how to set the range of lambda values for doing cross validiation in lasso regression. For the dataset I am using at near lambda value 10^(0), all feature coefficients becomes zero. If I take the range 10^(-1) to 10^(0) then I get 6 coeffcients (6 features) but if a broader range is taken such as 10^(-6) to 10^(0) then I endup with 12 coefficents. So what should be the correct approach of choosing the minimum and maximum value of the lambda?\n\nShould I think alone this line that if the model built with 6 features performs better than the 12 feature based model then I should choose the smaller range of lambda? Please provide me some guidance.\n\nhttps://preview.redd.it/0nqvgj62m4wd1.png?width=569&format=png&auto=webp&s=588b8a22c011150581333e88eb66ecb7f37a1482\n\n",
    "created_utc": "2024-10-21T08:29:53",
    "num_comments": 2,
    "comments": [
        "Classic use-case for cross-validation: fit many models with different lambdas, compute their errors for the validation dataset, choose lambda that produces lowest error.",
        "Thank you very much."
    ]
},
{
    "submission_id": "1g8s241",
    "title": "Mathematics for machine learning",
    "selftext": "i am interested in machine learning field, so i did some research on prerequisites for machine learning, then i got to know that mathematics (linear algebra, probability, calculus) are very important for someone who''s starts machine learning(because the concept behind all those algos in machine learning is purely based on mathematics), so can someone suggest some good way to complete math portion ( so far i have completed vector spaces, subspaces, basis, dimension, linear independence, systems of equations in linear algebra and single variable calculus )",
    "created_utc": "2024-10-21T08:12:52",
    "num_comments": 3,
    "comments": [
        "[deleted]",
        "How come you haven’t mentioned about statistics, this is also important mathematics topics to learn in ml",
        "sorry my bad ( linear algebra, probability and statistics, calculus), can you please recommend me some structured way to learn mathematics"
    ]
},
{
    "submission_id": "1g8ryda",
    "title": "Is it possible to Use pretrained  llms to perform classification (image)?",
    "selftext": "",
    "created_utc": "2024-10-21T08:08:43",
    "num_comments": 7,
    "comments": [
        "vision capabilities in multimodal llms can be used but not worth the computation cost and inference time",
        "A vision transformer is probably the closest thing, but I wouldn't call that a \"large language model\" and it couldn't be  pretrained on language.",
        "Yes. But why would you do that? Train a classifier. It is faster and more reliable.",
        "Well, Large Language Models, as the name already implies, primarily excel in language tasks. Therefore, they can be highly effective at classifying text. However, there are multimodal LLMs which can process images and could possibly be used for image classification. However, they usually work by just combining an image model with an LLM and the LLM part would just be pointless if you only want to perform image classification.\nTherefore, the answer is yes you could but a dedicated image classifier will probably be way faster and more accurate than a multimodal LLM. A more precise answer would depend on your actual use case.",
        "Ikr 💯💯💯",
        "😂just trying new waters but I get it thanks",
        "Thanks yeah did some research about it and I’d agree with you"
    ]
},
{
    "submission_id": "1g8r260",
    "title": "I want to practice, please show me how",
    "selftext": "\n\nHey guys, I am trying to learn statistics/machine learning from the book Introduction to Statistical Learning with Applications in Python (ISLP). I'm not going to lie, I really like the way this book explains different models and emphasizes the statistical aspects of data. Now I want to practice, but I can't find a resource that suits the way I want to learn. I don't want to just import a dataset, clean it, and train a model or different models. What I'm looking for is a mentor-like resource that will guide me step by step on how to build a strong analysis on a dataset, showing me the errors I should avoid. I'm not focused on getting the best result, but on learning how to reach it. Please, if you know of any book, resource (GitHub, website, course, etc.), let me know.",
    "created_utc": "2024-10-21T07:30:26",
    "num_comments": 9,
    "comments": [
        "Hopefully this might help but tbh I find that doing project(s) on something you have interest in really helps. It \"forces\" you think about a lot aspects of ML that you might've not thought about/expected. \n\nThe above is especially true for the feature engineering part, which is like the (large) majority of ML work anws. To that end, you'd get ample practice.",
        "Kaggle? HuggingFace?",
        "Maybe think of the problem (even same as in the book) that uses specific model and try to do it yourself from the beginning to the end. You can start with web scraping for data, then clean it and use the model you need to practice? Lots of websites like airbnb, linkedin or twitter allow to gather data from their websites ",
        ">I'm not going to lie, I really like the way this book explains different models and emphasizes the statistical aspects of data.\n\nThanks for telling the truth and admitting it. It's shameful but you'll be aight.\n\nGonna give a strong disagree with the Kaggle answers. In my experience, Kaggle has a divide between:\n\n* Literally fucking impossible / not even machine learning\n* Cookie cutter bs\n\nIf you approach Kaggle the right way, maybe. Especially do not recommend just quizzing yourself on the approaches of others.\n\nI would personally recommend to come up with some novel interesting idea yourelf and try to approach it. Then along the way, you'll discover challenges that you would've never conceived of when doing some cookie-cutter Kaggle playground.",
        "Jeremy Howard’s FastAi course",
        "Go for kaggle \nLook at different answers and try understanding the pattern but make sure that u have already learned python libraries \nFor statistics u may follow Harish garg playlist and they contain the whole information!!!",
        "Yes there are many are ml projects, but i find that they contain mainly the final result of the project, they lack the analysis side i am looking for. From what i learned, the choice of a ml model is not based on the accuracy only, but also on a lot of static informations about the dataset(variance, correlation....), the bias-variance trade-off...... So choosing a model is a final result of long analysis, and right now i am not able to do this analysis because i will be missing many informations",
        "Kaggle has some excellent write-ups from competition winners and also comments in notebooks explaining why they've chosen the paths they gone down.\n\nYou're going to hate this, but the #1 response to, \"I don't know if this is a good approach,\" is, \"Try it and find out.\" Remember the *science* bit of data science, you've got to do experiments and see what works. That doesn't mean blindly trying everything, but often the situation is too complicated for you to \"just know\" what will work and the most efficient thing to do is give it a go and see how well it works.",
        "Kaggle has a competition section which I think is pretty widely accepted to be one of the best ways to git gud."
    ]
},
{
    "submission_id": "1g8qga1",
    "title": "Flux.1 Dev can now be used with Google Colab (free tier) for image generation ",
    "selftext": "",
    "created_utc": "2024-10-21T07:03:18",
    "num_comments": 0,
    "comments": []
},
{
    "submission_id": "1g8pfwi",
    "title": "Question regarding laptops",
    "selftext": "I am currently in the middle of completing my Data Science and ML certification. My laptop is on its last legs, so I wanted to ask recommendations for replacement.\n\nThe configuration I am currently eyeing is as follows\n\n* Processor: 14th Generation Intel® Core™ i9-14900HX Processor (E-cores up to 4.10 GHz P-cores up to 5.80 GHz)*selected upgrade*\n* Operating System: Windows 11 Home Single Language 64\n* Microsoft Productivity Software: Microsoft Office Trial\n* Memory : 32 GB DDR5-5600MHz (SODIMM) - (2 x 16 GB)*selected upgrade*\n* First Solid State Drive: 1 TB SSD M.2 2280 PCIe Gen4 TLC*selected upgrade*\n* Second Solid State Drive: None\n* Display: 40.64cms (16) WQXGA (2560 x 1600), IPS, Anti-Glare, Non-Touch, HDR 400, 100%DCI-P3, 500 nits, 240Hz, Low Blue Light\n* Graphic Card: NVIDIA® GeForce RTX™ 4060 Laptop GPU 8GB GDDR6\n\nI heard advice on other threads to get a GPU with 16GB VRAM but I can't afford it.\n\nAlso wanted to ask if the i9-14900HX can work well with the 4060 without throttling.",
    "created_utc": "2024-10-21T06:16:33",
    "num_comments": 1,
    "comments": [
        "You can't do serious machine learning on a laptop with a constrained budget. \n\nHowever if you just need something for basic course work then the laptop you've described should be fine. Core i9 is probably overkill, even; I bet a core i7 would work just fine."
    ]
},
{
    "submission_id": "1g8ohf6",
    "title": "Machine Learning teaching/study group ",
    "selftext": "Hi all. We have a study group on discord that is based around teaching ML to each other. We have been holding sessions pretty regularly for the past two months, at a frequency of three meetings per week and are covering the basics of classical ML and Deep Learning. \n\nWe have a sizeable number of people in the group but very few of them are active participants in the sessions. It would be nice to have more people engage and learn from the discussions. So I’m here to look for more members. Drop me a dm if you would like to join the group. Please contact only if you are serious about studying ML and plan to actively join the sessions. Thanks. ",
    "created_utc": "2024-10-21T05:29:09",
    "num_comments": 3,
    "comments": [
        "Dm",
        "Dm",
        "DM"
    ]
},
{
    "submission_id": "1g8oet8",
    "title": "[Help] Building an AI Model for Generating Text & Q&A Based on Custom Data – Need Suggestions on Methods & Deployment Platforms",
    "selftext": "Hey everyone,\n\nI'm working on an enterprise-level AI project to build a model that can generate text or answer questions based on provided documents or data. I'm looking for advice on the best methods to achieve accurate results and the most suitable platforms for deployment.\n\n**Key Considerations:**\n\n* **Accuracy:** How can I ensure the generated text or answers are highly accurate and relevant to the given context?\n* **Deployment:** Which platforms are best suited for deploying such a model in an enterprise environment, considering factors like scalability, security, and integration with existing systems?\n\nI'm open to any suggestions or recommendations you may have regarding data preparation, model selection, fine-tuning, evaluation metrics, deployment platforms, or any other relevant aspects of building a text generation AI.\n\nThanks!",
    "created_utc": "2024-10-21T05:25:15",
    "num_comments": 1,
    "comments": [
        "You’re taking on a big project, especially for enterprise level deployment. But here are a few follow up questions/considerations:\n\n1) What is your experience with ML, NLP, and Gen AI? If you’re not experienced, you might consider partnering with a company who has experience deploying in enterprise environments.\n\n2) Do you want to fine tune an existing model or do you want the existing model to answer a prompt based on retrieved documents (RAG)? The answer will require completely different methods of approaching your solution, both now and in the future.\n\n3) Is your data unstructured only or structured and unstructured? The way you fine tune your model or build your document retrieval system (vector db with search) will depend on your data.\n\n4) How often will your data change? Static data may lead to one solution, but dynamic data will bring additional challenges.\n\n5) What platforms are you currently using and what platforms do you need to integrate with? The “best” solution for your enterprise will probably be different from the best solution for other organizations.\n\n6) Customer facing or internal? The amount and type of guardrails, observability, and evaluations will depend on the users and enterprise risk aversion.\n\n7) There many other questions… but these will form a good starting point. Good luck!"
    ]
},
{
    "submission_id": "1g8o24m",
    "title": "i need a dataset and which model or algorithm used for my whatsapp template selection model.",
    "selftext": "    ### this the code for create a dataset and i need more precious and perfect data ###\n    import pandas as pd\n    import json\n    \n    # Data for the WhatsApp Template Dataset (from previous example)\n    data = {\n        \"Template Name\": [\n            \"order_confirmation\",\n            \"delivery_update\",\n            \"authentication_otp\",\n            \"seasonal_promotion\",\n            \"product_launch\",\n            \"password_reset\",\n            \"appointment_confirmation\",\n            \"order_cancellation\",\n            \"service_feedback\",\n            \"limited_time_offer\",\n            \"support_followup\",\n            \"account_verification\",\n            \"subscription_renewal\",\n            \"survey_invitation\",\n        ],\n        \"Category\": [\n            \"UTILITY\",\n            \"UTILITY\",\n            \"AUTHENTICATION\",\n            \"MARKETING\",\n            \"MARKETING\",\n            \"AUTHENTICATION\",\n            \"UTILITY\",\n            \"UTILITY\",\n            \"MARKETING\",\n            \"MARKETING\",\n            \"UTILITY\",\n            \"AUTHENTICATION\",\n            \"MARKETING\",\n            \"MARKETING\",\n        ],\n        \"Language\": [\"en_US\"] * 14,\n        \"Body Text\": [\n            \"Thank you for your order, {{1}}! Your confirmation number is {{2}}. Use the buttons below to contact support.\",\n            \"Your package will be delivered by {{1}}. Click the link below to track your package.\",\n            \"Your one-time password (OTP) is {{1}}. Do not share this code with anyone. It expires in {{2}} minutes.\",\n            \"Shop now through {{1}} and use code {{2}} to get {{3}} off all merchandise. Click the buttons below to manage subscriptions.\",\n            \"We are excited to introduce {{1}}! Click the link to learn more or call us for details.\",\n            \"Reset your password using the one-time password (OTP) sent to your email. It expires in {{1}} minutes.\",\n            \"Your appointment is confirmed for {{1}}. If you need to reschedule, use the buttons below.\",\n            \"Your order for {{1}} has been cancelled successfully. If you have any questions, contact support below.\",\n            \"How satisfied are you with our service? Please click on the buttons below to share your feedback.\",\n            \"Hurry! Our limited-time offer ends soon. Click below to explore our catalog or call us for more details.\",\n            \"We have received your support request. A member of our team will contact you shortly.\",\n            \"Verify your account with the one-time password (OTP) below. Do not share this with anyone.\",\n            \"Your subscription to {{1}} will renew soon. Use the buttons below to manage your subscription or contact us for support.\",\n            \"We'd love to hear your thoughts on our recent service. Click the link below to take a short survey.\",\n        ],\n        \"Header Text\": [\n            \"Order Confirmation\",\n            \"Delivery Update\",\n            \"Authentication Required\",\n            \"Seasonal Promotion\",\n            \"New Product Launch\",\n            \"Password Reset\",\n            \"Appointment Confirmation\",\n            \"Order Cancellation\",\n            \"Feedback Request\",\n            \"Limited-Time Offer\",\n            \"Support Request Received\",\n            \"Account Verification\",\n            \"Subscription Renewal\",\n            \"Survey Invitation\",\n        ],\n        \"Footer Text\": [\n            \"Call support for help\",\n            \"Track your order\",\n            \"Do not share your OTP\",\n            \"Manage your preferences\",\n            \"Exciting offers available!\",\n            \"Contact support if needed\",\n            \"Contact support\",\n            \"Contact support\",\n            \"We value your feedback\",\n            \"Offers end soon!\",\n            \"Thank you for your patience\",\n            \"Code expires in {{1}} minutes\",\n            \"Manage your subscription\",\n            \"Your opinion matters\",\n        ],\n        \"Button 1 Type\": [\n            \"PHONE_NUMBER\",\n            \"URL\",\n            \"OTP\",\n            \"QUICK_REPLY\",\n            \"URL\",\n            \"OTP\",\n            \"PHONE_NUMBER\",\n            \"PHONE_NUMBER\",\n            \"QUICK_REPLY\",\n            \"CATALOG\",\n            \"QUICK_REPLY\",\n            \"OTP\",\n            \"QUICK_REPLY\",\n            \"URL\",\n        ],\n        \"Button 1 Action\": [\n            \"+15550051310\",\n            \"https://www.example.com/track/{{1}}\",\n            \"COPY_CODE\",\n            \"Unsubscribe from Promos\",\n            \"https://www.example.com/product/{{1}}\",\n            \"ONE_TAP\",\n            \"+16175551234\",\n            \"+15550051234\",\n            \"Very Satisfied\",\n            \"Browse our Catalog\",\n            \"Need more help?\",\n            \"ZERO_TAP\",\n            \"Unsubscribe\",\n            \"https://www.example.com/survey/{{1}}\",\n        ],\n        \"Button 2 Type\": [\n            \"URL\",\n            \"QUICK_REPLY\",\n            \"0\",\n            \"QUICK_REPLY\",\n            \"PHONE_NUMBER\",\n            \"QUICK_REPLY\",\n            \"URL\",\n            \"QUICK_REPLY\",\n            \"QUICK_REPLY\",\n            \"PHONE_NUMBER\",\n            \"VOICE_CALL\",\n            \"0\",\n            \"URL\",\n            \"0\",\n        ],\n        \"Button 2 Action\": [\n            \"https://www.example.com/order/{{1}}\",\n            '\"Need more help?\"',\n            \"0\",\n            \"Unsubscribe from All\",\n            \"+13051234567\",\n            \"Need more time?\",\n            \"https://www.example.com/appointments/{{1}}\",\n            \"Need to reorder?\",\n            \"0\",\n            \"+14155552345\",\n            \"+15555555555\",\n            \"0\",\n            \"https://www.example.com/subscription/{{1}}\",\n            \"0\",\n        ],\n        \"Button 3 Type\": [\n            \"0\",\n            \"0\",\n            \"0\",\n            \"0\",\n            \"0\",\n            \"0\",\n            \"0\",\n            \"0\",\n            \"QUICK_REPLY\",\n            \"0\",\n            \"0\",\n            \"0\",\n            \"0\",\n            \"0\",\n        ],\n        \"Button 3 Action\": [\n            \"0\",\n            \"0\",\n            \"0\",\n            \"0\",\n            \"0\",\n            \"0\",\n            \"0\",\n            \"0\",\n            \"Neutral\",\n            \"0\",\n            \"0\",\n            \"0\",\n            \"0\",\n            \"0\",\n        ],\n        \"Allow Category Change\": [\n            True,\n            True,\n            True,\n            False,\n            True,\n            False,\n            True,\n            True,\n            True,\n            False,\n            True,\n            True,\n            False,\n            True,\n        ],\n    }\n    \n    # Create a DataFrame\n    df = pd.DataFrame(data)\n    \n    # Function to generate the JSON structure for each row\n    def generate_final_json(row):\n        buttons = []\n        if row['Button 1 Type'] and row['Button 1 Action']:\n            buttons.append({\n                \"type\": row['Button 1 Type'],\n                \"text\": \"Button 1\",  # Placeholder text\n                \"action\": row['Button 1 Action']\n            })\n        if row['Button 2 Type'] and row['Button 2 Action']:\n            buttons.append({\n                \"type\": row['Button 2 Type'],\n                \"text\": \"Button 2\",  # Placeholder text\n                \"action\": row['Button 2 Action']\n            })\n        if row['Button 3 Type'] and row['Button 3 Action']:\n            buttons.append({\n                \"type\": row['Button 3 Type'],\n                \"text\": \"Button 3\",  # Placeholder text\n                \"action\": row['Button 3 Action']\n            })\n        \n        return json.dumps([\n            {\n                \"type\": \"BODY\",\n                \"text\": row['Body Text'],\n                \"example\": {\n                    \"body_text\": [[\"Example1\", \"Example2\"]]  # Placeholder examples\n                }\n            },\n            {\n                \"type\": \"BUTTONS\",\n                \"buttons\": buttons\n            }\n        ], indent=2)\n    \n    # Create a new column 'Final JSON'\n    df['Final JSON'] = df.apply(generate_final_json, axis=1)\n    \n    # Save to CSV\n    csv_file_path = \"./whatsapp_template_dataset.csv\"\n    df.to_csv(csv_file_path, index=False)\n    \n    csv_file_path\n\n    [\n      {\n        \"type\": \"BODY\",\n        \"text\": \"Thank you for your order, {{1}}! Your confirmation number is {{2}}. If you have any questions, please use the buttons below to contact support. Thank you for being a customer!\",\n        \"example\": {\n          \"body_text\": [\n            [\n              \"Pablo\",\"860198-230332\"\n            ]\n          ]\n        }\n      },\n      {\n        \"type\": \"BUTTONS\",\n        \"buttons\": [\n          {\n            \"type\": \"PHONE_NUMBER\",\n            \"text\": \"Call\",\n            \"phone_number\": \"15550051310\"\n          },\n          {\n            \"type\": \"URL\",\n            \"text\": \"Contact Support\",\n            \"url\": \"https://www.luckyshrub.com/support\"\n          }\n        ]\n      }\n    ]### the above  template like i need a outut from model what can i do\n     ",
    "created_utc": "2024-10-21T05:06:12",
    "num_comments": 0,
    "comments": []
},
{
    "submission_id": "1g8m88y",
    "title": "beginner books recommendation for machine learning ",
    "selftext": "I have seen a lot of people that recommended the Andrew Ng lectures to learn but personally I'm not a person who can sit still and watch a prerecorded lecture, I would rather read the material myself, I've gotten two books so far, Introduction to Statistical Learning in Python and Machine Learning with Pytorch and Scikit learn from similar posts asked before, I just want to make sure that these books are good for me, if not what book would you guys recommend? And assume that I have basic calculus and linear algebra knowledge as I have completed a level. Since I only have about 8 months of free time, which book should I prioritise first? Tqvm !!",
    "created_utc": "2024-10-21T03:15:11",
    "num_comments": 0,
    "comments": []
},
{
    "submission_id": "1g8lx1y",
    "title": "How would you approach this if you were starting again?",
    "selftext": "I'm a daytrader who's had to become a quant. I'm comfortable with data, Linux, cudf etc.\n\nFor the past few months I've been playing around with AutoML, CatBoost, XGBoost, SK Learn etc and I'm in a position where I can create models that work for me and my usecase, with pretty high scores, but I don't really understand the models of what they are doing lmfao, I need to basically learn how to make them, I understand nothing about HPO, architecture etc, when to use what model, how to improve etc. If you were to start again, how would you do it ?\n\nI understand that there is a learning curve but the other avenues I've pursued have steep learning curves too so I'm not scared of effort. Don't worry I'm not the type to come in here and and throw in Linear Regression at the closing price and think I'm gonna be rich, I have multiple use cases, ranging from confirmation, classification, etc ",
    "created_utc": "2024-10-21T02:53:48",
    "num_comments": 6,
    "comments": [
        "It seems to me you are using a lot of libraries that do the dirty work for you, which is perhaps why you don't really understand what's happening under the hood. \n\nAn idea might be to implement some of the models for your use cases by hand in, say, numpy/torch. Try to implement the backprop by hand so you *really* understand everything that's happening. Then you can go on to more complex concepts, like making your model be robust to an insignificant change in price (just an example, I don't even know if that's desirable, never did ML for finance). Or use Gaussian Processes to quantify the uncertainty of a price prediction. \n\nMy point is, trying to cater to a use case by implementing by hand something that's novel and only you might know it is needed because of your domain specific knowledge, will help you tinker with a lot of the intricacies of the models and make you realize how they actually work.",
        "How are able to consistently crank out models that work well without any understanding of what it is that makes a model work well?",
        "Thank you, this is very good advice I'll cut out time to start doing this",
        "AutoML libs are surprisingly good + I know what factors will likely lead to good predictability via domain knowledge",
        "Andrej Karpathy on youtube has a lot of 0 to hero tutorial series, which do exactly that - e.g. on Neural Nets - not sure if this would be useful to your case are all. Amazing learning resources from a legend in the field.",
        "I use NNs a lot, I'll be sure to have a look, thank you"
    ]
},
{
    "submission_id": "1g8lqff",
    "title": "\"The slow death of Li-ion batteries\"",
    "selftext": "\n\nhttps://preview.redd.it/jrrfs3t6y2wd1.png?width=1280&format=png&auto=webp&s=3a29566919cca89a4cd1ff2dcf61836a119edc25\n\nIf you use a battery very frequently, it will degrade fast. This is called cycle degradation.\n\n\n\nIt will still degrade if you don't use a battery at all. This is called calendar degradation.\n\n\n\nDegradation of capacity is a major challenge for Lithium-ion Batteries in EVs.\n\n\n\nLithium is scarce and there are no better alternatives. So it adds great value to be able to predict how much longer can the battery itself (not the charge) last.\n\n\n\nThere are very complicated theoretical models that predict calendar and cycle degradation. However, we noticed that all such models use very difficult to estimate empirical parameters. These parameters vary based on the batter type and yet the models are not very accurate.\n\n\n\nWe wanted to create a better model, that does not rely so much on these empirical parameters and can yet capture the dynamics of battery health degradation.\n\n\n\nFor the last few months, Sharv (a 15-year old) and Hrishikesh have been working with Raj, Rajat and me to solve this problem of modeling battery degradation using Scientific Machine learning. We have successfully modeled the battery health using physical models + neural networks and our results match exceptionally well with analytic models, numerical models, and experimental data.\n\n\n\nOur work was accepted at MIT URTC - one of the most prestigious conferences for undergrad students.\n\n\n\nNow our latest results are also published on arXiv: [https://arxiv.org/abs/2410.14347](https://arxiv.org/abs/2410.14347)\n\n\n\nIn this video I published recently on Vizuara's YouTube channel, I explain the work in detail and provide a simple yet detailed review of the paper. Check this out. I am sure you will enjoy the video: [https://www.youtube.com/watch?v=y2EPnu8km2s&feature=youtu.be](https://www.youtube.com/watch?v=y2EPnu8km2s&feature=youtu.be)\n\n",
    "created_utc": "2024-10-21T02:40:16",
    "num_comments": 4,
    "comments": [
        "This is incredible work. You guys are amazing!",
        "Reddit tells me that Lithium is no longer considered scarce.",
        "No relevant code picked up just yet for \"A Scientific Machine Learning Approach for Predicting and Forecasting Battery Degradation in Electric Vehicles\".\n\n[Request code](https://www.catalyzex.com/paper/arxiv:2410.14347?requestCode=true) from the authors or [ask a question](https://www.catalyzex.com/paper/arxiv:2410.14347?autofocus=question).\n\nIf you have code to share with the community, please add it [here](https://www.catalyzex.com/add_code?paper_url=https://arxiv.org/abs/2410.14347&title=A+Scientific+Machine+Learning+Approach+for+Predicting+and+Forecasting+Battery+Degradation+in+Electric+Vehicles) 😊🙏\n\nCreate an alert for new code releases here [here](https://www.catalyzex.com/alerts/code/create?paper_url=https://arxiv.org/abs/2410.14347&paper_title=A Scientific Machine Learning Approach for Predicting and Forecasting Battery Degradation in Electric Vehicles&paper_arxiv_id=2410.14347)\n\n--\n\nTo opt out from receiving code links, DM me.",
        "Cool and impressive work!!!"
    ]
},
{
    "submission_id": "1g8lbs4",
    "title": "Need suggestions on learning ML",
    "selftext": "Hi, I started my data Science ML Journey around in September before that I did some python and basic data analysis visualization courses and now I'm learning SQL as well as properly learning the ML basics. I'm doing the coursera Andrew NG specialization and I'm halfway through. So I thought maybe I should start doing some projects alongside now on machine learning. But I have no idea how and where to start. When I look at other people's projects there seems a lot of knowledge gap between me and them I feel like l haven't learned anything at all. ",
    "created_utc": "2024-10-21T02:09:01",
    "num_comments": 2,
    "comments": [
        "You can do all the tutorials and read all the textbooks in the world, but if you can’t implement anything you’ve learned into code, you’re wasting time. It sounds like you have enough foundational knowledge to start building.\n\nAs you’re going along with these learnings, chose your own problem to solve and solve it your way. If you need to google things like syntax, what layer to use, etc. that’s fine. Start with exploring open source data sets (ex: Kaggle) and see what resonates with you.\n\nBuild something based on what you’ve learned -> look at well written Git repos -> refine what you’ve built -> repeat",
        "+1"
    ]
},
{
    "submission_id": "1g8krt1",
    "title": "Can someone help me with few shot longformer finetuning",
    "selftext": "I'm trying to finetune the longformer model for text classification on only 2 classes. I'm using the DAIC-WOZ dataset for depression detection. The problem is there is class imbalance (which i solved through synonym replacement) and the number of samples per class is very less (\\~120 per class). \n\nThe model doesnt seem to reduce its training and validation loss. The best accuracy I got was about 50% and the best F1 score was about 67%. \n\nI want to look into few shot learning methods or any way I can finetune this big model on my small dataset. Thanks!",
    "created_utc": "2024-10-21T01:25:25",
    "num_comments": 0,
    "comments": []
},
{
    "submission_id": "1g8k205",
    "title": "Can I use NN on non-image classification?",
    "selftext": "I have a school project on machine learning and was wondering if neural network can be used on classification tasks other than images. \n\nThe dataset is tabular with categorical variables, would it be feasible to try implementing a neural network or would we be better off using decision trees and logistic regression?",
    "created_utc": "2024-10-21T00:27:51",
    "num_comments": 15,
    "comments": [
        "It is possible but NNs are notoriously bad with tabular data, and you should one-hot encode categorical variables",
        "Gradient boosting classifiers are the go-to solution for classification on tabular data. Check scikit-learn's histogram-based gradient boosting classifier, xgboost, lightgbm, etc usually NN fall behind these in performance while requiring far more hyperparameter tuning, longer training times, and larger datasets.",
        "Yes, you can use a neural network for classification of tabular data. \n\n\nIs it an open-ended project? If you have time, you could try all three methods and compare the results.\n\n\nAlso, this is from a quick Google search: https://towardsdatascience.com/the-complete-guide-to-neural-networks-multinomial-classification-4fe88bde7839",
        "Can you?  Yes\n\nShould you?  Probably not",
        "NN can work really well on tabular data especially if you have 500K+ records in your dataset. \n\nYou'll have to encode the categorical variables somehow, generally either one-hot, ordinal or sometimes weight of evidence binning can work well.",
        "Yes but in most cases with spreadsheet problems you're better served with something like random forests. NNs are better with exotic data like images. \n\nYou could always test it out. A basic NN is very simple to code in R or Python. You should be aware though that unless you have a lot of data, it will really struggle, (it will usually be worse with tabular data, but it would be horrendous without much data).",
        "I think NNs are very good, but yes you must one-hot encode all categorical variables",
        "I see, thank u for the suggestion! we r planning to implement a few models (logistic regression, dt + sthg) and compare the results, but since xgboost is a tree based model, wld u recommend using logistic+dt+xgboost or sthg more different like logistic+xgboost+svm for better comparison?",
        "Thank you for the link! We are planning to implement multiple methods and evaluate them together, but we wasnt sure if nn wld be a suitable model. Regardless, the link helped us understand how we cld have used it, rly appreciate that!",
        "thank you for your response! our dataset isnt that big so i suppose nn is not a good idea afterall :'\"\")  I did abit of google research and most say NN seem to say wld not work well as tabular data is structured and does not have enough information for NN so it will tend to overfit. Does it fit better to large datasets because it can generalize better and capture more complex relationships? Please correct me if im wrong",
        "NNs can predict tabular data, but most of the time simpler models like rfrs and xgboost performs just as good if not better.",
        "I definitely agree. You could use NNs but Gradient Boosting is state of the art when it comes to tabular data. They also handle categorical values very well. You can try what ever model. Probably no approach will be able to beat the Gradient Boosting. Since sklearn provides an easy interface for all of these it should be very quick and simple to test them all out.",
        "That's basically correct. Intuitively, each node can be thought of as a feature, (they are features for future layers), and one of the ways to overfit with older ml algos is to include too many features. However, with a ton of data, the weights can be forced to generalize, instead of perfectly predict a few training datapoints.",
        "Yepp we will def include gradient boosting! thank you for your input :)",
        "i see, definitely understanding it better now, thank u for your detailed response!"
    ]
},
{
    "submission_id": "1g8jzmb",
    "title": "Hard negative mining and Embedding enhancement ",
    "selftext": "I have an embedding task for legal document retrieval that needs to enhance the MRR score for the relevant retrieved document. My dataset contains roughly 200k pairs of {query: str,legal\\_doc: str}. So I have several questions \n\n1. I have inferred the BAAI/bge-large-en and the result is quite impressive but the model still performs poorly in a long query context. Is there any <1B model that performs well?\n2. To achieve more accuracy and recall during rerank, I need to hard negative mining before feed data to the model but I have searched and don't found any pipeline related to this workflow, how to mine hard negative and is there any constraint to mine hard negative ? \n\nThanks in advance",
    "created_utc": "2024-10-21T00:22:39",
    "num_comments": 0,
    "comments": []
},
{
    "submission_id": "1g8im6k",
    "title": "The Prompt Report: Prompting techniques survey",
    "selftext": "Prompt engineering, while not universally liked, has shown improved performance for specific datasets and use cases. Prompting has changed the model training paradigm, allowing for faster iteration without the need for extensive retraining.\n\n>**Follow the Blog for more such articles:** [https://medium.com/aiguys](https://medium.com/aiguys)\n\n**Six major categories of prompting techniques are identified: Zero-Shot, Few-Shot, Thought Generation, Decomposition, Ensembling, and Self-Criticism. But in total there are 58 prompting techniques.**\n\n**1. Zero-shot Prompting**\n\nZero-shot prompting involves asking the model to perform a task without providing any examples or specific training. This technique relies on the model's pre-existing knowledge and its ability to understand and execute instructions.\n\nKey aspects:\n\n* Straightforward and quick to implement\n* Useful for simple tasks or when examples aren't readily available\n* Can be less accurate for complex or nuanced tasks\n\n*Prompt: \"Classify the following sentence as positive, negative, or neutral: 'The weather today is absolutely gorgeous!'\"*\n\n**2. Few-shot Prompting**\n\nFew-shot prompting provides the model with a small number of examples before asking it to perform a task. This technique helps guide the model's behavior by demonstrating the expected input-output pattern.\n\nKey aspects:\n\n* More effective than zero-shot for complex tasks\n* Helps align the model's output with specific expectations\n* Requires careful selection of examples to avoid biasing the model\n\n*Prompt:* *\"Classify the sentiment of the following sentences:*\n\n*1. 'I love this movie!' - Positive*\n\n*2. 'This book is terrible.' - Negative*\n\n*3. 'The weather is cloudy today.' - Neutral*\n\n*Now classify: 'The service at the restaurant was outstanding!'\"*\n\n**3. Thought Generation Techniques**\n\nThought generation techniques, like Chain-of-Thought (CoT) prompting, encourage the model to articulate its reasoning process step-by-step. This approach often leads to more accurate and transparent results.\n\nKey aspects:\n\n* Improves performance on complex reasoning tasks\n* Provides insight into the model's decision-making process\n* Can be combined with few-shot prompting for better results\n\n*Prompt: \"Solve this problem step-by-step:*\n\n*If a train travels 120 miles in 2 hours, what is its average speed in miles per hour?*\n\n*Step 1: Identify the given information*\n\n*Step 2: Recall the formula for average speed*\n\n*Step 3: Plug in the values and calculate*\n\n*Step 4: State the final answer\"*\n\n**4. Decomposition Methods**\n\nDecomposition methods involve breaking down complex problems into smaller, more manageable sub-problems. This approach helps the model tackle difficult tasks by addressing each component separately.\n\nKey aspects:\n\n* Useful for multi-step or multi-part problems\n* Can improve accuracy on complex tasks\n* Allows for more focused prompting on each sub-problem\n\nExample:\n\n*Prompt: \"Let's solve this problem step-by-step:*\n\n*1. Calculate the area of a rectangle with length 8m and width 5m.*\n\n*2. If this rectangle is the base of a prism with height 3m, what is the volume of the prism?*\n\n*Step 1: Calculate the area of the rectangle*\n\n*Step 2: Use the area to calculate the volume of the prism\"*\n\n**5. Ensembling**\n\nEnsembling in prompting involves using multiple different prompts for the same task and then aggregating the responses to arrive at a final answer. This technique can help reduce errors and increase overall accuracy.\n\nKey aspects:\n\n* Can improve reliability and reduce biases\n* Useful for critical applications where accuracy is crucial\n* May require more computational resources and time\n\n*Prompt 1: \"What is the capital of France?\"*\n\n*Prompt 2: \"Name the city where the Eiffel Tower is located.\"*\n\n*Prompt 3: \"Which European capital is known as the 'City of Light'?\"*\n\n*(Aggregate responses to determine the most common answer)*\n\n**6. Self-Criticism Techniques**\n\nSelf-criticism techniques involve prompting the model to evaluate and refine its own responses. This approach can lead to more accurate and thoughtful outputs.\n\nKey aspects:\n\n* Can improve the quality and accuracy of responses\n* Helps identify potential errors or biases in initial responses\n* May require multiple rounds of prompting\n\n*Initial Prompt: \"Explain the process of photosynthesis.\"*\n\n*Follow-up Prompt: \"Review your explanation of photosynthesis. Are there any inaccuracies or missing key points? If so, provide a revised and more comprehensive explanation.\"*\n\nPrompt engineering, while not universally liked, has shown improved performance for specific datasets and use cases. Prompting has changed the model training paradigm, allowing for faster iteration without the need for extensive retraining.\n\n>\n\n**Six major categories of prompting techniques are identified: Zero-Shot, Few-Shot, Thought Generation, Decomposition, Ensembling, and Self-Criticism. But in total there are 58 prompting techniques.**\n\n**1. Zero-shot Prompting**\n\nZero-shot prompting involves asking the model to perform a task without providing any examples or specific training. This technique relies on the model's pre-existing knowledge and its ability to understand and execute instructions.\n\nKey aspects:\n\n* Straightforward and quick to implement\n* Useful for simple tasks or when examples aren't readily available\n* Can be less accurate for complex or nuanced tasks\n\n*Prompt: \"Classify the following sentence as positive, negative, or neutral: 'The weather today is absolutely gorgeous!'\"*\n\n**2. Few-shot Prompting**\n\nFew-shot prompting provides the model with a small number of examples before asking it to perform a task. This technique helps guide the model's behavior by demonstrating the expected input-output pattern.\n\nKey aspects:\n\n* More effective than zero-shot for complex tasks\n* Helps align the model's output with specific expectations\n* Requires careful selection of examples to avoid biasing the model\n\n*Prompt:* *\"Classify the sentiment of the following sentences:*\n\n*1. 'I love this movie!' - Positive*\n\n*2. 'This book is terrible.' - Negative*\n\n*3. 'The weather is cloudy today.' - Neutral*\n\n*Now classify: 'The service at the restaurant was outstanding!'\"*\n\n**3. Thought Generation Techniques**\n\nThought generation techniques, like Chain-of-Thought (CoT) prompting, encourage the model to articulate its reasoning process step-by-step. This approach often leads to more accurate and transparent results.\n\nKey aspects:\n\n* Improves performance on complex reasoning tasks\n* Provides insight into the model's decision-making process\n* Can be combined with few-shot prompting for better results\n\n*Prompt: \"Solve this problem step-by-step:*\n\n*If a train travels 120 miles in 2 hours, what is its average speed in miles per hour?*\n\n*Step 1: Identify the given information*\n\n*Step 2: Recall the formula for average speed*\n\n*Step 3: Plug in the values and calculate*\n\n*Step 4: State the final answer\"*\n\n**4. Decomposition Methods**\n\nDecomposition methods involve breaking down complex problems into smaller, more manageable sub-problems. This approach helps the model tackle difficult tasks by addressing each component separately.\n\nKey aspects:\n\n* Useful for multi-step or multi-part problems\n* Can improve accuracy on complex tasks\n* Allows for more focused prompting on each sub-problem\n\nExample:\n\n*Prompt: \"Let's solve this problem step-by-step:*\n\n*1. Calculate the area of a rectangle with length 8m and width 5m.*\n\n*2. If this rectangle is the base of a prism with height 3m, what is the volume of the prism?*\n\n*Step 1: Calculate the area of the rectangle*\n\n*Step 2: Use the area to calculate the volume of the prism\"*\n\n**5. Ensembling**\n\nEnsembling in prompting involves using multiple different prompts for the same task and then aggregating the responses to arrive at a final answer. This technique can help reduce errors and increase overall accuracy.\n\nKey aspects:\n\n* Can improve reliability and reduce biases\n* Useful for critical applications where accuracy is crucial\n* May require more computational resources and time\n\n*Prompt 1: \"What is the capital of France?\"*\n\n*Prompt 2: \"Name the city where the Eiffel Tower is located.\"*\n\n*Prompt 3: \"Which European capital is known as the 'City of Light'?\"*\n\n*(Aggregate responses to determine the most common answer)*\n\n**6. Self-Criticism Techniques**\n\nSelf-criticism techniques involve prompting the model to evaluate and refine its own responses. This approach can lead to more accurate and thoughtful outputs.\n\nKey aspects:\n\n* Can improve the quality and accuracy of responses\n* Helps identify potential errors or biases in initial responses\n* May require multiple rounds of prompting\n\n*Initial Prompt: \"Explain the process of photosynthesis.\"*\n\n*Follow-up Prompt: \"Review your explanation of photosynthesis. Are there any inaccuracies or missing key points? If so, provide a revised and more comprehensive explanation.\"*\n\nhttps://preview.redd.it/htzkrq8cr1wd1.png?width=650&format=png&auto=webp&s=38374af9f024cd5dcf92837a36339c9516e99c1f",
    "created_utc": "2024-10-20T22:40:04",
    "num_comments": 0,
    "comments": []
},
{
    "submission_id": "1g8d2h5",
    "title": "How to take notes? ",
    "selftext": "Those who taught ML themselves, how did you go about taking notes and how you revise them later? Is there any particular note taking method or an App that you recommend? ",
    "created_utc": "2024-10-20T17:16:22",
    "num_comments": 5,
    "comments": [
        "I use note-taking apps like Joplin, OneNote and Obsidian. Joplin has markdown support along with inbuilt latex and vim bindings. Also, I have configured Joplin to intake freehand drawings.  \n  \nEnd-goal of studying is note-making. This makes the session productive, effective and engaging. \n\nThe best way to revise is retrieval practice. Take a piece of paper and start teaching yourself the concepts without looking at your notes. Only consult them, when you fail to retrieve them. Study methods that inculcate question-answering format to revision, are the cornell-method and anki flash cards .",
        "I love paper or my Kindle paper white.\nIt helps me to draw diagrams and such.\nBefore I then used notion, and now obsidian to organize and make them digital. \n\nI still have a backlog. But putting it on paper. Refining and trimming it often helped me a lot with intuition.",
        "I use a combination of obsidian excalidraw (to chart out the flow of the model) and Latex for math equations, I have done this for basic ML topics and Transformers which I am now studying",
        "Anki and supermemo",
        "I just use wall mounted white-board and then explain the topic i just learn."
    ]
},
{
    "submission_id": "1g8clmu",
    "title": "Started ML recently",
    "selftext": "Hello, I am a senior software engineer in backend/ data engineering side and started learning machine learning recently. Are we too late in the field? I see very less job postings compared to the people who know it already. Also the amount of people who are also learning like me are increasing daily. If Someone from the AIML industry could help me understand this situation , it would be great. ",
    "created_utc": "2024-10-20T16:52:25",
    "num_comments": 37,
    "comments": [
        "My mixed thoughts on this:\n\nSometimes I just feel like its the same as the data scientists bubble\n\nI remember people wanting to switch to it and then realizing its not worth it\n\nToday for an example, a company that has 2000 software engineers has 20-30 data scientists…\n\nBut also maybe the wave is not here yet\n\nBut one thing is certain, AI is going into every industry and every field",
        "I work as ML engineer for the past 3 years, and I tried to come up with something insightful and useful for you; however, I feel like there are so many dimensions to this question its hard to really say. \n\nFor example answering this requires an understanding of the job markets, and historical trends; however, also being able to make decent predictions given this new confounding factor of LLMs which are increasingly more capable its very hard to predict. Even those who work on ML like myself are split in the impacts of AI and the future of work. \n\nOne piece of data which I do find rather interesting, is that Open AI just published a new bench mark called the \"MLE Benchmark\" which is basically evaluating how well a model can perform on MLE tasks like training and deploying models. So top AI companies are actively seeking to automate ML Engineering and research, as many see this as a way to keep improving model performance, which makes sense logically.",
        "The future will bring a lot more work to do than the people who can do it. You are not \"too late\". You can only say \"too late\" when the saturation of demand for such work has passed its peak. We are not even close to the peak yet.",
        "It is definitely seeing a boom amongst younger kids currently as well",
        "Can you invent your own job to get in the field instead of searching for current ones?\nIn my company I found a great usecase for ML in detecting fraud and built up the product from the ground up, learning as I developed. The role was blurrying the limits between back end engineering and ML engineering and the spectrum is ever moving.\nI think just taking some classes and changing companies might be a way more difficult career move than expanding your current role.",
        "As someone that's been in this for almost 10 years now here's my blunt truth:\n\n1.  There's a lot of idiots in this field that got jobs since COVID that lack any real understanding of the underlying principles of this stuff, which is very important\n\n2.  There will hopefully be an exodus of said idiots in the next few, but I'm not optismitic\n\n3.  Pretty much anyone that doesn't fit into the following categories lack the proper knowledge to keep up with the field:  PhD, Masters with several years of experience, self taughts that are master/grandmasters on kaggle.\n\nYou can do it, but it's like you already said, the field is over saturated and it's not going to be easy to get and then maintain that job.  I have coworkers, all nice people, that fit into the lacking the underlying principles.  Some of the things they say, some of the consultant work they've done, I just look at it and shake my head.  I try to assist them and point them in the right direction, but their Dunning Kruger is just too strong at this point.  Like these people are good at taking the data, throwing into a basic model and giving output, but when their model selection doesn't work, I'll ask basic questions like what does the data look like, and they have no clue, they don't even understand what EDA is, and I've met TONS of people in this field like this.\n\nSo like for now yeah the field is oversaturated, but I feel it'll hopefully die down once the fat starts getting chewed a bit.  If you really want to get into this, I highly suggest you come at it with a fresh mind of assuming you don't know anything, for several years, break things, fix them.  Keep a portfolio.  Read books on statistics and modelling, read research papers, and really sit down and understand what they're saying and why they're saying it, and use DS challenges like on Kaggle and other places to refine things.  Reach out to grandmasters and masters and interface with them.",
        "Coming up on 10YOE as a staff MLE, I would not recommend joining the field right now the cost to benefit ratio sucks. If you invest a fraction of the time you would learning ML into upskilling as a DE/BE Eng you will see much faster career growth.",
        "why do you want to switch to ML though ?? from what i know , data engineering is quite in demand too , it also pays well ...",
        "Hello. Can you help me understand your meaning for being “late” ?   What are you late for?",
        "Hey, I am an sophomore aiming for MLE roles, I know it's a little unrealistic, but so far I have a good grasp on backend, deployment stuff like devOPS and MLOps, and using frameworks/libraries adjacent to ML/DL, but most of my projects have been scratch implementations and RAG frameworks which I think leans more towards academia... Could you give some perspective of what is expected of an MLE?",
        "I never looked at it this way but how true.",
        "Yes I understand but I am not in touch with the team who works on these to know what is covered and what is not . Not sure if I ask I will get the details either.",
        "u/hellobutno Thanks for this feedback. I generally agree with most of things you stated at least the general message being communicated. On your 3rd point tho, I do want folks to know that there are other pathways to be a competent MLE that don't fall under Ph.D., Masters, Kaggle Master. I have known of Kaggle masters and that couldn't explain or see why increase P value from .05 to 0.1 was a better decision path for the problem at hand, didn't demonstrate understanding on the why - just an example. \"Doing\" and applying to real problems in an environment that challenges your output is the best. Do a project and put the results out there, let people \"pick it apart\" and ask hard questions. Ultimately, the key point is vital - you cannot skip the foundational stuff like understanding data collection, statistics, being able to translate data problems to business problems etc. \n\n  \nI advise folks to look into starting unconventional path of Lean Six Sigma Yellow Belt and Green Belt taking an enterprise / experiential training not the cheap training you see online. It is a good way to get a good grasp for Industrial statistics and an alternate path to forming that foundation with a significant business bent!. You learn to plan data collection almost by hand.  AND DO NOT take training that automatically selects the best models or takes you straight to pipelines...you should practise each step the your ML workflow and understand the impact - sample, when to use stratified shuffle, what the impact is on the result, that kind of fiddling is where the learning comes from. Try to learn the 2019 way vs, the GPT way. - All the best",
        "Thanks for the reply, yes I am studying the details of the model , EDA etc . Didn’t know many have jobs without knowing the background of the models.",
        "Jesus that's condescending and gatekeeping so much.\n\nDo you think someone with a masters or PhD is the one keeping up with every trend in ML while doing a fucking PhD? What are you on about.\n\nAnd no you don't need to be a grandmaster on kaggle to learn good information about ML. \n\nLearn your basics, learn your stats and probability, understand the depth of concepts and learn on the job.\n\nSomeone really fucking hurt you, cause this is written like a true reddit post.",
        "How do you do exploratory data analysis for high dimensional inputs?",
        "I thought there were few roles for DE too plus I wanted a change. Looks like all are not in demand. Job market will have a shift I guess eventually",
        "Startups are looking for ML engineers with a wider breadth of knowledge, encapsulating the entire ML life cycle, from data collection to deployment and monitoring. Larger companies are looking for what I call \"super specialists,\" those who have very deep knowledge in a smaller area, and who know enough to impact a business that has already been working on ML for many years. \n\nHowever, even at the startups, you should still be a specialist, just not a super-specialist. For example, you should focus on the entire ML lifecycle for a specific type of ML problem, data, or stack. For example, you can be very knowledgeable of working with audio data and also have some experience in C++ to build audio apps. Or you could be interested in finances, and be an expert in SQL and building models in Google Cloud. Or you could be very interested in LLMs, so you build several different chatbot applications and agents to solve different problems, orchestrating them using a cloud provider. \n\nSo try to find an area that you are interested in, and dive deep. Either go super deep to become a super-specialist and work at a larger company, or go somewhat deep to be a specialist and go for startups. \n\nBoth paths these days unfortunately would prefer graduate-level degrees; however, if you can land internships, this is not as important. I got my job as an MLE with only a bachelors; however, things are more competitive now compared to 3 years ago during the tail end of the COVID tech job boom.",
        "I never said that being any one of those things will guarantee you're not an idiot.  I am saying however that it massively increases the likelihood that you won't be an idiot.  I even said \"pretty much\" everyone.  It doesn't mean everyone.  There's obvious niche cases here and there especially when someone has a specialty such as a doctor coming in and providing medical advice while being competent in the field.  For the most part though yeah, no.",
        "No I'm with you on this. I don't think the original comment makes much sense other than \"old man scream at kids\".\n\nThere are people who you consider \"idiots\" that are in every subfield, he thinks that a person not knowing something equates to them not learning it, rather than not being exposed to it in the given work they've done uptil now.\n\nLike how often do you get exposed to data drift in a masters or kaggle comps? Not really but it's a concept you need to know and you get exposed to at work.",
        "I don't really care if you don't like.  It's my observations after having been in this field for nearly a decade.  It's not about \"keeping up with the trends\".  If you were actually doing well in this industry you'd know that.  Keeping up with the trends is simply making sure there isn't a obvious substitution for something else you're doing.  If you're using models simply because \"keep up with the trends\" then you're doing it wrong.  You're spending too much time chasing diminishing returns for returns that probably don't even have value to the end user.",
        "What type of data is it?  ",
        "a DE guy struggling with few number of jobs ...\n\ni am definitely cooked",
        "Thanks a lot for the insights, there is much to do haha, but like you stated, I will probably try to reach a specialist level, I am not smart enough for super specialist, also do you perhaps have any companies that I should keep an eye out for if I am looking to intern there?",
        "You were correct.",
        "Any kind you like. Image data. Consumer purchases. Whatever's illustrative.",
        "Herp derp how do EDA on image",
        "Since when is image data or consumer purchases \"high dimensional inputs\"?",
        "There are 236,160 pixels in one frame of a 360p video.\n\nMost consumers buy hundreds of products a year.",
        "An image has exactly 3 dimensions, 2 if it's gray scale.  Consumer purchases are 1 dimensional.  Can you at least try and pretend like you know what you're talking about?",
        "In scientific modeling and machine learning, a \"dimension\" isn't typically spatial. If you need N values to uniquely specify one particular input, then your data are N-dimensional.",
        "My man, you're trying to argue that an image, that you can open up and look at, is some mysterious high dimensional item.  It's 3 dimensional.  It has a mean and standard deviation.  You can open it and look at it and go \"it's a cow, what am i supposed to do with a cow\".  It's not a mysterious thing.",
        "This isn't me being idiosyncratic and having my own unique understanding of what high dimensional inputs are. This is accepted by everyone working in machine learning. You are way too confident in judging and dismissing things you don't understand.\n\nIf you are open to updating your knowledge, this might help you: https://en.m.wikipedia.org/wiki/Dimension#:~:text=In%20physics%20and%20mathematics%2C%20the,specify%20any%20point%20within%20it.",
        "You are asking how to do EDA on basic data that you can look on Kaggle and find answers for in 20s, and claiming it's some sort of \"difficult\" high dimensional data.  Maybe you should just pick up a book.",
        "God help whoever has to work with you.",
        "God help whoever is paying you to work for them, because you clearly ain't getting anything done."
    ]
},
{
    "submission_id": "1g8biui",
    "title": "GPU on macbook",
    "selftext": "So I wanted to buy a pc with Nvidia rtx 4060 ti (for training smaller models of ai) and I'm between buying a Macbook air (m2 base model with 8gb of RAM) or a pc( Asus rog strix g13ch). I relay like macOS and the Apple design but I'm nit sure how well would the model train on a 8 core GPU on macbook vs the Nvidia GPU on a pc. Please help",
    "created_utc": "2024-10-20T15:59:03",
    "num_comments": 13,
    "comments": [
        "I think you have a misunderstanding of how these things work. Let's step back, what kinds of models are you looking to build?",
        "Try it with Google Colab or Paperspace or even kaggle before investing the money. And for this kind of online stuff,  i do suggest going mac for the longer battery life and it had support for ollama, lm studio (llm) and Drawthings( Image Generation)",
        "I'm actually a beginner when it comes to these stuff (as you can see) I want to explore the field. For a start I was thinking to make some basic models and progress over time and I would like to buy something that could train almost any model i give it to",
        "Yeah, I picked up on that lol. Honestly, depending on how small you're talking... most modern laptops are fine. Like a basic linear regression model takes nothing to run it. If I was in your shoes, I wouldn't invest in any new hardware until I've started maxing out my current hardware.\n\nAs for your comment on being able to toss any model at a machine, you start talking about getting into models that require larger requirements than any consumer grade machine can handle. There are more of these than you'd expect and that's when you need to move into cloud computing. \n\nSo really, don't upgrade anything. Just max out what you have and continue saving money until you're maxing the current hardware out then buy a new machine. You'll definitely want a bigger GPU than the one you planned for.",
        "You will need a lot more than 8 gb ram to train any model on your own machine\n\nYou should get at least 64gb and even that will be limited to small models\n\nIt sounds like you don’t really need to train a model for what you want to do though",
        "on the note , i can not even max out my laptop with a 4gb gpu.. kinda embarrased",
        "Wtf are you talking about. He def needs more than 8gb, but he can get by with 16 or 32gb. He does not need 64gb.\nWhere I work I mostly use 16Gb which is fine. \n\nObviously, there are limitations on what and how fast you can train with limited resources, but for exploring the field that's enough.\n\nSome rough rule is to have twice the system ram to your gpu vram.",
        "Linear regression models take almost nothing to train. 8gb of ram would be overkill if OP was just doing models like that.",
        "“You need a lot more than 8GB of RAM to train any model on your own machine” - entirely incorrect. Trained hundreds of deep learning models with 8GB MacBook. Had huge memory swap, but still managed it.",
        "If you want to run something like a 70b parameter model you will need a lot more than 16gb ram model",
        "Does a beginner need to train such large models or he cannot get into the field ?",
        "Did you read his comments? \n\nHe said he wants it to be able to train any model he can throw at it",
        "He can also use collab pro+ if he want performance"
    ]
},
{
    "submission_id": "1g8an1v",
    "title": "Switching from Self-Studied Machine Learning to Data Engineering: Seeking Advice ",
    "selftext": "I'm a graduate from a third-tier government college in India, and I've been passionate about Machine Learning. After graduation, I spent a year dedicating myself to learning the math, statistics, and technologies behind classical ML, along with Cloud and MLOps. Instead of pursuing a job or an ME, I chose to self-study. However, with the current job market heavily favoring experienced candidates and internships mostly going to enrolled students, it's been challenging. Additionally, ML roles now often require skills in Deep Learning and Generative AI, and I can't devote enough time to learning these at home. So, I've decided to pivot toward Data Engineering roles. Any suggestions or advice would be appreciated!\n\n",
    "created_utc": "2024-10-20T15:16:17",
    "num_comments": 0,
    "comments": []
},
{
    "submission_id": "1g89o8d",
    "title": "I want to run a local LLM that I can ask about historic conversations. Where to start?",
    "selftext": "Hi,\n\nI should preface by saying, I’m a complete novice in this area!\n\nI want to run a local language model which I can ask about a bunch of old “tech support” conversations and resolutions. There are about 30,000 documents in total but being able to ask about just a subset of them (~3k) would still be very useful. Each conversation can be uniquely identified.\n\nI’ve done some reading, with most instructional material suggesting a llama model. Some of the things I’ve looked at are suggesting RAG as a way to get more relevant answers without having to get too deep in the weeds, but I don’t know if that’s practical for this much data.\n\nSo, any advice on how complicated would it be set something like this, and where I would be best to get started?\n\nThanks!",
    "created_utc": "2024-10-20T14:32:24",
    "num_comments": 2,
    "comments": [
        "Look into LangChain. There should be examples there of setting up a RAG with a local Ollama model. You will need to choose a text parser (like Unstructured, an embedding model (also from Ollama), a vector db (Chroma, Deep Lake, Weaviate, etc). But like I said, you should find plenty of examples at the LangChain site.",
        "Thank you, this sounds very promising! I will take a good look."
    ]
},
{
    "submission_id": "1g876j8",
    "title": "Why do DDPMs implement a different sinusoidal positional encoding from transformers? ",
    "selftext": "Hi,\n\nI'm trying to implement a sinusoidal positional encoding for DDPM. I found two solutions that compute different embeddings for the same position/timestep with the same embedding dimensions. I am wondering if one of them is wrong or both are correct. DDPMs official source code does not uses the original sinusoidal positional encoding used in transformers paper... why?\n\n**1) Original sinusoidal positional encoding from \"Attention is all you need\" paper.**\n\n[Original sinusoidal](https://preview.redd.it/aa34wg76qzvd1.png?width=1410&format=png&auto=webp&s=ab776adf327a05bc52b30b4013c2d1e9c4d44c0c)\n\nhttps://preview.redd.it/a6atl1v0rzvd1.png?width=1494&format=png&auto=webp&s=fdad3639837b88f5dc266105e0645c37043302a4\n\n**2) Sinusoidal positional encoding used in the official code of DDPM paper**\n\n[Sinusoidal positional encoding used in DDPMs](https://preview.redd.it/8s82abvjqzvd1.png?width=1494&format=png&auto=webp&s=8a5fe3653d9a5d8e80e25073d574e362f1aa251e)\n\nhttps://preview.redd.it/aps58hwvqzvd1.png?width=1478&format=png&auto=webp&s=c9f0c15547615727479ee2a270fa3684712dd88b\n\n**Why does the official code for DDPMs uses a different encoding (option 2) than the original sinusoidal positional encoding used in transformers paper? Is the second option better for DDPMs?**\n\nI noticed the sinusoidal positional encoding used in the official DDPM code implementation was borrowed from tensor2tensor. The difference in implementations was even highlighted in one of the [PR](https://github.com/tensorflow/tensor2tensor/pull/177) submissions to the official tensor2tensor implementation. Why did the authors of DDPM used this implementation (option 2) rather than the original from transformers (option 1)?\n\nps: If you want to check the code it's here [https://stackoverflow.com/questions/79103455/should-i-interleave-sin-and-cosine-in-sinusoidal-positional-encoding](https://stackoverflow.com/questions/79103455/should-i-interleave-sin-and-cosine-in-sinusoidal-positional-encoding)",
    "created_utc": "2024-10-20T12:43:10",
    "num_comments": 3,
    "comments": [
        "I dont think they matter much in terms of results as it is pretty much the same just a different order but it may have an impact on speed due to some low level cuda tricks",
        "FYI, those four images (if that’s what they are) aren’t loading for me.",
        "Omg dude!! Thank you so much, in my account I could see the images, but once I logged out the images disappeared... Well thanks mate <3"
    ]
},
{
    "submission_id": "1g86cd4",
    "title": "Am trying to run a script in Colab and would appreciate any help",
    "selftext": "I am helping out on some research and was told to reproduce some results using the psg-mit/llm-random-number-gen. I am struggling with the set up and am unable to run it on my colab due to some conflicting packages but am lost on how to proceed. Would appreciate any help.",
    "created_utc": "2024-10-20T12:07:07",
    "num_comments": 0,
    "comments": []
},
{
    "submission_id": "1g85cms",
    "title": " 🚀 Get Private AI with Qwen 2.5 API Templates on Vast.ai🔒💡 ",
    "selftext": "Hey everyone!  Are you looking for an affordable, easy-to-use way to access powerful AI models on top quality hardware while keeping your data completely private? Well, I want to share some instance templates I've made for a variety of Qwen 2.5 models on [Vast.ai](https://cloud.vast.ai/?ref_id=167881).\n\n[Vast.ai](https://cloud.vast.ai/?ref_id=167881) offers a cloud marketplace where you can rent powerful GPUs at a fraction of the cost of traditional providers. And now with the Qwen 2.5 templates I've made, you can easily run an openai compatible API serving a state-of-the-art AI models without worrying about data privacy, complicated setup, or high costs.\n\nThese templates might be perfect for you if you want...\n\n# 🔒 Complete Privacy\n\nNo logging. No third-party data exposure. Just complete control over your own data. You totally control your instance and can stop it or delete it in seconds. If you're handling sensitive documents or working on private research this gives you the privacy you need.\n\n# ⚙️ Simple Setup\n\nGetting started is a breeze. No long setup guides or hours spent troubleshooting. Just a few clicks, and you're ready to roll.\n\n# 💰 Cost-Effective\n\nForget about token fees or hidden charges. With [Vast.ai](http://Vast.ai), you only pay for the uptime you use.  Getting access to powerful GPUs without the huge costs. (H100s for <$3ph!)\n\n# 🤖 Use Cases\n\n* **Private Document Processing**: Perfect for those dealing with confidential information.\n\n* **Agent Workflows**: Run AI agents without worrying about token-based fees—just pay for uptime.\n\n* **R&D**: Great for researchers who need temporary access to high quality  private LLMs.\n\nHere are a few templates you can try:\n\n* **Qwen 2.5-72B q8 for 1x H100 GPU (get 30t/s)**:\n\n[Launch Template](https://cloud.vast.ai/?ref_id=167881&template_id=581b0df498f0062ba1449630e4646e4a)\n\n* **Qwen 2.5-14B q8 for 1x RTX GPU (3090/4090) (get 20t/s)**:\n\n[Launch Template](https://cloud.vast.ai/?ref_id=167881&template_id=49097547d4e3219247f395358f5993f6)\n\n* **Qwen 2.5-72B q4.25 for 2x RTX GPUs (get 15t/s)**:\n\n[Launch Template](https://cloud.vast.ai/?ref_id=167881&template_id=02f5a7ee57f6e9aece4c4d6b8b6748c7)\n\nIf you're curious, check out [Vast.ai](http://Vast.ai) using [my referral link](https://cloud.vast.ai/?ref_id=167881) and see if it could work for you. It's a game changer if you're looking for a flexible, cost-effective way to run the biggest Qwen2.5 models while keeping your data private.\n\nFeel free to ask questions below—happy to share more about my experience! 🤖✨\n\n",
    "created_utc": "2024-10-20T11:25:12",
    "num_comments": 0,
    "comments": []
},
{
    "submission_id": "1g838vo",
    "title": "Join our ML Discord server to learn, collaborate, and work on projects with fellow machine learning enthusiasts!",
    "selftext": "Here is the link 👇👇\n\n[Discord](https://discord.com/invite/95vCDNc3rX)",
    "created_utc": "2024-10-20T09:54:25",
    "num_comments": 0,
    "comments": []
},
{
    "submission_id": "1g80riz",
    "title": "Ml beginner",
    "selftext": "Hello ml community , I am beginner in ml and dl journey , I belong to IIT and hence my maths is good but since I have started ml I feel very disappointed and lonely I am making very baby steps progress I feel like quiting it but when I see coding then it seems tough too .\nWhere am I lacking or this feeling of nostalgia is common at the Starting .\n\nPlease help me out .. \nWould mean a lot to me ",
    "created_utc": "2024-10-20T08:05:31",
    "num_comments": 6,
    "comments": [
        "You need a basic program training, and try to find achievements from the training (such as building some program-related portfolios), and when you have the ability to implement machine learning and deep learning side projects, you will do better than the average person, because your math skills will make you know more than others.",
        "How good is your programming? You can learn any library you want at anytime but do you know the basics?",
        "DM",
        "I am learning numpy , pandas , matplotlib, seaborn like python libraries recently \nEarlier without doing them I was learning algorithms like decision trees from ANDREW NG sir course but logic I was getting but in optional labs I could understand the code partially but couldn't find why that was happening .\nThis is my whole journey till now . \nCan u guide what's the correct approach in both ???",
        "My programming is good I can understand the programming in python but yup I am learning lib bcz without them my life is getting hell.\nAlso when to start working on datasets on ur own ..\nAlso do u know when to learn data cleaning...",
        "AI conversation platforms like ChatGPT or Claude have done a great job of answering common procedural questions. You might as well provide some or all of the code to the conversation platform (paste your code or the code you're spending time understanding), then ask the conversation platform what the code is doing, it will give you a preliminary answer to the structure of your program, and then you can copy part of the program snippets (maybe the part you don't fully understand), ask him what the paragraph is doing, and they will explain it to you as much as possible. I have a subscription to ChatGPT Plus, so the functions are more complete, and the model used may be better, if you don't have a subscription, the free version is enough, but you just need a better user prompt."
    ]
}
]