[
{
    "submission_id": "1gukj52",
    "title": "Older fellow trying to grasp some concepts, Comments welcomed...",
    "selftext": "Hi guys.  Hoping not to get flamed im closer to 50, NO formal developer training, just a tech savvy engineer.  \nAbout two years ago trying to keep my brain snappy i got myself into ESP32 microcontroller and a bit of hobby electronics with help of tutorials and Chat GPT got  some fun projects done. (Thanks to reddit community too)\n\nSo i have some questions that arose after a bunch of Object detection videos on Youtube. That i need someone kind enough to help me clear some questions im unsure of which tech should be used where...\n\ni managed to get an object detection tutorial using Yolo8 on my mac running, and i noticed it detected a bunch of classes when i swapped the video from the tutorial for some capture of a security camera on a shop i have access to.   Got ,   \n1 person, 1 cup, 1 knife, 1 bowl, 1 chair, 1 tv, 1 mouse, 1 keyboard, 1 book, 33.4ms etc etc.\n\nThen i threw the code into Gemini AI since i can't code, and asked it to tune it to detect just cell phones, and then tuned again for cell phones being used (in motion against a the static background) and it did a pretty decent job.  \n[https://imgur.com/a/xmm0Ifd](https://imgur.com/a/xmm0Ifd)\n\nBut...   questions again\n\n1-  I noticed in the terminal it kept printing the rest of the classes it detected as this was part of the tutorial code.  This means that this software will detect all its classes and just display what i asked for.  \nBut is this efficient?  Or is it like a human that needs to detect everything and then tell stuff apart?  \nIf you were interested in just detecting cell phones in use, would this be the best way, resource wise CPU/GPU/energy?\n\n2- Also watched some videos about object detection training using your own datasets, and then running them on some small $50-$75 TPU boards.  \nMy question is, is this training done from zero?  \nFor example YOLO seems to be able to detect an object pretty good.  But if i train my own model,  \nWon't this be less efficient?  \nI would need millions of samples to get it done better wont i?  Why would i want to do it if this works already?\n\n3. Is there a way to build/tune on top of an existing object detection system and get it smarter? For example, i ran the videos from the security camera looking for cell phones , but it frequently mistook portable credit card terminals that actually look pretty much like cell phones (large screen icons, the only difference is the printer).  Is this something it can thought?  show it a bunch of terminals and tell it this are NOT cell phones?  \nhttps://imgur.com/a/xmm0Ifd. image2\n\nand last i promise\n\n4. Is training a model from images from your actual end use case (hundreds), better than using a gigantic data set from google better event do some images are way off the particular use case?\n\nI know this may be basic for some, but could not get straight answers from AI .  I appreciate your patience and comments.  thanks!",
    "created_utc": "2024-11-18T16:41:50",
    "num_comments": 3,
    "comments": [
        "When you train your own model you are adjusting the weights of your model but the structure of your model doesn’t change. So people normally do this when they want to detect objects not found in the COCO dataset. Normally all the pretrained model was trained on coco so any objects from there works. If it’s already detecting the objects that you want reliably then you probably don’t need to train it",
        "But can you fine tune it? for example the case where it mistakes an object that actually looks alike? How can it do better?",
        "Oh yeah in that case would be better to train it with some new images in those corner cases"
    ]
},
{
    "submission_id": "1guepdu",
    "title": "Mentorship/guidance required for university CV project (non-profit)",
    "selftext": "Hey there! I'm a university student doing my final year project in making a computer vision based automated clothing store. We just get confused sometimes and need some guidance. Will not be hectic I assure you :)",
    "created_utc": "2024-11-18T12:30:57",
    "num_comments": 4,
    "comments": [
        "You are welcome to contact me. I will try my best to help whatever I can.",
        "Bring it on\nDMs and I'm fine with discord",
        "Thanks that'd be very helpful. Can I dm?",
        "Yes, sure"
    ]
},
{
    "submission_id": "1guc4j7",
    "title": "COCO annotation format",
    "selftext": "Hi guys, im currently making a tool for generating datasets and i wanted to export the annotation data using the COCO Annotation Format, since it has tons of libaries to help users load and manipulate datasets in this format.  \nFor segmentation, both instance and semantic, do i need the bbox for the annotation? i get that with a polygon is quite easy to get the bbox, but i wonder if you could omit this data.",
    "created_utc": "2024-11-18T10:45:37",
    "num_comments": 2,
    "comments": [
        "I would just follow the standard, although it can be redundant. The point of standardising a dataset is to integrate it with other tools. Who knew if a tool checked for the bbox key and errored out :(",
        "Yeah, my issue is that since I'm using a 3D graphics engine to generate the annotations, the segmentations bbox might be too small in some scenarios and end up hurting training. I might use an area rule to exclude segmentations that are too small in a sample."
    ]
},
{
    "submission_id": "1guaz5c",
    "title": "[Help] Small object detection without SAHI",
    "selftext": "Hi everyone, I hope I have come to the right place. \n\nI am currently working on a project which needs to detect the very small objects with a messy background in phone camera. These objects only has 10\\~20 pixels out of 3024 x 4032 pictures.\n\nI have trained a yolov8 model with SAHI and tiling. To me, the results are good enough with map of 80%, making some false positive in the background but basically detect all the small ones. But my supervisor wasn't very happy about it since there is still false positives and SAHI can't work in real time in a phone.\n\nWould you have any suggestions, that could be implement in a phone setting?",
    "created_utc": "2024-11-18T09:59:29",
    "num_comments": 1,
    "comments": [
        "Are there ever any large objects?"
    ]
},
{
    "submission_id": "1guatgt",
    "title": "Models for Image regression",
    "selftext": "Hi, I am looking for models to predict the % of grass in a image. I am not able to use a segmentation approach, as I have a base dataset with the % of grass in each of thousands of pics.\nIt would be grateful if you tell me how is the SOTA in this field. \n\nI only found ViTs and some modifications of classical architectures (such as adding the needed layers to a resnet). Thanks in advance!",
    "created_utc": "2024-11-18T09:52:49",
    "num_comments": 2,
    "comments": [
        "Why are you starting with ViT while there are plenty of easier to experiment with models. As someone already mentioned, train a ResNet with custom laywr at the end. I am suggesting this too because I have recently done similar task and it is smoothly running so far.",
        "You could just use something like resnet then modify the head to do regression. ChatGPT can help you with preparing the data/training scripts. I recommend prompting it to use PyTorch."
    ]
},
{
    "submission_id": "1gu7myl",
    "title": "How much variance is needed for proper training?",
    "selftext": "I’m training a computer vision model to detect applications and websites based on logo. The training images we have are of entire monitors screenshots. We’re able to generate around 60k images to train with from 30 sources. However, I’m curious if anyone can give me any advice. A lot of these images may be exactly the same as each other and could potentially have more than one label in each image. \n\nIf anyone has any advice or resources that I should read I’d greatly appreciate it. ",
    "created_utc": "2024-11-18T07:43:13",
    "num_comments": 1,
    "comments": [
        "Although it depends on your situation, given the lack of context (sample images), you can try to make a detection dataset with each logo as a class and throw YOLO on it."
    ]
},
{
    "submission_id": "1gu75am",
    "title": "PaddleOCR Python Tutorial: A Must-Try OCR Model for Image to Text!",
    "selftext": "\nFinally got a chance to try out paddleocr and it’s pretty good. Much better than easyocr. It’s also really easy to use. Go ahead and check it out in this short demo: https://youtu.be/PBNLWywfSpI",
    "created_utc": "2024-11-18T07:22:11",
    "num_comments": 1,
    "comments": [
        "could you provide a tutoriels to fine tune to other languages ?"
    ]
},
{
    "submission_id": "1gu5ha5",
    "title": "Optical Character Recognition and making report on prescribed format",
    "selftext": "Hello everyone, \n\nIs it possible to recognize hand written data of various parameters (through Optical Character Recognition) and generating reports in a prescribed format from those data?? ",
    "created_utc": "2024-11-18T06:06:34",
    "num_comments": 1,
    "comments": [
        "Should be doable."
    ]
},
{
    "submission_id": "1gu4e03",
    "title": "[Hands-on Workshop] Real-time Video Analytics with Nvidia DeepStream and Python",
    "selftext": "https://preview.redd.it/h9pzqlu5sn1e1.jpg?width=1280&format=pjpg&auto=webp&s=be45d9675effedfd77f53893049b50eee5c37026\n\nExcited to announce our upcoming live, hands-on workshop:\"**Real-time Video Analytics with Nvidia DeepStream and Python**\"\n\nCCTV setups are everywhere, providing live video feeds 24/7. However, most systems only capture video—they don’t truly understand what’s happening in it. Building a computer vision system that interprets video content can enable real-time alerts and actionable insights.\n\nNvidia’s DeepStream, built on top of GStreamer, is a flagship software which can process multiple camera streams in real time and run deep learning models on each stream in parallel. Optimized for Nvidia GPUs using TensorRT, it’s a powerful tool for developing video analytics applications.\n\nIn this hands-on online workshop, you will learn:\n\n1. The fundamentals of DeepStream\n2. How to build a working DeepStream pipeline\n3. How to run multiple deep learning models on each stream (object detection, image classification, object tracking)\n4. How to handle file input/output and process live RTSP/RTMP streams\n5. How to develop a real-world application with DeepStream (Real-time Entry/Exit Counter)\n\n🗓️ Date and Time: Nov 30, 2024 | 10:00 AM - 1:00 PM IST\n\n📜 E-certificate provided to participants\n\nThis is a **live, hands-on workshop** where you can follow along, apply what you learn immediately, and build practical skills. There will also be a **live Q&A session**, so participants can ask questions and clarify doubts right then and there!\n\n**Who Should Join?**\n\nThis workshop is ideal for Python programmers with basic computer vision experience. Whether you're new to video analytics or looking to enhance your skills, all levels are welcome!\n\n**Why Attend?**\n\nGain practical experience in building real-time video analytics applications and learn directly from an expert with a decade of industry experience.\n\n**About the Instructor**\n\n**Arun Ponnusamy** holds a Bachelor’s degree in Electronics and Communication Engineering from PSG College of Technology, Coimbatore. With a decade of experience as a Computer Vision Engineer in various AI startups, he has specialized in areas such as image classification, object detection, object tracking, human activity detection, and face recognition. As the founder of Vision Geek, an AI education startup, and the creator of the open-source Python library “cvlib,” Arun is committed to making computer vision and machine learning accessible to all. He has led workshops at institutions like VIT and IIT and spoken at various community events, always aiming to simplify complex concepts.\n\nLinkedIn: [linkedin.com/in/arun-ponnusamy](https://linkedin.com/in/arun-ponnusamy)\n\nRegister now and take your first step toward mastering real-time video analytics.\n\nRegistration Link: [https://topmate.io/visiongeek/1301823](https://topmate.io/visiongeek/1301823) ",
    "created_utc": "2024-11-18T05:12:59",
    "num_comments": 0,
    "comments": []
},
{
    "submission_id": "1gu45t9",
    "title": "Did yall see the new SOTA realtime object detection? I just learned about it. YOLO has not been meaningfully dethroned in so long.",
    "selftext": "I hope that title isn’t stupid. I’m just a strong hobbiest, you know so Someone might say I’m dumb and it’s pretty much just another flavor, but I don’t think that’s accurate. \n\nI’ve been playing with Yolo since the dark net repo days. And with the changes that ultralytics sneakily did recently to their license, Timing couldn’t be any better. I’m just surprised that the new repo only has like 600 stars. I would’ve imagined like 10 K overnight. \n\nIt just feels cool. I don’t know it’s been like five years since it’s really been anybody that really stood up against the map/speed combo of yolo. \n\nhttps://github.com/Peterande/D-FINE",
    "created_utc": "2024-11-18T05:01:14",
    "num_comments": 30,
    "comments": [
        "I've also discovered this model just last week, thinking on fine tuning it to my use case in near future. Coincidentally I'm also working on RTDETR at this moment and will do a performance comparison later",
        "good to see the apache 2.0 license there, honestly fuck ultralytics",
        "Very interesting development, thank you.\n\nNot sure it dethroned yolo, as it is \"strategically positioned\" in between yolo11 models, having slightly more parameters and slightly better mAP to not be able to tell which one is actually better.\n\nAnother outstanding feature of yolo is that it can be exported to almost any platform. D-FINE does not mention anything about model exporting, which is worrying.\n\nCould be a good model to add to the ensemble. Architecture seems significantly different.",
        "It's interesting but the codebase in terms of readability needs huge improvements. This is inherited from RT-DETR, which has the same project / components structure, which just.. sucks imho. \n\nIf you're building a new model, ready to be used by people, at scale, just structure it like:\n\n```\nmodel/ -> place NN components here, with meaningful names. Add a final model.py with the higher level neural components, e.g. DetectionDFINE\ntrainer.py\neval.py\ndataset.py -> allow people to override the dataset, don 't take for granted that coco or other notorious benchmarks format are good for everybody. Just design a dataset interface, provide a default one and show an example on how implement a custom ds.\nutils/\nconfigs/\nREADME.md\n```",
        "Finally something else than ultralytics. We are blessed",
        " D-FINE is basically the RT-DETR, but they came up with an innovative approach for bounding box optimization. There is a dire need for a research to explore the effectivrness of these optimization approaches on different models. Also, I believe we are passed CNNs and now it's time for the reign of transformers.",
        "Please reply to this comment if you ever do a comparison. Much help.",
        "I mean, let’s not go that far. Listen I think whatever they’re doing there is kind of weird. The bots in the comment section is really weird.\n\nBut you know what Glenn did a lot of work. I understand that he took whoever else’s work or whatever. They built one of the most comprehensive popular ML tool chains on the planet. Capital is getting tight. I mean, if it’s such a big deal lose the 5% map and go back to yolov5. I think I like that tool chain more anyways.\n\nI think the license change is dishonest. I’m actually building a product right now and I’m very glad I saw that somebody posted that on this site. That’s really something you need to headline on your site and then on your repo. Maybe they did too and I just didn’t notice. But it should be in the CLI popping up with red warnings on that that’s a big deal. \n\nAll that to say is. It’s the same thing with like loot boxes and video games. I will never bemoan a company, pursuing a market position or trying to make money. But it’s like the games that add micro transactions after launch or it’s like changing your license after five years. That’s dishonest. And that I will bemoan.\n\nEdit: I don’t get the downvotes. I wonder if these are the same people that cry in the issue sections when everybody doesn’t release their n variance right away. I am nothing but grateful to anybody in this space because they have incepted things I could never start to. And then they made it so I could understand it. \n\nI feel like the proliferation of ideas has belittled the value of people‘s work in some cases and created too much expectation from the community to get something for nothing",
        "There is tools section with exporting no ?\nOr I am lost here?",
        "I don’t know I think they just released their n model and latency is better too. Or at least directly on par. Correct me if I’m wrong, but even the initial DETR stuff wasn’t that performative.",
        "I planned on writing rt detr using keras, saw the codebase and said \"another time\" (a year ago)",
        "This is what it’s like to do anything on windows. Massive pch files with a billion headers and hyper oo with aggressive using statements so you have no idea where anything is come from. I hate it.",
        "+1",
        "Let's begin with the fact that Ultralytics has very bad rep in this sub, why?\n\nThey have turned a GPL-licensed, god's work, aka Joseph Redmon's YoloV3... into a pay-to use software.\n\nHave they contributed to it? Sure. Can someone turn GPL into AGPL? Sure.\n\nIs it correct with respect to the open source community? fuck me, no. \n\nThis is why vision will always stay behind language in the AI world. Weird, restrictive licenses everywhere. \n\nRT-DETR and this one will hopefully reverse the trend.",
        "yah but the enterprise license starts at like +20k USD per year, who can afford that if you're prototyping something unless you're funded? either you just don't pay the license or wait until you have a bunch of customers (and the pricing makes sense to whatever you're building).  it should scale on a per app basis, I'd pay like max 100 USD per month for their webapp, and maybe buy tokens for extra training at a fixed cost per run. It's also built on something that used to be open source. That AI bot is extremely annoying.",
        "You are right, I didn't see that.",
        "D‑FINE‑N 2.12 ms seems slower than YOLO11n 1.5 ms for T4 using TRT10. Makes sense since they have 7 GFLOPS vs 6.5 GFLOPS and better mAP. This is what I was referring to as \"strategical positioning\".\n\nNo CPU ONNX timing either, which is also worrying.",
        "Anything profitable is eventually bound to head that way. If anything, CV breakthroughs gave the bean counters and sophists something to fixate on so that NLP model sharing could fly under their radar",
        "so what it’s not your right. Go build it yourself or be grateful that you’ve got something that somebody infinitely smarter than you made that you can use.\n\nI know my place. I’ve got nothing but respect for all of these people.",
        "Seeing the real-time factor, I will definitely try it using trrt, especially on tensor gpus.\nI am tired of small incremental improvements from a yolo to another.",
        "We’ll find out, but that n localization is actually so bad. I only have so much bandwidth for my project so like I’m starting to integrate some other models like SGM. Various key point detection. So I understand like a lot of folks probably have robust pipelines and that and model is just gonna be an initial flag to see if something is on a scene. But it performs so bad with small objects and like everything else that to me, it’s darn near worthless. And I trained pretty robust custom data sets. If a single class it it’s not terrible and It can do simple stuff pretty well. I have some other stuff I’m working on. I gotta get back to it, but it will be nice to see if it feels like it even outperforms small. Cause it’s a lot less work for me if I don’t have to clean up the Yolo detections.",
        "\"Go build it yourself\"\n\nUltralytics has only introduced AutoAnchor and a few other utilities. They have done an impressive work of making the code a plug and play type model, which is their USP.\nBut they didn't build the entire architecture themselves. They stood on the shoulders of a lot of researchers and then put a price on all of their collective work. I would love to know if the authors of the original work are being compensated",
        "Hi, Glenn Jocher",
        "Yeah, I wonder which opset they are exporting to and if it will run on older TRT ~8.2. Have those old Jetsons to support...",
        "Yeah I feel you. We work with 11n and recently totally revised our dataset, retrained the model and got almost zero improvement. Same old fake boxes on the same tough images.\n\nDiscouraging to say the least, but this is the price.",
        "Then get to it if it’s so easy",
        "Ha. Just Ryan.",
        "I feel you...\nI have to deal with xavier and orin for production hardware.\nNvidia and their jetpack...",
        "At this point models are starting to “overfit” on COCO…so it makes sense when they don’t improve as expected on your own data. It’s why sometimes an older model is actually superior to the fancy new ones, because they tend to be simpler and more easily generalized. ",
        "It might be the case. We've tried v5 and v8 too though, but they were slightly worse.\n\n11s was much better, so I assume 11n is just too small for our dataset. Too bad 11s+ are too slow for our use case."
    ]
},
{
    "submission_id": "1gu2gk9",
    "title": "Adapting TIMM Transformer models for semantic segmentation in Unet via SMP",
    "selftext": "Hello everyone, \n\nI'm working with SMP trying to implement a Unet with a custom ViT-based encoder following this documentation: [https://smp.readthedocs.io/en/latest/insights.html#creating-your-own-encoder](https://smp.readthedocs.io/en/latest/insights.html#creating-your-own-encoder)\n\nI'm trying to use this model: [https://huggingface.co/facebook/deit-tiny-patch16-224](https://huggingface.co/facebook/deit-tiny-patch16-224)\n\nSo far my problem is that each of the attention layers output a fixed (196, 192), ie. 196 tokens, each of length 192. I need these to be turned into hierarchal features that can be used by the Unet decoder to generate the mask. My research so far has led me to this paper: [https://arxiv.org/pdf/2103.13413](https://arxiv.org/pdf/2103.13413) but the author builds a ViT architecture that seems to fundamentally ditch the Unet architecture. There are also these pre-existing model architectures [https://arxiv.org/pdf/2102.04306](https://arxiv.org/pdf/2102.04306) [https://arxiv.org/pdf/2305.08396](https://arxiv.org/pdf/2305.08396) [https://link.springer.com/chapter/10.1007/978-3-031-25066-8\\_9](https://link.springer.com/chapter/10.1007/978-3-031-25066-8_9)\n\nBut i'm unsure on how to recreate this with SMP. Will I have to build a new model class on top of SMP?",
    "created_utc": "2024-11-18T03:16:45",
    "num_comments": 1,
    "comments": [
        "Found [1 relevant code implementation](https://www.catalyzex.com/paper/arxiv:2103.13413/code) for \"Vision Transformers for Dense Prediction\".\n\n[Ask the author(s) a question](https://www.catalyzex.com/paper/arxiv:2103.13413?autofocus=question) about the paper or code.\n\nIf you have code to share with the community, please add it [here](https://www.catalyzex.com/add_code?paper_url=https://arxiv.org/abs/2103.13413&title=Vision+Transformers+for+Dense+Prediction) 😊🙏\n\nCreate an alert for new code releases here [here](https://www.catalyzex.com/alerts/code/create?paper_url=https://arxiv.org/abs/2103.13413&paper_title=Vision Transformers for Dense Prediction&paper_arxiv_id=2103.13413)\n\n --\n\nFound [1 relevant code implementation](https://www.catalyzex.com/paper/arxiv:2102.04306/code) for \"TransUNet: Transformers Make Strong Encoders for Medical Image Segmentation\".\n\nIf you have code to share with the community, please add it [here](https://www.catalyzex.com/add_code?paper_url=https://arxiv.org/abs/2102.04306&title=TransUNet%3A+Transformers+Make+Strong+Encoders+for+Medical+Image+Segmentation) 😊🙏\n\nCreate an alert for new code releases here [here](https://www.catalyzex.com/alerts/code/create?paper_url=https://arxiv.org/abs/2102.04306&paper_title=TransUNet: Transformers Make Strong Encoders for Medical Image Segmentation&paper_arxiv_id=2102.04306)\n\n --\n\nFound [1 relevant code implementation](https://www.catalyzex.com/paper/arxiv:2305.08396/code) for \"MaxViT-UNet: Multi-Axis Attention for Medical Image Segmentation\".\n\n[Ask the author(s) a question](https://www.catalyzex.com/paper/arxiv:2305.08396?autofocus=question) about the paper or code.\n\nIf you have code to share with the community, please add it [here](https://www.catalyzex.com/add_code?paper_url=https://arxiv.org/abs/2305.08396&title=MaxViT-UNet%3A+Multi-Axis+Attention+for+Medical+Image+Segmentation) 😊🙏\n\nCreate an alert for new code releases here [here](https://www.catalyzex.com/alerts/code/create?paper_url=https://arxiv.org/abs/2305.08396&paper_title=MaxViT-UNet: Multi-Axis Attention for Medical Image Segmentation&paper_arxiv_id=2305.08396)\n\n--\n\nTo opt out from receiving code links, DM me."
    ]
},
{
    "submission_id": "1gtzsq6",
    "title": "Remember Clippy? He's back! In Lego form!",
    "selftext": "",
    "created_utc": "2024-11-17T23:54:45",
    "num_comments": 0,
    "comments": []
},
{
    "submission_id": "1gtz09p",
    "title": "How do I achieve advanced memory recall like Google Astra?",
    "selftext": "Hi! I am really interested in building a mini DIY version of the [Google Astra](https://www.youtube.com/watch?v=nXVvvRhiGjI) project. I understand that this can be basically achieved by running image analysis on a webcam's output every second, but I also want to integrate similar memory recall behavior. For example, I want to be able to say \"where did I leave my glasses\" and have them respond.\n\nI assume that I should be running object detection and other image analysis in the background every second, and storing this somewhere, but I am stuck on what to do when a user actually asks something. For example, should I extract keywords from user queries and search images, then feed that relevant image data into an LLM along with the user query? Or maybe it's better to keep all recent image data in context (e.g. a quick summary of objects seen in every frame). These are the two methods I've thought of so far, but neither of them seem right...\n\nPlease let me know if there are better ways of doing this. Thank you!",
    "created_utc": "2024-11-17T22:56:22",
    "num_comments": 0,
    "comments": []
},
{
    "submission_id": "1gtrftt",
    "title": "How to extract subtitles from videos with ocr",
    "selftext": "I would like to ask for help from someone who is familiar with programming.\n I am trying to extract subtitles (not in file format, hard coded) from a video using OCR, but it is not working.\n Does anyone know of a good way to do this?",
    "created_utc": "2024-11-17T15:57:56",
    "num_comments": 5,
    "comments": [
        "EasyOCR might be what you are looking for.",
        "You can split the video into frames. And for each frame you can use google OCR tools. Maybe this will work",
        "[deleted]",
        "Is the accuracy better than Tesseract OCR?",
        "Thank you for your reply.\n I am currently working on a prototype program to create a srt file that detects subtitles that are hardcoded and move around in the video, but it is not working.\n The system is running on google colab.",
        "Your mileage may vary but there are lots of settings to play with and it's dead simple to get working."
    ]
},
{
    "submission_id": "1gtrcmc",
    "title": "Consultant needed for CV projects",
    "selftext": "Hey all, I’m looking to hire an engineer who’s good at computer vision. He/She should’ve experience with object detection (more than just ultralytics) along with a decent understanding of classical CV concepts. Candidates from non US/non EU regions preferred due to cost. DM me your LinkedIn profile/website if possible.\n\nThanks!",
    "created_utc": "2024-11-17T15:53:39",
    "num_comments": 3,
    "comments": [
        "Someone I know - https://roboticsren.com pls contact him directly",
        "Messaged",
        "Happy to help :) \n\nwww.numberboost.com\nhttps://www.linkedin.com/in/alxcnwy "
    ]
},
{
    "submission_id": "1gtqk41",
    "title": "3D Surface Reconstruction for Sparse Points",
    "selftext": "I'm not sure if this is the right place for this. I need some help on how to get a rough surface for a sparse point cloud in 3D space. I'm looking for a learning based approach rather than optimization as it needs to be fairly fast and also differentiable. Most methods I've looked at either don't provide good results for my purposes ([Points as Shapes](https://arxiv.org/pdf/2106.03452)) or are optimization based ([DMesh](https://arxiv.org/pdf/2404.13445), [DeepSDF](https://arxiv.org/abs/1901.05103)). One of the datasets I'm working with is TartanAir, and the goal is to get rough surfaces given keypoints generated from a keypoint detector unprojected via depth. Any ideas or suggestions would be welcome!",
    "created_utc": "2024-11-17T15:15:44",
    "num_comments": 9,
    "comments": [
        "Found [7 relevant code implementations](https://www.catalyzex.com/paper/arxiv:2106.03452/code) for \"Shape As Points: A Differentiable Poisson Solver\".\n\nIf you have code to share with the community, please add it [here](https://www.catalyzex.com/add_code?paper_url=https://arxiv.org/abs/2106.03452&title=Shape+As+Points%3A+A+Differentiable+Poisson+Solver) 😊🙏\n\nCreate an alert for new code releases here [here](https://www.catalyzex.com/alerts/code/create?paper_url=https://arxiv.org/abs/2106.03452&paper_title=Shape As Points: A Differentiable Poisson Solver&paper_arxiv_id=2106.03452)\n\n --\n\nFound [1 relevant code implementation](https://www.catalyzex.com/paper/arxiv:2404.13445/code) for \"DMesh: A Differentiable Representation for General Meshes\".\n\nIf you have code to share with the community, please add it [here](https://www.catalyzex.com/add_code?paper_url=https://arxiv.org/abs/2404.13445&title=DMesh%3A+A+Differentiable+Representation+for+General+Meshes) 😊🙏\n\nCreate an alert for new code releases here [here](https://www.catalyzex.com/alerts/code/create?paper_url=https://arxiv.org/abs/2404.13445&paper_title=DMesh: A Differentiable Representation for General Meshes&paper_arxiv_id=2404.13445)\n\n --\n\nFound [6 relevant code implementations](https://www.catalyzex.com/paper/arxiv:1901.05103/code) for \"DeepSDF: Learning Continuous Signed Distance Functions for Shape Representation\".\n\nIf you have code to share with the community, please add it [here](https://www.catalyzex.com/add_code?paper_url=https://arxiv.org/abs/1901.05103&title=DeepSDF%3A+Learning+Continuous+Signed+Distance+Functions+for+Shape+Representation) 😊🙏\n\nCreate an alert for new code releases here [here](https://www.catalyzex.com/alerts/code/create?paper_url=https://arxiv.org/abs/1901.05103&paper_title=DeepSDF: Learning Continuous Signed Distance Functions for Shape Representation&paper_arxiv_id=1901.05103)\n\n--\n\nTo opt out from receiving code links, DM me.",
        "Delauney triangles?",
        "[deleted]",
        "I looked at Delauney Triangulation, but couldn't find a implementation that was differentiable. After seeing your comment, I looked again, and there is a implementation in torch\\_geometric. Perhaps I can try that. Also I found [this paper](https://arxiv.org/pdf/2109.10695) which looks promising as well.",
        "How would that help for 3D surface reconstruction given a sparse point cloud?",
        "I'm not sure I understand? DinoV2 can only provide the depth, yes? I'm not necessarily sure I want to predict depth. Rather I'm trying to get a coarse surface/mesh to then compare with the ground truth depth and learn keypoints that can then best describe the geometry.",
        "Open3d has direct function.",
        "[deleted]",
        "[deleted]",
        "[deleted]",
        "Total non-sequitur. OP is asking about surface reconstruction, not 6DPE.",
        "Right, of course. I can't say I understand all the capabilities of DinoV2, but I realize it's a feature detector and matcher. But you see, I don't want a dense feature detector since I'm trying to train a sparse one. My goal is to generate a 3D surface and train keypoints that can best fit a coarse surface to the ground truth depth.",
        "Surface reconstruction is a prerequisite to 6dpe in that papers implementation"
    ]
},
{
    "submission_id": "1gtocje",
    "title": "Pretrained models for detection of deer",
    "selftext": "Hello all\n\nI have a following problem. On my farm we have a problem with wild deer coming in and destroying crops. While most of it is behind fence small part of a farm cannot be fenced. So in order to minimize damage I need to be alerted to the presence of deer as soon as possible in order to drive them out. To do so I installed some cameras which cower areas where deer come in. I can collect movement detections in real time with images and videos of detections. I already have a simple system in place which chacks for new detections, run a Roboflow 3.0 Object Detection model trained on wild life and notifies me if there is deer on the images/videos. The problem is that that model is either badly suited for my local wildlife or overtrained, as I get a lot of detection on empty images and mismatch detection.\n\nI would love to dive into training my own model, but am short on time and suitable images. So I was wondering if there is a pretrained model that I could use in my project instead of current model. Is there some other site beside Roboflow with pretrained models?\n\nIf this sub is not suited for this kind of question please point me to the right one.",
    "created_utc": "2024-11-17T13:35:38",
    "num_comments": 5,
    "comments": [
        "If your local wildlife is unique, you will need to collect and annotate your own data and use it to fine tune a model. Don't think you'll find a fast and easy solution here. Pretrained models are good for general cases mostly.\n\nRegarding detections in empty images, it's probably due to some unlucky random noise. You could try a trick: if there's a detection in current frame, ignore it unless the same detection was also present in last 25 out of 30 frames. This way you'll filter out that isolated random noise which triggers the model.",
        "I suggest using Megadetector, a state-of-the-art wildlife detector, and then training a simple deer/not deer classifier: [https://github.com/microsoft/CameraTraps/blob/main/megadetector.md](https://github.com/microsoft/CameraTraps/blob/main/megadetector.md)",
        "Try using open-vocabulary detection models (e.g. Yolo-World, OwlViT) instead. You don't have to train them use them on detecting novel classes. In your case, just input the caption 'deer'.",
        "It is not really unique. Regarding deer any model trained on whitetails from northern America should also work on European whitetail and roe deer.\nRegarding empty images that is a good suggestion. I also noticed that those detection have rather low confidence so I think I will implement both filters.",
        "I will look into those models. Thank you for suggesting."
    ]
},
{
    "submission_id": "1gtm219",
    "title": "i want to learn about the latest in emotion recognition",
    "selftext": "so i am a computer science student and have a task to create an AI that can recognize emotions based on an image or video and would like to know what are the best aproches to this project.  \nso what are some studies and papers that would help me get a good result, thank you all in advance",
    "created_utc": "2024-11-17T11:54:59",
    "num_comments": 1,
    "comments": [
        "-1\n\nJust letting you know that you are a single google search away. There's perplexity, too."
    ]
},
{
    "submission_id": "1gtl5jq",
    "title": "How much FPS do I need",
    "selftext": "I'm trying to use a high speed camera to track where a lacrosse ball (potentially travelling up to 80mph or about 130 kph) will pass trough a lacrosse net. I will use a monochrome camera to detect infrared light bouncing of the ball. I found some 300 fps cameras that are reasonably priced for my budget. How do I tell if that is precise enough for my project? Thanks!",
    "created_utc": "2024-11-17T11:15:11",
    "num_comments": 8,
    "comments": [
        "Maths.\n\nWithout know the camera position/fov/you algo approach/etc. it's hard to know exactly but you can get started trying to estimate how far the ball will travel in the frame-to-frame time.\n\n130 kph \\~ 36 metres per sec\n\n1/300 fps = 0.003 s (per frame)\n\nSize of lacrosse ball \\~6cm = 0.06m\n\nTime to travel 6cm \\~ 0.06m / 36m/s \\~= 0.0017s\n\nWhich means your camera is sampling about 2x the diameter of the ball as it travels - is that good enough?",
        "Likely that will be enough, but unless you are doing processing offline it will take significant effort to make a model that runs near 300fps, so pipelines/models will likely be your bottleneck not the camera itself. You will need a small model and hardware acceleration like JAX/tensorrt to run live at speeds at hundreds of fps",
        "If you are only interested in finding the ball, and have the option of using an illuminator you can probably get away with using reasonable frame rates. The ball not changing velocity unless when bouncing means you can interpolate its position between frames. Minimizing motion blur is probably going to be more important than high frame rate. \n\nOr, you could look into event cameras which have no issues with those kinds of object speeds. They can easily image objects going 10x that speed, or more.",
        "Actually, the more distance between frames the more precise estimating the ball's direction would be. \n\ne.g. 30fps will give you 1.2m between two consecutive frame ball positions.\n\n\n\nI'd begin testing the algorithm with any cheap global shutter camera, or even a normal camera instead of assuming the most expensive I can afford would do much better.",
        "Before looking at camera speed, consider the camera field if view and resolution.\nTry to figure what is the smallest ball size (in pixels) you can detect in a single image, then you can select a combination of field of view and resolution, and then worry about speed.\n\nRemember that camera speed will only influence how much distance the ball travels between two consecutive frames.\nThe FOV+resolution will determine how hard it is to see the ball itself.",
        "math is great.",
        "Ok, thanks for the response! Would an algorithm be enough to detect where it passes, though? I don't need the trajectory of the ball, simply its location when passing through the net.",
        "You will need to do two things, both of which are non-trivial in outdoor space with lots of environmental noise. You need a good localization of the ball in 3D space relative to the goal boundary and you need a good 3D goal boundary. This means you will need good camera calibration and potentially multiple cameras to account for blind spots projecting 2D into 3D space (unless you can build the camera into the net and can see ALL of the boundary in one camera). When you move 2D into 3D you will be able to see movement orthogonal to the camera’s aim, but you will not get reliable depth without a lot of extra work (but you can try monocular depth libraries, like depth pro, but speed of these will not be 300fps as-is, so traditional CV camera calibration will require less engineering for a MVP)"
    ]
},
{
    "submission_id": "1gtcqsp",
    "title": "Sign Detection",
    "selftext": "Hi,\n\nI’m working on a freelance project for a friend. I work as a data scientist but not in computer vision. The project I’m working on is to detect signs (and if possible the type of sign like construction, speed limit, etc) in a large dataset of photos they have. The data is sensitive to the point that they do not want to upload it to a cloud based platform for me to be able to label the data. Additionally, they don’t want to use a pre trained algorithm because they feel training their own algorithm will increase the value of their business. I have a couple of questions:\n\n1. What options do I have to label data on my machine? Every search I do leads to me a cloud based platform. \n2. From your experience, is there business value in writing my own algorithm using something like cv2 or for businesses is the value really in the trained dataset? \n3. If I don’t use cv2 are there free options for commercial use?\n\nThanks!",
    "created_utc": "2024-11-17T04:43:36",
    "num_comments": 8,
    "comments": [
        "1. Use a local Docker Image of CVAT to annotate \n\n2. Get used to MMDetection and train a model on your labeled classes. You can start for example with a ResNet or RetinaNet. Depending on your dataset size and quality you can use other models. \n\n3. Write a small API Endpoint with FastAPI that takes the image as input and outputs the results in the specified format",
        "1. You can self-host CVAT in docker containers on your machine and label data this way (recommend). Or use something as simple as VIA annotator, which is basically an html file.\n\n2. No value in reinventing the wheel, use some pre-trained object detection model tested with time and fine-tune on your data.\n\n3. You would probably want to use ML and not pure CV. Training will happen in Pytorch or Tensorflow, OpenCV will only be used to pre-process images. In that case the question is moot.",
        "You can use a local annotation tool like labelimg or hack together your own quick to run offline.\n\nYou should always use a pre-trained model and then fine-tune (train more) in your data\n\n1. Use LabelStudio [https://github.com/HumanSignal/label-studio](https://github.com/HumanSignal/label-studio)\n\n2. No. You almost always should NOT write your own algorithm. In this case you should learn the algorithm using a deep neural network. \n\n3. [https://github.com/meituan/YOLOv6](https://github.com/meituan/YOLOv6) or [https://huggingface.co/docs/transformers/en/model\\_doc/owlv2](https://huggingface.co/docs/transformers/en/model_doc/owlv2)",
        "Run a local version of Segment Anything, combined with a local vision model and you ought to be able to label your data nearly automatically.",
        "Sounds like CVAT too",
        "Kinda. CVAT, at least the last time I tried the hosted version about six months ago, does support SAM and custom object detection models, but it couldn’t combine their functionality (you had to use one or the other) nor could it run them ahead of time on the entire dataset, so the user is stuck waiting for them to process each image during interactive labelling. \n\nInstead of relying on annotation tools to auto label stuff, I usually setup my own pipeline for that and then upload the results to an annotation tool where the user can quickly go through and make corrections without the multi-second waiting period. ",
        "I did write a QT based auto annotation tool before CVAT for a client, too"
    ]
},
{
    "submission_id": "1gtcobz",
    "title": "Help with ML Project for Damage Detection",
    "selftext": "Hey guys,\n\nI am currently working on creating a project that detects damage/dents on construction machinery(excavator,cement mixer etc.) rental and a machine learning model is used after the machine is returned to the rental company to detect damages and 'penalise the renters' accordingly. It is expected that we have the image of the machines pre-rental so there is a comparison we can look at as a benchmark\n\nWhat would you all suggest to do for this? Which models should i train/finetune? What data should i collect? Any other suggestion?\n\n  \nIf youll have any follow up questions , please ask ahead.",
    "created_utc": "2024-11-17T04:39:12",
    "num_comments": 3,
    "comments": [
        "RemindMe! 1 day",
        "Try mmsegmentation. If you have own data to train you might get some good results.",
        "I will be messaging you in 1 day on [**2024-11-18 15:39:23 UTC**](http://www.wolframalpha.com/input/?i=2024-11-18%2015:39:23%20UTC%20To%20Local%20Time) to remind you of [**this link**](https://www.reddit.com/r/computervision/comments/1gtcobz/help_with_ml_project_for_damage_detection/lxlshfx/?context=3)\n\n[**CLICK THIS LINK**](https://www.reddit.com/message/compose/?to=RemindMeBot&subject=Reminder&message=%5Bhttps%3A%2F%2Fwww.reddit.com%2Fr%2Fcomputervision%2Fcomments%2F1gtcobz%2Fhelp_with_ml_project_for_damage_detection%2Flxlshfx%2F%5D%0A%0ARemindMe%21%202024-11-18%2015%3A39%3A23%20UTC) to send a PM to also be reminded and to reduce spam.\n\n^(Parent commenter can ) [^(delete this message to hide from others.)](https://www.reddit.com/message/compose/?to=RemindMeBot&subject=Delete%20Comment&message=Delete%21%201gtcobz)\n\n*****\n\n|[^(Info)](https://www.reddit.com/r/RemindMeBot/comments/e1bko7/remindmebot_info_v21/)|[^(Custom)](https://www.reddit.com/message/compose/?to=RemindMeBot&subject=Reminder&message=%5BLink%20or%20message%20inside%20square%20brackets%5D%0A%0ARemindMe%21%20Time%20period%20here)|[^(Your Reminders)](https://www.reddit.com/message/compose/?to=RemindMeBot&subject=List%20Of%20Reminders&message=MyReminders%21)|[^(Feedback)](https://www.reddit.com/message/compose/?to=Watchful1&subject=RemindMeBot%20Feedback)|\n|-|-|-|-|"
    ]
},
{
    "submission_id": "1gt9iz0",
    "title": "Record seen objects and remember them ?",
    "selftext": "Lets say we have an object tracking system that gives id's to detected cars.\n\n- A car is detected, given the id 1\n- That car leaves the sight of the camera\n- After 15 seconds the same car enters the sight\n\nCan we somehow determine that car has been seen before and its id was 1 ?",
    "created_utc": "2024-11-17T00:54:39",
    "num_comments": 5,
    "comments": [
        "You can try matching the number plates.",
        "id use something similar to sam2 with reidentification",
        "also checkout [https://roboflow.com/](https://roboflow.com/)",
        "Car is just an example, it can be anything like an apple.",
        "You need a reidentification model. Usually an embedding model trained with metric learning. But something with very little features like an apple would probably not work."
    ]
},
{
    "submission_id": "1gt9be0",
    "title": "DINOv2 instance similarity",
    "selftext": "Hi everyone, \n\ncurrently I am working on finding objects similar to ones in a database. I thought by extracting feature vectors with DINOv2 for the cropped object instances could be a good possibility, as it works well for image similarity, but the resulting vector distances to the database entries are basically noise.. For reference, I am using FAISS for this.\n\nHas anyone ever looked into anything like that and could point me in a meaningful direction?",
    "created_utc": "2024-11-17T00:38:30",
    "num_comments": 6,
    "comments": [
        "If you could elaborate the problem a little more, then I could give some suggestions.",
        "If you generate spatial features, then Dino will not work for you. I tried resnet, efficient net and dino for that purpose. Because dino is vit based, the spatial embeddings are different for the same object in different scenes. However resnet and efficient net works better in this perspective. \nIf you will generate a single embedding (out layer) from dino, it may be useful. But otherwise, no.",
        "If you're using Ultralytics to detect objects, you could try [extracting the object features directly from YOLO](https://www.reddit.com/r/Ultralytics/comments/1g7tl0n/retrieving_objectlevel_features_from_yolo/).",
        "Okay, that might be the issue then. How were the CNNs trained that you used? Just simple ImageNet pre-training or did you mirror the self-supervised trainings?",
        "Detection is not the goal and I guess it would be hard to do with my data, but thanks for the good idea",
        "Simple imagenet is sufficient :) you can look for ssl trained backbones, maybe timm has it"
    ]
},
{
    "submission_id": "1gt5ia1",
    "title": "Dataset labeling advice",
    "selftext": "Hello everyone, I’m pretty new to CV and Deep learning. I have a project for school where I’m trying to detect if a punch or kick lands in a kickboxing fight. \n\nI’m trying to make a dataset and I want some advice as to how to go about doing this? I’m using videos as input so im going through frames in videos 1 by 1 and labeling them on CVAT. Currently im only working on detecting if a hit lands or not so I only have two classes, ‘hit’ or ‘no_hit’. When there is a hit I’m thinking of placing a bounding box around the area where the hit is landing but when there is no hit I’m not so sure as to what to do, should I just place a label on it as no hit. \n\nIn any case thank you for any advice, it much appreciated! ",
    "created_utc": "2024-11-16T20:20:38",
    "num_comments": 1,
    "comments": [
        "You would have to decide what type of model you're going to use.\n\nIf it's temporal like action recognition, then you would need a single label for the whole clip or sequence, not the frames. And you would need a lot of sequences for it to work. You can try a skeleton based action recognition model that works on the extracted skeletons of the people. Skeleton action recognition models are much faster than RGB frame based ones, but they lack the RGB context which may be important for something like hit detection.\n\nBut if you're not going to use something temporal, then you can just label the frames. I would draw a bounding box with label around both people if using RGB. Or you could use skeletons here too. Extract the skeletons and label the pairs as hit or not hit."
    ]
},
{
    "submission_id": "1gt0xx6",
    "title": "Problem with edge detection",
    "selftext": "I am working on developing a reliable method to detect contours (specifically, the sourdough level) in enclosed images using Python. I’ve explored several approaches for contour detection:\n\n    1.\tContour Detection Methods:\n    \n    •\tcv2.findContours\n    \n    •\tskimage.find\\_contours\n    \n    •\tskimage.feature.canny\n    \n    •\tcv2.Canny\n    \n    •\tcv2.convexHull\n    \n    2.\tPreprocessing Techniques:\n    \n    •\tGrayscale conversion\n    \n    •\tBlurring\n    \n    •\tWhite top hat filtering\n    \n    •\tNormalization\n    \n    •\tThresholding\n\nWhile these methods and preprocessing steps work for some cases (depending on the parameters used), they fail for others, and vice versa. This inconsistency is evident in the examples I’ve tested.\n\nI am looking for a more robust or simplified solution to reliably detect the sourdough level across different images.\n\nhttps://preview.redd.it/f9cm88y2uc1e1.jpg?width=1232&format=pjpg&auto=webp&s=6c2d7b77d63f8e99fe9955a4d61683a6d9f67784\n\nhttps://preview.redd.it/midhiay2uc1e1.jpg?width=1232&format=pjpg&auto=webp&s=c941ff49b33b75f3f3556b8dbbb52c16adf75d04\n\nhttps://preview.redd.it/x8mwz8y2uc1e1.jpg?width=1232&format=pjpg&auto=webp&s=b63bb62394bd85606f0724a4af2325e143bd1ccb\n\nhttps://preview.redd.it/8o31j9y2uc1e1.jpg?width=1232&format=pjpg&auto=webp&s=65ae62f80181da88b30fb4ee3e43754471e00ac7\n\n",
    "created_utc": "2024-11-16T16:14:21",
    "num_comments": 5,
    "comments": [
        "If its only the top edge you are looking for, thresholding, then contour detection should  do the job on the images you provided without any issues.",
        "How much experience do you have in computer vision?  If you have more data I could give it a shot at working a solution",
        "That's a lot of steps. I'd probably threshold and find the first row with a decent amount of white. That's your height.",
        "You can easily doing it without edge detection (at least with the images you provided). With a threshold or fillflood algorithm you can segment the two areas and then with findcontours you estimate the level",
        "I would recommend trying to understand how the different filters work and whats their pro and coms. then u can choose the one which fits best for your project:)"
    ]
},
{
    "submission_id": "1gt0k5t",
    "title": "Best techniques for clustering intersection points on a chessboard?",
    "selftext": "",
    "created_utc": "2024-11-16T15:55:55",
    "num_comments": 24,
    "comments": [
        "\\[SOLVED\\] \\* read last paragraph\n\nHi everyone,\n\nI'm developing a chess game recognition app and currently facing an issue with mapping the chessboard grid. My goal is to map the board squares (e.g., a1, a2, ... h7, h8) and save the vertices' coordinates. This will allow me to compare square positions with the positions of detected chess pieces' bounding boxes.\n\nHere's my current progress:\n\n1. I used a Canny edge detector to detect edges on the chessboard image.\n2. Then, I applied HoughLinesP to detect horizontal and vertical lines. Since some lines are interrupted by chess pieces, I extrapolated them to extend fully across the board (see **image 1**).\n3. I calculated intersections between the horizontal and vertical lines, giving me a set of intersection points (see **image 2**).\n\nI chose images of a \"bad situation\" on purpose, where the warp transform didn't work as intended because I need my solution to work in those situations. Image 3 shows the usual scenario.\n\nNow, I need help with **filtering and clustering these intersection points**:\n\n* **Filtering:** Exclude intersections that don't belong to the chessboard grid (e.g., those outside the grid or caused by irrelevant lines).\n* **Clustering:** Group nearby points into a single cluster to eliminate duplicates (caused by overlapping or slightly shifted lines).\n\nChatGPT suggested three possible techniques for clustering: **DBSCAN**, **K-means**, and a simple **Euclidean distance-based filter**. I'd love to hear from anyone with experience in similar tasks:\n\n* Which of these techniques would work best in this case?\n* Are there other methods I should consider?\n\nThanks in advance for any insights or suggestions!\n\nEdit: Thanks for all the suggestions. I managed to do what I wanted by applying a DBSCAN with the cluster position calculated using the median of all the cluster elements. It worked pretty well because of all the multiple overlapping points I had (I thought I had less, but actually there were over 20k points detected and the real corners had the highest density).\n\n    # python code generated by ChatGPT\n    def dbscan_cluster_points(intersections, eps=10, min_samples=2):\n        clustering = DBSCAN(eps=eps, min_samples=min_samples).fit(intersections)\n        labels = clustering.labels_\n        unique_labels = set(labels)\n        clusters = []\n        for label in unique_labels:\n            if label != -1:  # ignore outliers\n                cluster_points = intersections[labels == label]\n                centroid = np.median(cluster_points, axis=0)                   \n                clusters.append(centroid)\n        return np.array(clusters)\n    \n    filtered_intersections = dbscan_cluster_points(intersections, eps=img.shape[0]/12, min_samples=5)\n    \n    # eps value was chosen empirically\n    # 1/12 of the board width was good enough for me\n    # It just needs to be shorter than the distance between real corners",
        "If you know the best fit 4 corners then you know every vertex is appropriately evenly distributed between them. Can’t you just do a “best fit” chessboard based on the points? Basically find the four corner points the minimize error function, where error is the distance of the 81 expected vertices locations to the nearest point you have detected.",
        "I dont know if it will be helpful for your particular use case, I can provide fully labelled chess data in any number of images, fovs, angles etc if it's useful. here's an example page that I made a while ago (it's a bit dark)\n\nhttps://theperceptioncompany.com/generation/77\n\n\nhttps://theperceptioncompany.com/asset/king_135",
        "I usually solve these types of problems with graph analysis: I define every intersection as a node and every line as an edge. Then I start applying assumptions based on the topology I want to extract. In this case I would cluster edge lengths, then I remove lines of the shorter cluster and at the end remove unconnected points. Probably you can find stronger assumptions but I hope you got the main idea.",
        "Totally recommend this\nhttps://github.com/davidmallasen/LiveChess2FEN",
        "Cluster by distance — go through the list of points, for each point look for points in a small radius, average them, go through the new list. \n\nFor noise rejection/intersection signature, look at radon transforms. See fig 3, source 26 here. https://arxiv.org/pdf/2308.13823 . Unfortunate I don’t recall finding a cv2 implementation for this. Try it out yourself but if you’re stuck lmk, I might be able to share the paper implementation.\n\nAlternatively do some assumptions wrt the visibility of grid numbers/letters, and do some interpolation from there — much more brittle solution, but worth trying if you want to dabble in abit of OCR",
        "The chess board being green and white is actually very helpful. You can use Hsv segmentation to get the green blocks and use them to robustly capture corners.",
        "I don't really understand what you want to get out of clustering.\n\nClustering is for grouping data into unknown groups. You have target groups and furthermore quite rigidly defined groups.\n\nOr am I missing something?",
        "Maybe checking the area of each area formed by 4 vertices. The chess board areas are square or almost sqaure. The other parts are not. Or maybe just chekcing the aspect ratio could be enough",
        "Since they’re pretty close together already and you know the target number of vertices - just throw in a kmeans over the coordinates and either take the cluster centers or the closest points to them",
        "convolution matrix to find the vertices of all the black-white squares - [-1 -1 1 1;  -1 -1 1 ; 1 1 -1 -1; 1 1 -1 -1]. then to fint those most prominent of those - best to find close to 7x7 points, and from there to get lines through them - find the two average angles - make sure that your camera is calibrated - best to have almost no fish-eyeyness. but idk, I was doing CV more then 8y ago...",
        "So from reading some of the other comments it sounds like what you are most worried about is removing the clusters external to the chessboard.  As lots of people are saying some sort of clustering, be that: k-means, algomrative hierarchical is the first step to reduce noise around the desired points.\n\nI think the second step to remove the external edges can also be quite simple. Take the center of all the clusters calculated previously.  Use an algorithm such as jarvis march or monotone chain to find the convex hull of these points.  Remove all points that are in the convex hull as they will be the outermost points.\n\nI hope this helps.  If you have trouble trying it let me know and i will craft something rough for you to try.\n\nhttps://en.wikipedia.org/wiki/K-means_clustering?wprov=sfla1\n\nhttps://en.wikipedia.org/wiki/Hierarchical_clustering?wprov=sfla1\n\n\nhttps://en.wikipedia.org/wiki/Gift_wrapping_algorithm?wprov=sfla1\n\nhttps://en.wikipedia.org/wiki/Convex_hull?wprov=sfla1",
        "Kmeans could work, you want to optimize the euclidean distance between points for a cluster. However it'll probably be slow, you need to find the optimal parameter k (k as in k clusters), you can find it using the elbow or silhouette method for kmeans (search them up). These require you to run kmeans cluster over a range of k to find the optimal.\n\nHowever you might already know k it seems? If the number of clusters is the same for all images then you can just set k to what it needs to be.",
        "Try cv2.findChessboardCorners",
        "Your suggestion is really good, but there are the intersections outside the board grid. How would you remove them? If I manage to do it, I will probably use your solution.",
        "Thanks, but I already built a pretty decent dataset for now... Took me long enough tho hahaha",
        "Thanks! I will take a look at this project. But I can't really use it... I'm working on this app for my undergraduate dissertation and gotta do it by myself with what I have planned already.",
        "I don't believe aspect ratio alone is enough because of the small squares at the edges.  But both aspect ratio and area would be something cool to try.",
        "Thanks for the suggestion. I managed to do it using a DBSCAN clustering. Since the distance between the corners is already \"known\", it was the simplest thing I could think of. I just needed to set the DBSCAN radius to be shorter than the distance between real corners. Possibly k-means could also work, but I'm pretty happy with DBSCAN for now.\n\nI edited my original comment to include the solution I found (plus code).",
        "I wanted to keep only 81 clusters (for the chessboard grid), but often there are many more than 81 in the image. \n\nI need to remove the ones that are not part of the grid, this is my main problem... After that I think a simple k-means would work.",
        "This was my first try before using the warp perspective, but didn't work at all. Didn't try after adjusting the perspective tho.",
        "I think the extra points won’t hurt you much because they aren’t evenly spaced so even if they were considered as potential corner points, they would not be the ones selected because the error would be high.\n\nDon’t put too much weight on this, I’m just a tinkerer there are much more skilled people on here haha",
        "To add to drupadoo's suggestion, you could do metric rectification on the image as well so the images don't have to be looking directly down on the chess board.\n\nEdit: I also feel like there's a clever way to fit a RANSAC model using predetermined chessboard corners to filter out the false positives",
        "Ok. They are based on a good chess detector which is also worth reading  https://github.com/maciejczyzewski/neural-chessboard\n\n\nIf you are interested I could also send you my bachelor's thesis, which is about the same topic. I just improved the livechess2fen a little and compared."
    ]
},
{
    "submission_id": "1gt0atw",
    "title": "Interested in the research and topics at this year's ECCV conference but weren't able to attend? We're hosting an online speaker series with authors of research presented at ECCV 2024. Find out more at the link below.",
    "selftext": "",
    "created_utc": "2024-11-16T15:42:51",
    "num_comments": 0,
    "comments": []
},
{
    "submission_id": "1gt003n",
    "title": "[R] Holography Driven Novel View Synthesis - Literature Survey.",
    "selftext": "",
    "created_utc": "2024-11-16T15:28:33",
    "num_comments": 0,
    "comments": []
},
{
    "submission_id": "1gsy7pk",
    "title": "pytesseract not detect numbers.",
    "selftext": "hello. i'm new in this community, and english isn't my language. i try to extract number from an image. use python and pytesseract but not matter how i process the image not detected. some body have a clue?\n\n\n\nthis is the image.\n\n[https://imgur.com/a/iRmaDAU](https://imgur.com/a/iRmaDAU)\n\n I even managed to detect the area and crop it like this\n\n[https://imgur.com/a/iAIy25G](https://imgur.com/a/iAIy25G)",
    "created_utc": "2024-11-16T14:02:40",
    "num_comments": 7,
    "comments": [
        "Use another ocr\n\nBest OCR Models to Extract Text from Images (EasyOCR, PyTesseract, Idefics2, Claude, GPT-4, Gemini)\nhttps://youtu.be/00zR9rJnecA",
        "Just don't use it, it's the worst OCR you could find today. Try EasyOCR or DocTR",
        "Love EasyOCR",
        "thanks",
        "which one is better to start?",
        "My second option was to use EasyOCR, but first I wanted to see if I could fix it with pytesseract.",
        "Honestly you are just making work for yourself. EasyOCR is easier, faster and better than tesseract. Your choice though."
    ]
},
{
    "submission_id": "1gsulg0",
    "title": "Any tips state of the art \"camera motion tracking\" and depth estimation?",
    "selftext": "Hey! I'm looking for some computer vision based utilities for vfx work.\n\nWhat I look to achieve is to be able to edit objects into a landscape video such that it looks like the objects are in the scene.\n\nA couple things are needed I think:\n\n1. tracking camera movement over time - to match the object's movement with the scene.\n2. depth estimation over time - to know when the object it obscured.\n3. bonus: reconstruct the entire scene over time, e.g where is the floor, buildings.\n4. bonus 2: method for generating human motion capture from video?\n\nAny tips on keywords to search for to find state of the art methods for these? specific methods or github repos would be 1000% appreciated too : )\n\nAfter effects has some of this built in, but it has not performed very well.",
    "created_utc": "2024-11-16T11:13:42",
    "num_comments": 6,
    "comments": [
        "Best Depth Estimation Models (MiDaS, Depth Pro, Depth Anything v2, DepthCrafter, Marigold, Metric3D)\nhttps://youtu.be/egBNsSCajDg",
        "i have seen many e.g image-to-depth models, that i guess could be applied to video, frame by frame, but if there are models that leverage temporal information im guessing that could give better results for this use case?",
        "thanks!",
        "really nice video, thank you, depth crafter looks promising 👍",
        "do you know any good project for generating human motion capture from video? like this one [https://github.com/zju3dv/EasyMocap](https://github.com/zju3dv/EasyMocap)",
        "Thanks for your videos, man! I came across them some weeks ago."
    ]
},
{
    "submission_id": "1gst6sr",
    "title": "For NeRF/3DGS, how can I get the 3d reconstruction view for the ground truth image?",
    "selftext": "Hi, I am currently comparing bunch of different NeRF and 3DGS and I need to get the 3d reconstruction view for the ground truth image. But COLMAP output for the camera extrinsics is in binary format (not human readable) and even if I had the camera extrinsics, I don't know how to insert this into NeRF/3DGS process to get the view of the reconstructed model from the ground truth angle. Excuse me but can anyone help me on how to approach this..?? Thank you!",
    "created_utc": "2024-11-16T10:11:00",
    "num_comments": 1,
    "comments": [
        "You can export the camera, images and reconstructed sparse points into txt if you would like human readable details.\n\nMore helpful to your problem, [nerfstudio](https://docs.nerf.studio/) provides a platform for different kinds of NeRF models. If you train your NeRF with it, in the localhost web page you can monitor the changes during the training process. It will show you the camera positions and poses (as calculated from COLMAP), and you can see from the viewpoints of the ground truth images. Not sure if they include Gaussian Splatting"
    ]
},
{
    "submission_id": "1gssnjy",
    "title": "Predicting Disease Deformations in Standardized Bone Meshes",
    "selftext": "Hello, I'm looking for the best approach to predict disease-related deformations in bone meshes.\n\n**Dataset**: I have data from over 4,000 subjects, each with standardized femur and tibia meshes captured at 8 different time points (with 12 months between each). These meshes are standardized, meaning they have the same number of vertices and faces, and I have consistent vertex-to-vertex correspondence across all meshes for every subject and time point.\n\nThis setup allows me to track how the disease-related deformations evolve over time for a given subject (as illustrated in my figure: on the left is a healthy, undeformed femur; on the right, the same femur deformed by the disease).\n\n**Task**: I aim to predict the deformation occurring on the mesh 12 months after baseline. I'm curious about the most suitable architecture for this. I've considered Graph Neural Networks (GNNs) but want to ensure that whatever model I use takes full advantage of the standardized nature of my meshes. Additionally, I'm uncertain whether dimensionality reduction techniques, such as PCA, would be appropriate without compromising the integrity and unique characteristics of the mesh data.\n\nAny insights or recommendations or references would be greatly appreciated!\n\nhttps://preview.redd.it/mbcc3wuswa1e1.png?width=635&format=png&auto=webp&s=6c8eda80ef2e592ef88aa478c36e96247571a5ea\n\n",
    "created_utc": "2024-11-16T09:46:49",
    "num_comments": 1,
    "comments": []
},
{
    "submission_id": "1gss9zo",
    "title": "Best way to recognize the same (non-human) object in different pictures?",
    "selftext": "So, it seems person re-identification is pretty mature at this point. i.e. I can give a model a profile pic of someone and it can identify that person in any new photo the model sees.\n\nIs there something like this for objects? e.g. I can give you a coffee mug and it can identify the same coffee mug (same colour, shape, etc.) in new photos?\n\nI was thinking about using an embedding of a bounding box of the object in the new photo and finding the embedding of a previously identified object with the lowest euclidian distance, but is there a better way of doing this?",
    "created_utc": "2024-11-16T09:29:38",
    "num_comments": 3,
    "comments": [
        "\\*cosine, the magic word is cosine similarity. Otherwise you’ll suffer the curse of dimensionality. What do you mean by *better*? Faster? More accurate? The alternatives would be template matching or bags of (custom) words. Also, the SIFT patent has expired, so you might want to try this as well.",
        "you're right cosine similarity. better as in accuracy.\n\ni've used sift for keypoint tracking, how would you use it to re-identify objects against a library?\n\ncount number of matched keypoints?",
        "Yes, number of good quality matches; maybe in combination with a broad classifier. However, if you can limit the search space to only coffee mugs, and you have a large enough dataset, I'd suggest you pre-train a siamese network."
    ]
},
{
    "submission_id": "1gsrvfu",
    "title": "You could be wasting a lot of time labeling images manually…",
    "selftext": "In some cases you might be able to do it automatically with autodistill. Give it a try! \n\nAuto Label Images for YOLO Object Detection with AutoDistill (Annotate Images with Prompt)\n[https://youtu.be/dGiJur_Ae2I](https://youtu.be/dGiJur_Ae2I)",
    "created_utc": "2024-11-16T09:11:13",
    "num_comments": 1,
    "comments": [
        "Should be pointed out that even if the automatic labels are not very good in some cases, it often is very good pretraining"
    ]
},
{
    "submission_id": "1gsosye",
    "title": "Clean Java OpenCV Code With Chained Calls!",
    "selftext": "Hey guys, this is my first time posting here! I use OpenCV Java pretty much every day and it's a huge part of my workflow. Unfortunately the code always wound up being really messy and I wanted a better way of dealing with things. So basically I made this class that allows you to chain together OpenCV calls, and makes your code much cleaner and faster to write.\n\nEssentially, we can take this code that converts the color, blurs the image, does Canny, then adds a black box border:\n\n```java\nImgproc.cvtColor(mat, mat, Imgproc.COLOR\\_BGR2GRAY);\nImgproc.GaussianBlur(mat, mat, new Size(3, 3), 0);\nImgproc.Canny(mat, mat, 150, 200); \nint pixels = 10;\nCore.copyMakeBorder(mat, mat, pixels, pixels, pixels, pixels, Core.BORDER_CONSTANT);\n \n```\n\nAnd rewrite it as something much cleaner:\n\n```java\n// The chain is instantiated once\nImageProcessChain chain = new ImageProcessChain()\n\t.bgrToGray()\n\t.blur(3)\n\t.canny(150, 200)\n        .addBlackBoxPadding(10);\n\n// Then the chain acts on the Mat in-place\nchain.accept(mat);\n```\nA huge advantage is that the ImageProcessChain can be reused and passed around like any other object.\n\nThere's a few other methods on it, and you can certainly add more as you see fit. It essentially works by storing a list of Consumer<Mat> and executing this list whenever the chain is  invoked.\n\nHere's my entire code:\n```java\npackage processor.computer_vision;\n\nimport java.util.ArrayList;\nimport java.util.List;\nimport java.util.function.Consumer;\n\nimport org.opencv.core.Core;\nimport org.opencv.core.CvType;\nimport org.opencv.core.Mat;\nimport org.opencv.core.Scalar;\nimport org.opencv.core.Size;\nimport org.opencv.imgproc.Imgproc;\nimport org.slf4j.Logger;\nimport org.slf4j.LoggerFactory;\n\n/**\n * Execute chain of commands on image that modify the Mat object in-place\n */\npublic class ImageProcessChain implements Consumer<Mat> {\n\n\tprivate static final Logger logger = LoggerFactory.getLogger(ImageProcessChain.class);\n\n\tprivate final List<Consumer<Mat>> CHAIN = new ArrayList<>();\n\n\t@Override\n\t/**\n\t * Execute the chain of processinng functions on target Image\n\t */\n\tpublic void accept(Mat image) {\n\t\tCHAIN.forEach(function -> {\n\t\t\tfunction.accept(image);\n\t\t});\n\t}\n\n\tpublic ImageProcessChain sharpen() {\n\t\tCHAIN.add(mat -> {\n\t\t\tMat kernel = Mat.ones(3, 3, CvType.CV_32F);\n\t\t\tCore.multiply(kernel, new Scalar(-1), kernel);\n\t\t\tkernel.put(1, 1, new float[]{9.0f});\n\t\t\tImgproc.filter2D(mat, mat, -1, kernel);\n\t\t\tlogger.debug(\"executed sharpen on mat \" + System.identityHashCode(mat));\n\t\t\tkernel.release();\n\t\t});\n\t\tlogger.debug(\"added sharpen to chain\");\n\t\treturn this;\n\t}\n\t\n\tpublic ImageProcessChain addBlackBoxPadding(int pixels) {\n\t\tCHAIN.add(mat -> {\n\t\t\tCore.copyMakeBorder(mat, mat, pixels, pixels, pixels, pixels, Core.BORDER_CONSTANT);\n\t\t\tlogger.debug(\"executed add black box on mat \" + System.identityHashCode(mat));\n\t\t});\n\t\tlogger.debug(\"added pad to chain\");\n\t\treturn this;\n\t}\n\n\tpublic ImageProcessChain blur(int blurSize) {\n\t\tCHAIN.add(mat -> {\n\t\t\tImgproc.GaussianBlur(mat, mat, new Size(blurSize, blurSize), 0);\n\t\t\tlogger.debug(\"executed blur on mat \" + System.identityHashCode(mat));\n\t\t});\n\t\tlogger.debug(\"added blur to chain\");\n\t\treturn this;\n\t}\n\n\tpublic ImageProcessChain bgrToHsv() {\n\t\tCHAIN.add(mat -> {\n\t\t\tImgproc.cvtColor(mat, mat, Imgproc.COLOR_BGR2HSV);\n\t\t\tlogger.debug(\"executed bgr to hsv on mat \" + System.identityHashCode(mat));\n\t\t});\n\t\tlogger.debug(\"added bgr to hsv to chain\");\n\t\treturn this;\n\t}\n\n\tpublic ImageProcessChain bgrToGray() {\n\t\tCHAIN.add(mat -> {\n\t\t\tImgproc.cvtColor(mat, mat, Imgproc.COLOR_BGR2GRAY);\n\t\t\tlogger.debug(\"executed bgr to gray on mat \" + System.identityHashCode(mat));\n\t\t});\n\t\tlogger.debug(\"added bgr to gray to chain\");\n\t\treturn this;\n\t}\n\n\t/**\n\t * @param threshold an int between 0 and 255\n\t */\n\tpublic ImageProcessChain binaryThreshold(int threshold) {\n\t\tCHAIN.add(mat -> {\n\t\t\tImgproc.threshold(mat, mat, threshold, 255, Imgproc.THRESH_BINARY);\n\t\t\tlogger.debug(\"executed binary threshold on mat \" + System.identityHashCode(mat));\n\t\t});\n\t\tlogger.debug(\"added binary threshold to chain\");\n\t\treturn this;\n\t}\n\n\tpublic ImageProcessChain scharrEdgeDetect(double scale) {\n\t\tCHAIN.add(mat -> {\n\t\t\tint delta = 0;\n\t\t\tint ddepth = CvType.CV_16S;\n\t\t\tMat gradX = new Mat();\n\t\t\tMat gradY = new Mat();\n\t\t\tImgproc.Scharr(mat, gradX, ddepth, 1, 0, scale, delta, Core.BORDER_DEFAULT);\n\t\t\tImgproc.Scharr(mat, gradY, ddepth, 0, 1, scale, delta, Core.BORDER_DEFAULT);\n\t\t\tCore.convertScaleAbs(gradX, gradX);\n\t\t\tCore.convertScaleAbs(gradY, gradY);\n\t\t\tCore.addWeighted(gradX, 0.5, gradY, 0.5, 0, mat);\n\t\t\tgradX.release();\n\t\t\tgradY.release();\n\t\t\tlogger.debug(\"executed scharr edge detect on mat \" + System.identityHashCode(mat));\n\t\t});\n\t\tlogger.debug(\"added scharr edge detectto chain\");\n\t\treturn this;\n\t}\n\n\t/**\n\t * compute gradients (Scharr edge detect) with default scale = 1\n\t */\n\tpublic ImageProcessChain scharrEdgeDetect() {\n\t\tCHAIN.add(mat -> {\n\t\t\tdouble scale = 1.0;\n\t\t\tint delta = 0;\n\t\t\tint ddepth = CvType.CV_16S;\n\t\t\tMat gradX = new Mat();\n\t\t\tMat gradY = new Mat();\n\t\t\tImgproc.Scharr(mat, gradX, ddepth, 1, 0, scale, delta, Core.BORDER_DEFAULT);\n\t\t\tImgproc.Scharr(mat, gradY, ddepth, 0, 1, scale, delta, Core.BORDER_DEFAULT);\n\t\t\tCore.convertScaleAbs(gradX, gradX);\n\t\t\tCore.convertScaleAbs(gradY, gradY);\n\t\t\tCore.addWeighted(gradX, 0.5, gradY, 0.5, 0, mat);\n\t\t\tgradX.release();\n\t\t\tgradY.release();\n\t\t\tlogger.debug(\"executed scharr edge detect on mat \" + System.identityHashCode(mat));\n\t\t});\n\t\tlogger.debug(\"added scharr edge detect to chain\");\n\t\treturn this;\n\t}\n\n\tpublic ImageProcessChain canny(int lowerThresh, int upperThresh) {\n\t\tCHAIN.add(mat -> {\n\t\t\tImgproc.Canny(mat, mat, lowerThresh, upperThresh);\n\t\t\tlogger.debug(\"executed canny on mat \" + System.identityHashCode(mat));\n\t\t});\n\t\tlogger.debug(\"added canny edge detect to chain\");\n\t\treturn this;\n\t}\n\n\tpublic  ImageProcessChain erode(int pixels) {\n\t\tCHAIN.add(mat -> {\n\t\t\tImgproc.erode(mat, mat, Imgproc.getStructuringElement(Imgproc.MORPH_ELLIPSE,  new Size(pixels, pixels)));\n\t\t\tlogger.debug(\"executed erode on mat \" + System.identityHashCode(mat));\n\t\t});\n\t\tlogger.debug(\"added erode to chain\");\n\t\treturn this;\n\t}\n\n\tpublic  ImageProcessChain dilate(int pixels) {\n\t\tCHAIN.add(mat -> {\n\t\t\tImgproc.dilate(mat, mat, Imgproc.getStructuringElement(Imgproc.MORPH_ELLIPSE,  new Size(pixels, pixels)));\n\t\t\tlogger.debug(\"executed dilate on mat \" + System.identityHashCode(mat));\n\t\t});\n\t\tlogger.debug(\"added dilate to chain\");\n\t\treturn this;\n\t}\n\n\tpublic ImageProcessChain inRange(Scalar lowerThresh, Scalar upperThresh) {\n\t\tCHAIN.add(mat -> {\n\t\t\tCore.inRange(mat, lowerThresh, upperThresh, mat);\n\t\t\tlogger.debug(\"executed in range on mat \" + System.identityHashCode(mat));\n\t\t});\n\t\tlogger.debug(\"added in range to chain\");\n\t\treturn this;\n\t}\n\n}\n```",
    "created_utc": "2024-11-16T06:48:47",
    "num_comments": 0,
    "comments": []
},
{
    "submission_id": "1gso0zx",
    "title": "Raspberry pi 4 and with MobileNetV3 and unidirectional TSM ( Temporal shift module ) ",
    "selftext": "Hey everybody, I am currently doing a project with the goal of having real time Human activity recognition system , I want to use the pre-trained TSM model on UCF101 dataset ( for intruder detection tasks specifically ) as the system is an alerting system basically, there would be an extension on it where we use multi cameras , one is wide view and the one is an PTZ Camera that would start working on areas where the wide view camera would fail to classify the action happening in a specific localized area to have a better fault tolerance for the system, I was wondering if there is any documentation that would help me with the process of deploying such models and using them on raspberry pi 4 and the two cameras that we have basically, if there is an expert here familiar with such ideas that would be of great help as well, thanks in advance !!",
    "created_utc": "2024-11-16T06:08:31",
    "num_comments": 0,
    "comments": []
},
{
    "submission_id": "1gsn6y9",
    "title": "How is output0 tensor of YOLOv5 and YOLOv8 organised?",
    "selftext": "Considering detection task, I know the shape of the (single) output tensor \"output0\" is the following:\n\nYOLOv5: batch \\* 25200 \\* (numClasses + 5)  \nYOLOv8: batch \\* (numClasses + 4) \\*8400\n\nwhere the difference between 4 and 5 is due to YOLOv8 not having an objectness score.\n\nNow my question is: **class scores** are **AFTER** of **BEFORE** the other features? For example, for ***YOLOv5***, considering the tensor flattened to a vector (N = 25200, NC classes, batch = 1), which one is correct?\n\n    output = [x1, y1, w1, h1, conf1, class1_1, class2_1, ..., classNC_1,\n              x2, y2, w2, h2, conf2, class1_2, class2_2, ..., classNC_2,\n              .\n              .\n              .\n              xN, yN, wN, hN, confN, class1_N, class2_N, ..., classNC_N]\n    \n    output = [class1_1, class2_1, ..., classNC_1, x1, y1, w1, h1, conf1,\n              class1_2, class2_2, ..., classNC_2, x2, y2, w2, h2, conf2,\n              .\n              .\n              .\n              class1_N, class2_N, ..., classNC_N, xN, yN, wN, hN, confN]\n\nSimilarly, for ***YOLOv8*** (M = 8400, NC classes, batch = 1), which of the two:\n\n    output = [x1, x2, ..., xM, \n              y1, y2, ..., yM, \n              w1, w2, ..., wM, \n              h1, h2, ..., hM, \n              class1_1, class1_2, ..., class1_M, \n              class2_1, class2_2, ..., class2_M,\n              .\n              .\n              .\n              classNC_1, classNC_2, ..., classNC_M]\n    \n    output = [class1_1, class1_2, ..., class1_M, \n              class2_1, class2_2, ..., class2_M,\n              .\n              .\n              .\n              classNC_1, classNC_2, ..., classNC_M\n              x1, x2, ..., xM, \n              y1, y2, ..., yM, \n              w1, w2, ..., wM, \n              h1, h2, ..., hM]\n\nI hope it's clear.",
    "created_utc": "2024-11-16T05:24:25",
    "num_comments": 3,
    "comments": [
        "For YOLOv8 boxes first, then classes. I just checked Netron and it concats (1, 4, ...) with (1, num_classes, ...).\n\nI assume v5 is the same.",
        "The [source code](https://github.com/ultralytics/ultralytics/blob/2ccec61b5856614b3a230d16cf80c0af894b9ce1/ultralytics/engine/results.py#L1011-L1060) is also a good reference point. \n\n`data = [[x, y, x, y], id, conf, class]  # id for tracking only`",
        "Oh true, I could have checked with Netron🫠. Thank you!"
    ]
},
{
    "submission_id": "1gsm4lr",
    "title": "Best hardware for computer vision tasks",
    "selftext": "Hi there,  \n  \nI'm currently building a computer vision project which needs to run at a factory. We're **NOT** going to be doing anything like real time inference with YOLO or some other model, but **only** processing separate frames. So we save an image when movement stops, process it with a python script and do some Resnet-50 (convolutional neural network) predictions on the images to get results. We're expecting to do a maximum of 3 images per second.\n\nWe're also going to run a quite simple application to control the whole process, like starting and stopping the python scripts and some other basic tasks with a database with one or two tables.\n\nFor me it's really hard to determine what a good workstation or mini pc would be to run the python script and be fast enough to do the calculations in real time. We don't want to have lag or anything.\n\nThanks in advance for all your input and answers!\n\nKind regards",
    "created_utc": "2024-11-16T04:20:25",
    "num_comments": 7,
    "comments": [
        "Hi,\n\nThere are too many variables you haven't told us about in order to give a good estimate. So let's try to clarify a few things:\n\nQ1: How much lag can you afford between the process output and the model prediction? (I know you mentioned no lag, but everything has a lag)\n\nQ2: How many parameters will your model likely have to solve your problem? Do you know this or is Resnet-50 just your initial guess, without having done any testing?\n\nQ3: What is the dimensionality of your input data? (i.e. resolution of the images)?\n\nQ4: Related to Q3: Can you down-sample your input and still receive reliable predictions? What is the highest down-sampling factor that will still yield reliable performance?\n\nQ5: Do you anticipate that quantization will work for your problem?\n\nQ6: Why do you need to process three images per second? Where do you get this estimate from?\n\nQ7: Can you make a case that your problem can only be solved by using deep learning? There are many classical methods out there that could be much faster if they are suitable, and also don't require a beefy workstation.\n\nQ8: What is your budget? Maybe you don't have a choice and just have to go with one of the medium good GPUs and the decision of what you need is irrelevant due to budget constraints.",
        "It all comes down to your inference speed requirements and physical size / weight / power / cost envelope - it could be a as low as Pi5 or as high as a GPU.  \nYou say 3fps, what performance can you get from your network on some reference hardware?",
        "get a computer with a GPU, or possibly several GPUs",
        "I did a similar project. How would you know when movement stops accurately? You need good CPU, what i mean about good CPU, i got 7 fps of processed images, but i couldnt do it for along time, my laptop was overheating, and it will turn off. So i would go about gaming pc for about 800$. RTX 3060 or something similar should do the job. You can try with PC with good CPU, but for 200 $ more, you can get good GPU, and you can process whatever you want. I went with GPU. But model that i use is SSD Lite mobile net V3, and its object detection, ResNet 50 is image classification.",
        "Thanks for your answer! Here are answers to your questions:\n\n1. The ResNet50 model prediction using NVIDIA RTX 4000 Ada Generation Laptop GPU can be done in real-time. We have tested the speed using both GPU and CPU, trying to make the predictions faster than the given time per frame in seconds. With CPU we cannot still be real-time but with GPU we can.  \n  \n2. Resnet50 was chosen since in similar applications the performance is good. We will further test ResNet18 and ResNet34. I will let you know once I have the results.   \n  \n3. Currently the dimensionality of the input image is (224, 224). We checked the performance using both (100, 100). The accuracy was not as good as for  (224, 224) and again we could only do real-time with GPU, and not with CPU.  \n   \n4. We tried to downsample/resize to (100,100) and the accuracy of the model got much worse. After doing that, we also resized our images to (500, 500) and the accuracy became better. So we can assume that downsampling further from (224,224) would not be ideal in this case.   \n  \n5. We didn't apply quantization yet, but we could try this to make it more efficient. With the current GPU (1) we are real-time. So, we can check indeed if quantization will help with CPU. We use pytorch ResNet50 and I'm not sure how easy is to integrate that.   \n  \n6. Yes, I computed that based on the frame duration  \ncap = cv2.VideoCapture(video\\_path)  \nfps = cap.get(cv2.CAP\\_PROP\\_FPS)  \nframe\\_duration = 1 / fps  # time per frame in seconds  \nprint('Frame duration in seconds (s):', frame\\_duration)  \n   \nThen, I used another algorithm that makes predictions when a movement is detected. The movement is periodic and approximately occurs 3 times per second.   \n  \n7. We tried a method such as structural similarity algorithms but the results are easily affected by different lighting situations, just like many classical methods we tried, and therefore do not perform well. Do you have any ideas for classifying defective images using image processing or deep learning?  \n  \n8. Ideally, the budget would be a maximum of 500 euros for this piece of hardware. Do you think we can afford a GPU or only CPU with that budget?  \n  \n  \nIn general, do you know how can I define the hardware specifications only by knowing the machine learning model, camera/image resolution, and distance of the camera to the object I want to track?  \n  \nAre there any best practices in the industry I can follow to effectively choose?   \n  \nIs that any approach you suggest? any open-source/report/documentation tool?",
        "Thank you for taking the time to answer all my questions.  \nI'm not a GPU expert, but with that budget, I would either try one geforce rtx 4070, or two geforce rtx 3050. What you basically need is the best bang for your buck in terms of Nvidia GPUs (any other brand is trash for deep learning). \nCPU inference will most likely be way too slow.\n\nI don't think ResNet is your best option here. Developing efficient convolutional neural networks is an entire field in itself. One very promising architecture is efficientNet (eg efficientNetB0), which has about 5x fewer parameters than Resnet50. If you need to go even smaller, check out this work: [https://ieeexplore.ieee.org/abstract/document/9859729?casa\\_token=Q-txQj5737gAAAAA:EYoza9QfyVg2JELM2ULdBBNh\\_4VpIy9THeFXjNjnIz1VMx9688Ud1Sf-aX5e7knusFtlWV0po70x](https://ieeexplore.ieee.org/abstract/document/9859729?casa_token=Q-txQj5737gAAAAA:EYoza9QfyVg2JELM2ULdBBNh_4VpIy9THeFXjNjnIz1VMx9688Ud1Sf-aX5e7knusFtlWV0po70x) Prioritize already implemented solutions.\n\nCan you tell me a little more about the task you are trying to achieve? Are you trying to identify potentially defective parts of some item in a factory through image inputs? In order to have an opinion on whether there is a classical method that could work, I would need to not only know exactly what your task is, but also what kind of noise (apart from different lighting conditions) you are going to encounter."
    ]
},
{
    "submission_id": "1gsls08",
    "title": "Possiblity of mixing 2 or more models",
    "selftext": "I'm working on project of a system for traffic cameras,\n\nMy system contains 3 different models:\n\nVD: detecting vehicle's\n\nLPR: licence plates recognition\n\nOCR: reading licence plates characters\n\nThese 3 models should work together Parallel or sequential on a single image.\nFor example:\n\nImage=> VD => LPR => OCR\n(The images goes to VD model, results of VD goes to LPR and the results of LPR goes to OCR)\n\n\nThe question:\n\nIs it possible to Train a model that do the process of all 3 models?\nMeans:\nIt detect the vehicle, the license plates, and characters in each image, and then we do the pre processing of choosing the correct outcomes ?",
    "created_utc": "2024-11-16T03:57:44",
    "num_comments": 6,
    "comments": [
        "You could do it in 2 models one that detects cars and license plates, and another that does OCR.  You can maybe reduce it to 1, but I'm not immediately aware of how to do it.  The only problem with adding license plate recognition is you can't for sure match a license plate with a given car.  Like you can just do a quick check to see which license plate is contained within which detections, and I can't think of any circumstance where this might fail, but it could fail.  I guess one case where that matching might fail is if you have like traffic, and the bounding box of one car goes up and intersects with another.  But I'm not sure how likely that is to happen.",
        "PaddleOCR has an off-the-shelf shelf models to do ALPR.... Those are for Chinese Licence Plate Detection/Recognition..... You can train your data using the PaddleOCR configs.....",
        "Your order is correct.  Perhaps you could compact the model down and just detect the license plate.  Detecting the car is essentially a throwaway act because you are doing so to narrow down where the license plate is located.   OCR is a different problem set than detecting a license plate thus it should be a separate model that is good at OCR.",
        "You're absolutely right \nI also came up with this idea and found some examples in the GitHub, but couldn't find any with mixing OCR model with the other two.\n\nYou are right, the only way to make sure if a plate belongs to a car is to just check if the bounding boxes are in the right place compared to each other\n\nThe main problem that I think I'm gonna face is the character's size in OCR input image and VD/LPR process",
        "Thanks for the suggestion,\n\nI'll try them...\nI don't think I could use the OCR datasets but LPR and vehicle detection datasets must be good to go.",
        "Hmmm.\n\nYou are right.\n\nThe problem that I'm encountering right now is the time.\nI Trained my models on YOLOv9t with ultralytics.\n\nOn my laptop (cuda:12.5 /gtx1650m(4GB)) I'm getting 24~50ms per each model.\nAnd since the goal is to do the process under 100ms the models are slow (onnx format)\n\nTried to use orange pi 5 (it has npu of Rockchip rk3588) but stocked at the process of converting my models to rknn format Which is the only acceptable format for this Rockchip npu,\n\nSo now, I just want to come up to a new solution with this problem.\n\nFor the VD model, I don't think so, since there ganna be some vehicles without license plates in my region, so detecting only license plates means I'm forgetting about those types of vehicles."
    ]
},
{
    "submission_id": "1gsk6qw",
    "title": "Flexible solutions for receipt processing (IDP)?",
    "selftext": "I have a task that involves processing receipts: recognizing the country, retailer, products, and prices. From what I’ve read, this falls under the umbrella of Intelligent Document Processing (IDP).\n\nAre there any flexible, ready-made solutions for this that won’t hit a complexity wall? Or is it still a case of having to build the entire pipeline from scratch?\n\nWould love to hear your recommendations or experiences! Thanks!",
    "created_utc": "2024-11-16T02:00:21",
    "num_comments": 1,
    "comments": [
        "Hi OP, you can try my solution documentpro AI. Easily extract whatever information is needed from receipts. It works with images or pdfs. The API is also very quick to get started."
    ]
},
{
    "submission_id": "1gsjytp",
    "title": "Resources for Training a Face Detection Model with Custom Dataset and Applying it in Real-Time Using PyTorch",
    "selftext": "Hi everyone,\n\nI’m looking to train a custom face detection model using a dataset with images and XML annotation files. I plan to use PyTorch for training and save the model as a `.pth` file. I also want to apply this trained model in real-time using a webcam feed (e.g., with OpenCV).\n\nCan anyone recommend some comprehensive tutorials or resources that cover both:\n\n1. Training a face detection model with custom datasets (including XML annotations).\n2. Applying the trained model in real-time for face detection using a webcam.\n\nI’d really appreciate any guidance or links to detailed tutorials on these steps!\n\nThanks in advance!",
    "created_utc": "2024-11-16T01:43:00",
    "num_comments": 1,
    "comments": [
        "If you really want to learn sth, you should try and solve the same problem with different approaches, including the classic - I’m thinking Viola & Jones - then compare them against each other. You will have to convert the dataset to different formats and maybe do some additional labeling. This makes for 80 % of the *real* work outside of research, and since you probably have not curated that dataset yourself it’s also a great practice."
    ]
},
{
    "submission_id": "1gsjqp5",
    "title": "Image Data Quality",
    "selftext": "has anyone worked on a project where the data quality was the main hindrance? how do you get thru it??",
    "created_utc": "2024-11-16T01:25:40",
    "num_comments": 8,
    "comments": [
        "Seems to be most projects depending on what you mean by data quality\n\nOne way to fix it is to go back to whoever collected the data and work with them to get it right e.g. fix illumination or capture problems. Make sure they're actually looking at the data they take, loads of people will literally not do that. If it's metadata or tagging or whatever, you need to go back to whoever had that job and make sure they have the correct tools and instructions to do it to the specification you expect.\n\nYour time is important and valuable too, don't waste it working with bad data on impossible problems when there's a way to fix it before you encounter it\n\nElse if you really can't get new data, you're just going to have to throw out huge quantities of it",
        "Welcome to the life of a data / cv scientist.",
        "In most practical scenarios the biggest challenge is data quality. The best way to deal with it is to go through the dataset manually with the annotator to fix them, but that can be too time consuming. Most recently the Data Centric AI techniques can be helpful for highlighting cases which are suspected to have low quality or incorrect labels, but it would only help if the \"bad\" part of that dataset is like 10% ish or less.",
        "I'm facing the same issue due to the camera range. Either have multiple cameras/input sources or try smoothening the input if image features are not too important (if you are trying to detect small objects)\n\nCurrently having to manually fix the model detections",
        "Thanks!\nIt's just a bit annoying!",
        "Hey man, I took 2500 photos in five formats and using ten different types of camera ranging from phone to disposable film camera, all that needs to be done now is for you to count every mosquito in every image. Most are out of focus but you can just upscale that right? Can you send me the results by Monday btw, that's the deadline even though the data has been lying around for six months.",
        "I think the one critical step in project organisation/planning/management to help avoid this is to ensure that the technical CV people (and everyone else downstream) is involved in the project from day 1. Rather than e.g. some non-technical (or not-technical-any-more) project/people/product manager team specifying the data collection methodology then handing over the data to the technical people as a completed milestone. The CV people need to be part of specifying the collection methodology and then have the task to look at e.g. the first 10% of the data and already start building the models on subsets.",
        "Had me in the first half, ngl. But you gave it away when you didn’t ask for absolute size measurements of objects captured at various distances without so much as a reference in the (monocular) field of view."
    ]
},
{
    "submission_id": "1gshjte",
    "title": "What was the strangest computer vision project you’ve worked on?",
    "selftext": "What was the most unusual or unexpected computer vision project you’ve been involved in? Here are two from my experience:\n\n1. I had to integrate with a 40-year-old **bowling alley management system**. The simplest way to extract scores from the system was to use a camera to capture the monitor displaying the scores and then recognize the numbers with CV.\n2. A client requested a project to **classify people by their MBTI type** using CV. The main challenge: the two experts who prepared the training dataset often disagreed on how to type the same individuals.\n\nWhat about you?",
    "created_utc": "2024-11-15T22:45:00",
    "num_comments": 54,
    "comments": [
        "Classify burn degrees on children. I only briefly worked on the code but a colleague drew the short straw and had to sift through thousands of pictures of children with burn injuries and label the severity of the burn on the skin. \n\nIt really was for a noble cause but it's truly heartbreaking.",
        "I didn't do it and the task was kind of not odd, but the circumstance surrounding it was quite odd.  On a famous freelancer website I was contacted to do a project for a \"non-profit\" organization, where they wanted me to recognize chip stack counts in casinos.  Sounds real \"non-profit\" to me.",
        "It was a proof of concept, but using stereoscopic vision and people detection to identify people who wandered into unsafe areas near a lighthouse where rogue waves tend to claim victims.  This was integrated with a long-range acoustic device that could be pointed at the offender to tell them to return to a safe area.  The safe area was dynamic based on weather predictions and wave height forecasts.",
        "An official from law enforcement in Mexico sent us \"difficult facial recognition images\" to test our \"good to great facial recognition in difficult situations\" product... and it was a database of decapitated heads from a mass grave.",
        "Counting specs of dust on a piece of glass or a robot that chases birds off lawns.",
        "I created a vision system for an industrial machine for cutting fabric. 4 cameras calibrated that were looking a large area with a fabric on top. The software received a CAD file with t-shirts and trousers dimensions, apply them on the fabric according to the repetition/texture and deform them if the fabric was stretched. After all those computations, it send the coordinates to a robot that cut it. Two years of project, cannot expeess the difficulties in a reddit post😅.",
        "finding the best matching toilet seat by taking a photo of your toilet from within a web shop.  \nrectifying from device inclination, classifying, aligning, measuring, segmentation, shape comparison to find the best match.",
        "that second one…wtf. How did it go? I’m so curious",
        "Wow I feel #2 is bound to fail, did it work at all?\n\nMine was building a system for a VR skydiving experience where users actually make the leap in real life. I had to build an ML model to predict the exact time the user committed to a jump, milliseconds before the jump is performed.",
        "I wrote an opensource framework for connecting optical sensors to a camera using plastic optical fiber: https://hackaday.io/project/167317-fibergrid\nI am estimating you can connect on the order of 500 sensors to a single cam.  Intended to be used in robotics.\n\nSensors can be 3D printed etc...  For example, It took me about two hours to make this joystick: https://hackaday.io/project/172309-3d-printed-joystick\n\nThe code identifies the fibers in an image, saves their size and locations.  After that it takes just a few lines of code to sample the sensors.\n\nThe idea and implementation are really simple but the big picture is that it merges vision with other modalities.",
        "Ah I do remember one other sort of odd circumstance I ran into.  It started off innocent where I was just basically making a damaged product detector for stuff on a conveyor belt.  Once I finished my project manager complained I didn't fulfill all the requirements and then showed me there was a line where they wanted the customer to be able to control the detection threshold.  Like why on earth would you want to let the customer control that?  It's not even linear, no factory worker is going to understand what they're doing with that.",
        "How tf do you classify personality through computer vision 😭",
        "Score breast symmetry after plastic surgery.\n\nGenerally it was about breast cancer surgery but not exclusively. Especially strange I found one picture where the photographed woman is smiling very suggestively so that I wondered if that was actually the GF of the doc who initiated this project and sent me all the images he used for his initial testing lol (it wasn't taken from the internet as it had the same hospital room background)",
        "We built machines to count fish heads—yes, fish heads. These marvelous contraptions found their home in fishermen’s guilds, where they didn’t just tally fish noggins but also helped estimate their size or grain (units/kg). Turns out, fish are surprisingly bad at holding still for a measuring tape, so we thought, \"Why not let the machines do the hard work?\" Efficiency, accuracy, and fewer fish-related arguments ensued.",
        "I'm going to assume the second wasn't possible. MBTI is widely debunked. It's like 'intellectual' tarot or star signs. If you could accurately test for it, it would shake the psych community up. So would if you could tell someone was a Pisces, I guess.",
        "2 years ago, in my end of studies internship I have worked on virtual try on, the only project I really enjoyed and worked hard on it, it was a research because it's really hard problem but it worth every second spent on it really interesting project",
        "Put a camera on an inverted periscope device that went into the pile instead of a control rod in a live nuclear reactor. They needed to image the state and position of the graphite blocks.  Camera was single use before dead low grade nuclear waste.",
        "This isn’t something I worked on but I had a peer start a govt job that is working on a model that identifies CP on the dark web. Pretty fucked up, it was tough on him mentally and had to leave the job after a couple months",
        "OpenCV only: recognition of multiple gas stations prices, multiple shape, types, colours and price associations",
        "Ummmmm",
        "Gosh that’s nasty 😫",
        "Another one was classifying bee behavior. But it didn't really work well, the differences in behavior classes was way too subtle and there were hundreds of bees in the observation hive.",
        "what approach did you use to count the chips? Sounds really interesting, I assume the position on the table and color (player) was also of interest?",
        "wow! what a cool one!",
        "Warning Sign + Darwinian process in action > expensive CV saving the stupids\n\nbut a novel application for sure",
        "holy sh...",
        ".....whoa. I wouldn't have been able to do that. Photographic memory is a blessing, but also a curse.",
        "My graduation project is similar. Instead of using a robot to cut, I used a robot to draw ink lines on fabric. At first, I wanted to use a CAD file as input but I couldn't. So I switched to using a photo as a model. How do you read the CAD file and process it? Is there any keyword?",
        "Wow! This is making me rethink my fictional business venture that will replace hair salons with AI robots….",
        "I love this one. A perfect application of CV to something that's otherwise a PITA to resolve",
        "Didn't work out :) Like, at all. No signal :)",
        "How do you even start something like this? I would imagine you need to capture a heap of real data and then identify key markers that happen exactly before the jump ?",
        "I can't remember the exact task but I remember in like 2016 or so some paper came out where they were able to predict something you'd have assumed was internal about a person using computer vision.  I want to say it was disease based but it may have been psychological.",
        "Whoa, that sounds really cool!\n\nI’m sure you’ve heard of it and know the actual term, but what you describe reminds me something I read about once. It’s a type of camera that can be essentially glued onto a wall as a bunch of photovoltaic sensors in a large grid. It reassembles (very low resolution) images of the room based on the amount of light hitting each PV. They had a name for this kind of camera that doesn’t use a lens and at time it sounded really insane, but now that we have such powerful ML it’s still amazing but less surprising they it could work…",
        "Seems like a pretty reasonable request to me. “This one is damaged worse than that one”\n\nA good example of the disconnect between users and developers in understanding what’s possible. ",
        "No way to do it :)",
        "I guess its like astrology. You label based on real people and real info, and it doesnt really matter what the prediction is, you'll get it right 50% of the time.",
        "They did it with political leanings in the US",
        "neckbeard/trilby vs no neckbeard/trilby",
        "I started with \"I didn't do it\"",
        "Yes my apologies, i used the CAD word to make people understand the concept, the shapes of the shirt were in a ISO file, it was a sequence of points and different layers for different types of cut. We built our own parser for that. Ofc the client was producing such machines, he ordered us the vision system as plugin for their machines, so he had knowledge of the sector",
        "Some toilet seats are very comfortable. Try the cushioned ones.",
        "I’m not surprised haha. Hope you still got paid a bag",
        "Yep I was tracking keypoints on the body, and had some ground truth on where exactly the jump occurred",
        "I think you are talking about compound eyes like spiders have.  \nCompound eye sensors are in a grid and in my framework fibers are in a grid.   My framework can definitely act as a compound eye if you let natural light shine at the end of the fibers.\n\nFrom what I understand compound eyes have to limit the angle at which light can enter each sensor. So in your case you would need small tubes around each sensor on the wall. Although it could be useful without the tubes to detect motion etc...",
        "That's not what having control over the threshold means.",
        "Maybe not personality immediately, but psychological aspects or metrics definitely. \n\n\"Hold my beer\" -FAANG",
        "Right, I’m blind, anyways if you would do it which approach would you take? I figure yolo and some key point detection or segmentation models could work for a stack? Sounds like a fun problem to work on.",
        "To us engineers it’s not. But as a user he just wants to be able to adjust how sensitive the model is to damage. \n\nPerfectly reasonable request imo, but it obviously entails a completely different modeling method, which should be decision #1 before even quoting the project. So yeah if he didn’t say that upfront it’s kinda both of your fault…you as the expert know that it had to be an explicit design criteria, and he as the customer should know that when dealing with software development you need to be really clear with the requirements. \n\nFWIW when I’ve had to do damage assessment I usually try to get the customer to break out the training examples into a low/medium/high categories and then design the model to output continuous numbers. It’s not perfect obviously but it’s usually good enough to satisfy their desire for a dial to control. ",
        "*\"Physiognomy is the pseudoscience of assessing a person's personality based on their physical appearance, especially their face\"*\n\nspot the Left wing feminist in this crowd of normal people...\n\n  \nmeanwhile: [https://www.theguardian.com/technology/2017/sep/07/new-artificial-intelligence-can-tell-whether-youre-gay-or-straight-from-a-photograph](https://www.theguardian.com/technology/2017/sep/07/new-artificial-intelligence-can-tell-whether-youre-gay-or-straight-from-a-photograph)",
        "I'd really have to see the data first.  There would be ways to solve this without DL just using basic color matching and line detection.",
        "Even if you manage to output something continuous, that's still not how it works.  That's why it's a problem, and has nothing to do with me.  If they want a deep learning solution, they can't have control of the threshold.",
        "In most business cases there’s pre and post processing surrounding the DL models, so that’s where the threshold can be added if you don’t think it can be baked right into the model itself.\n\nLike for example you could use DL to measure the length of each defect, then have a threshold for that. \n\nBut this all requires a lot of upfront discussion and planning with the client. You can’t just go “yeah we can build an AI model to detect defects for $50,000” and expect them to be happy with the results! ",
        "You can keep saying this stuff, but you're already just lacking the fundamental knowledge of what it means to modify the threshold."
    ]
},
{
    "submission_id": "1gseqn5",
    "title": "An Introduction to Bitmask Representations and Encodings - RLE vs REE",
    "selftext": "Hey everyone, thought to share a little writeup our developer team has done as we are exploring different ways of storing bitmasks in a scalable manner for our users.\n\nI realised that visualising the various encoding methods help understand how the compression works and we look forward to getting your feedbacks!\n\n",
    "created_utc": "2024-11-15T19:46:33",
    "num_comments": 0,
    "comments": []
},
{
    "submission_id": "1gscwb7",
    "title": "what is a good approach for simple furniture recommender",
    "selftext": "hey guys i am trying to make a simple recommender that recommends a furniture from a set of furniture for a particular spot in the room. I tried cosine similarity between the spot and the furniture but it seems it doesn't work as expected, what other approaches can i try?",
    "created_utc": "2024-11-15T18:03:14",
    "num_comments": 1,
    "comments": [
        "The embeddings of the spot and the furniture won't be similar. \n\nHowever, maybe you can feed the embeddings for the spot to a VLM or a simple classifier and get tags like art deco, modern, industrial out of it?\n\nOr maybe hand craft a large library of spot embeddings that go well with furniture embeddings. Then given a spot embedding, you can pick among nearest neighbors in the furniture embeddings space? \n\nLet me know if you find a good solution, it is an interesting problem."
    ]
},
{
    "submission_id": "1gsbdj1",
    "title": "Faster Image Reading Options for UHD Sources",
    "selftext": "I’m running into an issue with a CV project where we’re able to to read in HD ProRes files at 200-300 fps, but moving to UHD takes us down to ~20-30 FPS. This is with heavy multi threading on a M1 Ultra Mac Studio fully maxing out the processor on OpenCV. I’d expect 1/4th performance, but 1/10th seems a bit excessive. \n\nHas anyone worked out a way to utilize the hardware Media Engine for ProRes acceleration. The same machine can read and write a UHD ProRes in Resolve at over 300fps. Processing the actual CV tasks is extremely fast (200+ FPS) after the read, but the read is super slow. Tried FFMPEG-Python, and it was comparable to multithreaded CV2.  \n\nIf anyone has found a library that can utilize the Mac’s Media Engine to get a NP of the uncompressed frames, I’d be eternally grateful. ",
    "created_utc": "2024-11-15T16:42:28",
    "num_comments": 0,
    "comments": []
},
{
    "submission_id": "1gs8xq9",
    "title": "Which is the best traditional image segmentation algorithm?",
    "selftext": "I am trying with Watershed, K-means applying various pre-processing steps, but the segmentation is not upto the mark. For edge detection I used multi-scale edge detection with CLAHE and canny but the results are very bad. \n\nThe images are not very crowded, but not very simple either.\n\n(This is just for my learning purposes, exploring with traditional CV segmentation methods)",
    "created_utc": "2024-11-15T14:45:09",
    "num_comments": 5,
    "comments": [
        "I used random forest for lane detection back in the day and it worked just as well as when we switched to mask rcnn.  k-means can be unreliable.",
        "I used k-means to segment an object from a background. O got better results by cropping into the obj to be detected, keeping a better balance between background and obj pixel counts. The results were much better than attempting kmeans on the entire image.",
        "Try grabcut",
        "is it possible to use it for ocr tasks ?",
        "Not really. In theory if you have all variables controlled (font type, illumination, width, rotation, etc - just about every single param) then you can do ocr by template matching. You don't even need segmentation. But that would be very unlikely. The fact that sometimes printers print crooked messes that up. There is a reason why OCR is done with ML. In many ways, it is simpler. Traditional OCR sounds too brittle. It would take too much development time. And OCRs are used to capture forms and the like. Those change all the time and they are not necessarily the most renevue critical aspects of a bussiness.\n\nBut it is possible."
    ]
},
{
    "submission_id": "1gs8wvm",
    "title": "Homography for matching images of two sides of a football field",
    "selftext": "Hi, everyone. Hope you're doing well. I'm from Argentina and I'm doing some digital image processing stuff applied to football. I want to extract some information about the distance that a player travelled on a match, and for that I took  some videos from a fotball match. The camera couldn't take all the pitch, even with the \"fish eye\" lens so I had to record two videos from the match. \n\nhttps://preview.redd.it/h9jawa0k451e1.png?width=1351&format=png&auto=webp&s=026fc9aa3cec1662c18a889bcd29c30760a9e750\n\nThe main problem is that the two images warp a lot! That makes very difficult the post-processing.  \n  \nI have two videos from the left and right side of a football pitch and I want to join them. I used this implementation of a \n\n  \n[https://github.com/OpenStitching/stitching](https://github.com/OpenStitching/stitching)\n\nI load two images from an instant of the match and then use the feature detector and later I match the features form the two images. Next step is warping the images, and here is the main problem, where the images bend a lot for matching the features.\n\nhttps://preview.redd.it/klvczhll451e1.png?width=1361&format=png&auto=webp&s=0d287e1a98d070b83ac7917f731eac479812cd81\n\nThe final result is like this:\n\nhttps://preview.redd.it/bqza9scn451e1.png?width=1370&format=png&auto=webp&s=71370314d01c170685007215b82d6f0a7c09b8e0\n\nI would like to know if there's something better in terms of less warping that I could use. I found a page that makes the joining of the two images with less distortion:\n\nhttps://preview.redd.it/ap8x8w5r551e1.png?width=1415&format=png&auto=webp&s=0fdd45e104568cf2eba15e9e0db3d9f474800d17\n\nI'm still not sure how they achieve that, any help would be very appreciated! Here is the code of the homography that I described before:  \n  \n[https://github.com/agusrol/homography\\_football/blob/main/homography\\_final.ipynb](https://github.com/agusrol/homography_football/blob/main/homography_final.ipynb)  \n\n\n\n\n",
    "created_utc": "2024-11-15T14:44:04",
    "num_comments": 9,
    "comments": [
        "How are you detecting the point correspondences? There's probably a lot of false positives.\n\nAre you using RANSAC to find the best point correspondences to then use when calculating your homography for the projection when doing the stitching?",
        "Check out dust3r!",
        "Record a video with a roomba mopping the field entirely, have its position recorded via DGPS or whatever at every moment/frame, and built a direct map for position in field coordinates to its pixel position in both frame(s). \n\n  \nIf cameras are glued in their position (or camera holders at least), then you can later use this direct mapping no need for other vision algorithm",
        "We are using the stitching feature\\_detector that uses ORB as default, the point correspondences looks like this: [https://i.imgur.com/Q6KYbHm.png](https://i.imgur.com/Q6KYbHm.png)",
        "we are gonna check that out! thanks",
        "Hello I'm his partner, the video must be recorded with two phones, so both views are fixed. The only thing that we can improve is that we can record the two sides more horizontally",
        "You might be able to utilize the midfield line to rotate the images to be the same angle, then do the stitching. But for typical image stitching, the camera needs to be at the same location in space and rotated across a single axis.\n\nHave you tried this by simply rotating one of the images to match the other?\n\nIt also looks like they are projecting one of the images before they stitch one onto the other\n\nEdit: ohhh it looks like they are first projecting the images such that the lines at infinity are parallel. Which is why the horizons are matched up. Then they do image stitching. Clever...",
        "you need markers in known locations for both, where more locations you can calibrate the better. QR codes or similar are the easiest, but anything you can identify the center to reliably will work well (eg a ball and have the user click on each image -> use SAM or some traditional CV to get ball center). The key is to have similar time-synced photos from both where the main difference is the changing location of the marker",
        "That would help, I imagine. It should be one camera being turned on a level plane for panoramic views. Those two views are so radically different that it is warping because it can't stitch them horizontally."
    ]
},
{
    "submission_id": "1gs7bdx",
    "title": "Need advice on building a dataset for an image classifier",
    "selftext": "Hi everyone! I am kind of new to CV and I wanted to try my hand at building an image classifier for the original Pokémon starters (Bulbasaur, Charmander, and Squirtle). I started off by building my dataset by creating a dataset by downloading all the images of every Pokémon card of the three starters but there is a surprising lack of images for those three. I only got a range of 20-30 different images for each. I plan to use data augmentation to increase the dataset size, but I think that this will still result in way too small of a dataset. \n\nIf anyone had some advice on some ways I can help increase my dataset size that would be fantastic.\n\nNote: I haven't included any pictures from the shows or fan art yet since I had some concerns about adding pictures like that for my dataset",
    "created_utc": "2024-11-15T13:32:43",
    "num_comments": 7,
    "comments": [
        "You’d be surprised how few images you can get away with if each class is obviously different from the others. \n\nAlso a very effective type of augmentation is to cut out the character and paste it into different backgrounds at random. You can write code that does this easily. ",
        "If this is just for funsies, you might be fine. \n\nYou can also just test it. It won't take long to train at all lol. A basic image classification conv net, even on CPU with that small of a dataset will train in a couple hours max. If it's on GPU, it'll take like a few minutes if that.\n\nAlso, you should be using pretrained models. No need to waste your time training a model to learn what horizontal lines are. It also saves on calculations.\n\nIf you want more Bulbasaur pics or whatever, I would Google \"how to scrape web Google image results\" and use beautiful soup or whatever. Don't worry about accuracy too much. For something as popular as Pokemon, results will be close enough, and enough results will average out the less conforming images. You'll have the labels obviously, because the label is whatever you searched for on Google.\n\nNot sure what your concerns are from using images from the show unless you are selling access to this dataset. It's already freely and publicly available on someone's database if you download it. Just don't sell stuff with IP. You couldn't do that with cards either.",
        "I had decent accuracy classfying the original 151 pokemon with only a few hundred images using a 4 layer deep CNN. As other commentors have said, just give it a shot and it might work pretty well. You can include non-card images of the pokemon as well.",
        "There are some datasets on kaggle you can check out.",
        "Would be interesting to know if stable diffusion or other Gen art tools can generate the three pokemon characters. If they can you can really pump up your training set with synthetic data.\nEven if not, I suppose you can crop the characters out and paste them onto different generated backgrounds with different brightness and shading",
        "that’s actually pretty genius, I wouldn’t have thought of changing the background. Thanks!",
        "Google Simple Copy Paste. Also Google rembg as a tool that might be helpful in the process. "
    ]
},
{
    "submission_id": "1gs2gez",
    "title": "How to Fine-Tune SAM-2.1 on a Custom Dataset",
    "selftext": "",
    "created_utc": "2024-11-15T10:01:27",
    "num_comments": 0,
    "comments": []
},
{
    "submission_id": "1gs14uy",
    "title": "UK Networking events, computer vision professionals?",
    "selftext": "Anyone been to any good networking events in the UK (especially London)? Cheers!",
    "created_utc": "2024-11-15T09:05:48",
    "num_comments": 5,
    "comments": [
        "I dont know of one. But would be interested to know if there are any.",
        "Search on meetup, I've been to a few in the past",
        "I’ll see what I can find and try to post back. Might even start one in London…",
        "Thanks, will do!",
        "Thanks"
    ]
},
{
    "submission_id": "1gs0lcn",
    "title": "Perspective N-Point for object pose estimation",
    "selftext": "Hi everyone! Sorry if my question is trivial, but I can't really understand this matter. \n\nI know that Perspective N-Point is very useful to find the camera pose with respect to an object (and so, the object pose relative to the camera), but I can't really get how to automatically select the most important points to get a 2d-3d correspondence and effectively use PnP.\n\nI'll give an example: I have the 3d models of some objects, and I can manually select some points on them. Then, with a Blender add-on I can select the same points on a photo and calculate the correct camera pose. At the same time, I don't really get how to automatize this process (without manually choosing the points) to match 2d and 3d points. Some researches made me discover solutions like feature detection and matching in images, but I don't get how to implement them.\n\nThanks in advance!",
    "created_utc": "2024-11-15T08:43:12",
    "num_comments": 6,
    "comments": [
        "Look at deep-learning based 6DPE papers. For the ones based on PnP, they train a graph network to perform 2d-3d matching e.g. OnePose/OnePose++.",
        "If you render your object in many positions and orientations, and many lighting conditions, you'll be able to build a big dataset of how the object points look, along with their 2D-3D coordinates pairs (you'd be able to filter the occluded ones, using the ZBuffer of course).\n\nOld school approach: If you select a few points by hand on a Blender model, then you could use the dataset to train classifiers to recognize each point. It could be something as simple as logistic regression over the SIFT coefficients on this point (SIFT being a well known, very decent, keypoint detector/descriptor method).\n\nOne of the very old school ways to select points is to retain those that are easily detectable by some keypoint detector (ex: SIFT, again): you run SIFT over your dataset, and check which points are the most reliably detected.\n\nLess oldschool: PnP is differentiable, so if the detector that detects your special points is trained with gradient descent, everything can be trained end to end. Here is a random example: [https://arxiv.org/pdf/2409.14249](https://arxiv.org/pdf/2409.14249) where facial landmarks are used with PnP. Notice the facial landmarks is also a small sparse set of points that need to be defined beforehand!\n\nWith an RGBD camera, this older work directly regresses 3D coordinates for all pixels: [https://projet.liris.cnrs.fr/imagine/pub/proceedings/ACCV-2014/pages/PDF/744.pdf](https://projet.liris.cnrs.fr/imagine/pub/proceedings/ACCV-2014/pages/PDF/744.pdf) \n\nThis a more recent method, that uses the same probabilistic differentiable PnP, but it's much more involved, see [https://arxiv.org/pdf/2303.12787](https://arxiv.org/pdf/2303.12787)",
        "First, use a detector like SIFT or ORB on both the 2D image and rendered views of your 3D model to find keypoints and descriptors. Then, match descriptors between the 2D image and 3D model using k-NN with Lowe’s ratio test to filter out weak matches. Apply RANSAC to remove outliers, ensuring more reliable correspondences for the PnP solver. With good matches, use OpenCV’s solvePnP to estimate camera pose.",
        "Thank you, I'll have a look at the papers!",
        "Wow that is a lot of useful informations! Thank you very much, you gave me a lot of starting points!",
        "Thank you very much, seems promising, I'll look into it!"
    ]
},
{
    "submission_id": "1grzpu7",
    "title": "Improving Accuracy ",
    "selftext": "https://preview.redd.it/79x0v7vn931e1.jpg?width=1912&format=pjpg&auto=webp&s=aa4759c53bf7f3de585ddd0f123654a9af5402d2\n\n  \nThere are a handful of reasons why computer vision models achieve low Mean Average Precision (mAP) ratings. One way to overcome this challenge is by using synthetic image datasets.   \nBut where do you find the best service provider?   \nGreat news!  \nI just launched my online directory of service providers in the synthetic image data generation and simulation industry. (More listings coming this week)   \n  \nIt's free to browse all providers, and I would appreciate it if you could check it out and share your feedback with me.   \nLink: [https://www.inkmanworkshop.com/](https://www.inkmanworkshop.com/)  \nThank you!  \n\\-Eli",
    "created_utc": "2024-11-15T08:05:55",
    "num_comments": 0,
    "comments": []
},
{
    "submission_id": "1grxzrf",
    "title": "Badminton court detection using pytorch and resnet50",
    "selftext": "https://preview.redd.it/o4408rbjt21e1.png?width=3018&format=png&auto=webp&s=aa9134f589e537831d1ce12b8185386e47e8516d\n\nHi everyone,\n\nI'm new in computervision and I'm blocked during several hours on this problem.\n\nI try to detect 32 Keypoints on a badminton court : all lines intersections and the feet of the net. For that I used pytorch on a resnet50 pretrained model. I have a Dataset of annotates images and I train the model like that :\n\n    class KeypointsDataset(Dataset):\n        def __init__(self, img_dir, data_file):\n            self.img_dir = img_dir\n            with open(data_file, \"r\") as f:\n                self.data = json.load(f)\n            \n            self.transforms = transforms.Compose([\n                transforms.ToPILImage(),\n                transforms.Resize((224, 224)),\n                transforms.ToTensor(),\n                transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n            ])\n        \n        def __len__(self):\n            return len(self.data)\n        \n        def __getitem__(self, idx):\n            item = self.data[idx]\n            img = cv2.imread(f\"{self.img_dir}/{item['id']}.jpeg\")\n            if img is None:\n                raise ValueError(f\"Image {item['id']} could not be loaded.\")\n            \n            h, w = img.shape[:2]\n            img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n            img = self.transforms(img)\n    \n            # Ensure keypoints are consistent\n            kps = np.array(item['kps']).flatten()\n            if len(kps) != 64:  # Check that we have 32 keypoints (x, y)\n                raise ValueError(f\"Expected 64 keypoint values, but got {len(kps)} for item {item['id']}\")\n            \n            kps = kps.astype(np.float32)\n            kps[::2] *= 224.0 / w  # Adjust x coordinates\n            kps[1::2] *= 224.0 / h  # Adjust y coordinates\n    \n            return img, kps\n\n    train_dataset = KeypointsDataset(\"data/images\",\"data/data_train.json\")\n    val_dataset = KeypointsDataset(\"data/images\",\"data/data_val.json\")\n    \n    train_loader = DataLoader(train_dataset, batch_size=8, shuffle=True)\n    val_loader = DataLoader(val_dataset, batch_size=8, shuffle=True)\n\n    from torchvision.models import resnet50, ResNet50_Weights\n    \n    # Load the model with pretrained weights\n    weights = ResNet50_Weights.DEFAULT\n    model = resnet50(weights=weights)\n    model.fc =  torch.nn.Linear(model.fc.in_features, 32*2)\n\n    criterion = torch.nn.MSELoss()\n    optimizer = torch.optim.Adam(model.parameters(), lr=1e-4)\n\n    epochs=100\n    best_val_loss = float(\"inf\")\n    patience = 10  # Number of epochs to wait for improvement before stopping\n    trigger_times = 0\n    \n    for epoch in range(epochs):\n        for i, (imgs,kps) in enumerate(train_loader):\n            imgs = imgs.to(device)\n            kps = kps.to(device)\n    \n            optimizer.zero_grad()\n            outputs = model(imgs)\n            loss = criterion(outputs, kps)\n            loss.backward()\n            optimizer.step()\n    \n            if i % 10 == 0:\n                print(f\"Epoch {epoch}, iter {i}, loss: {loss.item()}\")\n    \n        # Validation Phase\n        model.eval()\n        val_loss = 0.0\n        with torch.no_grad():\n            for imgs, kps in val_loader:\n                imgs = imgs.to(device)\n                kps = kps.to(device)\n    \n                outputs = model(imgs)\n                loss = criterion(outputs, kps)\n                val_loss += loss.item()\n    \n        val_loss /= len(val_loader)\n        print(f\"Epoch {epoch}, validation loss: {val_loss}\")\n    \n        # Early Stopping Logic\n        if val_loss < best_val_loss:\n            best_val_loss = val_loss\n            trigger_times = 0\n            torch.save(model.state_dict(), \"best_keypoints_model.pth\")  # Save best model\n        else:\n            trigger_times += 1\n            if trigger_times >= patience:\n                print(f\"Early stopping triggered at epoch {epoch}\")\n                break\n\nWhen I called the model, I make the resizing back like this :\n\n    class CourtLineDetector:\n        def __init__(self, model_path):\n            self.model = models.resnet50(pretrained=False)\n            self.model.fc = torch.nn.Linear(self.model.fc.in_features, 32*2)  # Adjust for the number of keypoints\n            self.model.load_state_dict(torch.load(model_path, map_location='cpu'))\n            self.transform = transforms.Compose([\n                transforms.ToPILImage(),\n                transforms.Resize((224, 224)),\n                transforms.ToTensor(),\n                transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n            ])\n        \n        def predict(self, frame):\n            img_rgb = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n            image_tensor = self.transform(img_rgb).unsqueeze(0)\n            with torch.no_grad():\n                self.model.eval()\n                outputs = self.model(image_tensor)\n            \n            keypoints = outputs.squeeze().cpu().numpy()\n            original_height, original_width = frame.shape[:2]\n    \n            # Scale keypoints back to the original frame dimensions\n            keypoints[::2] *= original_width / 224.0\n            keypoints[1::2] *= original_height / 224.0\n    \n            return keypoints\n        \n        def draw_keypoints(self, image, keypoints):\n            # Plot keypoints on the image\n            for i in range(0, len(keypoints), 2):\n                x = int(keypoints[i])\n                y = int(keypoints[i+1])\n                if x is not None and y is not None:\n                    cv2.putText(image, str(i//2), (x, y-10), cv2.FONT_HERSHEY_SIMPLEX, 0.5, (0, 0, 255), 2)\n                    cv2.circle(image, (x, y), 5, (0, 0, 255), -1)\n            return image\n        \n        def draw_keypoints_on_video(self, video_frames, keypoints):\n            output_video_frames = []\n            for frame in video_frames:\n                frame = self.draw_keypoints(frame, keypoints)\n                output_video_frames.append(frame)\n            return output_video_frames\n\nBut, as you can see, the dot look kind of in the good position, but not in good scale.\n\nI already try several method in order to correct that like padded the picure before training (and after) in order to keep the correct scale... but that don't work either.\n\nMy original dataset is really simple with\n\n    [\n        {\n            \"id\": \"badminton_court_keypoints_167\",\n            \"metric\": 0.0,\n            \"kps\": [\n                [\n                    385,\n                    353\n                ],\n                ...\n            ]\n        },\n        ...\n    ]\n\nFuthermore, do you think there is a better way to achieve the same results, I'll be happy to understand better this field.  \n  \nThank you so much for taking the time to help me.",
    "created_utc": "2024-11-15T06:48:58",
    "num_comments": 1,
    "comments": [
        "Hi everyone,\n\nI finally found where my error came from.\n\nThe training and the detector was good but the problem came from... the beginning : the annotates images :-(\n\nI don't know why, but my training folder had the original size image, but I scaled everything for the annotations.\n\nI still want to know if there's a best way to achieve the same result :-)"
    ]
},
{
    "submission_id": "1grxt2z",
    "title": "Building a wardrobe inventory & virtual try-on app: which technologies are there? where to start from?",
    "selftext": "Hi, I am wondering if it is possible to build an app that allows users to take a picture of themselves in a mirror that automatically adds their clothing items into an inventory, ideally a photo of the item and tags such as clothing category, color, fabric etc. Then the user should be able to mix and match the items in the wardrobe and see the outfit on a realistic avatar of themselves. Is this possible from a technological standpoint? If so, could you recommend which concepts, resources etc. to look into? What kind of expertise would one need to develop this product? Please bear in mind that I have engineering background and coding experience but I don't know anything about computer vision itself.  \n\n",
    "created_utc": "2024-11-15T06:40:07",
    "num_comments": 1,
    "comments": [
        "Try On Clothes Virtually with AI using Kolors Virtual Try-On in the Wild\nhttps://youtu.be/9uS9Vfk8-N8"
    ]
},
{
    "submission_id": "1gru82f",
    "title": "Real-world challenges of AI-driven visual sensing  ",
    "selftext": "I am in the process of writing a survey paper that explores the real-world challenges of AI-driven visual sensing across various sectors, such as wearable devices (VR/AR/smart glasses), construction, mining, oil, robotics, retail, and more. My focus is on the limitations and constraints posed by camera technology in these applications. Any insights or contributions on this topic would be greatly appreciated!",
    "created_utc": "2024-11-15T03:25:00",
    "num_comments": 6,
    "comments": [
        "Very interested in this.\n\nI think an exhaustive evaluation of camera vs lidar for real world scenarios with emphasis on those that matter most and have various costs associated with it would be very interesting. \n\nFor e.g. while lidar provides depth for free, does the camera based AI + stereo vision / temporal stitching *really* drop the ball in critical scenarios or does real world data have sufficient information to atleast allow an actuation system to respond to such situations even if they aren't fully identified.\n\nElon's claims that lidar is a local minimum, needs evaluation no matter what people think of him.",
        "What are the sensor limitations you’re focusing on? Resolution, fps, bit depth, wavelengths detected?",
        "The challenges are mainly answering questions like: where am I? What is surrounding me? How my environment will change?\nTaking in account that there is not a deterministic answer.",
        "Great question! Let me clarify. I primarily approach the survey from the perspective of application scenarios. For instance, current depth sensing technologies—such as RGB-D, LiDAR, and even radar—can struggle with transparent objects like glass or plastic wrappers. This is just one example, and I'm eager to explore more such challenges across different sectors.",
        "That's interesting! Could you please provide more details about the motivations behind these requirements? Thank you!",
        "The main idea is that visual sensing for whatever reason is motivated because there aren't other sensors able to do the job. Because of that, there are a plethora of algorithms that manipulate visual information and measure the quantity of interest. As any sensor, you have to deal with uncertainty and error. So that motivate the first question: where am I? You can combine a lot of sensors to answer that question (my location determines how to know find other things to measure), but you have to deal with a tradeoff between being very expensive or dealing with more uncertainty. But even the cutting-edge stuff will have specific uncertainty.\n\nThe second thing, specially in weareables, VR and autonomous driving (in general, autonomous stuff moving around us) is how is the environment changing. (what is  surrounding me?) That question is harder to answer because the complexity of the world. You need to precisely determine depth and the location of nearby objects to avoid an user hitting his head and suing your company in the best scenario.\n\nLastly, let's imagine a delivery robot trying to cross a street. The algorithms will have to check if there is people in front of it, cars driving, and predict if it is safe (not only for the robot, but for everyone) to cross in specific time. Let's make it harder, for example a street without traffic lights, bad light conditions and erratic human behavior.\n\nNow let's imagine a human with an augmented reality glasses or headset or helmet or whatever will came, trying to cross the street but distracted in the improved social media algorithms to keep your attention kidnapped."
    ]
},
{
    "submission_id": "1grtygk",
    "title": "Quaternion rotation for each of skybox panorama views",
    "selftext": "I have a skybox panorama image ( 360 view in bottom/up/left/right/front/back view ). I also have the camera position and rotation vectors, and I've noticed that the rotation vector is for camera \"bottom\" view.\n\nI'd want, having the bottom view rotation vector, to calculate rotation vectors for all other views ( left/right/up/front/back ), but I'd like to start with \"left\" view. The problem is if I only manipulate y axis, and rotate it 90 degrees, objects that must be on the bottom are on the left side of the image rendered from camera perspective, and if I additionally rotate it 90 degrees on Z axis, objects that must be on the bottom are slightly on the right if it makes sense.\n\nAs I understand, it happens because the Z axis rotates with Y rotation, and it is not perfectly aligned now. Is there a way to properly calculate rotation for panorama view?\n\nPS. Sorry if I explained that poorly, I'll try to create example if what I'm saying does not make sence.",
    "created_utc": "2024-11-15T03:06:40",
    "num_comments": 2,
    "comments": [
        "I assume by \"rotation vectors\" you mean quaternions represented by axis multiplied by magnitude.\n\nYou need to think about the order of rotations, as they are not commutative. v\\*A\\*B meaning \"rotate by global A and then by global B\" but simultaneously it means \"rotate by local B and then by (new) local A\". So depending on the conventions of your math library, you need to write either v\\*A\\*B or v\\*B\\*A or A\\*B\\*v or B\\*A\\*v for the correct results.\n\n... At least that's for rotation matrices. If you're using quaternions, then the same principle applies but the basic rotation is something like A\\*v\\*A\\_inv instead of just A\\*v.",
        "Yeah I really don’t get what you are trying to say… Let’s start again with what you trying to do first, what your strategies are, and what is wrong with the result you have got. Go!"
    ]
},
{
    "submission_id": "1grtuwd",
    "title": "Theia: Distilling Diverse Vision Foundation Models for Robot Learning",
    "selftext": "",
    "created_utc": "2024-11-15T03:00:19",
    "num_comments": 0,
    "comments": []
},
{
    "submission_id": "1grt39l",
    "title": "Blog post: Use cases of AI in driverless cars.",
    "selftext": "This [blog post](https://www.opencv.ai/blog/ai-in-driverless-cars) explores how CV solutions are used in autonomous driving cars. It is not a technical post, but it can be useful for general knowledge. If you have more interesting Use cases or projects, please add them to the thread. It would be very useful to me. Thank you.\n\n",
    "created_utc": "2024-11-15T02:03:21",
    "num_comments": 1,
    "comments": [
        "Okay, nobody seems to get this: a driverless car, correct? The passengers are not occupied by the car, correct? So, what do you think the passengers are probably going to be doing? Think for a minute what a driverless car once the novelty of being a driverless car wears off... what is a driverless car when the novelty is gone? It's a mobile room. In many many cases it will be a mobile office where people are expected to work while in transit. So, the AI needs of that mobile office thing are everything an office and in some cases a work shop might need."
    ]
},
{
    "submission_id": "1grr89r",
    "title": "Edge detection using MultiThreading",
    "selftext": "Hi, I've been trying to implement Edge detection using convolution (sobel operator) in C and utilizing the pthreads library for multithreading but when I compare the execution time of the program with and without threads I'm getting a slower execution time for the one without threads. I've tried to reduce the number of threads so less overhead to create and manage them, Increased the image resolution but nothing worked.\n\nHere is my code\n\n    Image mul_convolution(int **input_image, Kernel k, int width, int height) {\n        int num_threads = 4; // Number of threads\n        pthread_t threads[num_threads];\n        ThreadData thread_data[num_threads];\n        int rows_per_thread = height / num_threads;\n    \n    \n        Image img;\n        img.width = width;\n        img.height = height;\n        img.data = (int **)(malloc(sizeof(int *) * height));\n        for (int i = 0; i < height; i++) {\n            img.data[i] = (int *)(malloc(sizeof(int) * width));\n        }\n    \n    \n        for (int i = 0; i < num_threads; i++) {\n            thread_data[i].input_image = input_image;\n            // thread_data[i].output_image = img.data;\n            thread_data[i].k = k;\n            thread_data[i].width = width;\n            thread_data[i].height = height;\n            thread_data[i].start_row = i * rows_per_thread;\n            thread_data[i].end_row = (i == num_threads - 1) ? height : (i + 1) * rows_per_thread;\n    \n    \n      thread_data[i].output_image = malloc((thread_data[i].end_row - thread_data[i].start_row) * sizeof(int *));\n            for (int j = 0; j < (thread_data[i].end_row - thread_data[i].start_row); j++) {\n                thread_data[i].output_image[j] = malloc(width * sizeof(int));\n            }\n    \n            pthread_create(&threads[i], NULL, thread_convolution, &thread_data[i]);\n        }\n    \n        for (int i = 0; i < num_threads; i++) {\n           pthread_join(threads[i], NULL);\n        }\n    \n    \n        return img;\n    }\n    \n    void* thread_convolution(void* arg) {\n        ThreadData* data = (ThreadData*)arg;\n        int **input_image = data->input_image;\n        Kernel k = data->k;\n        int width = data->width;\n        int height = data->height;\n        int start_row = data->start_row;\n        int end_row = data->end_row;\n    \n        for (int i = start_row; i < end_row; i++) {\n            for (int j = 0; j < width; j++) {\n                int sum = 0;\n                for (int m = 0; m < k.size; m++) {\n                    for (int n = 0; n < k.size; n++) {\n                        int x = i + m - k.size / 2;\n                        int y = j + n - k.size / 2;\n                        if (x >= 0 && x < height && y >= 0 && y < width) {\n                            sum += input_image[x][y] * k.data[m][n];\n                        }\n                    }\n                }\n                data->output_image[i - start_row][j] = sum;\n            }\n        }\n    \n        pthread_exit(NULL);\n    }",
    "created_utc": "2024-11-14T23:41:00",
    "num_comments": 2,
    "comments": [
        "Instead of using threads, you can try to parallelize the for loop. It would be more efficient.",
        "Also probably would be a lot easier with omp"
    ]
},
{
    "submission_id": "1groc4v",
    "title": "Papers on calibrated multi-view geometry for beginners",
    "selftext": "Hi all, I'm looking for some papers that are beginner-friendly (I am only familiar with basic neural network concepts) that discuss the process of combining multiple perspectives of a photo into a 3D model.\n\nIdeally, I'm looking for something that supports calibration beforehand, so that the reconstruction is as quick as possible.\n\nRight now, I need to do a literature survey and would like some help in finding good direction. All the papers I've found were way too complicated for my skill level and I couldn't get through them at all.\n\nHere's a simple diagram to illustrate what I'm trying to look into: [https://imgur.com/a/MJue7I2](https://imgur.com/a/MJue7I2)\n\nThanks!",
    "created_utc": "2024-11-14T20:31:57",
    "num_comments": 4,
    "comments": [
        "If you're looking for a non-neural network based method, I would suggest to read Multi View Geometry by Hartley, Zisserman. I would suggest to skip part 1 and directly go into part 2. The math is very elegant. It should be okay for a beginner with a background in linear algebra to grasp concepts. The terms you should be looking for are 3d reconstruction, calibrated 3d reconstruction, fundamental matrix, essential matrix, etc. Happy to share more inputs if needed.",
        "i think rather than reading papers you should read relevant materials from a course and do assignments to better your understanding. see: https://geometric3d.github.io/\nthis course follows Ziaserman, Hartley. another plus",
        "https://arxiv.org/pdf/2208.02946",
        "[https://grail.cs.washington.edu/rome/](https://grail.cs.washington.edu/rome/) is the one they used to use in all the course I know of."
    ]
},
{
    "submission_id": "1grn3p1",
    "title": "Best Image Inpainting methods to naturally blend objects",
    "selftext": "Hi Folks,\n\nI have a use case where I am given two images. For notations let's call IMAGE1 and IMAGE2. My task is to select an object from IMAGE1  ( by selection, I mean to obtain the segmented mask of the object ).  Place this segmented mask object naturally in IMAGE2, where a masked region is provided by the user. We have to ensure that the object from IMAGE1 should be naturally blended into IMAGE2. Can someone shed light on what might be the best model or group of models to do this?\n\nExample: Place a tree from IMAGE1 into IMAGE2 ( group of people taking selfie on a grassland)\n\n1. I have to segment the tree from image1   \n2. I have to place the tree in the potion highlighted or provide a mask in IMAGE 2.3. I have to take care of the light, angle, and vibe (like selfie mode, wide angle, portrait, etc). Context awareness   \nSmooth edge blending, Shadows, etc.\n\nDataset: For now, I choose to work on the COCO dataset. A subset of 60K images\n\nSince painting has many techniques, It's confusing which set of models I need to pipeline for my use case, which might give a good, realistic, natural image.\n\nI have explored the following techniques but could not settle on one strategy. \n\n1. Partial Convolutionals.  \n2. Generative Adversarial Networks (GANs)  \n3. Autoencoders. \n\n4. Diffusion Models\n\n5. Context-based attention models etc.\n\nThanks for checking on my post. Please provide some insights if you have some experience or ideas working on such use cases.\n\n",
    "created_utc": "2024-11-14T19:22:36",
    "num_comments": 2,
    "comments": [
        "if I understood this correctly, this is exactly what i'm doing in the image editor I work on. I use diffusion on the borders of the \"transplant\", here's a writeup [https://medium.com/@geronimo7/photo-editing-with-ai-93b653f49030](https://medium.com/@geronimo7/photo-editing-with-ai-93b653f49030)"
    ]
},
{
    "submission_id": "1grmxhe",
    "title": "Is there a way to get the plates off this car?",
    "selftext": "I was wondering if there’s anything that could make these plates clear in this video any help would be greatly appreciated",
    "created_utc": "2024-11-14T19:13:04",
    "num_comments": 8,
    "comments": [
        "Enhance",
        "We really need an automod to flag for review all post that have the world “car” and “plates” in it.",
        "What’s the story here?",
        "How do I use that?",
        "My apologies is this not allowed?",
        "I basically picked worst spot to park and a Volkswagen atlas swiped my front bumper. Learned my lesson but I want to find out who did it. I wasn’t in the car when it happened.",
        "It's just not what this sub is really for. So it's annoying when a somewhat niche area gets the signal to noise ratio diluted with these types of requests. \n\nNot necessarily your fault. But it would be a good idea to get more active moderation.",
        "This sub is there to help you to write a software to automatically detect number plates. It's not there to enhance single images. Especially if it's simply impossible due to bad image quality."
    ]
},
{
    "submission_id": "1grmt43",
    "title": "Interview with David Forsyth, Computer Vision Giant. He talks about the biggest problem in vision right now",
    "selftext": "",
    "created_utc": "2024-11-14T19:06:25",
    "num_comments": 6,
    "comments": [
        "That's a philosophical problem, it's not a technical problem.",
        "Where?",
        "[deleted]",
        "RemindMe! 5hours",
        "RemindMe! 5hours",
        "Right behind you",
        "I will be messaging you in 5 hours on [**2024-11-15 17:34:22 UTC**](http://www.wolframalpha.com/input/?i=2024-11-15%2017:34:22%20UTC%20To%20Local%20Time) to remind you of [**this link**](https://www.reddit.com/r/computervision/comments/1grmt43/interview_with_david_forsyth_computer_vision/lx95l1p/?context=3)\n\n[**CLICK THIS LINK**](https://www.reddit.com/message/compose/?to=RemindMeBot&subject=Reminder&message=%5Bhttps%3A%2F%2Fwww.reddit.com%2Fr%2Fcomputervision%2Fcomments%2F1grmt43%2Finterview_with_david_forsyth_computer_vision%2Flx95l1p%2F%5D%0A%0ARemindMe%21%202024-11-15%2017%3A34%3A22%20UTC) to send a PM to also be reminded and to reduce spam.\n\n^(Parent commenter can ) [^(delete this message to hide from others.)](https://www.reddit.com/message/compose/?to=RemindMeBot&subject=Delete%20Comment&message=Delete%21%201grmt43)\n\n*****\n\n|[^(Info)](https://www.reddit.com/r/RemindMeBot/comments/e1bko7/remindmebot_info_v21/)|[^(Custom)](https://www.reddit.com/message/compose/?to=RemindMeBot&subject=Reminder&message=%5BLink%20or%20message%20inside%20square%20brackets%5D%0A%0ARemindMe%21%20Time%20period%20here)|[^(Your Reminders)](https://www.reddit.com/message/compose/?to=RemindMeBot&subject=List%20Of%20Reminders&message=MyReminders%21)|[^(Feedback)](https://www.reddit.com/message/compose/?to=Watchful1&subject=RemindMeBot%20Feedback)|\n|-|-|-|-|"
    ]
},
{
    "submission_id": "1grjv9v",
    "title": "Fine-Tune Mask RCNN PyTorch on Custom Dataset",
    "selftext": "Fine-Tune Mask RCNN PyTorch on Custom Dataset\n\n[https://debuggercafe.com/fine-tune-mask-rcnn-pytorch-on-custom-dataset/](https://debuggercafe.com/fine-tune-mask-rcnn-pytorch-on-custom-dataset/)\n\nInstance segmentation is an exciting topic with a lot of use cases. It combines both object detection and image segmentation to provide a complete solution. Instance segmentation is already making a mark in fields like agriculture and medical imaging. Crop monitoring and tumor segmentation are some of the practical aspects where it is extremely useful. But in deep learning, fine-tuning an instance segmentation model on a custom dataset often proves to be difficult. One of the reasons is the complex training pipeline. Another reason is being able to find good and customizable code to train instance segmentation models on custom datasets. To tackle this, in this article, we will learn **how to fine-tune the PyTorch Mask RCNN** model on a small custom dataset.\n\nhttps://preview.redd.it/0wnxvw2tny0e1.png?width=1000&format=png&auto=webp&s=c70db631f4f9c10243ec29711be93183c45d2154\n\n  \n",
    "created_utc": "2024-11-14T16:35:37",
    "num_comments": 2,
    "comments": [
        "How about you stop reposting your months old posts in every ML-related subs?\n\nhttps://www.reddit.com/r/pytorch/s/keLk3Pv2nd",
        "Noted. Thanks for the feedback. Only wanted to make this reach more beginners in the industry. However, will try to find a better posting strategy."
    ]
},
{
    "submission_id": "1grc6e1",
    "title": "Reflectance-Based DIRetinex and Real-ESRGAN Image Enhancement Pipeline",
    "selftext": "Hi everyone!\n\nI built a pipeline combining a Reflectance-Based Deep Retinex model with Real-ESRGAN to enhance low-light images. The Retinex model separates the image into reflectance and illumination components, allowing us to adjust brightness and contrast based on predicted coefficients. This helps to improve visibility in low-light images while keeping details natural. After this, I thought eh that was kinda just recreating a paper. So, I tried improving it with Real-ESRGAN. It steps in to upscale the images, adding super-resolution for clearer, high-quality results.\n\nThe model has shown decent results in handling challenging low-light conditions by producing images with better visibility and refined details. If you're interested, I’ve shared the code here: [Project](https://github.com/JishnuSethuraman/ReflectanceBased-DIRetinex-RealESRGAN-ImageEnhancement-Pipeline).\n\nI still wasn't exactly able to reproduce the results from the paper [here](https://arxiv.org/abs/2404.03327). But the final image is clearer and with a lot less noise than even the ground truth at some points.\n\nHere's an example:\n\nhttps://preview.redd.it/yhbbywyfyw0e1.png?width=1200&format=png&auto=webp&s=071213fb53b0d8d39a4b53e6b8a373daf1f96d02\n\nI’d love any feedback or thoughts for improvement using this method.\n\nP.S. I'm only a grad student, take it easy on me xD",
    "created_utc": "2024-11-14T10:53:49",
    "num_comments": 1,
    "comments": [
        "Found [2 relevant code implementations](https://www.catalyzex.com/paper/arxiv:2404.03327/code) for \"DI-Retinex: Digital-Imaging Retinex Theory for Low-Light Image Enhancement\".\n\n[Ask the author(s) a question](https://www.catalyzex.com/paper/arxiv:2404.03327?autofocus=question) about the paper or code.\n\nIf you have code to share with the community, please add it [here](https://www.catalyzex.com/add_code?paper_url=https://arxiv.org/abs/2404.03327&title=DI-Retinex%3A+Digital-Imaging+Retinex+Theory+for+Low-Light+Image+Enhancement) 😊🙏\n\nCreate an alert for new code releases here [here](https://www.catalyzex.com/alerts/code/create?paper_url=https://arxiv.org/abs/2404.03327&paper_title=DI-Retinex: Digital-Imaging Retinex Theory for Low-Light Image Enhancement&paper_arxiv_id=2404.03327)\n\n--\n\nTo opt out from receiving code links, DM me."
    ]
},
{
    "submission_id": "1gr1vko",
    "title": "Custom Code for Precision, Recall, and Confusion Matrix for YOLO Segmentation Metrics?",
    "selftext": "Has anyone written custom code to calculate metrics like precision, recall, and the confusion matrix for YOLO segmentation? I have my predicted label files, but since I've modified the way I'm getting inference results, the default `val` function in Ultralytics doesn’t work for me anymore. Any advice on implementing these metrics for a custom YOLO segmentation format would be really helpful!",
    "created_utc": "2024-11-14T02:28:57",
    "num_comments": 0,
    "comments": []
},
{
    "submission_id": "1gqxl34",
    "title": "Increase accuracy pose estimation",
    "selftext": "I am struggling to find a pose estimation model that is accurate enough to estimate poses consistently for sports footage (single person, 30fps, 17 key points)\n\nDo you have any tricks/tips for video post processing to increase accuracy?\n\nThanks!",
    "created_utc": "2024-11-13T21:15:36",
    "num_comments": 5,
    "comments": [
        "Google MediaPipe maybe: Pose landmark detection",
        "You may give a try to [RTMPose](https://github.com/open-mmlab/mmpose/tree/main/projects/rtmpose#body-2d), it was trained on multiple datasets, not only on COCO, so might give good results in broader use cases",
        "Filtering might help",
        "When I was working on tracking keypoints over a video, I found that Savitzky-Golay filtering worked the best to smoothen the keypoints.",
        "Thank you! Good luck with your channel!"
    ]
},
{
    "submission_id": "1gqwmrf",
    "title": "LG Ultra sharp 40\" VS the world",
    "selftext": "I've looked around and haven't found one of the 5K monitors I'm interested in on display. The only retailer that carries anything anymore is Best Buy, and I live in LA. They do have the LG 45\" OLED  which is big and beautiful in person, although probably too curved, not much of a hub, and sold as a gaming monitor. The size is nice being tall AND wide! I'm not a gamer except for some FPV Drone Simulation on occasion. \n\nWhat I am is a MAC creative who works in photoshop, InDesign, Illustrator and a fair amount of Premier.  I'm looking for a combination of color accuracy, size (but not a fan of narrow 49\" monitors) and resolution. I'm currently on an Imac 27\" which is what I'm used to with it's 5K resolution, and sometimes text is hard to read. Because I have a 23\" sidecar monitor I can't mount a VESA and pull it close to my face when needed. However, I do prefer to keep the monitor a little further from my face for eyeball tanning sake. 5K resolution comes in real handy as I'm often using screen grabs.\n\nWhat I like about the Dell is the resolution, the hub with ample USB C ports, the ambient light sensor. But Dell is not a name I associate with computer monitors. I'm also a fan of OLED screens. My TV is an LG OLED and it's been sweet! I like the idea of the screen emitting the light rather than an array of LED's from behind. I see that LG has a 5K OLED coming 2025/26\n\nI am still debating between an M2 Studio Ultra or an M4 Mini if you'd like to chime in on that feel free. If I found a screamin' deal on a M2 Ultra studio i'd probably get that. This next computer will likely be a placeholder till the M4 Ultra/Studio or whatever Apple does next is released. So an M4 mini might have better resale when that time comes.   \n  \nSo with black Friday looming, is it worth the extra scratch for the Dell or LG 40\"?  Or would I be happy with an LG OLED 38\" or 45\"?",
    "created_utc": "2024-11-13T20:19:42",
    "num_comments": 3,
    "comments": [
        "r/lostredditors",
        "This sub is for camera image processing, not vision for computers lol",
        "I was about to abuse reddit for it's shitty recommendations."
    ]
},
{
    "submission_id": "1gqtd06",
    "title": "3D Mesh inner vertices ",
    "selftext": "I hope this question is appropriate here.\n\nI have a 3D mesh generated from an array using marching cubes, and it roughly resembles a tube (from a medical image). I need to color the inner and outer parts of the mesh differently—imagine looking inside the tube and seeing a blue color on the inner surface, while the outer surface is red.\n\nThe most straightforward solution seems to be creating a slightly smaller, identical object that shrinks towards the axis centroid. However, rendering this approach is too slow for my use case.\n\nAre there more efficient methods to achieve this? If the object were hollow from the beginning, I could use an algorithm like flood fill to identify the inner vertices. But this isn't the case.",
    "created_utc": "2024-11-13T17:28:05",
    "num_comments": 2,
    "comments": [
        "[deleted]",
        "Use culling if you’re worried about rendering being slow",
        "Thanks for replying!\nI'm using go.Mesh3d for rendering"
    ]
},
{
    "submission_id": "1gqmj8i",
    "title": "voyage-multimodal-3: all-in-one embedding model for interleaved screenshots, photos, and text",
    "selftext": "Hey r/computervision community — we built `voyage-multimodal-3`, a natively multimodal embedding model, designed to handle *interleaved* images and text. We believe this is one of the first (if not the first) of its kind, where text, photos, figures, tables, screenshots of PDFs, etc can be projected directly into the transformer encoder to generate fully contextual embeddings.\n\nWe hope `voyage-multimodal-3` will generate interest in vision-language models and computer vision more broadly.\n\nCome check us out!\n\nBlog: [https://blog.voyageai.com/2024/11/12/voyage-multimodal-3/](https://blog.voyageai.com/2024/11/12/voyage-multimodal-3/)\n\nNotebook: [https://colab.research.google.com/drive/12aFvstG8YFAWXyw-Bx5IXtaOqOzliGt9](https://colab.research.google.com/drive/12aFvstG8YFAWXyw-Bx5IXtaOqOzliGt9)\n\nDocumentation: [https://docs.voyageai.com/docs/multimodal-embeddings](https://docs.voyageai.com/docs/multimodal-embeddings)",
    "created_utc": "2024-11-13T12:22:54",
    "num_comments": 1,
    "comments": [
        "nice! do you plan on releasing the weights?"
    ]
},
{
    "submission_id": "1gqj9rg",
    "title": "Machine recommendation",
    "selftext": "I am confused between buying an M2 MacBook Air vs Mac mini M4 as one is portable and other is not. The external display would be needed wherever Mac mini goes.\n\nAccording to you, which will be beneficial in long-term, I have a Windows laptop that is 7 years old (it even froze when loading the python interpreter, and computer vision is kind of a long shot)\n\nI want to do computer vision, machine learning tasks, and software development. \n\nPlease write the reason the comments\n\n[View Poll](https://www.reddit.com/poll/1gqj9rg)",
    "created_utc": "2024-11-13T10:07:06",
    "num_comments": 9,
    "comments": [
        "You can almost certainly get the windows laptop you have as good as new with a fresh install of windows.\n\nLearning CV and software dev does not need a specially high spec machine. Unless you get to where you want to train neural nets, use CUDA and so on, but by then you'll know what hardware you need, or likely use cloud compute for training.\n\nIf you need a laptop or a desktop computer is really for you to decide. Do you sit at a desk with a display you already have, or do you sit on the sofa or on a train or in a café?",
        "Why are you limited to those two options? Mac mini is supposed to be stationary - you don't carry it with you.",
        "I have the m2 air and I am very happy about it despite it can only be connected to 1 external display at a time and I am used to connect to more than 1 external display to work. Also for neural network training I am connected to a workstation with a dedicated GPU trough SSH.",
        "you're going to have to make your own decision about the external display. I would always go for the faster machine. be aware that for ml training you will probably want high amounts of ram (the 32 gb ram option is basically required for anything that isn't a toy dataset), so if you're trying to save money on the mac mini... I think the cost benefit goes away when you add ram.",
        "Please provide a reason for that , that will very helpful"
    ]
},
{
    "submission_id": "1gqiz9i",
    "title": "Highest quality video background removal pipeline (built on top of SAM 2)",
    "selftext": "",
    "created_utc": "2024-11-13T09:55:29",
    "num_comments": 2,
    "comments": [
        "Haha, typical scam project trying to farm money on open source!",
        "Hey folks! We were looking for good video background removers but found that most of them sucked. Especially on complex scenes where videos would flicker or miss objects. So we built a new video background solution by combining SAM 2 (from Meta) and BiRefNet Lite (a more traditional foreground model). We use BiRefNet Lite to create an initial mask that is propagated by SAM 2.\n\nWe wrote more about it here and there’s a link to try it too: [https://www.sievedata.com/blog/high-quality-ai-video-background-removal-for-developers](https://www.sievedata.com/blog/high-quality-ai-video-background-removal-for-developers)\n\nWould love the community’s feedback :)"
    ]
},
{
    "submission_id": "1gqhxkm",
    "title": "Submit your presentation proposal for the premier conference for innovators incorporating computer vision and AI in products",
    "selftext": "Join our lineup of expert speakers and share your insights with over 1,400 product creators, entrepreneurs and business decision-makers May 20-22 in Santa Clara, California at the 2025 Embedded Vision Summit! It’s the perfect event for you to get the word out about interesting new vision and AI technologies, algorithms, applications and more. \n\n[https://embeddedvisionsummit.com/call-proposals](https://embeddedvisionsummit.com/call-proposals)",
    "created_utc": "2024-11-13T09:12:49",
    "num_comments": 0,
    "comments": []
},
{
    "submission_id": "1gqhk2e",
    "title": "Unsupervised Quantum ML Pipeline for Medical Image Segmentation",
    "selftext": "AI-assisted image segmentation techniques, especially deep learning models like UNet, have significantly improved our ability to delineate tissue boundaries with remarkable precision. However, these methods often depend on large, expertly annotated datasets, which are scarce in the real world. As a result, models trained on these datasets may struggle to generalize to new, unseen cases. \n\nThat's why we've been developing an unsupervised pipeline for medical image segmentation aimed at breast cancer detection. This approach leverages quantum-inspired and quantum methods to enhance precision and accelerate the segmentation process. We formulated the segmentation task as a Quadratic Unconstrained Binary Optimization (QUBO) problem and tested several techniques to solve the problem.\n\nThe results are promising, and our paper will soon be released on arXiv. Ahead of the release of the paper we created a video to showcase the solution: [https://www.youtube.com/watch?v=QQ4\\_9\\_dKZFY](https://www.youtube.com/watch?v=QQ4_9_dKZFY)\n\n  \nWe will post an update when the paper is published and the accompanying free lessons in our QML course, coming soon here: [https://www.ingenii.io/qml-fundamentals](https://www.youtube.com/redirect?event=video_description&redir_token=QUFFLUhqbHRNN1E1dlh4VUJueUhvMEZFNzlzNlJNbTNGZ3xBQ3Jtc0tsMVJoQW5lSEJGTGpteGMxdEUyRUJJeEJwdi1sQlpva2VjZVdKdWJwa19vTFdzOTNMVVlpZjBYelVfTmJ2NE5DeldEeFMtLVcwaW5qRHlLejlQaF9keVdKd29EM3hJTWNfMllKVnRPVXJ1clZXWWhFaw&q=https%3A%2F%2Fwww.ingenii.io%2Fqml-fundamentals&v=QQ4_9_dKZFY)",
    "created_utc": "2024-11-13T08:57:36",
    "num_comments": 2,
    "comments": [
        "I mean it's fantastic you've developed this pipeline, but isn't it kind of limiting to be quantum?  Unless you allow people access this tool remotely, but that comes with a ton of drawbacks and legal red tape since it's medical information.",
        "Thanks! We have proposed a formulation of medical imaging segmentation for breast cancer as a QUBO problem, that can be solved with many classical, quantum-inspired or fully-quantum methods. We explore the advantages and drawbacks of each of these methods, so that the end user can decide what methodology (and hardware) is more suitable for them. Also, access to quantum hardware is accessible on cloud by many platforms, such as D-wave, as we will show in the paper. \n\nI'll follow up with a link to the paper when it's published in the coming weeks!"
    ]
},
{
    "submission_id": "1gqaz0z",
    "title": "OCR for different documents ",
    "selftext": "I’m looking to build a pipeline that allows users to upload various documents, and the model will parse them, generating a JSON output. The document types can be categorized into three types: identification documents (such as licenses or passports), transcripts (related to education), and degree certificates. For each type, there’s a predefined set of JSON output requirements. I’ve been exploring Open Source solutions for this task, and the new small language vision models appear to be a flexible approach. I’d like to know if there’s a simpler way to achieve this, or if these models will be an overkill.",
    "created_utc": "2024-11-13T03:40:38",
    "num_comments": 0,
    "comments": []
},
{
    "submission_id": "1gqafgx",
    "title": "Which program to apply for master's in Europe?",
    "selftext": "I am currently in my final year of bachelor's in management information systems. I would like to apply to master's degree in Europe but I don't know where to start or how to choose. I will also need scholarship since the currency of my country is nothing compared to euro.   \n  \nAbout myself, I can say I have 3.5+ GPA and I had 2 months internship experience in object detection app development and currently having 3.5 months part time job experience in LLM and automated speech recognition model research and development. My main goal is to do my master's related to computer vision, object detection etc. but anything related to machine learning would also do. \n\nWhere should I apply? How can I find a program to apply? Is it possible for me to get a scholarship (tuition free + some funding for living expenses)? \n\n(ps. I'm not sure what flair to put for this, so I just put help theory)",
    "created_utc": "2024-11-13T03:04:38",
    "num_comments": 1,
    "comments": [
        "Europe is not a country, so the master programs, the admission, the requirements and all other things differ much from country to country.\n\nThe only general advice is: read the requirements, fullfill the requirements and speak the local language."
    ]
},
{
    "submission_id": "1gq9utd",
    "title": "Is There a way to get PhD supervisors to find you?",
    "selftext": "I have a graduate degree but I have managed to do many research internships over the past two years and have a good research background. I am working a full time job as a computer vision engineer at the moment and I want to go for a PhD. I have given a lot of time to finding PhD supervisors and reaching out to them. However, only very few reply back and all of them were to let me know that the supervisors are not looking for PhD candidates at the moment. The whole process is absolutely exhausting and I hardly have any time now.\n\n  \nIs there a way to get PhD supervisors to find me?",
    "created_utc": "2024-11-13T02:23:55",
    "num_comments": 21,
    "comments": [
        "I don't understand you did \"many\" research internships, yet you didn't put together any network?  Surely there's someone you've met that can at least refer you to someone.",
        "These profs are so busy working on the state of the art futuristic tech they don’t have time to find you. You approach the profs directly which is the wrong way to get into graduate school. The right way is to apply officially the university application form, pay the application fee, a board of professors look at your track record resume and decide if you are in. Few months later, if you are a top choice, you get an admission letter. The school then arranges for full day get to know your prof lectures, each prof presents his lab’s direction to the students, and students decide their , choices of which lab to go to. Each lab has different funding and will choose their candidates applicants based on how well interests fit. You are doing it the other way round, finding the prof first, this method only works if you inform the prof you have a scholarship that will fund your entire phd, hiring you will not cost the lab or school anything.",
        "Hard to give advice without knowing how you're approaching them.",
        "What area are you interested in? What country do you want to go to?",
        "I had done these research internships with private organisations and while I had been able to put together a network that had ultimately helped me in getting a job, I'm not so sure about this network is good enough to refer me for a PhD because if anything, through these internships, I realized how bad the research community here is in my country. So I've been trying to apply to universities out of the country.",
        "This makes sense but Im not sure if this is how it works for all countries? I refer the university websites and what they suggest is to reach out to the professors first and if a professor agrees to take you in, then apply",
        "This is very wrong if you are in Europe and UK. If you are in EU/UK you should definitely reach out to a prof before applying through the official portal, otherwise your application will most likely be dropped when it reaches a prioritisation meeting.",
        "I started with talking a little about their research and how their research interests align with mine, then one of the professors I reached out to suggested to create a research proposal before reaching out to supervisors, so I created a basic proposal and started reaching out with it",
        "Yeah, so you didn't talk to anyone in those organizations?  You were doing research entirely by yourself?  Without the guidance of a postdoc or PhD?  Nothing personal, but I'd be skeptical of that work experience then.",
        "Yeah this answer seems to be culturally specific. My process was:\n\n1. Reach out to professor’s \n2. Get in contact and learn about potential research projects\n3. Wait for admissions, where an acceptance means you have a high likelihood of professors you contacted selecting you to be part of their lab. \n\nthis is how it works for me.",
        "Yeah, in Switzerland for eg you have to reach out to specific professors,  I believe it's not the case in tbe US.",
        "That's approximately the right strategy. A few things to keep in mind:\n\n* The length of an email is inversely proportional to how likely they are to read it\n* Flattery does not work, it just makes the email longer.\n* If you say your research interests are \"deep learning and computer vision\" you will probably be ignored. You need to be much more specific than that.\n* Identify an open problem that is related to their work---preferably their \\*recent\\* work.\n* Identify whether you are wanting to work in an application domain or on general purpose methods. Use this to inform which prospective supervisors you approach.\n* Don't worry too much about highlighting GPA or class ranks. They don't actually care that much, as long these numbers are not completely terrible. They're much more interested in your ability to work on an open-ended project with more autonomy than a typical piece of coursework.",
        "Have you tried mailing new Assistant Proffs, they are more likely to reply that Associate or Full proffs.",
        "yeah for the most part, I've interacted with my supervisor there weren't PostDocs or PhDs involved to be honest. If you don't mind me asking what would make you skeptical of this work experience?",
        "why would he need a PhD or a postdoc to guide him if he himself knows what needs to be done and how research works?",
        "Yep, I've pretty much followed this, I've even applied to projects open to PhD candidates with a proposal but I never heard back from them. I guess I don't have a Masters' degree that could have contributed to me not getting any response",
        "I never thought of this....but I guess most assistant or ass professors dont have a good research profile as the full profs, I think thats why I overlooked them",
        "You do realize the distinction between masters and PhD right?",
        "I do, but I still don't get what you are getting at? you seem to believe in order to be able to do proper research, you have to have a PhD degree! which is absolutely not the case!",
        ">research internships \n\nNo institution would accept it as a research internship if it didn't have proper moderation.",
        "that's a fair point."
    ]
},
{
    "submission_id": "1gq9so2",
    "title": "SAM2 running in the browser with onnxruntime-web ",
    "selftext": "Hello everyone!\n\nI've built a minimal implementation of Meta's Segment Anything Model V2 (SAM2) running in the browser on the CPU with onnxruntime-web. This means that all the segmentation is done on your computer, and none of the data is sent to the server.\n\nYou can check out the live demo [here](https://sam2-seven.vercel.app/) and the code (Next.js) is available on GitHub [here](https://github.com/geronimi73/next-sam).\n\nI've been working on an image editor for the past few months, and for segmentation, I've been using [SlimSAM](https://huggingface.co/Zigeng/SlimSAM-uniform-77), a pruned version of Meta's SAM (V1). With the release of SAM2, I wanted to take a closer look and see how it compares. Unfortunately, [transformers.js](https://github.com/huggingface/transformers.js) has not yet integrated SAM2, so I decided to build a minimal implementation with onnxruntime-web.\n\nThis project might be useful for anyone who wants to experiment with image segmentation in the browser or integrate SAM2 into their own projects. I hope you find it interesting and useful! \n\nUpdate: A more thorough [writeup](https://medium.com/@geronimo7/in-browser-image-segmentation-with-segment-anything-model-2-c72680170d92) of the experience\n\nhttps://reddit.com/link/1gq9so2/video/9c79mbccan0e1/player",
    "created_utc": "2024-11-13T02:19:25",
    "num_comments": 9,
    "comments": [
        "Cool, good job, you live demo link does not work however!",
        "good job",
        "Nice!",
        "Wow quite impresive. Any chance you use GPU? Feels slow :(",
        "fixed! thanks\n\n[https://sam2-seven.vercel.app/](https://sam2-seven.vercel.app/)",
        "thanks!",
        "Changed my mind. Added webgpu support with fallback to CPU",
        "Unfortunately I'm looking for solutions without a GPU in this case. It should not be too hard (I guess) to switch to WebGPU but I want something that runs on as many devices as possible. Depending on WebGPU would currently restrict it to users running Chrome or Edge AND have a GPU. I agree, it's slow but I'm optimistic that there's still ways to make it faster: quantization (which I will try) and I hope the SlimSAM team will released a pruned version of SAM2, apparently they are [already working on it](https://github.com/czg1225/SlimSAM/issues/23). \n\nKeep an eye on the repo, I will try to put in any optimization I can find to make it faster. Note that I'm a total newbie to onnxruntime, that's my first try, god knows in what ways I fucked up there.",
        "I don't know how much more space it will use as I never used ONNXRuntime on web (it was in my pending list). But for x86 you can use some backends that would try to load if they are available if not default to CPU.\n\nIt's quite nice because whoever has the hardware and drivers will have the max speed. Is not only GPU but also NPUs in phones/tablets that can be loaded with the same code, just compiling it with more backends.\n\nI successfully did this in a previous project and it was able to run Windows, Linux, MacOS in their most performant backend when available with just one code base."
    ]
},
{
    "submission_id": "1gq8x9h",
    "title": "Texture segmentation ",
    "selftext": "Hey! \nI was searching for texture segmentation with neural networks and found nothing, not even a useful survey!!! Does anyone know how can i find one? I really can’t believe there’s no review paper on this topic.\nPs: I did find some codes on github using filter banks, I’m searching for a review paper to see which method is better and suitable for my thesis and then code it.\n",
    "created_utc": "2024-11-13T01:14:18",
    "num_comments": 0,
    "comments": []
},
{
    "submission_id": "1gq8q6g",
    "title": "Thoughts on pyimagesearch ? ",
    "selftext": "Especially the tutorials and paid subscription. Is it legit ? Is it worth it ? Do you recommend better resources ?\n\nThanks in advance.\n\n(Sorry I couldn't find a better flair)\n\nedit : thanks everyone for the answers. To sum them up so far : it used to be really good, but given the improvement or appearance of other resources, pyimagesearch's free courses are as good as any other course.\n\nThanks 👍",
    "created_utc": "2024-11-13T00:58:32",
    "num_comments": 8,
    "comments": [
        "In my opinion, no. If what you are trying to get is general image processing/ hands on opencv, then his content is no better than geeksforgeeks, or opencv content on youtube. I have read almost all of his blog, and they all seems to follow this youtube-hands on format: lots of code, just explaining the python code line by line (alot on the syntax) without much explanation on how to structure the code, nor how to use the functions, nor general theory in broader scope. This approach could do okay with Python and Opencv beginners, but it fares exceptionally bad when things get harder. His neural network and CNN content are kinda outdated too, you could find better content on Github, or the d2l.ai course which is completely free. \n\nAlso, the paid subscription is quite off-putting. I got onto his page and it said enter email to receive the code for the blog post, so I entered the email accordingly. The email came like 2 days later saying “sorry, but this content is for paid subscription only”. He than proceeded to mail me irrelevant course ads for the next 20 days which kinda irritated me mad. Yeah, felt like a bit of transparency issue there.",
        "back in the day, yes.  \ntoday, not so much.\n\nVast amounts of resources freely available everywhere for the subject now.",
        "In 2024 not worth it at all even back in the day it wasn’t worth it but many learning resources were also not easily available. There’s many free resources that are pretty well organized. Plus I think Adrian sold the company to someone else who runs it now.",
        "Not worth it. I did buy a couple of his books years back and they are ok but everything can be found on the web free or way cheaper than what they are charging now. Seems like pure greed at the moment where they even hide some of the articles and example code behind a paywall.",
        "Adrian was providing a good leading path to me when i first entered to the computer vision area. He teaches well, I don't know the current state because it's been while. If you are new to the area, he will definitely broaden your vision and make you in a state where you can do personal projects to yourself.\n\nI say all these by looking at my experience back then. It was all free. It did not buy its courses or the book. Maybe someone having more recent experience would provide better info",
        "Helped me a lot when I was learning. I don't think I'd get anything from it now except when I forget the specific syntax or order of operations for a specific method or something",
        "Always Liked his articles",
        "No."
    ]
},
{
    "submission_id": "1gq6tfz",
    "title": "Manual OCR - what level of dilation is best?",
    "selftext": "Hi, for a CV course I'm taking we're starting by learning about image processing, using an example reuters article. While playing around with dilation and erosion, I found a level of dilation which manages to keep good separation between each word, while also having each word be its own connected component.\n\nHowever, this comes with the exception of the letter lowercase i, which it detects the dot and the rest of the letter as separate words. I can enlarge the dilation kernel of course, but then there are entire strings of words which are viewed as a single component.\n\nWhich is generally better - over-separating or over-combining into separate components?\n\n[Here](https://i.imgur.com/tzACbHM.png) is our output for example, the real wordcount is 314 words, ours detected 519 components (where ideally 1 component = 1 word). Not ideal.\n\nOf course I can improve this outcome by dilating with a larger kernel, but I'm not sure that the number of components is necessarily the best metric, especially if it means multiple words get merged into a single component",
    "created_utc": "2024-11-12T22:35:07",
    "num_comments": 4,
    "comments": [
        "Feels like you are over-tuning for this specific problem. Honestly I would just go with whatever gives you best results. With a real variety of images there are loads of tiny decisions depending on your image set.",
        "Overseparation is generally better, because combining is easier than separating. You can use some heuristic to combine the dots of the i with the rest. For example, you could take the surface area of the dot as an indicator that it is a dot and then check the shape of the letter right underneath the dot\n\nOne hurdle here is that you're dealing with different text sizes, but maybe clustering could help you out here",
        "Hi what CV course are you taking?",
        "https://www.aiismath.com/"
    ]
},
{
    "submission_id": "1gpujtz",
    "title": "Need help for Object counting task",
    "selftext": "So, this is my first time delving into computer vision and working on a project as well. I have basic understanding of DL and digital image processing, took them as elective courses last sem. \n\nThe project is counting the number of pizzas made in a day at multiple restaurants through their CCTV cameras. The feeds are of various quality some are clear some are low quality, lighting conditions also vary a lil. I have about 2500 annotated images from their CCTV of pizzas and have trained on a pretrained ultralytics **yoloV8s**, but the accuracy isn't great, like after 25 epochs of training the class loss stays at 0.5, after that does not improve (maybe I wasn't running it for longer), and the model, when ran on a video from the test set, the result is pretty bad. I don't understand how I'm supposed to go on from here, use a bigger model? Are my hyperparameters are incorrect, if so, how do I find optimal ones? Is it cuz of insufficient data?  Any other way of going about doing it? Any help would be really appreciated, please help my dumbass. \n\nCan you guys give me insights on how you would approach this problem in the first place.",
    "created_utc": "2024-11-12T12:27:32",
    "num_comments": 1,
    "comments": [
        "Since your footage varies in quality and lighting, apply augmentations like brightness adjustments, rotations, etc., to help with generalization. YOLOv8s is lightweight but might not capture enough detail for tougher conditions, so try YOLOv8m or YOLOv8l if possible. If your loss is stuck, lower the learning rate or try more epochs, and a smaller batch size might help too. To address class imbalance, try focal loss to focus more on pizzas if there’s a lot of background in your images. 2500 images may be a bit low for varied conditions, so if possible, get more data or use synthetic data. You could also consider combining detection with tracking to help with counting, even if detection sometimes fails. Hope this helps!\n\nYOLO itself is mainly for object detection and doesn’t track objects across frames. It detects objects independently in each frame, so if you need to count pizzas over time in a video, you’ll need a tracking algorithm on top of YOLO. You can combine YOLO with trackers like DeepSORT, ByteTrack, or a Kalman filter with optical flow to create continuous tracks for each detected pizza."
    ]
},
{
    "submission_id": "1gpui26",
    "title": "CV Experts: what parts of your workflow have the worst usability?",
    "selftext": "I often hear that CV tools have a tough UX - even for industry professionals. While there are a lot of great tools available, the complexity of using them can be a barrier. If the learning curve were lower, CV could potentially be adopted more widely in sectors with lower tech expertise, like retail, agriculture, and small-scale manufacturing.\n\nIn your CV workflow, where do you find usability issues are the worst? Which part of the flow is the most challenging or frustrating to work with?\n\nThanks for sharing any insights!",
    "created_utc": "2024-11-12T12:25:29",
    "num_comments": 11,
    "comments": [
        "The \"Catch 22\" of Computer Vision -- There's no way to know the latency/accuracy tradeoffs of your system architecture without actually building it.",
        "Most of industry and business opportunities are literally just making open source tools actually usable.\n\nFrom a development standpoint, deployment is awful.\n\nFrom a user standpoint, everything is awful.",
        "Oh that’s easy. It’s whatever cobbled together process I built six projects ago and need to reuse now! That’s the one with the worst workflow. ",
        "Depends on what you’re doing with it. But if you’re trying to create a low-code, no-code workflow with trained models for laypeople there’s probably a few areas that are getting to maturity from a technological standpoint and may be worth creating a more user friendly workflow for: object detection, keypoint detection, keypoint tracking, human pose detection, person identification, optical character recognition, text recognition, etc. \n\nNote that the cloud platforms and mobile os’s have relatively easy to use (though in the case of cloud providers, expensive) api’s for most this stuff so you’re really talking about a gap in the market that is low-code no-code.\n\nIf you’re talking about training models, the bottleneck is labelling and training time, there are some very nicely designed solutions for labelling (label-studio) but the problem is time. \n\nDeployment of models is another area that is receiving attention as of late, you can check out the recent developments in mlops solutions and see if there’s anything you feel there is a gap in. \n\nOne thing that is universal across ml is what you do with the data after you get your predictions. How are you going to store it, use it, present it, etc. this is an area that ux can improve, but it’s going to be very use case specific what the best user flow will be.",
        "The biggest issue I see is people not knowing how to approach a problem. Do they need detection? classification? segmentation? a lot of people don't realise the tradeoffs.",
        "RemindMe! 1 day",
        "[removed]",
        "I hate when someone pitches an idea in a meeting and then immediately starts throwing around how fast the algorithm should run.",
        "I will be messaging you in 1 day on [**2024-11-15 12:13:32 UTC**](http://www.wolframalpha.com/input/?i=2024-11-15%2012:13:32%20UTC%20To%20Local%20Time) to remind you of [**this link**](https://www.reddit.com/r/computervision/comments/1gpui26/cv_experts_what_parts_of_your_workflow_have_the/lx2q1it/?context=3)\n\n[**CLICK THIS LINK**](https://www.reddit.com/message/compose/?to=RemindMeBot&subject=Reminder&message=%5Bhttps%3A%2F%2Fwww.reddit.com%2Fr%2Fcomputervision%2Fcomments%2F1gpui26%2Fcv_experts_what_parts_of_your_workflow_have_the%2Flx2q1it%2F%5D%0A%0ARemindMe%21%202024-11-15%2012%3A13%3A32%20UTC) to send a PM to also be reminded and to reduce spam.\n\n^(Parent commenter can ) [^(delete this message to hide from others.)](https://www.reddit.com/message/compose/?to=RemindMeBot&subject=Delete%20Comment&message=Delete%21%201gpui26)\n\n*****\n\n|[^(Info)](https://www.reddit.com/r/RemindMeBot/comments/e1bko7/remindmebot_info_v21/)|[^(Custom)](https://www.reddit.com/message/compose/?to=RemindMeBot&subject=Reminder&message=%5BLink%20or%20message%20inside%20square%20brackets%5D%0A%0ARemindMe%21%20Time%20period%20here)|[^(Your Reminders)](https://www.reddit.com/message/compose/?to=RemindMeBot&subject=List%20Of%20Reminders&message=MyReminders%21)|[^(Feedback)](https://www.reddit.com/message/compose/?to=Watchful1&subject=RemindMeBot%20Feedback)|\n|-|-|-|-|",
        "FYI, this account purely exists to promote oslo.vision.",
        "A company representative doesn't disclose their affiliation in a promo = instant blacklist for shitty practice"
    ]
},
{
    "submission_id": "1gpt61q",
    "title": "Question for labeling",
    "selftext": "Hello all, I am new in the whole annotating, or even training models for computer vision, so I'd appreciate some feedback. I am annotating some objects that are quite large. I tried making tight bounding boxes, but I am afraid there is also too much background due to the size of these. So would it be better if I made like 8-10 smaller boxes to cover the entire object, instead of 1 big bounding box? I usually create many smaller pieces if there are other objects in front, blocking an object, but I am not sure if it would be wise in this case\n\nhttps://preview.redd.it/qby4677avi0e1.png?width=1058&format=png&auto=webp&s=8dafbe354d68eb5d8ed1f1b273c833166b99ebc9\n\n",
    "created_utc": "2024-11-12T11:30:34",
    "num_comments": 6,
    "comments": [
        "If you think background is too much, then segmentation is the way to go but not small boxes to cover the whole. Cuz it will get complicated later to calculate metrics.",
        "you can use label-studio or label-me to create polygons surrounding the object or just tiltted bounding box to cover only the object.",
        "Thank you, I will look into segmentation although I never used this method before, time to educate myself 😅",
        "I already use label studio but didn't see such an option, I guess I didn't search enough 🥲 I will look for some videos, thanks 😊",
        "In label studio there's an option to plot points to create polygons. I guess it'll help."
    ]
},
{
    "submission_id": "1gpsud2",
    "title": "OpenCV Cpp can't load image",
    "selftext": "I've looked up the Error before but no post I found was able to help me.\n\nI have a file, called \"map.png\" in my folder. Let's say \"C:/Folder/map.png\".\n\nFor demonstration I made a simple project. This is all of the code: [https://pastebin.com/wp0YyiLr](https://pastebin.com/wp0YyiLr)\n\nYet when I try to run it I get the error\n\n\\[ WARN:0@0.060\\] global loadsave.cpp:241 cv::findDecoder imread\\_(''): can't open/read file: check file path/integrity\n\nError: Could not load the image.\n\nYet the image itself is completely fine and can be read without opencv\n\nPS: It does find the image, in the code it only states \"map.png\" but it really is \"C:/Folder/map.png\", that doesn't change anything though",
    "created_utc": "2024-11-12T11:17:26",
    "num_comments": 5,
    "comments": [
        "So, I have no idea why, imread still doesn't work, but I circumvented it by using a filestream instead",
        "Does it work when you put the image in the working directory?",
        "I remember that opencv documentation states that some image formats require additional libraries. I think png is always supported, but I’d double check it just in case:\n\nhttps://docs.opencv.org/3.4/d4/da8/group__imgcodecs.html\n\nEdit: have you tried adding “./“ at the beginning of the path?",
        "Sadly no. I now completely use filestreams to use opencv, which does work so it has to be a problem with the imread function itself"
    ]
},
{
    "submission_id": "1gpskud",
    "title": "Create Street map from aerial image.",
    "selftext": "The image is binary,  in this image I see r roads that wander in different directions and intersect.\n\nI'm for a software solution that will take an image like this, Identify each pathway, and label them.  Presumably it will be easy to calculate the length of each street, once the identifying process is completed.\n\n  \nThoughts welcome\n\nhttps://preview.redd.it/vi5llpvalh0e1.png?width=258&format=png&auto=webp&s=c23d867816f57344689428eac1932e254323a414\n\n",
    "created_utc": "2024-11-12T11:06:43",
    "num_comments": 4,
    "comments": [
        "So you basically want to detect solid color lines, you’re not actually using photographs right? ",
        "Could you please post some actual photos of the fibers? The exact kind of photo you would need to run the algorithm on.  I did come up with some code (using CharGPT, so it’s not great) that could work with the sketches, but it would definitely not work with a real photo. Maybe it could be combined with an AI model that segments the fibers. But what would probably work best is an AI model that directly outputs the coordinates of a line that follows the middle of each fiber…but I doubt you want to learn how to develop that! It would be like this model:\nhttps://github.com/uw-loci/collagen-fiber-metrics",
        " This is a machine vision application,  I'm looking at fibers, and trying to generate a table of fiber lengths.  It dawned on me that these images look like street maps, and assume someone has worked on a system to take an image like the one I sent and break it into segments, as if they were streets.  I have a few ideas, but just because I'm old does not make me smart.  In this image I see 4 fibers.  Need to figure out how to get the computer to see the same.",
        "Ah I see. \n\nI’ll try to come back later with a more detailed reply, but “contour” detection is a place to start. Or at least it’ll probably be a step of the process. \n\nhttps://docs.opencv.org/3.4/d4/d73/tutorial_py_contours_begin.html\n\nAnd here’s an AI generated response:\n\nTo detect curved lines in a sketch using OpenCV, you can follow these modified steps:\n\n1. **Read Image**: Load your image file into OpenCV.\n2. **Convert to Grayscale**: Change the image to grayscale to simplify the analysis.\n3. **Apply Gaussian Blur**: Blur the image slightly to reduce noise and smooth the image.\n4. **Detect Edges using Canny**: Use the Canny edge detector to find edges in the image.\n5. **Find Contours**: Use `findContours` to detect contours from the edge-detected image. Contours can represent curved lines.\n6. **Approximate Contours**: Optionally, use `approxPolyDP` to approximate the contours to simpler shapes or to smooth the detected curves.\n7. **Filter Contours**: Filter out the contours based on criteria such as contour length or area to focus only on significant curves. \n\nThis approach helps in identifying and isolating curved lines and shapes from the sketch."
    ]
},
{
    "submission_id": "1gppyuc",
    "title": "Action Recognition for Abuse Detection.",
    "selftext": "So I'm wokring on this project to detect abuse in public places(schools), I curated a clean dataset segregating into hitting, fighting and pushing and neutral, I tried to fine-tune a vision transformer like VideoMAE because it performed really well on Kinetics but the predictions are going horribly wrong. Are there any techniques or key points I should make sure before I finetune the model. Need some basic suggestions to build by model to perfection.  Any help would be great. Thanks!",
    "created_utc": "2024-11-12T09:20:41",
    "num_comments": 7,
    "comments": [
        "Months ago I was also trying to train video classification models and I found that these models need a lot of data to get even an average score.",
        "Can you use audio signal too?",
        "I'm working on one as well and man training it even for simple things has been a slog. Glad to see I am not the only one",
        "I was able to curate a dataset with around 3500 videos with proper segregation of actions but yea still no use, would you suggest something in lines of mixing openpose and action recognition models.",
        "The videos I get don't have audio yet(on which the models are running), so the main criteria is to detect the action",
        "I do not have much success experience in these so I can not say for sure. :)"
    ]
},
{
    "submission_id": "1gpoukj",
    "title": "A complete guide on how to extract text from a board or on paper",
    "selftext": "",
    "created_utc": "2024-11-12T08:34:47",
    "num_comments": 0,
    "comments": []
},
{
    "submission_id": "1gpong0",
    "title": "Need Advice and Resources for Interview Preparation: Research Position in Machine Learning and Deep Learning",
    "selftext": "Hi everyone!\n\nI’m a sophomore in college preparing for an interview for a research position in **machine learning** and **deep learning** with a focus on **artificial societies**. I’ll be working with a team mostly composed of PhD and Master’s students in computer science, so I’d love to come as prepared as possible.\n\n**A bit about my background:**\n\n* **Project experience**: Voice gender classification, UNET-based image segmentation for lunar crater detection, and Traveling Salesman Problem (using Simulated Annealing).\n* **Research interests**: Primarily in deep learning and computer vision applications.\n\nI’d appreciate any advice or resources! Specifically, I’m looking for:\n\n1. **Interview tips**: What should I focus on for research-oriented ML roles, especially when working with advanced researchers?\n2. **Key concepts**: Are there technical/theoretical ML or deep learning topics that are especially important for a research setting?\n3. **Recommended resources**: Any must-read papers, books, or courses to help me prepare?\n\nThanks so much for any advice or insights you can share!",
    "created_utc": "2024-11-12T08:26:40",
    "num_comments": 0,
    "comments": []
},
{
    "submission_id": "1gpoagz",
    "title": "Tools for Person-Detection and Tracking with IDs",
    "selftext": "I'm currently planning a project in which we will analyze social interaction features based on videotaped structured observation measures.\n\nFor keypoint extraction / pose estimation, I intend to use MMPose. As far as I'm concerned, the JSON output from MMPose does not include any data that could be used to identify and consistently track the depicted people (please correct me if I'm wrong). Since the videos include tester, children, and their parents, I will need to create IDs to properly analyze the keypoints, to link observations from frame to frame, and to be able to focus on / exclude individuals from the data. I'm a bit overwhelmed by the various approaches that seem to exist for object detection / tracking.\n\nWhat is the best method to achieve this task?",
    "created_utc": "2024-11-12T08:11:48",
    "num_comments": 5,
    "comments": [
        "See [Bytetrack ](https://github.com/ifzhang/ByteTrack)",
        "Checkout the supervision library, it has builtin functions for such use cases.",
        "Thanks a lot! This has been a great recommendation."
    ]
},
{
    "submission_id": "1gpo2vs",
    "title": "Crowd counting without ML/DL",
    "selftext": "I have some images that I have annotated of people on the beach. I want to count the number of people on the beach using basic operations. I have some preprocessing techniques on mind like CLAHE. This is a project for my school, of course I don't want any solutions, just want some interesting ideas on how this can be done without using any ML/DL. Thanks.\n\nEdit: I added an example image.\n\nhttps://preview.redd.it/touhzhvb9o0e1.jpg?width=1920&format=pjpg&auto=webp&s=b44266cbec5460d3e6bad6b75eed57dd1fe744d3\n\n",
    "created_utc": "2024-11-12T08:02:45",
    "num_comments": 4,
    "comments": [
        "Impossible to say without seeing the images.",
        "You could maybe do something like SURF or SIFT features, then fit bag of words model using your annotated images and count the number of occurrences of visual \"words\" which might sort of correlate with the number of people.\n\nKeep in mind that this would be nowhere near as accurate or generalizable as even a basic CNN object detector, which is why researcher into this type of thing has stopped entirely after 2013.",
        "[deleted]",
        "Thanks, I added an example image to the post.",
        "This is a very difficult one, since there’s so much going on in this scene. The people and all the beach utensils are so far away that it’s virtually impossible to construct meaningful features to disgusting the two. Maybe you can try something like a high-pass filtering and then apply an empirical model that estimates how many blobs in the image are actual humans."
    ]
},
{
    "submission_id": "1gpnckm",
    "title": "Best real time models for small OD?",
    "selftext": "Hello there!\nI've been working on training an object detector for small to tiny objects.\nWhat are the best real-time or semi-real time models/architectures in your experience?\nI'd love some pointers too boost the current performance I reached.\nNote: I have already evaluated all small yolo versions from ultralytics (n & s).",
    "created_utc": "2024-11-12T07:31:27",
    "num_comments": 25,
    "comments": [
        "Real time could mean 5 FPS or 60 FPS. What is your expectation?\n\nWhat's the available hardware? Yolo11n can run 20 FPS (400x400) on Core i5 CPU, for example, but not on Raspberry Pi3.\n\nRegarding YOLO models, you can make them sub-nano by removing backbone layers in the config. At the cost of accuracy, of course. You also need to convert them to the target platform to get the best latency.",
        "What resolution are you processing at? Did you try SAHI or some similar slicing based method?",
        "If you have an Orin AGX (as mentioned in another reply) you can actually run up to YOLO V10 M with a 1280x1280 input size in greater than 30 FPS. The key is you need to have as much running on your GPU as possible. Ultralytics is good for getting a working model quickly, but their performance (especially on Jetson platforms) leaves some to be desired.\n\nYou can check out this example: [https://github.com/justincdavis/trtutils/blob/main/examples/impls/yolo.py](https://github.com/justincdavis/trtutils/blob/main/examples/impls/yolo.py)  \nWhich showcases how to do end2end inference. I would recommend using only V10 models since they remove the NMS operations. \n\nIf you use the above library, make sure to export the V10 model to ONNX from the official V10 repo and then build the TensorRT engine using trtexec (should be already present on your Orin). I measured \\~25ms end2end time on my Orin AGX for V10 M with 1280x1280, but as low as \\~11ms for V10 N 1280x1280.",
        "What kinds of objects do you have?",
        "FOMO based architecture works well. Check out: https://arxiv.org/pdf/2311.07163",
        "Thank you for the input.\nLet's say 30 FPS as a minimum.\nHardware is a jetson Orin AGX.",
        "How 5 fps could mean real time?",
        "At the moment, quite low: 740×740.\nSAHI is a potential method. The question is which model to use?\nFor example, would a transformer based architecture surpass a convolutional one for small objects at the same image resolution ?",
        "Noted for the speed.  \n11ms at 1280x1280 is quite impressive on a jetson.  \nin your experience, any other model speed benchmark on jetson for OD?  \nBtw, there as a link for a table with the different inference speeds for yolo on jetson, or am I wrong ?",
        "checking if a worker in a factory left a tool behind. Like a screw driver or a pen for example.",
        "I am getting hyped up from the name alone...",
        "will do.  \nhave you tried it ?  \nhow does it compare to other famous architectures (like yolo)?",
        "On Jetson Orin you can run yolo11n inference every ~5ms (or even faster with batch) at 400x400 resolution. You need to export to onnx and use specialized onnx-runtime from [here](https://elinux.org/Jetson_Zoo#ONNX_Runtime). Use TRT execution provider.\n\nDon't know if it is good enough for specifically small objects, but in my experience with FasterRCNN, SSD (various backbones) and YOLO (v5+) the latter wins on all tasks.\n\nMaybe Unet can be better with very small objects, but it is segmenting rather than detecting. And is also much slower I think.",
        "Depends on the use case is what I meant. If you need to make decisions once every second then even 2 FPS is real-time.",
        "I don't think transformer based models would be faster than cnns",
        "It really depends on so many factors. The most important one is that you train on data that resembles what you’re inferring on. Make sure the training samples take up the same proportion of the image , same number of pixels. ",
        "NVIDIA may provide some tables of benchmarks across various Jetsons with a few different models. The library I linked does not have anything like that, but maybe it is something that should be added...",
        "Yeah I co-authored that paper. Albeit that paper is about tiling approaches for detecting small objects, FOMO has a few kinks but kinda works. It is much smaller than YOLO which is what counted for us. Check table 1 for more info.\n\nBut if you do tiling i.e. allowing higher input resolution for the CNN, then YOLO will get you better performance as in: https://arxiv.org/pdf/2410.16769",
        "Yes exactly. Two stage detectors are too slow here and a bit outdated.\nI am just curious if there is an architecture that excels in small OD.",
        "fastervit",
        "yes of course.  \njust looking if there is an architecture that excels for small object!",
        "Most probably I saw it somewhere else then. I thought i clicked on a link from this thread.",
        "ever tried it for smaller object detection?",
        "What proportion of the image do these objects take up? "
    ]
},
{
    "submission_id": "1gpm5do",
    "title": "Enhance Six Dof Localization",
    "selftext": "I am working on an augmented reality application in a know environment. To do so, i have two stages, calibration and live-tracking. In the calibration i got as input a video of a moving camera, from which i reconstruct the point cloud of the scene using COLMAP. Still during this process, I associate to each 3d point a vector of descriptors (each taken from an image where such points is visible). During live phase, i should be able to match such pointcloud a new image (from the same environment). At the moment i initialize the tracking using the same frames from the calibration, I perform some feature matching from the live image with some of the calibration ones, and drag the 3d points id onto the live frame then use solvePnp to recover camera pose. After such initial pose estimation, i project the cloud on the live frame and match the projected points to the keypoints in a radius. Then refine the pose again with all the matches. The approach is very similar to what is described in the tracking part of ORB-SLAM paper. I have two main issue:\n\n1) it is really hard to perform the feature matching between the descriptors associated to the 3d point and the live frame. The perspective/zoom difference might be significant and the matching sometimes fails. I have tried SURF and Superpoint. Are there any better approaches than the one i am currently using? better feature?\n\n2) my average reprojection error is around 3 pixels, even if i have more than 500 correspondances. I am trying to estimate simultaneously 3 params for rotation, 3 for translation, zoom and a single distortion coefficient model (tried with 3 but it was worse). Any idea to improve this or it's a lost battle? the cloud has an intrinsic reprojection error of 1.5 pixel on average",
    "created_utc": "2024-11-12T06:37:55",
    "num_comments": 3,
    "comments": [
        "Have you tried Dino?",
        "No, just discovered the existance. I am not quite an expert of deep features techniques, i just used superpoint with a standard knn matcher because they were easy to integrate. Do you think the performances might rise just by having a better 2d matches?"
    ]
},
{
    "submission_id": "1gpm4et",
    "title": "[ Traffic Solutions ] Datasets and model for transportation ",
    "selftext": "Traffic monitor systems \n\nSource code and datasets have available on my Github.\n\nhttps://github.com/Devision789\n\nE-mail: forwork.tivasolutions@gmail.com\n#cctvsolution\n#TrafficChallenge\n#motorcycle",
    "created_utc": "2024-11-12T06:36:39",
    "num_comments": 3,
    "comments": [
        "Do you have yolo license for your business pal =)))))",
        " I understand that you're a CEO and very busy.\n\n[https://github.com/Devision789/CameraAI/tree/main/camera\\_AI\\_systems](https://github.com/Devision789/CameraAI/tree/main/camera_AI_systems) \n\nBut please write in English and fix your link."
    ]
},
{
    "submission_id": "1gpksoc",
    "title": "Labeling tool for object connectivity ?",
    "selftext": "I'm working on a project where I have to build certain connectivity between objects. 90% of the automatic connection is build correctly. However, there is 10% wrongly made and I need to correct them manually. \n\n  \nDo you have an idea of a labeling tool to allow me correct connections. I tried to use label-studio connection but they don't offer a possibility to select connection and you have to run on the entire image (waste of time). \n\nI tried to convert my objects to SVG and use Inkscape but the app crash on heavy connections. \n\nWe can imagine them as graphs with spatial information. I just need an app allow me to delete connections and draw new ones. \n\n\n\nhttps://preview.redd.it/1uinxwp63h0e1.png?width=1228&format=png&auto=webp&s=01fb580d4758fc22489a2e8184f4e1700b752a15\n\n",
    "created_utc": "2024-11-12T05:32:44",
    "num_comments": 0,
    "comments": []
},
{
    "submission_id": "1gpk7su",
    "title": "Frameworks for Real-time Object Detection: MMdetection or alternatives?",
    "selftext": "Hello everyone.\n\nI am currently working on the development of a real-time object detection model to be integrated into an app for commercial use. Therefore, I cannot use any of Ultralytics' YOLO models.  \nMy plan is to explore SOTA architectures, established ones and probably vision transformers. So that leads me to MMDetection, which I have already tried. However the models that I have trained using this framework (nothing too big, for example RetinaNet with a MobileNet backbone) are extremely slow, with inference times using the cpu around 500ms which is a dealbreaker for real-time use in mobile. Even converting to ONNX and quantizing it, the times are still too large.\n\nHas anyone else had the same problem? What other suggestions do you have? Thanks for your help!",
    "created_utc": "2024-11-12T05:03:00",
    "num_comments": 7,
    "comments": [
        "Try rtdetrv2,it’s Apache licensed",
        "You can try the EfficientDet family, created by the TFLite model maker, tends to be fast on CPU bound system. Otherwise you might want to look at SSDs with a small fixed size or similarly something like FasterRCNN when it's input size is limited can be got down to around 30ms on CPU. You've just got to be happy with the accuracy trade off though, so be warned.",
        "Another issue to think about is the preprocessing of the video feed and the input resolution. Since you are using a CPU even downscaling images can take some time. I had that problem when working with video input as well. Also I wouldn’t recommend to use transformers since the encoding of the image can take time.",
        "If you are looking at an alternate framework, please explore PaddleDetection.... Their GitHub repository is well maintained and have SOTA pretrained models....Transformers are part of the framework....PP-Yolo can be a good alternative to all the Yolo architectures....",
        "YOLOX, RTMDet, YOLO-NAS",
        "For inference on mobile (CPU), I think you need to work with YOLO since it offers huge speed up compared to other architectures.   \nUltralytics is a framework that presents an implementation of YOLO but doesn't own it. You can find other implementations with MIT license like [https://github.com/WongKinYiu/YOLO](https://github.com/WongKinYiu/YOLO)  \nOf course, this will not offer the same features as Ultralytics. So, be prepared to put some effort into it"
    ]
},
{
    "submission_id": "1gpiwmu",
    "title": "🕰️ Turn Modern Websites into 90s Style Using AI — Cozy Retro Hack with $1.5K in Prizes",
    "selftext": "",
    "created_utc": "2024-11-12T03:48:27",
    "num_comments": 1,
    "comments": [
        "Build a tool that transforms modern websites into authentic 90s designs! You can use AI, rule-based systems, or both.\n\n**Prizes:**\n\n* 1st: $1,000\n* 2nd: $300 + JetBrains All Products Pack\n* 3rd: $200 + Mistery gift\n\n**Quick Details:**\n\n* 72-hour hackathon (Dec 13-16)\n* Solo or teams of up to 5\n* Use any tech stack\n* Judged by industry experts from Samsung, Accenture & Terra Quantum\n\nJoin us in bringing Web 1.0 aesthetics back to life! Register by Dec 13.\n\nRegistration form: [https://neuronostalgia.raptors.dev](https://neuronostalgia.raptors.dev/)"
    ]
},
{
    "submission_id": "1gpfcpk",
    "title": "[D] Looking for a project partner who's published in top conferences [cvpr, neurips, wacv, iccv, etc]",
    "selftext": "Hello y'all. Deep into my master's degree, I am in a dire need of a mentor/partner for my research partner. Some of the professors at the academia who claim to specialize in the field of computer vision/ai doesnt know how to clone an existing model from github or provide gpu alternatives and solutions who doesnt have fancy things to speed up the process. \n\nso if you do feel the same way and is interested to work on some cool research gap leading to a publication. drop a comment on what excites you most. thankss.",
    "created_utc": "2024-11-11T23:26:13",
    "num_comments": 5,
    "comments": [
        "It's normal for established professors to not be so in tune with menial programming tasks. They probably did their own stuff a while ago but at a certain point your focus changes from doing experiments to planning and supervising projects, getting funding, teaching, etc. If you need a mentor that is more hands on look for either assistant professors, postdocs, or late stage PhD students who should have enough experience to mentor but still not so deep into administrative tasks that do not have time to code and run their own experiments.\n\nAlso, it's easier if you already have a project or field in mind when reaching out to people, shows that you're proactive, passionate and willing to put in the extra effort.",
        "2nd year phD, just got accepted in wacv as independent work, we can have a chat ✌️",
        "Hey, I'm also looking for collaboration to work with someone and publish in top tier conference. I have publishes in BMVC and WACV, but I'm now targeting top tier venue. Would love to connect and work together.",
        "Agreed! i did reach out with specific project statements previously but not much of a supervision or help from their side, but still got your point.",
        "cool. send me your email id, i'll mail you or any other forum you're comfortable with?"
    ]
},
{
    "submission_id": "1gpd8in",
    "title": "Does Overfitting Matter If \"IRL\" Examples Can Only Exactly Match Training Data? ",
    "selftext": "I'm working on a solo project where I have a bot that automatically revives fossil Pokemon from Pokemon Sword & Shield, and I want to whip up a Computer Vision program that automatically stops the program if it detects that the Pokemon is shiny. With how the bot is set up, there's not going to be a lot of variation between what the visuals will be, mostly just the Pokemon showing up, shiny or otherwise, and the area in the map that lets me revive the fossils. \n\nAs I work on getting training data for this, it made me wonder, given the minimal scope of visuals that could show up in the game, if overfitting would be a concern I'd have at all. Or to speak more broadly, in a computer vision program, if the target we're looking for can only exist in a limited fashion, does overfitting matter at all (if that question makes sense)? \n\n(As an aside, I'm doing this program because I'm still inexperienced to machine learning and want to buff up my resume. Would this be a good project to list, or is it perhaps too small to be worth it, even if I don't have much else on there?)",
    "created_utc": "2024-11-11T21:04:16",
    "num_comments": 10,
    "comments": [
        "How I might think of 'overfitting' in your circumstance is more like this:\n\n* You have a finite list of pokemon. Let's even assume that each pokemon is just a sprite without any animation, so just one fixed image per pokemon.\n* However, the pokemon can be displayed on various backgrounds etc.\n\nSo to me, a relevant example of overfitting might be a scenario where your model is trained on images of a type of pokemon only over a desert background. And so when your model sees that same pokemon over the ocean, it fails to recognize the pokemon correctly. This is because it used the background image as a helpful cue to infer the 'correct' answer.",
        "Then it's not really overfitting since you'll have 100% metrics (or at least very high scores) on your train, validation AND test sets.",
        "Overfitting isn't always the issues it's made out to be, like you say, if the problem domain is very limited it can be perfectly acceptable to overfit to that use case. This can be particularly useful in manufacturing - I might not need a model that can detection object X in all colors, lighting conditions, orientations, etc. It's not person detection for an autonomous car. The conditions of operation are totally different in each use case.",
        "Can you not just do matching of pixel values",
        "I don't think the background would be a factor in this case, at least not notably. For the fossil Pokemon, they're all given to you, showing up first on this screen: [https://imgur.com/a/0zCl24P](https://imgur.com/a/0zCl24P)\n\nIn this case, it'd just be identifying whether the Pokemon on this background is A) one of the four Pokemon that's available from fossils, and B) whether it has the proper shiny colors. I do recognize though that this would probably want to have some extra negative pictures when I'm going in and out of menus, but at least for the target data I'm looking for, overfitting doesn't seem like it'd be a concern since it'd just have to match an image like this anyway?",
        "Right? If your goal is to fit a model very closely to your dataset, and don't want it to cover novel cases, then it's not overfit. It's good-fit.",
        "You referring to seeing if the colors on the screen at certain pixels would match?",
        "If all you need to do is figure out which one of four known images a new image is identical to, why not just select a patch of pixels in the middle and make an if-else statement?",
        "more or less, you know where the shiny sprite is going to be, you know what it looks like so you can just mask out the background and whenever there's a 90% or so match of pixel values just consider it a shiny and call it good. Maybe give it a bit of leeway to account for occlusion?",
        "Perhaps that could work. (Although I feel like if part of the intent of this is to build up my resume, this would be less useful)"
    ]
},
{
    "submission_id": "1gpd42g",
    "title": "Exploring Fooocus for interior design generation",
    "selftext": "I came across some examples of using **Fooocus** (GitHub link: https://github.com/lllyasviel/Fooocus) to generate interior designs by transforming photos of empty rooms. The examples aren't mine, but I thought they were worth sharing here.\n\nThe showcase creators mention that Fooocus works well for an overall inpainting approach, but there are a few challenges:\n\n* When applied to walls, it sometimes adds extra space or gaps.\n* When used on floors, it tends to replace the original material entirely instead of blending with it.\n\nOne approach that seemed effective was to **use inpainting interactively**, generating furniture and decor piece by piece (for example, starting with the bed, then adding surrounding items). This helped avoid the larger changes that Fooocus can introduce when working on bigger areas.\n\nExamples:  \n\\#1\n\n[Empty room](https://preview.redd.it/sm5vpoqkie0e1.png?width=2266&format=png&auto=webp&s=9a183d038ef22aa9d0c693c5fd17630b9cf25df9)\n\n[Style 1](https://preview.redd.it/9itg0egmie0e1.png?width=2266&format=png&auto=webp&s=9d098f534e3115456fa4b0d71ee937006c524f97)\n\n[Style 2](https://preview.redd.it/dztrtaiyie0e1.png?width=2266&format=png&auto=webp&s=235cdf2905edea0d4765970cb5ff26ab65269e12)\n\n\\#2\n\n[Empty room](https://preview.redd.it/j1wk4nwsie0e1.png?width=2266&format=png&auto=webp&s=36e635ae2125aa70f1ac349b5ea3473e506982f9)\n\n[Style 1](https://preview.redd.it/iujvdlluie0e1.png?width=2266&format=png&auto=webp&s=ae48283fa419638011e5bd9206e2787c5013ddbd)\n\n[Style 2](https://preview.redd.it/6r94k851je0e1.png?width=2266&format=png&auto=webp&s=acaa6bb4cf1efbccd26b1e6726c49f66534bb5f8)\n\n\\#3\n\n[Empty room](https://preview.redd.it/4vi28azaje0e1.png?width=2262&format=png&auto=webp&s=160ce898fcd41b8a0fd9579330bc563a46147662)\n\n[Style 1](https://preview.redd.it/h8nwbmocje0e1.png?width=2262&format=png&auto=webp&s=ec183d937cfc395a1f6888bc2a00bc2275b3746c)\n\n[Style 2](https://preview.redd.it/69fl2c6eje0e1.png?width=2262&format=png&auto=webp&s=0906065dddedaa3028c6b5877093d5d37feebf5e)\n\nCredit to the author: [epoch8.co](https://epoch8.co/)",
    "created_utc": "2024-11-11T20:57:23",
    "num_comments": 0,
    "comments": []
},
{
    "submission_id": "1gpba3y",
    "title": "[D] How to Choose an AI-Focused Master's Program?",
    "selftext": "I'm currently applying for AI-focused Master's programs, and I could really use some advice. I love working in computer vision, and I think I’m genuinely passionate about research. I presented my first paper at an affinity workshop at ICML, and I’ll be attending NeurIPS as a workshop presenter. This experience has been a blast, and I'm hoping to continue down this path.\n\nRight now, I'm feeling overwhelmed by all the options and the looming deadlines. The only program I’m truly excited about is at UvA (University of Amsterdam). But I know I need to consider more options to keep my career moving forward.\n\nHere’s what I'm interested in:\n\n* **Self-Supervised Learning (SSL):** I have experience in this area and would love to deepen my expertise.\n* **Video Understanding and GNNs:** These are becoming my newest interests, and I’d love to join a program where I can explore these topics.\n* **Research-oriented environments:** I’m currently collaborating with a professor and have found that I really enjoy the collaborative, exploratory nature of research.\n\nThe problem? I don’t want to settle for a program that doesn’t align with these interests or doesn’t offer strong mentorship and research opportunities. I’m also worried I might be *too* picky, which is making the process even more stressful. I’d love to hear from anyone who’s been in a similar position:\n\n1. **How did you prioritize which programs to apply to?**\n2. **Did you find a strategy that helped you balance your interests with program options?**\n3. **Any advice on picking a program that will help with a long-term research-focused career?**\n\nThanks so much for any insights you can share!",
    "created_utc": "2024-11-11T19:16:02",
    "num_comments": 1,
    "comments": [
        "I did my master's in CS at the KU Leuven. My capita selecta was in AI. Here, a lot of people joined our classes from the master in AI (https://onderwijsaanbod.kuleuven.be/opleidingen/e/SC_51016880.htm#bl=all).\nI chose a lot of my courses based on reading the course reviews in this master and made sure that it was broad and theoretical because, in my opinion, you will learn the practical stuff later.\n\nNow that I am working in a research related job in computer vision, I can tell you with certainty that you don't need the practical knowledge that much. It's easy to look up to take a quick course on these things. People I collaborate with, coming from different backgrounds, not being able to compute what models do or even fully understand what is under the hood make weird decisions on what track to follow and make a lot of mistakes. Even though they have more experience in the field than me.\n\nSo my tip would just be to take a broad and theoretical set of courses on AI. The rest will follow. Maybe join a discord where people leave course reviews 😊"
    ]
},
{
    "submission_id": "1gp7eom",
    "title": "How do people usually manage large video datasets and annotations?",
    "selftext": "I'm relatively new to computer vision industry and Google hasn't offered much other than advertisements for a lot of services. I basically have terabytes of video datasets (which will ideally be annotated by a tool like CVAT). Each dataset ideally should have some metadata attached to it such as who collected it, when it was collected, what camera was used and some tags on the attributes involved. \n\n  \nThe current strategy is to store all video data on a blob storage like S3 or Azure and use a SQL database to store metadata on the datasets which would include a link to the actual videos on blob storage. Maybe throw in DVC in there somewhere for versioning the data. Is this standard in the industry? And if not, what's best practise? I've seen a lot of advertisements for services like Supervisely and Roboflow for these type of tasks as a one stop solution",
    "created_utc": "2024-11-11T16:05:50",
    "num_comments": 12,
    "comments": [
        "When you are processing all that video, the last thing you'll want to do is process it at real time playback speed, far too slow. Standard video playback processes audio, which you do not need, and delays the processing of frames until their appropriate timecode, also unnecessary when training off video frames. I wrote FOSS to address that portion of the training pipeline. My application, named ffvideo, ignores audio completely, so your memory and other resources are not being consumed by the audio, and frame timecodes are ignored so the frames are processed as fast as your underlying hardware will process. https://github.com/bsenftner/ffvideo  It's using an older version of ffmpeg, but in this use case, should not matter. It's also Windows based, which the place I worked just prior to writing this based all their work, so there is that.",
        "At least for annotation part, you can use SAM2, pretty good at moderate object tracking",
        "Hire 3 to 4 annotator on internship make them extract the frames and annotate the data \nLike 80 to 1000 images per day",
        "My company developed Tator to deal with the difficulties of video, especially if you need to retain the full motion video. It includes the ability to transcode, host, and stream. It is fully open source under AGPLv3 if you want to try it out. We have many customers with multiple TBs of data, and it has the ability to annotate at the media, frame, and localization level. It also tracks changes to metadata over time and can isolate annotations to specific versions/layers. Let me know if you try it out and have feedback.\n\nhttps://github.com/cvisionai/tator\n\nhttps://tator.io",
        "I wrote my own solution as all the tools available are cumbersome, have slow workflow processes, and didn't operate the way I wanted.",
        "Following!",
        "Are you planning on annotating it by frames or by video clips? What does the task look like?",
        "I have good experience using FiftyOne, made by Voxel51. They integrate with cvat and is open source. However, you need to buy their enterprise solution in order to be able to store data on AWS as far as I know. And I don't know the price for that.\n\nhttps://voxel51.com/",
        "I am wondering the same. Thank you for asking & sharing your thoughts",
        "If one person can only handle 80 to 1000 images per day, you need to rethink your process, methods, and tools being used.",
        "It's for segmentation so I'm annotating masks frame by frame"
    ]
},
{
    "submission_id": "1gp6trb",
    "title": "Suggestion for feet dataset to train a foot pose tracking model",
    "selftext": "I spend about 4 hours today trying to find an annotated foot dataset to develop a foot tracking application. I want to use it for Shoe try on project I am working on. My idea is once I can track the pose, I can probably use a depth estimator to locate the feet and use three js to put shoe 3d model in there. \n\nI found a couple of foot dataset, one from the coco, which I could not download, the download website does not really work. Second one is from university of cambridge, they have a 50000 feet dataset, but I had to submit a form, they did not get back  \n\n\n",
    "created_utc": "2024-11-11T15:39:21",
    "num_comments": 8,
    "comments": [
        "You just need an object aligned bounding box then take a straight line through it.  Feet don't have poses they only have angles.",
        "But I need to first detect it right? For that I should need some kind of tracking/ detection mechanism",
        "You could potentially use a key point detector and filter out everything but the foot keypoints",
        "Thanks for the suggestion. Sorry if my question is noob, but I need to train an ml model first to do the key point detector, right? My main issue is that I can’t find any dataset to train on feet",
        "Take a look at the papers here:\nhttps://paperswithcode.com/task/keypoint-detection\n\nSome of the algorithms are pretrained to detect human key points (wrist, elbow, knee, ankle etc).  You could probably use these off the shelf algorithms to identify where feet may be in an image",
        "Yeah, I will look into these, but all the models I have found so far are trained on full human body. Like if I just pass photo of the feet, it won’t be able to detect the key point.",
        "Dropping it for anyone who comes here later on. Ended up searching again and found this \n\nhttps://universe.roboflow.com/business-qcddc/whole_dataset"
    ]
},
{
    "submission_id": "1gp56qp",
    "title": "Regression Models in CV",
    "selftext": "I have the book “Computer Vision: Models, Learning, and Inference” by Simon Prince and mainly use it as a reference for Geometric Computer Vision problems. However, I recently read chapters 8 and 9 which discuss using various regression models for detection and classification. The book was published in 2012 and there has obviously been huge advancements in the problems of detection and classification since then. But I’m curious if anyone has come across applications recently where simple regression models perform as well as or are more appropriate than neural network based approaches for these tasks? ",
    "created_utc": "2024-11-11T14:28:33",
    "num_comments": 7,
    "comments": [
        "Most \"old\" regression techniques perform better than neural network regressions.  XGBoost is still sort of the gold standard.  Within CV I can't say, there's not much regression needed that often except maybe keypoints (CNN) and line fitting (linear regression tools).",
        "Thanks for the info. Though like I mentioned, I am wondering specifically about the detection and classification problems. In the book they discuss Bayesian logistic regression, dual logistic regression, etc. for the classification problem. Wondering if anyone has used these methods recently and can compare to NN based approach.",
        "Exactly this\n\nWhile all the trendy people want to use a neural network for everything, many common classes of problems are much much better solved with \"classical\" techniques. Especially if you e.g. need to quantify the performance, validity, charactaristics, assumptions and so on to stakeholders, or you have limited training data, or stakeholders have some model they need to align your results with.\n\nLinear regressions still come up all the time if you're trying to e.g. calibrate type of sensors, model noise, and so on (maybe this pushes a little more outside the realm of CV as a discipline of CS though)",
        "There are many vision regression tasks like image restoration that involve regression, e.g. denoising, deblurring, dehazing, super resolution etc. I don't think you can solve them efficiently with classical approaches, if the output variable is also another image.",
        "The whole reason CNNs created the hype for deep learning was the superior success on the ImageNet classification challenge. \n\nApplying Bayesian logistic regression on images directly would require a parameter for each pixel. It also generally only describes a linear model which limits the model complexity. Computing any kind of probabilities or ML-estimation becomes computationally intractable, especially if you also have a huge dataset. So you would require some dimensionality reduction, feature extraction or approximations. Alternatively you apply classical models only patch-wise.\n\nAt this point you might as well just use a CNN that learns features on its own in an end-to-end way and optimizes itself stochastically over a random batch.\n\nOn smaller datasets and lower-dimensional data classical methods are generally superior though and more interpretable.",
        "I mean other than filling in gaping holes, all of those have classical solutions that are sufficient for most jobs."
    ]
},
{
    "submission_id": "1gp390q",
    "title": "[D] How to report without a test set",
    "selftext": "\nThe dataset I am using has no splits. And previous work do k-fold without a test set. I think I have to follow the same if I want to benchmark against theirs. But my Val accuracy on each fold is keeping fluctuating. What should I report for my result? ",
    "created_utc": "2024-11-11T13:08:31",
    "num_comments": 4,
    "comments": [
        "In short, k-fold automatically 'splits' the dataset into training and testing parts based on how many 'k' folds you specify and then iterates over the different datasets generated. It's hard to say what you mean by fluctuating and why without knowing how your model looks or what data you are using.\n\nCompare your model to the one on the previous work you mention, understand why it is built like it is and try to integrate the knowledge into your own model.",
        "I thought k-fold split it into train and validation set and you usually need another test set? My dataset is very small with only 53 videos. My model is a normal u-net with 9 channels of input.",
        "Typically your test set is used to determine your final model metrics. It should contain images that were not used during the training/validation stage. If you can find a couple of hundred images that are in the same domain (similar camera, vantage, and subject matter), then that can serve well as a test set.",
        "It is a public dataset and dose not have a testing set"
    ]
},
{
    "submission_id": "1goxohv",
    "title": "Looking for On-Device Model for Basic Image Recognition (Flow Charts/Org Charts)",
    "selftext": "Hi,  \nI am looking for an on-device model that can handle basic image recognition for flow charts images or org charts images. Are there any existing tools or models out there that can help with this, or would I need to develop an image processing library from scratch for this use case?\n\nAny suggestions or insights would be appreciated!",
    "created_utc": "2024-11-11T09:25:44",
    "num_comments": 0,
    "comments": []
},
{
    "submission_id": "1gox0xa",
    "title": "[Need guidance] Data Extraction from documents",
    "selftext": "I am not a ML/AI guy but I have some knowledge and I have trained a few classifiers/object detectors before. I have been working on a business problem of trying to extract certain fields from documents. Challenges are we have a lot of documents, each document can have a variety of layouts and even similar layouts might have  little variations or font/design changes.\n\nWe are looking at a commercial solution like Abbyyy flexi capture. But I also have promised the stakeholders I am going to build them something in-house meanwhile.\n\nThe proposed pipeline is we run Google's enterprise (basic) OCR on all docs, and then try to use regular expressions to parse fields from the OCRed data (and a few NLP techniques like POS Tagging etc.). I am thinking of trying to implement this for as many documents as I can, to establish a baseline.\n\nApart from that what else is realistic? Models like Donut aren't working well out of the box. LLM seems like an overkill and would require lot of customization and finetuning.. Thinking maybe we can use an ensemble of small models?\n\nWhat are your thoughts?",
    "created_utc": "2024-11-11T09:00:06",
    "num_comments": 4,
    "comments": [
        "Flow: document -> OCR -> document type detection based on keywords -> regular expressions for that document type -> cleaning + standardization+ parts-of-speech tagging -> list of possible candidates for each field\n\nWe have tried this on one document (type) and seems to be working fine on 200-odd samples.\n\nSome documents have tables etc. and that increases complexity (OCR output is jumbled). We want to detect stamps, qrcodes, signatures too but priority issue is field extraction.",
        "If you can tun Llama vision go for it it's good with ocr, there is two versions choose the best based on your hardware, for complex tasks + hardware limits you may try ocr+Ner, If you have any questions DM I will be happy to help!",
        "Ran into this post recently: Implementing Intelligent Document Processing system (IDP) for structuring invoice data\nhttps://www.linkedin.com/pulse/how-implement-ai-system-parsing-data-from-invoice-scans-epoch8-a11of\n\nMaybe it helps.",
        "We spent a lot of time on the problem for our own product. The regex method will work but with very lower accuracy, defeating the purpose. \n\nWe actually shipped our document processing pipeline as an API for others to use. It is very easy to get started and I'd love to know if it works for your use case. \n\nWebsite: [https://tile.run](https://tile.run)  \nQuickstart with Python: [https://github.com/tile-run/python-quick-start](https://github.com/tile-run/python-quick-start)"
    ]
},
{
    "submission_id": "1gowf0m",
    "title": "Master thesis topic advice\n",
    "selftext": "Hi,\n\nI currently have the opportunity to do my master's thesis. The area is around \"Synthetic Data creation for vision/ lidar\". I am interested in this area since I wanted to do my thesis also related to computer vision.\n\nThey are flexible in terms of the final topic that I work on, so I had these ideas:\n\n**1) Synthetic Data creation for vision/LiDAR Images and Comparison with Real-World Data**\n\nUsing Generative Adversarial Networks (GANs), to generate synthetic images for either vision or LiDAR data separately. By creating high-quality synthetic images that mimic real-world conditions, the goal is to enable the generated data to be a viable training and evaluation resource. This approach helps assess the effectiveness of synthetic data in model training, aiming to reduce the dependency on costly real-world data collection.\n\n**2) Vision-to-LiDAR Image Conversion Using GANs**\n\nAims to convert standard vision images to LiDAR-like depth images using GANs, enabling environments without LiDAR sensors to gain depth perception from camera data alone. The project would involve training a GAN to learn depth representation from paired image data.\n\n3)  **Generating Natural Language Descriptions for LiDAR-Based Scene Understanding Using Vision-Language Models**\n\nThis project would focus on developing a vision-language model to generate natural language descriptions of scenes captured by LiDAR data. The aim would be to create a system that can interpret spatial and object data from LiDAR sensors and generate descriptive sentences or captions, making the data more accessible and interpretable.\n\nWhat are your thoughts on these topics? Which of these 2 topics would be more valuable to do in terms of real-world application? Or is there another interesting topic that I should think about?\n\nI would appreciate any suggestions. Thanks!",
    "created_utc": "2024-11-11T08:35:49",
    "num_comments": 6,
    "comments": [
        "Shouldn’t a master’s thesis provide scientific value? Both topics have already been thoroughly researched, and there are numerous papers on this subject… Nowadays, do people only ‘research’ just to get their degree or what?",
        "A Review of Synthetic Image Data and Its Use in Computer Vision (Man & Chahl)\n\nReview and Analysis of Synthetic Dataset Generation Methods and Techniques (Paulin & Ivasic‐Kos)\n\nDomain-Transferred Synthetic Data Generation for Monocular Depth Estimation (Peterson et al.)\n\nUnity Perception: Generate Synthetic Data for Computer Vision (Dhakad et al.)\n\nSynthetic LiDAR-Labeled Data Set Generation for Object Classification (Wisultschew et al.)\n\nTransfer Learning from Synthetic to Real LiDAR Point Cloud for Semantic Segmentation (Xiao et al.)\n\nThe SYNTHIA Dataset: Synthetic Images for Urban Scene Segmentation (Ros et al.)\n\nLiDARsim: Realistic LiDAR Simulation by Leveraging the Real World (Manivasagam et al.)\n\nSurfelGAN: Synthesizing Realistic Sensor Data for Autonomous Driving (Yang et al.)",
        "Yes, but this would be more specific to a company's proprietary data so that's why it is allowed. Any suggestions on what would be better to go for ?",
        "I have a question. Is exploring a path, which is not explored by scientific community yet, in a subject that is not solved even though has been around for over 2 decades, considered to have scientific value?",
        "Edge-Case Synthetic Data: Generate synthetic data for rare conditions like adverse weather, enhancing robustness in safety-critical models.\n\nHybrid Data Fusion: Combine synthetic and real-world data to reduce reliance on real data while improving model accuracy through fusion.\n\nReal-Time Adaptive GANs: Create GANs that adapt synthetic data based on real-world feedback, enabling practical, real-time data adjustment.",
        "[deleted]",
        "Bro WTF? I asked you this because it's kind of  the deal with my master thesis and I wanted to make sure that the path I've taken is not bad. I meant it it was not rhetorical. Seriously WTF?\n\nI am doing the exact thing I asked you about and I wanted to hear your opinion on it.\nMy question was literally about camera radar fusion in Transformer based object detectors."
    ]
},
{
    "submission_id": "1govofl",
    "title": "YOLO 11 Tutorial",
    "selftext": "Hi! In just 30 minutes, we'll be premiering an in-depth tutorial(In spanish) that will guide you through this powerful neural network. Installing, using from commannd line, using to create python apps o fine-tunning. \n\nDon't miss out! [https://www.reddit.com/r/NullSafeArchitect/comments/1glm588/super\\_tutorial\\_yolo\\_11/](https://www.reddit.com/r/NullSafeArchitect/comments/1glm588/super_tutorial_yolo_11/)",
    "created_utc": "2024-11-11T08:05:05",
    "num_comments": 0,
    "comments": []
},
{
    "submission_id": "1govmge",
    "title": "Problem with windows restarting any solutions?",
    "selftext": "I have an old laptop who keeps restarting (it has some important programs) \nDoes anyone have any solutions?\n\nOn bottom left corner it says press escape for startup menu  and shows :\nF1 System information \nF2 system diagnostics \nF7 HP sparekey\nF9 Boot device Options\nF10 BIOS  Setup\nF12 Network boot\nEnter continue startup(but it keeps just restarting)\n",
    "created_utc": "2024-11-11T08:02:50",
    "num_comments": 3,
    "comments": [
        "Nothing to do with computer vision, this is general IT",
        "You might have better success at r/techsupport. If English wasn't your first language, I could see how you might mistake Computer Vision for \"Computer Expertise\", but unfortunately, that is very far from the truth haha.",
        "There was no windows bro😂\nProbably faulty hdd or ssd."
    ]
},
{
    "submission_id": "1gonpea",
    "title": "Philosophical question: What’s next for computer vision in the age of LLM hype?",
    "selftext": "As someone interested in the field, I’m curious - what major challenges or open problems remain in computer vision? With so much hype around large language models, do you ever feel a bit of “field envy”? Is there an urge to pivot to LLMs for those quick wins everyone’s talking about?\n\nAnd where do you see computer vision going from here? Will it become commoditized in the way NLP has?\n\nThanks in advance for any thoughts!",
    "created_utc": "2024-11-11T00:36:44",
    "num_comments": 58,
    "comments": [
        "multimodal LLMs are really useful for computer vision - i've been getting great results for few-shot inspection using MLLMs. They're also really good at extracting structured data out of images. But they suck for other applications. They're just a tool IMO",
        "So far from what I have seen in the industry:  \nLLMs can barely be used due to licensing issues on most models and legal departments waiting for lawsuits to settle to get better guidance. For specific industrial cases such as manufacturing, logistics etc the big models such as SAM2 are having big issues due to low amount of data available online and works better for general cases.\n\nI feel the \"LLMs for Everything\" hype is also hurting the CV industry, especially for things that needs to run on edge devices by trying to force LLMs and Generative AI into every project due to hype...  \nMore focus on smaller models, better training methodologies, domain generalization etc is where I see the actual gold to be, LLMs in industry is more like fake gold in comparison, usable for smaller proof of concepts but not products.",
        "YOLOv1000000",
        "We can improve self supervision methods for video and multi-modal models such that they can extract longer term temporal knowledge and build a more human-like understanding of the world. The current methods are too much focussed on low level features like pixels and frames, which carry too little semantic value in and of themselves, unlike language tokens.",
        "I maintain that [language is a crutch](https://x.com/chrisoffner3d/status/1854633156076359968) and a distraction for computer vision.\n\nThere are countless animals that have excellent visual perception of the world – both in terms of object detection and semantics, as well as spatial 3D understanding – without relying on language at all.\n\nLanguage models can be useful as an *interface* for humans to express their intent to a computer, but it's entirely orthogonal to the problem of visual perception.",
        "If you're okay with my gut reaction — I would like the hype to value designing expert user interfaces around CV tech a bit more. There's tons of great stuff out there, just they don't get traction because they're hellishly difficult to use even for people in the same field.\n\nLLM or DL or ML or classical CV or whatever technology (and I would advocate that a LLM should be a very, very last resort), everybody has either a shitty chatbot or a poorly designed native/web/mobile UI that falls down whenever you try to do something slightly out of the norm. I'm including the product I'm working on on that, our tech behind has some nice stuff but I'm very critical of the choices in our user interfaces.",
        "Personally, I believe we need another big break through like the Transformers. Let's be real, classical computer vision, even though is useful in many cases, has failed to solve the core problems such as object detection or image registration. Moreover, current state of the deep learning has also failed to solve these problems. So, in my perspective, the sooner we start trying to come up with another approach, the sooner we can overcome current challenges.",
        "I'm waiting for an MLLM that can do everything that Segment Anything does. Only flamingo is close enough but it's quite small and not capable on documents. It will be a game changer to see an LLM that can do object detection, segmentation, tracking, object counting, OCR all in one place",
        "VLMs.",
        "metric depth estimation",
        "I'm thinking long term. The goal is to develop an artificial visual cortex. I think focusing on human visual cortex and trying to replicate that is a potential path made possible day by day with advances in neuroimaging. I'm more interested in biology inspired vision, rather than shots in the dark for better neural network architectures, not that that is not a viable approach. We already have vision, why not leverage that.",
        "I am particularly curious what will come out of the startup World Lab AI from Feifei and others. They seem to focus on doing the research about spatial intelligence which like building a foundational model for language but for space. My money is on them.",
        "Industry isn't using LLMs.  That's about all you need to know about LLMs.",
        "Ideally some kind of high dimensional \"thinking\" space where the semantic references of an LLM meet the capacity to use the recognized objects via CV. That space will grant context to the objects being recognized.",
        "Calculating spacetime probabilities with quantum computers",
        "Can 3D Computer Vision compete with LLM hype? If yes, what are the applications of 3D CV which are gaining traction these days in the industry?",
        "Maybe one more thing to remember aside from all the mentioned topics…Well AI isn’t particularly new and CV isn’t as well. It always baffles me how much research, how many decades and how many disciplines had to come together to archive the results we have today. I believe modern efforts to teach machines intelligence are made since the 1940s, most prominent maybe the Turing Test or the Dartmouth Workshop. Over the years so many completely different breakthroughs had to be made, maybe look up Hebbian Learning from Neuroscience, think about the enormous hardware requirements that we can utilise nowadays or about the mathematical foundations for backprop that we now can use but back when they were invented maybe nobody had a clue about that. While all these systems evolved over the decades, there were always hypes and on the other hand so-called AI-Winters when hope and hype in these efforts were lost. Oftentimes different techniques were developed afterwards and the field moved to new ideas. Remember that even back then governments and companies spent very large amounts of money to find out that things doesn’t really worked out the way they thought they would. But progress was made and things became possible that we couldn’t imagine. Sometimes failed or very old approaches come back like the so-called connectionist approaches, sometimes they don’t like expert systems or symbolic AI.\n\nMy point is that besides from the incremental performance increase we see so often, or apart from revolutionary new architectures like the transformer was, some CV problems could require a new way of machine learning. Or maybe we still won’t be able solve some problems in the near future. Maybe the AI hype could cool down a bit again. I personally don’t believe that all this is about to happen soon, but why should neural network training be the end of CV ideas?! And who knows, maybe in some years people might think that throwing very large amounts of data in networks is stupid, or maybe they can process that amount on small devices under their skin and say: Cute, 100 Terrabytes. Who knows?!\n\nWell, maybe all this unstructured ideas don’t really answer the initial question, but sometimes I believe it’s forgotten that AI is around for some time.",
        "Vision is for perception, and language is for communication. \nThe way i see it, there is no purely vision task. The computer needs to communicate what it perceives, making it a vision-language mix. \nIn this line of thinking, I sometimes feel that there is no vision, this field should be named computer perception and generalise perceiving the world with any set of sensors, be it one or a million. All sensors are finally going to provide electric signals and the computer perceives the world through the signals.\nThere's also a lot to be done to develop sensors  sufficiently to capture the whole spectrum of energy beyond our regular RGB for that.\nI didn't include image generation here as I see no value in it. Maybe project holographic content into the real world is a direction for generative models in vision. Enlighten me please. \nLLMs, due to their generative capability, will be very helpful in extending our perception. \nThere is still a lot of work needed to reduce the noise in both the tasks. Once the noise in language is reduced, i feel that we can expand our perception a lot, both at the microscopic and macroscopic level.",
        "Multi modal (audio video language) and temporally aware models who could learn dynamically just as we are continuously learning and adapting. Solving the problem of running out of data on the internet, either by synthesized data or building AIOTs such as meta glasses (but for cheap ofc), starting with giving access to specialists and researchers for their work and the model becomes efficient in the task that humans perform. I personally think AI won't consume all the current jobs if we stop panicking that AI would consume all the jobs and instead think of complex tasks that humans can be made an expert in using AI assistance and this way we'll shape future generations to focus on solving important problems like efficient harnessing and distribution of solar energy at global scale and leave normie tasks like building a website to an AI. We should ideally be able to spend more time on thinking than smashing keyboard buttons.\n(Just a wild thought)",
        "Definitely multimodal LLMs - plus value is not the model it is the output, so the faster we can demonstrate outcomes from our models the better we off we are as a practice.",
        "What do you mean by few shot inspection?",
        "Yolo 33 1/3",
        "This^",
        "So sell me this product.  How will this help my business?  This doesn't sound useful to anyone.",
        "How has classical and deep-learning-based computer vision failed to solve object detection? Can you elaborate?",
        "Amen",
        "There's nothing in computer vision that isn't really working.  There's no need to a breakthrough, except in maybe tracking.  And that need for tracking to be more robust has been there since DeepSORT came out.",
        "That's just untrue, most of the projects my company has developed for clients have involved LLMs in the last year.",
        "I don't know where you are getting this. Everybody is starting to use or is using LLMs. Even my freaking neighbor is using that for his work at the municipality. The guy started talking about RAG to me and he's just some project manager of an IT department.",
        "inspection where you don't have enough data to train a computer vision model",
        "I'm not selling anything to anyone. This probably won't help you business. Go and buy the competitors product :)",
        "\"working\" but how well? Most clients aren’t interested in CV solutions even if it works well 95% of the time, and getting there is already a big challenge in and of itself. They want 99% or 100% accuracy because if the CV solution can't remove humans from the loop, it's not worth the investment for them (and they are right).\n\nThere is a need for a breakthrough, especially in deep learning-based CV, so that you don't have to rely on mountains of data just to get models performing at a barely acceptable level. Humans don’t need tens of thousands of examples to recognize a car and don't break down just because you switched to a different view; we intuitively get it with very little exposure. CV is nowhere near that level of efficiency or understanding.",
        "That's fantastic for you.  Except, most people and most significant projects, are not.",
        "There's a difference between using LLMs as an assistant and using LLMs in your products.",
        "Interesting, I have been also tinkering with different VLMs on their zero and few shot capabilities on visual inspection. Specifically on anomaly detection. Whats your experience so far? Apart from gpt4o and claude I find most of them not very useful.",
        "So what you're saying is it's pointless.",
        "Also regarding your statement about need tens of thousands.  The bar is already much lower, regardless DL != CV.  Just because DL requires thousands of images to do something doesn't mean there isn't an equivalent or better CV solution that requires no training.",
        "Nothing is going to hit that accuracy from purely CV.  It's a pipe dream.  So the applications you're looking for are already moot to bring up.",
        "i'm using claude in production. works v well",
        "Yup. Totally pointless. I won't sell it to you nor will it make your business any money.",
        "\n>Just because DL requires thousands of images to do something doesn't mean there isn't an equivalent or better CV solution that requires no training.\n\nWhich CV solution can detect something simple as cars with no training equal to or better than DL? Or even remotely close?\n\nThat's more of a \"pipe dream\" than DL-based CV solutions reaching human level accuracy.",
        "Weren't [you the one](https://www.reddit.com/r/computervision/comments/1gonpea/comment/lwkpwtn) talking about products that businesses actually find useful?\n\nThey don't want to invest in half-baked solutions that are supposed to \"automate\" things using CV for them yet can't remove humans from the loop. The only type of clients I have seen investing into these half-baked solutions are clients with so much money that they don't know what to do with them. They invest in the solutions, not because they're convinced of their utility, but just so that they can boast about using \"AI\" in their products or pipeline.\n\nIf Apple's Vision Pro only recognized the hand gestures right 95% of the time, customers would've mauled them for having 5% error rate for a product that costs that much. And that's the state of majority of CV products currently. Expensive products with very little ROI. That's not considered \"working\" by client/customer standards.",
        "Yeah so how did it answer the OPs question?",
        "What are you talking about?  Did you not actually study CV or did you just take an Andrew Ng course?  You can easily create features and eigenvectors based on an object and detect them in images.  We had face detection in like 1992, you think we were using CNN's for that?\n\nAlso you keep saying human level accuracy, I don't think you actually know what that is.  First, human level accuracy for most tasks can vary from like 90-95%.  It's very rarely above 95%.  Second of all, no a single CV solution using DL solution will not hit 99% or 100%.  This is just fundamentals understanding statistics.  Did you actually study anything?",
        ">Weren't [you the one](https://www.reddit.com/r/computervision/comments/1gonpea/comment/lwkpwtn) talking about products that businesses actually find useful?\n\nYes I was, and I stand by that statement.\n\n>They don't want to invest in half-baked solutions that are supposed to \"automate\" things using CV for them yet can't remove humans from the loop\n\nNothing is half baked, all the solutions work as per client requirements.\n\n>The only type of clients I have seen investing into these half-baked solutions are clients with so much money that they don't know what to do with them. \n\nI mean that's a long winded way of saying you aren't part of the industry, but you do you.\n\n>They invest in the solutions, not because they're convinced of their utility, but just so that they can boast about using \"AI\" in their products or pipeline.\n\nOf course they do.  But you're also wrong about the second half of that statement.  I've seen plenty of companies do this.  We've told them up and down other solutions would work better, don't use a DL solution because you want to sound cutting edge.  They always end up getting burned.\n\n>If Apple's Vision Pro only recognized the hand gestures right 95% of the time, customers would've mauled them for having 5% error rate for a product that costs that much. And that's the state of majority of CV products currently.\n\nAt 90 frames per second, you only need to capture the gesture for about 10 of those frames.  So being wrong 80 times out of 90, still means they are right.  It's funny how you can't differentiate between videos and images.\n\nIt's also funny how you keep trying to argue about DL solutions as CV.  DL is like 5% of CV.  Go read a book please.",
        "It didn't. I take it back.",
        "Is OP asking about products?",
        "> What are you talking about?  Did you not actually study CV or did you just take an Andrew Ng course?  You can easily create features and eigenvectors based on an object and detect them in images.  We had face detection in like 1992, you think we were using CNN's for that?\n\nThe question wasn't even if CV solutions can perform detections without DL. Did you actually read what I asked?\n\n> Which CV solution can detect something simple as cars with no training *equal to or better than DL*? Or even remotely close?\n\nwhich was in response to you saying\n\n> Just because DL requires thousands of images to do something doesn't mean there *isn't an equivalent or better CV solution* that requires no training.\n\nWhy is it so hard for you to read before responding?",
        "\n> Also you keep saying human level accuracy, I don't think you actually know what that is.  First, human level accuracy for most tasks can vary from like 90-95%.  It's very rarely above 95%.  Second of all, no a single CV solution using DL solution will not hit 99% or 100%.  This is just fundamentals understanding statistics.  Did you actually study anything?\n\n95% of what? \"frames\" like you mentioned in your other response? So a human would fail to recognize a car in 5 out of 100 frames? Or get 5% of the text on a form wrong while reading?\n\nYou don't give any examples, as usual. It's all just broad claims with no substantiation.",
        "> Nothing is half baked, all the solutions work as per client requirements.\n\nYeah, that's why you keep mentioning these \"CV solutions\" without providing any actual examples of them (and what percentage of CV clients demand for it to qualify as a general statement that \"all the solutions work\"). Just constant waffling. \n\n> I mean that's a long winded way of saying you aren't part of the industry, but you do you.\n\nLol. I'm starting to think that about you given how out of touch you're with reality and what clients want.\n\n> Of course they do. But you're also wrong about the second half of that statement. I've seen plenty of companies do this. We've told them up and down other solutions would work better, don't use a DL solution because you want to sound cutting edge. They always end up getting burned.\n\nHere we again. You keep mentioning all these \"CV solutions that don't require DL\" yet you can't name them or show that's what makes up most of what clients demand when it comes to CV. Let me give you an example. A client wants OCR to parse forms and get the texts  (something that has a very significant demand in CV). Please let me know this non DL-based solution of yourself that works better than DL for the use-case. Or maybe your solution is to handwave and say to the client \"Who cares about OCR? We produce the best optical mouse sensors. That's what real CV is about. Not these DL nonsense. Are you interested?\" just like how you do here.\n\n> At 90 frames per second, you only need to capture the gesture for about 10 of those frames. So being wrong 80 times out of 90, still means they are right. It's funny how you can't differentiate between videos and images.\n\nHow is that even relevant or counter to anything I said? Did you even read what I said? Where did I mention anything about videos or images or frames?\n\n> It's also funny how you keep trying to argue about DL solutions as CV.  DL is like 5% of CV.  Go read a book please.\n\nWhat's funny and ironic is you bringing that up repeatedly when I qualified that explicitly in my original reply with \"especially in deep learning-based CV\".\n\n> DL is 5% of CV\n\nI am not talking about CV as a field; I am talking about client demands in CV and I have been explicit about that since the first reply, but reading is too hard apparently.\n\nEDIT: Lol. [This guy](https://www.reddit.com/r/computervision/comments/1fd24g2/comment/lwrm8t4/) is so worked up he had to find a reply of mine on a different thread that he can actually respond to to feel good about himself. And then blocks me. (and the error stacks up because the final outcome is dependent on all of them being correct; but you can't read, so eh).",
        ">And where do you see computer vision going from here? Will it become commoditized\n\nIt's astounding you felt the need to comment this.",
        ">Why is it so hard for you to read before responding?\n\nThe answer is in the post.  I think you need to take your own advice.  If you're not satisfied with that one, again you can use an SVM.  Both these techniques are taught in introduction to computer vision courses still to this day.",
        "Yes exactly that.  Also a human isn't examining frame by frame anyway.  I don't think that would be real practical, but for some reason you seem to think it is.  I've dealt with annotation enough to know what human error rates are.",
        ">Lol. I'm starting to think that about you given how out of touch you're with reality and what clients want.\n\nI'm starting to think you've never even talked to a client\n\n>What's funny and ironic is you bringing that up repeatedly when I qualified that explicitly in my original reply with \"especially in deep learning-based CV\".\n\nWhat's funny, is you should just already know if you're in the field.\n\n> A client wants OCR to parse forms and get the texts (something that has a very significant demand in CV). Please let me know this non DL-based solution of yourself that works better than DL for the use-case.\n\n Pati, P.B.; Ramakrishnan, A.G. (May 29, 1987). \"Word Level Multi-script Identification\"\n\nOr you know, an SVM.  Or even random forest can do it, and a lot of tools to still use these, and work just fine.  Isn't crazy how we had OCR since 1987, but you're acting like DL revolutionized it?\n\n>How is that even relevant or counter to anything I said? Did you even read what I said? Where did I mention anything about videos or images or frames?\n\nBecause even if something can only detect something 95% of the time, it doesn't need to detect it more than that if you are in the right application.  Which, if you were in this industry, you would know is basically EVERY TIME except when dealing with a still image like a CT scan or xray.\n\n>What's funny and ironic is you bringing that up repeatedly when I qualified that explicitly in my original reply with \"especially in deep learning-based CV\".\n\nThere is no \"deep learning-based CV\".  There is CV, and there are its tools.  DL is a tool, one of many.\n\nIt's funny you like to point to my old posts.  If you keep digging you'll find one where I talk about there being a lot of idiots in ML and CV, and they'll eventually get purged.  I suggest you take that as life advice.",
        "Why shall the future of LLMs be directly bound to a product? When backprop was proposed, it was far from being a product, but you know what? Look at it now.",
        "Do you know why multi object offline tracking hasn't had any major breakthroughs in the last several years?  Because no one needs it.  People don't research things that people don't need.  Why would you spend years of your life developing a system that no one will use?",
        "Research is often driven by curiosity, and this is often true in the big tech industry. Multi-object tracking is not only interesting but also a very complex challenge to tackle. However, with the significant advancements in detection algorithms over the past decade, we have made substantial progress in this area. I don’t believe that tracking is a topic of minor interest in the industry; on the contrary, it is quite significant.",
        ">Research is often driven by curiosity\n\nWrong.  Research is driven by funding\n\n> and this is often true in the big tech industry\n\nLOL\n\n> However, with the significant advancements in detection algorithms\n\nDetection algorithms have nothing to do with tracking accuracy\n\n> we have made substantial progress in this area.\n\nWe have not.  The only \"advancements\" have been made in online multiobject tracking, and even those are minimal.  Offline tracking hasn't been touched.\n\n> I don’t believe that tracking is a topic of minor interest in the industry; on the contrary, it is quite significant.\n\nOnline tracking is significant, but people don't research it because DeepSORT is good enough for most application.  Offline tracking is not significant because almost no industries rely on examining past broadcast footage, the money is all in live tracking."
    ]
},
{
    "submission_id": "1gokdny",
    "title": "Improving Training a small Vision Language Model",
    "selftext": "Hey all, I am trying to train a small VLM that's \\~200mil params just to learn more about VLM (partly inspired by the moondream2 model). I have a 4090 to do this but the training speed seems a bit too slow, I have trained florence-2-large in the past which is \\~770million params and that seems to be way faster. I assume there has been some optimization done that I can add. Looking to see what improvements can be made.\n\nCode - [https://github.com/04RR/SmolVLM](https://github.com/04RR/SmolVLM)\n\nPlease let me know what changed can be made. Right now one epoch on 20k samples is taking \\~5hrs.",
    "created_utc": "2024-11-10T20:55:33",
    "num_comments": 5,
    "comments": [
        "0. Refactor SmolVLM such that you move tokenization out of it, but put it in the dataset getitem. Avoid max_length padding, rather use \"longest\" strategy and edit the collate_fn accordingly (tokenize w/o padding and in the collate use tokenizer.pad for instance). Even better, pre tokenize your whole dataset if is possible. For the sake of refactor, you should also separate a proper forward and generate functions. They serve different purposes. \n1. Use AMP training w/ bf16, be careful with grad accum. tho\n2. AdamW(..., fused=True) to save some memory + gain speed or even use Adam to save some more memory. Regarding optimizers, you could try bits and biases implementations, which use quantized states, bringing more memory avail for the backwards => increase batch size if you're not gpu bottlenecked yet\n3. torch.compile (try to compile separate components first and monitor the gains, expect a degree of cold start time)\n\nWill edit this if I get more ideas, but these are the basic ones",
        "could you give me guidance on train florence for srabic ocr tasks",
        "Thank you! Will make these changes. \n\nLooking forward to more gems (?xD)",
        "https://github.com/04RR/florence-training/blob/main/training_distributed.py\n\nThis is the code.",
        "could you also provide the dataset format to use to 0ass it ro the model ?"
    ]
},
{
    "submission_id": "1gois46",
    "title": "Best way to segment objects with a similarly colored-background?",
    "selftext": "I'm attempting to create image masks for garments placed on tan-colored mannequins with a white background. I want these masks to only contain the outline of the garment itself, and I am struggling to find the right approach for colors that are similar to the mannequin background. Does this approach seem like the best way to do this via Python?\n\n1. Crop out the white background via HSV thresholding (assume there are different thresholds based on the color of the garment)\n2. Perform edge detection of the garment to generate a binary mask (i.e., using Sobel or Canny algorithms) if the mannequin is still visible in the mask\n\n  \n**Example 1:** This is a working example that has the desired output\n\n[Example Input Image](https://preview.redd.it/ioq6s5efx60e1.png?width=353&format=png&auto=webp&s=81eb56027f50e71fe92eeb6e6574d22c97507590)\n\n[Example Result - This is the desired output](https://preview.redd.it/a208nm0ix60e1.png?width=275&format=png&auto=webp&s=8fdf317bc83d9d41d219d47d30b26ee38eea533e)\n\n  \n**Example 2:** This example isn't working based on the similarly colored background\n\n[Another example - the item is similarly colored to the background](https://preview.redd.it/g61qhn7mx60e1.png?width=1536&format=png&auto=webp&s=b20fbec83af0439f62ce763cfc4129ad3f5c216b)\n\n\n\n[The HSV filter struggles to crop out the mannequin from the garment \\(non-desired outcome\\)](https://preview.redd.it/0xp8d4itx60e1.png?width=859&format=png&auto=webp&s=203531b845711d68efc2593b60cce25dfab4d4d3)\n\n  \n\n\nShould I be performing contrast boosting, or a different type of methodology instead to handle these similarly colored items? I'd open to any/all suggestions, or recommendations on literature to review to learn more about this topic. TIA!  \n\n\n",
    "created_utc": "2024-11-10T19:24:23",
    "num_comments": 2,
    "comments": [
        "You can use any segmentation models for this task like segment anything model",
        "I found out that Grounded SAM works perfectly. Thanks!"
    ]
},
{
    "submission_id": "1goi5lf",
    "title": "Thoughts on your future cameras? - Update 2",
    "selftext": "Hey you wonderful people!\n\nI work at [e-con Systems](https://www.e-consystems.com/default.asp); an OEM of embedded cameras. This is a follow up to my previous posts. [Post 1](https://www.reddit.com/r/computervision/comments/17w8nzp/thoughts_on_your_future_cameras/). [Post 2](https://www.reddit.com/r/computervision/comments/17w8o2k/thoughts_on_your_future_cameras/).\n\nYou guys been extremely helpful with your suggestions in my previous posts. So this post is kind of an update for the previous suggestions + to know you guys' thoughts on your future cameras, as it's nearly been a year.\n\n**For convenience, adding the suggestions from the previous posts below:**\n\n1. **Realtime ROI (Similar to event cameras)** \\- For this, the engineering research is going on. Recently, we made Multi-ROI accessible in [this camera](https://www.e-consystems.com/blog/camera/products/how-to-achieve-a-high-frame-rate-of-up-to-1164-fps-using-e-cam56_cuoagxs-multi-roi-feature/).\n2. **Ultra low light cameras with more than 9μm pixel size** \\- We might come up with this in the near future. This was suggested for aviation. Could this sensor be also useful for any of you guys' application?\n3. **Cheap event cameras** \\- Currently in consideration for development.\n4. **Controllable IR/Laser Emitter (Intensity control)** \\- I forgot to tell this to my team :'-). I'll update on the next post.\n\n**Some of my favourite new things I think could be helpful to you:**\n\n* [Robotic Computing Platform](https://www.e-consystems.com/robotics-dev-kit/ambarella-cv72s-ai-robotic-kit.asp) \\- This comes with an SDK that supports the robotics stack and includes pre-implemented algorithms for navigation and mapping. Useful for robotics developers.\n* [World's first Hot-pluggable 140db HDR camera](https://www.e-consystems.com/automotive-cameras/3mp-ar0341at-ip69k-gmsl2-140db-hdr-camera.asp) \\- One of the best HDR outputs I've seen. Do any of you find this hot pluggable feature useful for your application?\n* [World's Tiniest 4K Automotive camera](https://www.e-consystems.com/automotive-cameras/4k-ar0823at-ip69k-gmsl2-150db-hdr-camera.asp) \\- About to be launched.\n* [Qualcomm - e-con AI Vision Kit](https://www.e-consystems.com/qualcomm-embedded-cameras/qcs610-ai-vision-kit-imx415.asp) \\- This multi camera kit cost is relatively cheaper to the popular options. Could be useful to Engineers looking for low cost solution.\n\nAbout anything, you are welcome to share.",
    "created_utc": "2024-11-10T18:50:35",
    "num_comments": 7,
    "comments": [
        "RGB-IR camera with ISP that actually removes IR contamination from RGB images.",
        "What do you think about [this newly launched camera](https://www.e-consystems.com/usb-cameras/8mp-4k-rgb-ir-usb3-camera.asp)? This uses our patented RGB-IR separation technology where there is little to no contamination of IR in the RGB output.",
        "My team has tried it but the rolling shutter is a big deal breaker.",
        "That's great to know! We may be coming up with a GS version of this. May I know the application for which you need this camera?",
        "Detecting and tracking moving objects in outdoor seating (Day and Night)",
        "Thank you. Will check and tell you.",
        "Thank you. Will check and tell you."
    ]
},
{
    "submission_id": "1gohqtw",
    "title": "Suggestions for good image data processing tool in the market",
    "selftext": "We’ve been working with tons of video data lately at my retail computer vision startup. Frame extraction and annotation are taking forever. We have to do it all manually because we're at a very early stage and the founders are trying to save money.\n\n\n\nWe’re looking to scale our data processing without creating data inconsistencies that will hurt our models and cause us to spend even more time cleaning down the line.\n\n\n\nDoes anyone have any recommendations on a tool that could help us do this at scale? Preferably cheap!!\n\n",
    "created_utc": "2024-11-10T18:28:46",
    "num_comments": 8,
    "comments": [
        "Hey. Take a look at what people recommended me at my last post. It may help you.\n\nI'm now using roboflow for annotating with AI assist using a model I trained. It is doing pretty well now. \n\nFirst ~30 images were annotated manually. After that I trained a model to help me do it faster, then after 130 images I trained again and now It is pretty good after ~370 images.\n\nP.S: AI assist is completely free, but you only have 3 free training credits.",
        "I stumbled upon this open source tool that we recently implemented at our startup (video surveillance data). It's definitely going to help us as we need to process more and more data\n\n[https://github.com/cortal-insight/cortalinsight-example-workflows](https://github.com/cortal-insight/cortalinsight-example-workflows)\n\n[https://www.cortalinsight.com/blog/transforming-video-data-processing-in-computer-vision-with-cortal-v2i](https://www.cortalinsight.com/blog/transforming-video-data-processing-in-computer-vision-with-cortal-v2i)",
        "Take a look at CVAT or MLFlow as tools",
        "Use Python. This will guarantee reproducibility and consistency with whatever processing you’re doing.\n\nSpeaking of, can you tell us more about this processing?",
        "Solid. Thank you.",
        "Thanks for the share, looks promising! How much of a speed up did you see?",
        "Amazing, This works for creating new datasets with youtube videos as well, this is something i was looking for long time",
        "The speed gains have been nearly 1.5x because we've been able to process dozens of videos at a time without running out of memory"
    ]
},
{
    "submission_id": "1goeegf",
    "title": "Birds-eye-view Multi-Person Pose-Tracking ",
    "selftext": "Heyo, \nI want to track multiple persons (4 for now) from a birds-eye-perspective (Camera directly over them, tilted 90°, pointing directly at the ground). The persons would be in a ~2m*2m (~7ft) square and I want to track their movement. Arms hands/shoulders/heads. Maybe fingers too, but first things first ig. \n\nI tried some libraries/models with JS and they seem to track persons good enough but cant track in birds eye view? Does someone have any tips or maybe a finetuned model? \n\nI want to track the persons and use that data to control some paddles in a game. The \"screen\" with the game would be between them. \nI hope someone here can help, or point me to another place to seek help 😂",
    "created_utc": "2024-11-10T15:40:23",
    "num_comments": 0,
    "comments": []
},
{
    "submission_id": "1gob055",
    "title": "Reimplemented ViT from Scratch – Looking for Insights on Data Needs and Ideas for My Next Vision Project",
    "selftext": "Hello! I’m a CS student currently learning deep learning, with a primary focus on NLP. Recently, I implemented GPT-2 from scratch, inspired by the approach in Karpathy’s video. I've always been curious about how transformers work with images, so I decided to reimplement the original \"An Image is Worth 16x16 Words\" (ViT) paper from scratch and train it on smaller datasets like MNIST and CIFAR-10. You can find my implementation here: [GitHub link](https://github.com/ilyasoulk/mini-vit).\n\nI achieved decent results, but I noticed that training a ViT model requires many more epochs and data augmentations compared to a ConvNet. I've read that transformers are more data-hungry than standard ConvNets — is there an intuition or explanation behind this?\n\nFor my next project, I'm considering reimplementing DETR, but I’m open to any other interesting vision transformer papers. If you have suggestions, please feel free to share!",
    "created_utc": "2024-11-10T13:09:33",
    "num_comments": 4,
    "comments": [
        "I can't remember if its ViT that optimized it or not, but there was one that sort of optimized transformers for images a little bit.  But basically Conv nets use a small kernel (1x1, 3x3, 5x5, 7x7 at most) For the convolution, so in parallel it doesn't eat much GPU.  However a transformer tends to compare every pixel in the image to every other pixel in the image.",
        "You should read about the ViT versions of Meta & Apple",
        "Hi here are a few suggestions \n\n1. The original authors of ViT suggested some improvements in the original ViT to achieve decent results. You can look that up. I believe it was giving good results on cifar10 from scratch.(Cannot find the link ugh!!)\n\n2. Usually, ViT is trained using DeiT strategy by meta, using a distillation token, which is expected since not everybody has a database of 300M images :). Maybe continue along those lines. Then check out CLIP as suggested in another comment.\n\nWhy ViT are data hungry intuition. Not an expert on ViT, but think about this. \n\nConvolutions have a strong assumption that information from one pixel will come from (k×k) kernel only. ViTs do not make this assumption and are more \"open-minded\". They allow all vectors to interact with each other. So since ViTs are more \"open-minded\", they need more examples to properly understand images. So they are data hungry.",
        "Oh sorry that’s CLiP not ViT."
    ]
},
{
    "submission_id": "1go9r7s",
    "title": "Best practices for managing large, continuously-growing image datasets?",
    "selftext": "I’m working with a massive image dataset (hundreds of thousands of images), which is continuously updated with new labeled data. I’d love to hear how other teams or individuals approach challenges like these:\n\n1. **Dataset Management**: How do you handle the management of such large datasets that are constantly expanding? Are there specific tools or workflows that help keep things organized and efficient?\n2. **Retraining Process**: How do you manage model retraining? Is it automated, and if so, what tooling do you rely on to streamline this?\n3. **Pipeline for Detection and Classification**: If your workflow involves running images through a pipeline (e.g., detection -> classification), what tools or platforms do you find most effective for setting up and managing these pipelines?\n\nAny insights or advice would be greatly appreciated! Thank you in advance.",
    "created_utc": "2024-11-10T12:16:21",
    "num_comments": 9,
    "comments": [
        "1. SQL DB to hold metadata per image. So far, for the imagery itself, everything fits in one large rack server, with tape backups off-site.\n2. Not automated, because we only retrain after we have done quality control on new imagery. Bad data tanks performance. \n3. Our models aren't for external use so it's just a remote procedure call, or, end users handle putting it into production, we just supply the model and train it.",
        "1. Internally we use a self-hosted lakefs, which offers version control, branches and tags. I also couple it with Weight and Biases to track models experiments.\n2. Retraining depends on multiple factors, the most important one are: do we have significantly more data, were there a problem in the initial dataset. Anyway, because model training is not cheap, the decision is manual.\n3. I'm not totally sure I understand your use case, but I am satisfied with the git + weight and biases for any kind of experiments. Usually when something takes multiple steps, we take and optimize the first step prior to the second one, this allows to work on intermediate representations and saves time and money, as you'll work and optimize your second step without having to rerun the first all the time.",
        "1 & 2) i have a postgresql database running on the cloud to manage Metadata about datasets. \n\nFor us, the data collector first uploads raw datasets to the cloud. I have a python script that checks raw datasets against processed ones and pulls down new ones and does pre processing, then uploads it to the cloud. I typically run this on my personal machine so I can quickly verify it.\n\nNow I have a set of good annotated datasets.\n\nI have another script that looks for any new datasets that haven't been seen by a given model yet, combines them with all existing datasets and initializes a new training execution. This is done on a cloud GPU server.\n\nNone of this is 100% automated. There needs to be some monitoring of the data. But it's only a few commands. \n\n3) Python and c++ all the way!",
        "Keep it simple. Assign each image a unique id, you can use a UUID or incrementing. Have a sql database or even just text/json annotation files that store the annotations. \n\nThis is something a single computer can easily handle. Not until you get into the hundreds of millions do you need anything fancy. \n\nOnly pursue automatic retrwjning  if you know for certain the new data is accurate: ",
        "For managing large and growing datasets, automating your ETL process can save a lot of time. Tools like Hevo can help streamline the ingestion, storage, and processing of image data. This way, the dataset stays organized as it grows. For retraining models, setting up an automated workflow that triggers new training sessions whenever fresh data is available can help maintain accuracy. When it comes to managing detection and classification pipelines, using platforms like TensorFlow alongside automated ETL tools can make everything more efficient.\n\nIs anyone else automating this process?",
        "Look into Fiftyone Teams with obejct storage.\n\nIn my experience the biggest problem with large and growing datasets is understanding the data and how it is changing over time.\n\nFiftyone is absolutely amazing for that.\n\nYou can then schedule automatic training/eval on every update or every X days.\n\nWeight and biases is quite nice to track these experiments but there are many other similar tools that are good and less expensive.",
        "[https://lakefs.io/](https://lakefs.io/) \\- cool, thanks for reference  \n[https://wandb.ai/](https://wandb.ai/) \\- will try it out, thanx!",
        "Plus one to this comment. Plus you can always start with Fiftyone OSS with pip install fiftyone to try it out. Handles all the database and management part for you so you just focus on the CV."
    ]
},
{
    "submission_id": "1go6qll",
    "title": "Missing Object Detection [Python, OpenCV]",
    "selftext": "Saw the missing object detection video the other day on here and over the weekend, gave it a try myself.",
    "created_utc": "2024-11-10T10:06:32",
    "num_comments": 16,
    "comments": [
        "lol now show a video of putting the item back and it saying ‘not missing’. This is like half of hot dog / not hot dog.",
        "Repo or it didnt  happen",
        "What would happen if you move the object?",
        "What happens when you \"slightly\" rotate the object?",
        "This can be considered as anomaly detection. You need to apply different tests. As others said, change the coordinates of the objects, rotate them, replace with different objects but the same type (remove bottle a and put bottle b), augment the image and observe. Otherwise, this can be done with a simple pixel subtraction.",
        "Well done bro 👏",
        "Car gone, car gone!",
        "Move da câmera :)",
        "This is cool! What's your motivation / what need do you hope to fulfill with this?",
        "My objective was this to detect if the object is missing or moved and creating an alert based on that.\n\nI am working on an inventory restocking problem as well where putting the item back is an actual use case, for this project it was not.",
        "I don't have it on git yet. Apologies. But I can tell the steps here:\n\n1. I compared various corner detectors and feature extraction techniques. Found ORB the most balanced one in terms of robustness and accuracy. AKAZE was faster but it was missing some points.\n\n2. Once I had the feature points, my next step was to cluster them to form BBs. I stored the first frame as template and matched subsequent frames for the same BBs from features.\n\n3. Used IOU to compare BBs and if the score dropped below a threshold, concluded that object is missing.\n\n4. I displayed the heatmap based on density calculated by feature points in a grid. Kept the grid pixel size small to have more accurate small areas.\n\n5. Using the point, put X on the missing areas spaced by some pixels and avoiding overlap.",
        "ctrl v ctrl c gang 😆",
        "To show how simple it is to do something like this [post](https://www.reddit.com/r/computervision/s/ZQwdK1epGe) earlier this week.",
        "~~OP also just copied from another example that was posted on reddit recently; but OP is not sharing any link, not even to the one where they copied it from. Shady af.~~\n\nThe original video didn't have link either: [https://www.reddit.com/r/computervision/comments/1gk66vg/missing\\_object\\_detection\\_c\\_opencv/](https://www.reddit.com/r/computervision/comments/1gk66vg/missing_object_detection_c_opencv/)\n\n  \nSorry about that."
    ]
},
{
    "submission_id": "1go5ugf",
    "title": "What would be a good strategy of detecting individual strands or groups of 4 strands in this pattern? I want to detect the bigger holes here, but simple \"threshold + blob detection\" is not very reliable.",
    "selftext": "",
    "created_utc": "2024-11-10T09:28:29",
    "num_comments": 12,
    "comments": [
        "Have you tried a band-notch filter? These patterns look regular enough to try some frequency filtering. Maybe preceded by a bit of smoothing. What control do you have over the acquisition process?",
        "Gut feeling says I should be looking for ML powered solution here, but having no experience in the field. Not looking for full answer, just some pointers for further research.",
        "Maybe do some line detection on localized regions assuming you can localize yourself. Would need more context to understand what limitations you’re working with",
        "You should go toward some kind of deep learning app",
        "This looks like an interesting problem..! I'd be interested to know how much and what kind of data you have?   \nDo you have any labelled data? I bet there are many feasible ways to label it, either with segmentations or bounding boxes, or even keypoints, depending on the desired result. Does your data vary a lot, or are the images mostly similar to this one? \n\nMy best guess would be to use some neural net model (e.g. YOLO), and combine that with some more classical computer vision methods, if necessary. Again depends on the desired outcome. \n\nIf you have some more data you would like to share, I'd be happy to take a look and experiment with a few different things.. I work in a small data-curation/model-understanding software company (not trying to sell you anything! :)) and I am experimenting with all sorts of different computer vision datasets recently, trying to use our product to get better and faster results in different domains. And this looks like a fun challenge.",
        "Whoa what is this? SEM imaging of a fabric weave? What fabric? Is this for detecting flaws in production? So fascinating",
        "And you are right. If it's a commercial project, look if cognex vidi or Halcon ML is feasible. Cognex even had a stitching error as one of their samples.",
        "the data I have is actual fabric, they vary a bit in weaving style but keep the feature of grid-like pattern of larger holes in it.  \nThis photo is snapped with iphone, but I can up the quality of photos with actual full frame camera if needed, I feel like iphone sharpening drops way too much detail in there.\n\ni'm currently researching labeling approach, but can't decide what objects I want to detect there, and actual labeling is very labor intensive, if not going for such extreme closeup, each photo might be 100+ objects, regardless of labeling strategy chosen.\n\nCurrently thinking to invest time into labeling intersection squares (where each bigger hole is a corner of that square), and just brute-force something. I'm just afraid that this strategy won't give very precise location of the holes though, but will see if post-processing will yield something.",
        "tbh, just very early research for a potential toy project at the moment:) this is a cross-stitch fabric, that's why it has a grid-like pattern of bigger  holes in it.",
        "What are you trying to achieve in the end? Determine weaving pattern type, find defects, count strands, etc?",
        "My suggestion would be to try out a lazy annotation loop. You might be surprised how little manual labelling is required, and how little extra data you need to fine-tune. \n\nI advocate a iterative labelling technique where the user and the model take turns generating labels. \n\nFor example: \n\n\\+ Start by manually labelling a small number of samples.\n\n\\+ Fine-tune a detection/segmentation model on the labelled samples.\n\n\\+ Use the model to infer labels on all the samples.  \n\\+ Go through the samples and inspect the predictions. Where the predictions are good, accept them as new labels. Where they are incorrect, correct them and add them to the label set. \n\n  \nRepeat the steps; fine-tune, infer, inspect. \n\nI have had great success with this strategy, across a wide range of computer vision domains. Many hours of manual labelling have been saved!\n\nSend me a message if interested to hear more, as I mentioned, if you have some images to share I can take a look! (I'm always looking for new examples to apply the interactive labelling/training approach ;))"
    ]
},
{
    "submission_id": "1go1r70",
    "title": "[R] Can I publish dataset with baselines as a paper?",
    "selftext": "I am working on a dataset for educational video understanding. I used existing lecture video datasets (ClassX, Slideshare-1M, etc.,), but restructured them, added annotations, and did some more preprocessing algorithms specific to my task to get the final version. I thought that this dataset might be useful for slide document analysis, and text and image querying in educational videos. Could I publish this dataset along with the baselines and preprocessing methods as a paper? I don't think I could publish in any high-impact journals. Also I am not sure whether I could publish as I got the initial raw data from previously published datasets, as it would be tedious to collect videos and slides from scratch. Any advice or suggestions would be greatly helpful. Thank you in advance!",
    "created_utc": "2024-11-10T06:24:10",
    "num_comments": 5,
    "comments": [
        "Yeah you’d be able to publish a new dataset/benchmark, the MLSys has a chapter on benchmarking that might be relevant: https://mlsysbook.ai/contents/benchmarking/benchmarking.html\n\nAnd you could use a previously existing dataset and add new labels to it, or further curate it. For example, LVIS is just COCO with more classes, and Ref-COCO is just COCO with captions. Of course, goes without saying, be sure to cite the original paper. \n\nHere’s another paper that might be helpful for you as well: https://insightsimaging.springeropen.com/articles/10.1186/s13244-024-01833-2\n\nIf there’s anyway I can help you on this, let me know!",
        "Thank you so much for sharing! I will look into them.",
        "The mlsys source looks very useful, can't believe I've never seen it before. Thanks for sharing",
        "I see that most of the benchmarks are published in conferences. Can it be published in journals?",
        "There’s some great modules in it, especially the labs"
    ]
},
{
    "submission_id": "1go0w43",
    "title": "[Dataset Request] Looking for Animal Behavior Detection Dataset with Bounding Boxes",
    "selftext": "Hi everyone,\nI'm a college student working on an animal behavior detection and monitoring project. I'm specifically looking for datasets that include:\n\nPhotos/videos of animals\nBounding box annotations\nBehavior labels/classifications\n\nMost datasets I've found either have just the images/videos without bounding boxes, or have bounding boxes but no behavior labels. I need both for my project.\nFor example, I'm looking for data where:\n\nAnimals are marked with bounding boxes\nTheir behaviors are labeled (e.g., eating, running, sleeping, hunting) like in the photo given.\nPreferably with temporal annotations for videos\n\n\nHas anyone worked with such datasets or can point me in the right direction? Any suggestions would be greatly appreciated!\nThanks in advance!",
    "created_utc": "2024-11-10T05:40:22",
    "num_comments": 7,
    "comments": [
        "Checkout the Animal Kingdom dataset: \n\nhttps://openaccess.thecvf.com/content/CVPR2022/papers/Ng_Animal_Kingdom_A_Large_and_Diverse_Dataset_for_Animal_Behavior_CVPR_2022_paper.pdf\n\n•50 hours of annotated videos\n•30K video sequences for action recognition\n•33K annotated frames with bounding boxes\n•Covers 850 species across 6 major animal classes\n•Includes behavior annotations and temporal labels",
        "Quick tip, if you just have the bounding boxes themselves, you can crop them and write a prompt for a local VLLM (e.g. Qwen2-VL) asking it to return one of the choices for the cow in the image.\n\nEven COCO has quite a few animal annotations, but without behaviour of course.\n\nGood luck!",
        "This has behavior patterns but it doesn't contain the bounding boxes for particular animal. It detects the behavior for the complete photo. This can cause problems if we have more than one species doing more than one behavior.",
        "Thanks! I will try doing that",
        "ah yeah, I see I was wrong about that...they have the poses but not boxes. Would having the poses be useful? You could use something like YOLOWorld to get boxes though",
        "Could you tell me more about YOLOWorld? How can it help?",
        "It’s a zero shot detector. You can prompt it to detect the particular animal you’re interested in, or maybe just the phylum of the animal (whichever one makes most sense for you)"
    ]
},
{
    "submission_id": "1gnwgub",
    "title": "Help needed to run 3D model generation code with .ckpt files on CUDA 12.5 GPU (RTX A6000)\n\n",
    "selftext": "[https://github.com/zhenpeiyang/FvOR/](https://github.com/zhenpeiyang/FvOR/)\n\nHey everyone,\n\nI'm working on a project to generate 3D models from a few images, using code from a GitHub repo related to a recent paper. The repository provides pretrained `.ckpt` files, which I want to use to generate 3D models from image inputs. I have access to a server equipped with an NVIDIA RTX A6000 GPU, but I'm running into issues with CUDA compatibility.\n\n# Issue\n\nThe code is designed for CUDA 10.1, but the server GPU operates on CUDA 12.5. When I try to execute the code, I get an error indicating a CUDA version mismatch, which stops me from running the model. Downgrading CUDA isn’t an option here since I’m using a shared server environment.\n\n# Things I've Considered\n\n1. **Docker**: I’m thinking about setting up a Docker container with CUDA 10.1 to run the code in a compatible environment. However, I’m new to Docker, so if anyone has advice on how to set up and configure it specifically for CUDA 10.1 compatibility, I’d appreciate it.\n2. **PyTorch Compatibility**: I’m using PyTorch 1.7.1. Would upgrading/downgrading PyTorch in Docker help avoid any CUDA compatibility issues? Any tips on which version to target?\n\n# Additional Info\n\n* **GPU**: NVIDIA RTX A6000\n* **CUDA Version**: 12.5 (host)\n* **PyTorch Version**: 1.7.1\n* **Goal**: Use the provided `.ckpt` files with input images to get a 3D model output.\n\nAny guidance on setting up Docker or resolving the compatibility issue would be greatly appreciated. Thanks in advance!",
    "created_utc": "2024-11-10T00:51:05",
    "num_comments": 3,
    "comments": [
        "I'd say if your final goal is just to generate the outputs without considering the optimization or latency, setup the code in a second device in which you can install Cuda 10.1. Next create an http server to handle requests then use docker (its fairly easy to set up a container) and create a stand alone container.\nFew considerations, although it's pretty straightforward to set up a container, setting up a Cuda enabled container is not that easy, especially when there is a mismatch between the host Cuda version and the guest Cuda version.\nAn alternative choice could be rewriting the code in C++, but I suppose you don't want that. But using C++ you can easily compile your code in any Cuda available machine and run it elsewhere if you maintain the shared library dependencies. This is pretty rough but I think it's the best way possible.\n\nEdit:  by rewriting I mean torch.nn.Module => torchscript (or onnx) => libtorch api ( or onnx runtime)",
        "I usually solve cuda mismatches with installing older cuda toolkit and nvcc in a new conda enviroment, works like a charm",
        "You could honestly probably just upgrade pytorch and you'll be fine.  IDK have the rubric in front of me about pytorch and CUDA compatibility but one exists.  It might even be your version of pytorch is fine, but you need the one compiled for your current version of CUDA."
    ]
},
{
    "submission_id": "1gnq6rz",
    "title": "Opencv + Harvester + DALSA GigE",
    "selftext": "Hello.\nI'm having a problem acquiring images from Harvester, which in turn will use Opencv to display and record the images.\nHas anyone used these two libraries together and managed to use them on a DALSA Linear camera?\nI really need some help on this topic. I can send settings (acquisitions and triggers) but when I get to the buffer it's empty.",
    "created_utc": "2024-11-09T18:13:00",
    "num_comments": 2,
    "comments": [
        "First, ensure your acquisition and trigger settings in Harvester match the camera’s specifications. Test capturing a single frame with Harvester alone to confirm the camera connection. Properly release and clear each buffer after acquisition, as improper buffer management can lead to issues. Also, check that your acquisition timing and trigger settings are synchronized, as line-scan modes require precise timing. If the issue persists, enable debugging logs in Harvester to check for dropped frames or trigger issues.\n\nSharing your settings may help further troubleshoot.",
        "Hi, sorry for the late reply.\n\nOf course I'll share the settings I'm sending to the camera. I checked the Linea Color GigE manual and it contains these settings.  \nI had to reduce the code a lot to fit in the comment!\n\n    h = Harvester()\n    \n    \n            gentl_path = 'C:/Program Files/MATRIX VISION/mvIMPACT Acquire/bin/x64/mvGenTLProducer.cti'\n            h.update()\n    \n            print(f\"Number of devices found: {len(h.device_info_list)}\")\n            for device in h.device_info_list:\n                print(f\"Device: {device.vendor} {device.model}\")\n    \n            if len(h.device_info_list) == 0:\n                print(\"No devices found. Exiting.\")\n                return\n    \n            device_info = h.device_info_list[0]        ia = h.create(0)\n            ia.num_buffers = 3        # Check the node map\n            print(\"Inspecting node map\")\n            node_map = ia.remote_device.node_map\n    \n            # Setup image\n            line_rate_node = ia.remote_device.node_map.get_node('AcquisitionLineRate')\n            line_rate_node.value = 1000  \n            exposure_time_node = node_map.get_node('ExposureTime')\n            exposure_time_node.value = 500.0\n            width_node = node_map.get_node('Width')\n            width_node.value = 2048\n            height_node = node_map.get_node('Height')\n            height_node.value = 300\n            pixel_format_node = node_map.get_node('PixelFormat')\n            pixel_format_node.value = 'BiColorRGBG12p' \n            acquisition_mode_node = ia.remote_device.node_map.get_node('AcquisitionMode')\n            acquisition_mode_node.value = 'Continuous'            \n            trigger_mode_node = node_map.get_node('TriggerMode')\n            trigger_mode_node.value = 'On'\n            trigger_source_node = node_map.get_node('TriggerSource')\n            trigger_source_node.value = 'Software'        \n            ia.start()\n            print(f\"Acquisition status: {ia.is_acquiring()}\")\n    \n            for i in range(3):\n                print(f\"Fetching buffer {i + 1}\")\n    \n                ia.remote_device.node_map.TriggerSoftware.execute()\n    \n                buffer = ia.try_fetch(timeout=5)\n                print(f\"Buffer info: {buffer}\")\n                print(f\"Buffer acquisition status: {buffer}\")\n                print(f\"Is the device still acquiring: {ia.is_acquiring()}\")"
    ]
},
{
    "submission_id": "1gnj6im",
    "title": "Is it feasible to use drones and computer vision to detect stains on golf course grass?",
    "selftext": "Hello r/computervision community! I am working on a project that seeks to apply computer vision to optimize the maintenance of golf courses. The idea is to capture images and videos of fields using drones, and then process this data with an AI model capable of identifying spots and other anomalies in the grass, such as dry or disease-affected areas.\n\nMy current approach:\n\nData Capture: I plan to use drones to obtain high resolution aerial images. My main question is about best practices for capture: what would be the optimal flight height and camera settings to capture relevant details?\n\nProcessing model: My idea is to use image segmentation and classification techniques to detect deterioration patterns in grass. I'm considering methods, but I'm open to suggestions on more efficient algorithms and approaches.\n\nQueries and doubts:\n\nWhat specific computer vision algorithms could improve accuracy in identifying spots or irregularities in grass?\n\nDoes anyone have experience handling data captured by drones in outdoor environments? What aspects should I take into account to ensure quality data (such as lighting conditions, shadows, etc.)?\n\nDo you think this approach is viable to create a predictive and automated system that can help golf course maintenance managers?\n\nI appreciate any advice, experience or resources you can share. Any suggestion is welcome to improve this project.\n\nFor more information I leave my account [https://www.linkedin.com/in/ranger-visi%C3%B3n/](https://www.linkedin.com/in/ranger-visi%C3%B3n/)\n\nThank you for your time!\n\nhttps://preview.redd.it/72uez0darxzd1.jpg?width=800&format=pjpg&auto=webp&s=006b01463b0b0cc7358890fa0d7dd6770fff7b24",
    "created_utc": "2024-11-09T12:29:58",
    "num_comments": 13,
    "comments": [
        "You could use a greenness or NIR index (eg NDVI or similar) if you have access to a multi spectral camera. Apply some thresholding and basic spatial analysis and you'll have your spots",
        ">what would be the optimal flight height and camera settings to capture relevant details?\n\nSlow and above the trees if at all possible \n\n>What specific computer vision algorithms could improve accuracy in identifying spots or irregularities in grass?\n\nIn my experience unets are good for segmentation based on texture analysis. You could train one as an anomaly detector\n\n>Does anyone have experience handling data captured by drones in outdoor environments? What aspects should I take into account to ensure quality data (such as lighting conditions, shadows, etc.)?\n\nGet data from multiple times of day and from multiple weather conditions if at all possible, add a hue shift in your preprocessor, and if shadows are a big problem, try a fast retinex filter to the input. Keep in mind that any filtering you add to the data to train the model with will need to be done in real, or near real, time once you're running inference.\n\n>Do you think this approach is viable to create a predictive and automated system that can help golf course maintenance managers?\n\nI think you should ask a golf course maintenance manager this question.\n\nAlso I would recommend trying it with, or in conjunction with, data with a thermal camera. Idk if these spots have a difference in temp, but in my experience thermal data can be very effective",
        "Not a computervision expert like some of my coworkers, but working on the drone side. \n\nA few things to keep in mind when doing mapping tasks with the drone. I am here assuming you want to do a typical drone mapping of an area. You might have an usecase where you don't need that detail or can get away with a few images from high up to spot areas, and don't need a irl coordinate encoded into the image.   \n  \nIf you want to make a orthomosaic (stitched image) of the area you are flying over, you need to figure out what GSD (Ground sampling distance) you want. Essentially how much distance in the real world do you want 1px on your image to cover. \n\nGSD controls your flight-height based on your camera specifications, and thus also your flight-time. The lower flight-height, the lower GSD usually. But also longer flight to cover the entire area and more images to stitch together, as you want 80% overlap of images. \n\nThe camera can also limit the flight-speed of your drone. Cameras with rolling shutter means you can't move too quick otherwise you get motion blur on the images. Hence why most mapping drones utilize global shutter cameras where you can fly as fast as you want essentially. \n\nMake sure you check your drone regulations and rules. If you are in EU, you would fly in open category, unless you do risk-assessments (SORA) for specific category. That will give you some limits to flight-height, and distance away from operator. VLOS vs. BVLOS. Easa have pretty decent websites explaining the rules in general: [https://www.easa.europa.eu/en/domains/drones-air-mobility/operating-drone/open-category-low-risk-civil-drones](https://www.easa.europa.eu/en/domains/drones-air-mobility/operating-drone/open-category-low-risk-civil-drones) \n\nThe benefit of working on a orthomosaic image is that you get one big image that covers the entire area, and each pixel also have coordinates encoded. So you know where in the real world this pixel corresponds to\\*. (To the limit of the GPS precision you have when flying. If you want high precision, you need known ground-control spots and RTK GPS). \n\nDownside is that it is extra steps and the resulting file you get can get fairly big in size, due to the detail it has. So you have to take that into account when running it through your computervision algorithms and might need to process segments of the image at a time, instead of the entire thing. \n\nYou can use the excellent opensource GIS program QGIS, to view orthomosaics after they are build. There also exists open-source programs to make orthomosaics like WebODM (OpenDroneMap) that gives alright results. (Metashape and other more professional solutions costs money, but I have been told also gives better results)\n\nFor drone capture, there is a ton of paid and free solutions with each their limitations or support to 2D map an area. Pix4D Mapper is a free program for the mapping. But here it often depends on what drone you are flying with and who support it, or does it have inbuilt mission-generation for mapping tasks. \n\nIn terms of lighting conditions when capturing, my computervision coworkers usually tell me the best days are overcast As you ideally don't want the lightning conditions to change while you are flying to get the images, as you will end up with some images in very different lighting than others, which makes getting a good stitching result much harder. Overcast means no shadows and generally most of the time similar lighting level as there is no direct sunlight.  \n  \nIf you fly a day with no clouds and just full sun, do it as close to 12.00 as possible, to ensure as little shadows as possible when seen from above.\n\nRun the exposure so the image is generally slightly darker than you would like. Overexposure looses information, so you want it slightly darker, and then in post, you can stretch the image to get the light you want, without risking loosing details.   \n  \nChecking your histogram on the drone liveview and ensuring that you are not cutting anything off at the bottom or the top is a good way to go about it.",
        "Instead of drones I would see if you can do it with commercial satellite sources that image the surface of the earth on a recurring basis. That is a far more scalable solution and you could eventually be selling subscriptions to thousands of golf courses around the world without the high cost of drones and operators.\n\nCost varies but it could be surprisingly cheap to get high resolution multi spectral image tiles for an entire golf course that are updated more often than you could afford to pay drone operators for. ",
        "For more information I leave my account [https://www.linkedin.com/in/ranger-visi%C3%B3n/](https://www.linkedin.com/in/ranger-visi%C3%B3n/)",
        "I think everyone here has good suggestions. Really what type of model you use is dependent on what type of dataset you have and how you want to deploy it. UNets are great, but pixel level analysis with something like PLS-DA or linear regression can be quick proof-of-concept models to test.\n\nI actually worked on this exact same problem not long ago. Data quality was a much bigger hurdle than model selection. Get good ground truth data from course managers: types of damage, areas of damage, types of grass (sometimes there may be multiple species of grass on the same hole), and any management practices such as tarping or fertilizer. Also, make sure to get calibration ground truth at each flight if possible: reflectance ground panels and irradiance measurements from a DLS. \n\nAlso, take good ground-level photos with something like a cell phone camera for reference. At the same time remember the view from the drone may be different than you expect from the ground. For instance, we found that grass looked healthy and uniformly green from the ground, but spots seemed dead from the Drone POV due to more sand in the soil. \n\nAlso try to note time of year and weather conditions. Greenness will vary, and it does not necessarily mean the grass is experiencing undue stress. Also different turf grass may have different responses to these conditions, and if they are mixed. 50/50 bluegrass and bentgrass mix may behave differently from a 75/25 mix. Also sometimes course will patch greens with different grasses. A green may be 100% bluegrass the. They patch a section with bentgrass. I don’t know why they do this, but we saw it in our dataset.",
        "Excellent explanation, thanks for your time.",
        "Tk",
        "That’s where my mind went as well. There are dozens of spectral indices that could be used as a starting point\n\nhttps://github.com/awesome-spectral-indices/awesome-spectral-indices\n\nhttps://github.com/awesome-spectral-indices/spyndex?tab=readme-ov-file",
        "+1 on the above, especially speaking with the maintenance supervisor. Computer vision is the “automation of human sight,” so nothing better than getting opinions of the expert that does the role.\n\nAnother thing I’d add is layering classical CV techniques for pixel color comparison would also be helpful based on the sample image in your post.",
        "How are you? Thank you for your opinion, it is planned to add satellite images to the software, it is also more precise.",
        "Tk",
        ">Another thing I’d add is layering classical CV techniques for pixel color comparison would also be helpful based on the sample image in your post.\n\nThis is kind of what I was saying about including a retinex step. It's very much a classical method that  increases local contrast at smaller scales it's excellent as an edge enhancer or local contrast comparison. It's a *very* versatile filter op, however in my experience most implementations are *far* from real time and I know from experience that they can be made *much* faster (at least 60fps on single layer matrices"
    ]
},
{
    "submission_id": "1gnducj",
    "title": "Please check my convolution approach.",
    "selftext": "I am using armadillo library C++  here. I have written 1d convolution function below. Kindly say any improvement here that is proper way to perform convolution on computer. I see there are some different approach in mathematical  formula  of convolution and how it is implemented (like padding). I am here writing convolution for first time and want to  do it properly. I can clearly see difference in formulation of this operation vs the implementation on computer and there is a proper addressable gap\n\n    void conv1D(row_space signal, row_space kernel)\n    {\n    signal.insert_cols(0, 1);\n    signal.print(\"padded signal\");\n    row_space response(signal.n_cols+kernel.n_cols);\n    for (int n = 0; n <signal.n_cols; n++)\n    {\n    \n     float sigmasum = 0.0f;\n    \n    for (int m = 0; m < kernel.n_cols; m++)\n    {\n    \n    if(n-m>=0)\n    sigmasum += (signal[n - m] * kernel[m]);\n    \n    }\n    response[n] = sigmasum;\n    }\n    \n    response.print(\"response\");\n    \n    return;\n    }\n    \n\nnote :I know armadillo has convolution function. Yet I am implementing.",
    "created_utc": "2024-11-09T08:29:28",
    "num_comments": 0,
    "comments": []
},
{
    "submission_id": "1gnd1kt",
    "title": "How to pass objects between models running in different conda environments? ",
    "selftext": "At a basic level, what are the best practices to building pipelines that involve conflicting dependancies?\n\nSay for example I want to loa a large image once then simultaneously pass it into model A that requires PyTorch 2.* and also model B that requires PyTorch 1.*, then combine the results and pass them into a third model that has even more conflicting dependancies.   \n\nHow would I go about setting up something like this? I already have each model working in its own conda environment. What I'm hoping to have some kind of \"master process\" that coordinates the others. This is all being done on a Windows 11 PC.\n",
    "created_utc": "2024-11-09T07:52:39",
    "num_comments": 19,
    "comments": [
        "You can containerize (docker) each app separatelly and connect them via rabbitmq (also docker container). \n\nFinally, run them together using docker compose.",
        "Use a microservice approach where each model has its own docker container. Or use existing tools to host your models such as torchserve or triton",
        "Is this for a home project or for a production solution? \n\nThe multiple docker-micro services approach / save to disk i think is fine for a home project, proof of concept, etc, but for a production setup or edge app that users run, id consider:\n\n* converting your models to a standard serialized format, like ONNX or updated Pytorch 1 code to 2\n* remove the multiple pytorch dependencies and run your models in the same process where with the same execution environment, so they can share GPU memory outputs and avoid all overhead, like ONNX or TensorRT.",
        "Depending on what you are trying to achieve, you can go a long way just writing data via the filesystem using standard python serialization (or for torch, use their save/load functions for safetensors assuming compatibility between pytorch versions you're using). And yeah, a master process to control invocation of the other environments and models and to aggregate results.\n\nAt some point it might make sense to get into RPC and/or message passing queue systems. But that's more about architecting a production system. If you're just running something on a single PC, no need to make things over engineered.",
        "unless you have to run them on different machines for hardware constraints, convert them to onnx.",
        "You could use shared memory with torch.\n\nhttps://stackoverflow.com/questions/50735493/how-to-share-a-list-of-tensors-in-pytorch-multiprocessing",
        "You can use grpc protocol for transferring the data between scripts. Chatgpt can help",
        "If each model is ready working in its own environment then you basically have a bunch of processes that need to talk to one another. Then RPC would be the way to go.\n\n\nYou can use any communication software (e.g., rabbitmq like one of the commenters suggests) but no need to containerise (since it only add more overheads).",
        "Honestly don't, you'll just add to the sea of papers where nobody knows if the numbers you're showing are an artifact of implementation quirks, random seed or something else entirely, and to the mass of git repos where the issues tab is full of people saying they can't reproduce with no solutions being offered.",
        "Obviously a total noob question, but doesn’t that mean having an entire OS running for each model? I’m constrained to Windows 11 and am not allowed to touch Linux or any other free alternatives. ",
        "For production something like NVIDIA Triton is an Option.",
        "Do you think serialization and safetensors is reasonably efficient? The inputs to the entire pipeline are 24 megapixel JPG photos and some preprocessing is needed after loading before I can feed them into the different models. What I’m hoping to do is keep the preprocessed bytes in memory rather than saving them to disk, although I suppose a benchmark is in order to see if this is even worth worrying about..",
        "Would that include the different preprocessing functions and stuff like that? ",
        "Oh this is good to know! I’ll Google that some more…",
        "This is for actual work not research. Nothing will be published. ",
        "Run Docker through WSL. ",
        "Docker does special things internally that makes it act like a VM, but its not a VM and uses a lot less memory",
        "Given that modern computers tend to have NVMe or at least SSD drives, disk is pretty quick.\n\nIf you are working with images, then writing out PNGs or other lossless images makes it trivial to inspect and debug your pipeline.\n\nBut it obviously depends on your usecase and latency requirements. If you are doing realtime processing and need 30fps processing of 24MP imagery then that might very well be an argument to keep it in memory, and ideally in a GPUs VRAM. But that leans towards having to ensure your components use the same version of pytorch.",
        "Preprocessing can easily be rewritten into which ever environments version"
    ]
},
{
    "submission_id": "1gna93p",
    "title": "Open source drone localization using RasPi 0w + RasPi cameras - details in comments",
    "selftext": "",
    "created_utc": "2024-11-09T05:36:33",
    "num_comments": 2,
    "comments": [
        "Basic localization using cameras. Not the most accurate nor fast, but hopefully still a good proof of concept. I used raspberry pi 0ws with socket to send images to my computer, where it calculates the relative positioning. Also makes use of ADXL345 accelerometers for rotational invariance. More details including the shopping list on my blog: [https://matthew-bird.com/blogs/Drone-Rel-Pos.html](https://matthew-bird.com/blogs/Drone-Rel-Pos.html)\n\nGitHub Repo: [https://github.com/mbird1258/Drone-relative-positioning](https://github.com/mbird1258/Drone-relative-positioning)",
        "bro lowk I'm a huge fan of ur desk setup"
    ]
},
{
    "submission_id": "1gmw6ii",
    "title": "Segment Anything - Too Much Details ",
    "selftext": "Hi there, I need to segment out each individual DVD cases from photos, most of the times, they are assorted and I tried to use the [Auto Mask Generator](https://github.com/facebookresearch/segment-anything/blob/main/notebooks/automatic_mask_generator_example.ipynb) from SAM. The outcome is great, too great that they overly divided one instance of a DVD into many smaller segments. (for example, the DVD logo, the publisher logo, even individual characters of the movie title). I tried to [tune the parameters](https://github.com/facebookresearch/segment-anything/blob/main/segment_anything/automatic_mask_generator.py) but not that much luck. \n\nHere are my questions: \n\n1. Is there any levers from SAM that I should focus on tuning to combine those details with the case and turn each DVD into one mask per DVD? \n\n2. Given the unique requirements of my use cases, is there any other easier/better techniques I should explore as SAM feels like a bit heavy and time consuming. (taking almost 1 minute to segment one image). \n\n3. if I will have to retrain/fine tune my own segmentation model, can you point me to the right direction? \n\nWhat I have tried:   \n1. there is parameter called min\\_mask\\_region\\_area but doesn't seem to work at all, I still get a lot of small masks and SAM's github repo issues are not that active. \n\n2. As I have detailed location/area of the masks, worst scenario, I can run some clustering to combine different masks. (eg. if a small mask exist within another mask and the other mask looks like a rectangule, combine it), but it feels like hacking to me. \n\nhttps://preview.redd.it/twf5lba9frzd1.png?width=720&format=png&auto=webp&s=30a83b42aea367e73a774bf1c11c343447b3afae\n\n",
    "created_utc": "2024-11-08T15:23:04",
    "num_comments": 13,
    "comments": [
        "Increase stability_score_thresh for fewer, larger segments by merging smaller ones. Lower points_per_side for coarser segmentation. Adjust pred_iou_thresh to control segment merging. Or just use YOLO if you’re looking for a streamlined object detection model. It’s faster and efficient for broad segmentation tasks without much parameter tweaking.",
        "What’s your end goal? Can you do ocr instead to get each one?",
        "You could have a look at Grounded Sam as well.\n\nIt's Grounding Dino + Sam",
        "I wonder if you’d be better served with classical vision methods maybe combined with a simpler type of domain-specific object detection or segmentation model. \n\nAre all of your images this easy, where simply detecting parallel lines of similar length and/or spacing would do the trick? \n\nNot opposed to foundation models but they’re usually handily beat by domain specific methods. Breadth be depth of knowledge.",
        "You can try semantic-SAM and adjust the levels of granularity with it to get the desired outcome. Most prolly 1 or 2 will do the work.",
        "\nI see you've posted a GitHub link to a Jupyter Notebook! GitHub doesn't \nrender large Jupyter Notebooks, so just in case, here is an \n[nbviewer](https://nbviewer.jupyter.org/) link to the notebook:\n\nhttps://nbviewer.jupyter.org/url/github.com/facebookresearch/segment-anything/blob/main/notebooks/automatic_mask_generator_example.ipynb\n\nWant to run the code yourself? Here is a [binder](https://mybinder.org/) \nlink to start your own Jupyter server and try it out!\n\nhttps://mybinder.org/v2/gh/facebookresearch/segment-anything/main?filepath=notebooks%2Fautomatic_mask_generator_example.ipynb\n\n\n\n------\n\n^(I am a bot.) \n[^(Feedback)](https://www.reddit.com/message/compose/?to=jd_paton) ^(|) \n[^(GitHub)](https://github.com/JohnPaton/nbviewerbot) ^(|) \n[^(Author)](https://johnpaton.net/)",
        "Points per side is tricky, in this photo, we can use only two points per horizontal line but clearly there are many DVDs in one photo, which requires sufficient amount of points vertically, maybe even more than the default 32. \n\nAbout Yolo, I heard people use Yolo to get the bounding box first, and then use it as a prompt to feed SAM, or you are just suggesting to directly use Yolo for segmentation task. Like here? \n\n[https://docs.ultralytics.com/tasks/segment/#models](https://docs.ultralytics.com/tasks/segment/#models)",
        "The end goal(s) are several folds:   \n1. to digitally catalog all the collections (which requires to do instance level detection and identification). \n\n2. a reverse search (why accurate mask) needed, do I have the movie \"Forrest Gump\" if so, where it is. \n\nTraditionally OCR doesn't work really well on this application (maybe accurate only 50%), slightly artistic font styles that tend to be associated with media (Movies/DVDs) are hard to be recognized (I tried Tesseract).",
        "For this application, you are probably right but I really do not have the know-how of \"detecting parallel lines of similar length / or spacing\", indeed, very majority of the images will be stacked objects that follow certain pattern, I wish there is a way I can prompt SAM \"hey, they are all boxes, only show me boxes\". ChatGPT told me I can run some postprocessing to handle connected components, if a mask is a rectangle that looks like a DVD case, connect it with all the sub components that fall spatially within it - dilation and erosion, but I do not know how well that will generalize.",
        "looks promising. I tried to set it up on my windows but no luck. their demo also stopped working.",
        "If you just need to count DVDs, you could use OCR to detect each text line as a separate DVD spine. This is quick and works well if all DVDs have readable text on the spines.\n\nIf you need precise masking for each DVD, try training YOLOv8 with instance segmentation. Label each DVD as a separate object to create masks, so YOLO can both count and mask each DVD case accurately.",
        "Tesseract is probably the worse one. \n\nBest OCR Models to Extract Text from Images (EasyOCR, PyTesseract, Idefics2, Claude, GPT-4, Gemini)\nhttps://youtu.be/00zR9rJnecA",
        "I am using [yolo11x-seg.pt](http://yolo11x-seg.pt) and it indeed did a good job. I just need to tune down the confidence to be really small and all the DVDs will be recognized as books. Thanks for the help. \n\nI will now follow your advice to train my own model and see how well it will perform. Stay tuned."
    ]
},
{
    "submission_id": "1gmtxf5",
    "title": "How to generate 3D CAD model from many images with known (perfect) object pose and camera intrinsics?",
    "selftext": "[https://kelvins.esa.int/pose-estimation-2021/data/](https://kelvins.esa.int/pose-estimation-2021/data/)\n\nI'd like to use the dataset above and some sort of photogrammetric approach to reconstruct a 3D model of the satellite. The dataset contains thousands of renders of the satellite and gives accurate 6dof pose and camera intrinsics for every image. I'd probably simplify things by filtering out the images that have the Earth in the background.\n\nI'm decent with python and could slog through this myself but thought I'd ask here to make sure I don't have any blindspots for useful packages or new approaches. I don't care about state of the art - I just want a CAD model with reasonable geometric accuracy and a grayscale texture map. And I don't want to pay $1000 for professional photogrammetry software. I know with basically perfect information this should be easier/better than typical photogrammetry.\n\nThank you so much in advance!\n\n----------\nDoing this myself, I'd probably just try using cv2 SIFT to find keypoints, project those into 3D with the camera matrix, and inverse the ground truth rotation and translation for every image to build a sparse point cloud. Then find some tool to turn that into a mesh. Then I'd project images back into the point cloud to compute a mean grayscale value to assign to each portion of the mesh.",
    "created_utc": "2024-11-08T13:42:08",
    "num_comments": 6,
    "comments": [
        "NeRF.  I'd use NVIDIA's Instant NGP (https://github.com/NVlabs/instant-ngp)  because it's very fast and basically works out of the box.  You can generate a colored OBJ file of the final model when it's finished training.\n\nCOLMAP is usually the slowdown here, but if you already have very (very!) accurate 6DOF camera poses, you may be able to skip that step entirely, in which case it will take seconds to generate a final 3D model.",
        "Skip NeRF, go straight to Gaussian Splatting. AFAIK, colmap is still an obligatory step tho.",
        "Thank you. I was previously reading up on COLMAP, but I look forward to trying Instant NGP. The images are synthetic renders so the poses and intrinsics should be perfect down to whatever floating point precision they provide in the json. The real images were generated with a satellite on a robotic arm and also have near-perfect poses, but I plan to try only using the synthetic images since the real images are supposed to be a fully independent test set. I'll report back if I learn anything useful. Thank you!",
        "It looks very nice as a NeRF but like garbage when I create a mesh from it and load it into blender.",
        "You need to turn up the resolution; I can't remember the name of the parameter but it's one of the sliders in the UI.  It will take more memory, so you'll have to experiment to find the quallity/doesn't-run-out-of-RAM compromise.",
        "Ty. I adjusted the density threshold and that got rid of most of the floaters in the mesh."
    ]
},
{
    "submission_id": "1gmp3km",
    "title": "Liveness model problem? ",
    "selftext": "I am working on creating a liveness model to classify real Or spoof. I have only two class which real person and second is  photo of screen/photo. I have dataset of around 80k images still not getting good result on resnet 152. Any suggestion? ",
    "created_utc": "2024-11-08T10:14:21",
    "num_comments": 15,
    "comments": [
        "https://www.reddit.com/r/computervision/s/i4Mkiif9vI",
        "You might take inspiration from the following:\n\n- https://github.com/sakethbachu/Face-Liveness-Detection\n\n- https://github.com/MehediZ/face-liveness-detection",
        "It should be pretty straightforward with 80k images provided that your classes are balanced and the intraclass variability is not that high.\n\nI’m thinking that your model might be too huge, and it overfits? Even something as small as mobilenetv2 with alpha 1.0 should be enough to get pretty decent results for this task",
        "CV is basically all about the images. Share a sample so we know what you’re working with. \n\nMy guess is you need a model they can detect subtle textural differences like aliasing from an LCD screen. ",
        "Thanks but i already tried them  it is not working good.",
        "i have also tried this but not effective",
        "Yes, If I click a picture of a picture, it should be able to detect it's a spoof.",
        "Then it’s something wrong with your data, or the way you prepare it. Cause otherwise, it’s a pretty simple task where you’d get at least 95% accuracy with almost any model, and even then it’s not hard to bump it up to 99% from there\n\nRegardless, given minimal info you provided, it’s hard to suggest anything meaningful",
        "I would just hardcode the filenames that are pictures of pictures and use a lookup. that’s guaranteed to work 100%\n\nMy point is you need to give us some actual detail! Your model probably needs 1000+ photos to be trained. Post 0.1% of them here so we can see what exactly you’re working with ",
        "Maybe it is the data, I don't know. I even tried implementing many research papers but it only works on training data but does not generalize on production data. One more problem is that the images we get on production are very low quality data so not sure what I should try?",
        "I was also trying to train the detector to detect a phone screen with a face in an image but too many false positives.",
        "If you don’t have **any** examples of your production data in your training pipeline - then you have a data drift phenomenon - your prod data is too different from what you are training on. You **have to** make them as similar as possible if you want to get meaningful results.\n\nPlus if possible, your production data has to be somewhat good, cause in tasks like this model looks at the minuscule clues like moire etc.",
        "Detectors are no use, cause easy to bypass by just putting the screen closer to the camera",
        "Our training data is actually the production data but the quality is not good.",
        "Most of the time fake images that generally get passed from the model have mobile screens visible that is why I was thinking about detector."
    ]
},
{
    "submission_id": "1gmnfh7",
    "title": "Problems with TartanAir",
    "selftext": "I'm attempting to create a TartanAir dataset in [glue-factory](https://github.com/cvg/glue-factory). For my purposes, just importing the stereo images provides little information, so I attempted to create pairs between the frames as well. Doing this provides some pose inconsistencies that results in ground truth correspondences being slightly off. Here's an image of the dense warp to show what's wrong:\n\nhttps://preview.redd.it/bq80k3qeupzd1.png?width=1854&format=png&auto=webp&s=c1babcb4ae265de903d8c0bebef5f1f266101968\n\nWhat am I doing wrong? I've taken most of my information from [this repository](https://github.com/castacks/tartanair_tools/tree/master), specifically [this file showing the conventions and format of the dataset](https://github.com/castacks/tartanair_tools/blob/master/data_type.md). I've tried a lot of things including changing conventions from NED to EDN, accounting for baseline in the camera intrinsics, manually converting the quaternions to pose matrix and using libraries. I'm at my wits end, hence coming here for suggestions.",
    "created_utc": "2024-11-08T09:05:04",
    "num_comments": 1,
    "comments": [
        "The issue is fixed. I just needed to invert the rotation matrix to change from camera-to-world to world-to-camera convention."
    ]
},
{
    "submission_id": "1gmmfi8",
    "title": "[Question] How to remove white dotted border (See image)",
    "selftext": "",
    "created_utc": "2024-11-08T08:23:18",
    "num_comments": 3,
    "comments": [
        "Here's what I've tried so far:\n\n1. Converting the image to grayscale.\n2. Applying Gaussian Blur to reduce noise.\n3. Enhancing contrast using histogram equalization.\n4. Applying adaptive thresholding to highlight the white dotted borders.\n5. Using morphological operations to clean up the mask.\n6. Applying Canny edge detection.\n7. Finding contours and creating a mask for the contours near the bounding box edges.\n8. Using inpainting to remove the borders.\n\nHowever, the results are not satisfactory, and the white dotted borders are not being detected very well.",
        "Tell whoever made/gave you that bad dataset to fix it themselves (by remaking the dataset from scratch w/ no white boarders & handing it back off to you). There are some things that are not worth your time.\n\nThe jpeg compression in this one image alone is CRAZY. The tiling patterns are so bad. Adding onto the fact that you have that stupid white boarder (who thought this would be a good idea??). Not sure what the downstream task is, but training on a bad dataset gives you a bad model fwiw.",
        "Define a region around each vehicle using the ground truth bounding boxes to focus on the white dotted border area. Use Hough Line Transform and a custom kernel to detect and enhance the dotted pattern. Apply color segmentation in the HSV or LAB space to isolate white regions, then boost contrast with CLAHE. Use morphological operations with a dot-shaped structuring element to refine the border detection, and, if needed, template matching for consistency. Create a mask of the detected border, dilate it, and apply inpainting (Telea or Navier-Stokes) to seamlessly remove the border."
    ]
},
{
    "submission_id": "1gmlybg",
    "title": "Stable Fast 3D Meets Marvel Bobbleheads",
    "selftext": "",
    "created_utc": "2024-11-08T08:03:17",
    "num_comments": 18,
    "comments": [
        "You scraped a website's data (possibly violating their TOS), openly admitted to it and made a joke, then posted it on HF as your own without giving any credit? Disgusting",
        "robots.txt files are for search engines, not a guide on what data can be downloaded or not. Plus, the T&Cs are for the products that they sell, not the content that they used for their website. You could be violating copyright laws. T&Cs are non exhaustive.\n\nYou seem very immature for an adult. Stop being so emotional and defensive. Do better.\n\nEdit: in the video, you clearly state you’re violating T&Cs before going into this, but also defend yourself in the comments that you’re not violating any. Are you missing a brain? Holy shit there are some stupid people on this planet.",
        "The normalization of stealing content for AI is getting ridiculous. Another creator to avoid",
        "[Dataset on Hugging Face](https://huggingface.co/datasets/harpreetsahota/marvel-bobbleheads)\n\n[Notebook on Colab](https://colab.research.google.com/drive/1R2zpQBX_QHv4CCbQ1qk86DrCXieqOhFE?usp=sharing)",
        "Where no one has gone before...",
        "Alright, so if you wanna get pedantic about it:\n\nHere's the [robots.txt](https://www.popcultcha.com.au/robots.txt) file.\n\nThe images that were scraped, are from `media/catalog/product/cache`, which is not listed in any of the Disallow directives in the robots.txt file.\n\nIf you want to go EVEN deeper, then here's the websites [terms and conditions](https://www.popcultcha.com.au/terms-conditions), which makes no mention of prohibiting scraping of the images.\n\nDo you still wanna argue this point? If not then: https://tenor.com/search/shut-the-fuck-up-gifs",
        "You’re right man, thanks for calling me out",
        "I didn't train a model on this data for the purposed of any financial gain. I literally just scraped the data to see if the model could be used to produce some decent results. And I supported the website by purchasing from it to.",
        "I try!",
        "What a pleasant content creator. I'm sure people will be motivated to follow you after such responses\n\nThe funny thing is you didn't bother to check their TOS before making the video or commenting here. You're doing this only because of damage control a la \"I'm sorry I got caught\". And this has nothing to do with pedantry, respecting rules regarding data is the most basic thing a data specialist should do.\n\nP.S. since your suck at PR so much, here's a tip: a simple response like \"I couldn't find a an alternative solution to scraping this data. But their TOS doesn't prohibit it, so it should be fine\" is all that's needed. This way you're not looking like a total clown and people will actually want to hear more from you.",
        "Website TOS forbids it, you didn't get permission. The financial aspect is irrelevant and debatable since youre using it for social media content which is a form of commercial use.",
        "Yeah you’re right man, I did go about it wrong. Appreciate you calling me out",
        "Alright, so if you wanna get pedantic about it:\n\nHere's the [robots.txt](https://www.popcultcha.com.au/robots.txt) file.\n\nThe images that were scraped, are from `media/catalog/product/cache`, which is not listed in any of the Disallow directives in the robots.txt file.\n\nIf you want to go EVEN deeper, then here's the websites [terms and conditions](https://www.popcultcha.com.au/terms-conditions), which makes no mention of prohibiting scraping of the images.\n\nDo you still wanna argue this point? If not then: https://tenor.com/search/shut-the-fuck-up-gifs",
        "Good one!",
        "I guess I should explicitly tell everybody I interact with 'Dont murder me' otherwise that would be seen as implicit consent in your eyes lol.",
        "I'm more than happy to go back and forth with you all. day. long. All that does is pushes this post to the top of the sub's feed...",
        "And equating scraping websites makes me murderer in your eyes? WTF is wrong with you.",
        "Do I really need to explain what an analogy is to a grown ass man"
    ]
},
{
    "submission_id": "1gmkhhx",
    "title": "Automotive CV RAW16 or RAW24",
    "selftext": "Is there any need for RAW24 when it comes to ADAS? What I have been hearing is that RAW16 works just fine. Does having RAW24 help in low light conditions/glare or anything like that? Would love to hear your thoughts.",
    "created_utc": "2024-11-08T07:00:03",
    "num_comments": 0,
    "comments": []
},
{
    "submission_id": "1gmh1pv",
    "title": "Advice for how to create a remote sensing model architecture ",
    "selftext": "Hello! I am currently writing my thesis on the applications of remote sensing to rural satellite imagery. I was initially planning on comparing the performance of state of the art models for this domain (like vision transformers, LFAGCU, etc.) on a custom dataset. For my thesis, now I am considering creating my own model (even if it isn't groundbreaking) to compare it against the candidate models. I would imagine I'd use python since it has a ton of useful CV libraries, and my dataset is currently a .gpkg vector layer I've edited in QGIS with all pixels semantically labeled for a 100 million pixel raster. The idea is to augment my data by rotating tiles, shifting them across the raster (so that there can be a tile which overlaps two adjacent ones for example) and create either 512x512 of 1024x1024 tiles from the 100 million pixel raster and maybe even labeled another raster if necessary. \n\nThe only thing is that I feel completely lost on how to create my own model...where could I start? If anyone has experience with creating their own computer vision architectures, maybe even with remote sensing particularly, where did you start and what factors did you take into consideration when designing and programming it? I know that for now my goal is to semantically segment the entire satellite imagery to create covers for unique features within the image, but beyond that I feel lost about how to proceed. I've read some literature about the models I am interested in, but when I try to Google more information about how people create their own architectures the results aren't very helpful. This is my first time on the subreddit, so I hope my question makes sense. Thanks for reading",
    "created_utc": "2024-11-08T04:10:33",
    "num_comments": 1,
    "comments": [
        "Start with a simpler model like U-Net or FCN for segmentation as a baseline. Once you get that working, you could add custom tweaks or experiment with elements from Vision Transformers."
    ]
},
{
    "submission_id": "1gmg7pi",
    "title": "Foveal encoding ",
    "selftext": "Hi, here-s the problem I try to solve: \n\nFrom any arbitrary size image and a patch specifier (xy patch position, its size, and patch resolution), relative to the image size, I want to get a simplified, fixed resolution crop of the specified patch.\n\nExample:\n\n* position = (0,0) - the patch  is in the dead center of the image \n* size =  0.5 - its x,y size are 1/2 the original image (that's 1/4 area) \n* resolution  = 32 - regardless the size of the cropped patch, rescale it to a 32x32 pixels size. \n\n  \nThe question I have is anyone here encountered a similar problem or (hopefully) simple library for it. \n\n  \nWhy I'm interested in this - I'm curious whether, starting with a lightweight, pre-trained, low-resolution image classifier (e.g on MNIST or CIFAR-10), is possible to train an agent that learns to efficiently search for specified small query images in larger scenery images. by \"moving\" its \"foveal patch\" left-right and zoom in-out. \n\nThis kind of problem seems suitable for a RL pipeline in which, at every step, the agent \"sees\" only the content of its fovea patch, and its actions are instructions for a 3D motion of the same patch - horizontal, vertical, depth motions\n\n    \nFor now just hobby curiosity. \n\n  ",
    "created_utc": "2024-11-08T03:17:07",
    "num_comments": 0,
    "comments": []
},
{
    "submission_id": "1gmf2zi",
    "title": "How do I find the angle of rotation of an object?",
    "selftext": "Let say, I have an object that is stationary. It then rotates a specified angle about an axis.\nHow do I measure that angle using computer vision?\n\n\nEDIT:\nYou guys are legends, thank you for the help",
    "created_utc": "2024-11-08T01:57:55",
    "num_comments": 5,
    "comments": [
        "Match some points on the object (maybe you should put e.g. aruco markers on it, else you'll have to do some complicated segmentation stuff to determine what is object and what is background) in the before and after photos, write down the projection equations, and solve the equations. Not aware of an out of the box solution for that.\n\nMaybe there is a neat solution using optical flow e.g. along the lines of this paper (which I didn't read) [https://www.cv-foundation.org/openaccess/content\\_cvpr\\_2016/papers/Sevilla-Lara\\_Optical\\_Flow\\_With\\_CVPR\\_2016\\_paper.pdf](https://www.cv-foundation.org/openaccess/content_cvpr_2016/papers/Sevilla-Lara_Optical_Flow_With_CVPR_2016_paper.pdf)",
        "Keypoint model or train a model predicting an angle, not only a bb. E.g. YOLO OBB: https://docs.ultralytics.com/tasks/obb/",
        "A few assumptions are needed:  \n\\- Your camera must  be calibrated (known internal  parameters)  \n\\- The object has some texture so  you can track points on it as they  move. \n\nTrack features between 2 images. If the object is not planar,, you can compute the  epipolar geometry (i.e.  essential matrix) from the point  matches.  From that  geometry you get  the rotation  of  the object and the translation magnitude so  you  can eventually compute the center of rotation. Look for  \"pose from epipolar geometry\".  Remember that the pose of a camera looking  at a fixed  object is the  same  as a fixed  camera looking at a rotating object.\n\nIf the object if  planar, then you must  compute a homography and derive the rotation/center of rotation from  that.\n\nIf the object is hard to track, the best thing to do is stick an aruco tag on it and track that. Easy, fast, and accurate.\n\nIf you want to go the neural net route, beware  that most  models won't work because the training dataset  will likely be  very different from your case. If you  can  build a dataset of the specific objects you intend  to track, then it might possibly work,  but don't waste your time(and GPU). Try the classic simple way first...",
        "do you mean pose estimation? [https://www.youtube.com/watch?v=mlXs5kIQ5p4&ab\\_channel=KevinWood%7CRobotics%26AI](https://www.youtube.com/watch?v=mlXs5kIQ5p4&ab_channel=KevinWood%7CRobotics%26AI)",
        "Threshold the image to make it binary then extract the objects in it with connected components, then choose the object with the biggest area and extract its bounding rectangle and its angle.\n\nimport cv2  \nimport numpy as np  \n  \n*# ... (assuming you have your binary image 'binary\\_img')*  \n  \n*# Find connected components*  \nnum\\_labels, labels, stats, centroids = cv2.connectedComponentsWithStats(binary\\_img)  \n  \n*# Calculate angle for a specific component (e.g., component 1)*  \ncomponent\\_label = 1  \ncomponent\\_mask = np.where(labels == component\\_label, 255, 0).astype(np.uint8)  \n  \n*# Find the rotated bounding box*  \nrect = cv2.minAreaRect(cv2.findNonZero(component\\_mask))  \nangle = rect\\[2\\]  \n  \nprint(\"Angle:\", angle)"
    ]
},
{
    "submission_id": "1gmd05k",
    "title": "How do I preprocess this image to extract the relevant document like Adobe Scan does?",
    "selftext": "Hi!\n\nI've been working on a project to extract and perspective-correct a receipt from an image. For this, I've tried numerous approaches such as:\n\n1. Blur -> Canny -> FindContours -> Pick the largest quadrilateral contour\n\n2. Blur -> Canny -> HoughLines -> Crop\n\n3. Blur -> Dilate -> Erode -> Threshold\n\n  \nBut none of these approaches are working as I want them to in terms of identifying the receipt in the image. Had some ideas in terms of using the HSV colorspace, but don't know how to proceed from there.\n\nWould love some help in figuring this out, thanks a ton!\n\n\n\n  \nReference Image for what I'm trying (the blue lines are only there because I wanted to hide the address):\n\nhttps://preview.redd.it/sx54cqnepmzd1.jpg?width=1200&format=pjpg&auto=webp&s=1667114ba1c47b5e192d84a7e534f84def9c6171\n\n",
    "created_utc": "2024-11-07T23:19:09",
    "num_comments": 7,
    "comments": [
        "Try perspective transformation and maybe adjust lighting for the image",
        "You can look up a term gradient space.",
        "Update:\n\nI have reached a satisfactory intermediate step by applying the following:\n\n    color = cv2.imread('images/receipt_image.jpg')\n    hsv = cv2.cvtColor(color, cv2.COLOR_BGR2HSV)\n    \n    hue, saturation, value = cv2.split(hsv)\n\n    thresh = cv2.threshold(hue, 0, 255, cv2.THRESH_BINARY_INV + cv2.THRESH_OTSU)[1]\n    # show(thresh)\n    \n    \n    eroded = cv2.erode(thresh, kernel=None, iterations=7)\n    dilated = cv2.dilate(eroded, kernel=None, iterations=15)\n    eroded = cv2.erode(dilated, kernel=None, iterations=100)\n    dilated = cv2.dilate(eroded, kernel=None, iterations=100)\n\n  \nAfter that, I can apply:\n\n    def extract_largest_rectangular_contour(color_image, preprocessed_image):\n        contours, _ = cv2.findContours(preprocessed_image, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)\n        largest_contour = None\n        largest_area = 0\n        largest_approx = None\n        \n        for contour in contours:\n            peri = cv2.arcLength(contour, True)\n            approx = cv2.approxPolyDP(contour, 0.02 * peri, True)\n            if len(approx) == 4:\n                area = cv2.contourArea(contour)\n                if area > largest_area:\n                    largest_area = area\n                    largest_contour = contour\n                    largest_approx = approx\n                    \n        if largest_contour is not None:\n            cv2.drawContours(color_image, [largest_approx], -1, (0, 255, 0), 3)\n            x, y, w, h = cv2.boundingRect(largest_contour)\n            cv2.rectangle(color_image, (x, y), (x + w, y + h), (0, 0, 255), 2)\n            return color_image, largest_approx\n        \n        return color_image, None\n    def extract_largest_rectangular_contour(color_image, preprocessed_image):\n        contours, _ = cv2.findContours(preprocessed_image, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)\n        largest_contour = None\n        largest_area = 0\n        largest_approx = None\n        \n        for contour in contours:\n            peri = cv2.arcLength(contour, True)\n            approx = cv2.approxPolyDP(contour, 0.02 * peri, True)\n            if len(approx) == 4:\n                area = cv2.contourArea(contour)\n                if area > largest_area:\n                    largest_area = area\n                    largest_contour = contour\n                    largest_approx = approx\n                    \n        if largest_contour is not None:\n            cv2.drawContours(color_image, [largest_approx], -1, (0, 255, 0), 3)\n            x, y, w, h = cv2.boundingRect(largest_contour)\n            cv2.rectangle(color_image, (x, y), (x + w, y + h), (0, 0, 255), 2)\n            return color_image, largest_approx\n        \n        return color_image, None\n\n\n\nThis gives me the output I wanted!",
        "We just shipped an API to do this. Just submit the file and the schema you want: [https://tile.run](https://tile.run)",
        "Use openai o Claude for image processing.",
        "Exploit the fact that the lines have only one slope. Whereas the letters in the bill have varying slopes because they are curve essentially. Then you can differentiate the edges of bill from the letters. \n\nThen you can find the intersection of 2 lines and then you get a corner! After calculating the 4 corners, you can do some geometry to perspectively distord the image",
        "Also you can exploit that receipts are generally white colored. Though this approach is affected by illumination. \n\nYou just need 4 corners of the receipt."
    ]
},
{
    "submission_id": "1gmbnye",
    "title": "Good available tools for UI element labelling?",
    "selftext": "I don’t need interpretation or descriptions of them (eg. “Firefox logo”), I just need to recognize and draw boxes around all of the UI elements. I used OmniParser but perhaps there was a trade off in quality for the interpretation feature ",
    "created_utc": "2024-11-07T21:47:42",
    "num_comments": 0,
    "comments": []
},
{
    "submission_id": "1gmbesd",
    "title": "Ivy x Kornia: Now Supporting TensorFlow, JAX, and NumPy! 🚀",
    "selftext": "Hey r/computervision!\n\nJust wanted to share something exciting for those of you working across multiple ML frameworks.\n\nIvy is a Python package that allows you to seamlessly convert ML models and code between frameworks like PyTorch, TensorFlow, JAX, and NumPy. With Ivy, you can take a model you’ve built in PyTorch and easily bring it over to TensorFlow without needing to rewrite everything. Great for experimenting, collaborating, or deploying across different setups!\n\nOn top of that, we’ve just partnered with [Kornia](https://kornia.readthedocs.io/en/latest/get-started/multi-framework-support.html), a popular differentiable computer vision library built on PyTorch, so now Kornia can also be used in TensorFlow, JAX, and NumPy. You can check it out in the latest Kornia release (v0.7.4) with the new methods:\n\n* `kornia.to_tensorflow()`\n* `kornia.to_jax()`\n* `kornia.to_numpy()`\n\nThese new methods leverage Ivy’s transpiler, letting you switch between frameworks seamlessly without rewriting your code. Whether you're prototyping in PyTorch, optimizing with JAX, or deploying with TensorFlow, it's all smoother now.\n\nGive it a try and let us know what you think! You can check out Ivy and some demos here:\n\n* [Ivy on GitHub](https://github.com/ivy-llc/ivy)\n* [Ivy Demos](https://www.docs.ivy.dev/demos/examples_and_demos.html)\n* [Ivy Discord](https://discord.com/invite/vKqazsCK2Y)\n\nHappy coding!\n\nhttps://preview.redd.it/a7kawqkl6mzd1.jpg?width=1104&format=pjpg&auto=webp&s=d14253cdba9f0064229c0e3e78b5cf8ddf52f6c6",
    "created_utc": "2024-11-07T21:31:35",
    "num_comments": 4,
    "comments": [
        "What’s Ivy? What’s Kornia? Genuinely asking. TL;DR? ELI5?",
        "Hey! Sorry about that—should’ve given more context upfront. Totally fair point!\n\n**Kornia** is a differentiable computer vision library that’s built on top of PyTorch. It’s packed with differentiable image processing and geometric vision algorithms, so you can do things like image transformations, augmentations, and even some AI-driven image processing directly in PyTorch.\n\n**Ivy**, on the other hand, is a framework-agnostic tool we’ve been working on that lets you convert models and code between different ML frameworks (like PyTorch, TensorFlow, JAX, and NumPy) without having to rewrite everything. It’s especially useful if you want to work across multiple frameworks or take advantage of specific framework features. With our new Kornia integration, you can now use Kornia’s functions in JAX, TensorFlow, and NumPy, too, opening up more cross-framework flexibility.\n\nThanks for calling it out—hope this clears things up! Let me know if you have any other questions!",
        "Exactly, I've never heard about any of them. Unless you're an industry wide package like numpy/pandas, you have to introduce yourself",
        "Kornia is fairly popular  \nNever heard of Ivy on the other hand."
    ]
},
{
    "submission_id": "1gmawc5",
    "title": "Is there any way I can input mask of a certain object to SAM and generate segment similar objects from an input image?",
    "selftext": "**Help!**",
    "created_utc": "2024-11-07T21:01:02",
    "num_comments": 0,
    "comments": []
},
{
    "submission_id": "1gm5ty7",
    "title": "Multi-Class Semantic Segmentation Training using PyTorch",
    "selftext": "Multi-Class Semantic Segmentation Training using PyTorch\n\n[https://debuggercafe.com/multi-class-semantic-segmentation-training-using-pytorch/](https://debuggercafe.com/multi-class-semantic-segmentation-training-using-pytorch/)\n\nWe can fine-tune the Torchvision pretrained semantic segmentation models on our own dataset. This has the added benefit of using pretrained weights which leads to faster convergence. As such, we can use these models for **multi-class semantic segmentation training** which otherwise can be too difficult to solve. In this article, we will train one such Torchvsiion model on a complex dataset. Training the model on this multi-class dataset will show us how we can achieve good results even with a small number of samples.\n\nhttps://preview.redd.it/rjz2t35apkzd1.png?width=1000&format=png&auto=webp&s=ae30c261e1034d7e677401b6f7438327bdecba09\n\n",
    "created_utc": "2024-11-07T16:34:49",
    "num_comments": 4,
    "comments": [
        "I like how you setup the core to be more flexible, like by using albumentstions instead of the built in TorchVision transforms, and by splitting up the code into separate files.\n\nWould love to see another tutorial that turns this into an instance segmentation, and maybe uses some kind of fast transformer based model. ",
        "Nice Work!",
        "Thank you. I will surely try to write an article on Transformer based instance segmentation model.",
        "Thanks."
    ]
},
{
    "submission_id": "1gm421g",
    "title": "What are different optimization techniques you guys used to improve accuracy in multiclass object detection models ? ",
    "selftext": "Same as title ",
    "created_utc": "2024-11-07T15:12:23",
    "num_comments": 8,
    "comments": [
        "To boost multiclass detection accuracy, try data augmentation, anchor box tuning, class imbalance handling, model ensembling, fine-tuning pre-trained models, NMS tweaks, feature pyramid networks, and hyperparameter optimization.",
        "validate what classes is your model missing increase the data for that  \nmake sure the data is cleaned   \nmake sure the data is balanced  \nand the option u/Zealousideal-Fix3307 provided are great too",
        "One surprising method that works for me is experimenting with kernel sizes.. im not soo sure why!",
        "Whats a NMS tweaks? and how do you tune anchor boxes?",
        "NMS (Non-Maximum Suppression) tweaks involve adjusting the IoU or confidence thresholds to reduce duplicate detections without losing true positives.\nTuning anchor boxes means setting the right sizes and aspect ratios to better match the objects in your dataset, often using K-means clustering to find optimal values.",
        "This 100%. You've pretty much summed up every mainstream technique for improving training in 5 lines.",
        "I've been looking for a comment like this that succinctly summarizes all of the preprocessing techniques. Bravo!",
        "Lol.. this is it"
    ]
},
{
    "submission_id": "1gm40zi",
    "title": "How yolo or other object detection model handle images of different sizes ? ",
    "selftext": "I want to know how yolo or other object detection model handle images of different sizes for training as well as testing. Like if we resize the image then we also would need to change the bounding box coordinates. Can some one clarify one the same ? \n",
    "created_utc": "2024-11-07T15:11:04",
    "num_comments": 5,
    "comments": [
        "YOLO resizes images to a standard size, adjusting bounding box coordinates proportionally. It uses relative coordinates, making it size-agnostic, and often employs padding (letterboxing) to maintain the aspect ratio, preventing distortion.",
        "You either scale with or without padding or do mosaic inference",
        "Yolo expects the input image to be of a fixed size, both for training and for inference. This is because when designing the network, the size of the input layer has to be fixed. \n\nWhen you pass an image on to yolo, it first resizes the image so the dimensions are the same as the input layer. The bounding boxes are calculated based on that resized image, and then adjusted for the original image.\n\nThe quirk of this approach is that if you have a very large image (like 8K) and the object that you're trying to detect is a very small part of the image (like 300x300 px), when you downsample the 8k image, that same object will now be so small that it might not even be able to be detected anymore.\n\nTo solve this particular quirk you have to train a different yolo model with a larger input layer. This is why you find different versions of yolo with input layers of different sizes - some versions work with 1024x1024 images, others with 608x608 images and so on.",
        "^[Sokka-Haiku](https://www.reddit.com/r/SokkaHaikuBot/comments/15kyv9r/what_is_a_sokka_haiku/) ^by ^ivan_kudryavtsev:\n\n*You either scale with*\n\n*Or without padding or do*\n\n*Mosaic inference*\n\n---\n^Remember ^that ^one ^time ^Sokka ^accidentally ^used ^an ^extra ^syllable ^in ^that ^Haiku ^Battle ^in ^Ba ^Sing ^Se? ^That ^was ^a ^Sokka ^Haiku ^and ^you ^just ^made ^one.",
        "YOLO is a fully convolutional network. So it doesn't require a fixed input size. It can run on any input size as long as it's divisible by the stride. Increasing/decreasing the input size increases/decreases the number of grid cells and the number of possible output boxes. For example, YOLOv8 with input size 640x640 produces an output of shape (1, 4+num_classes, 8400), so there are 8400 boxes (most with confidence below threshold) that needs to be filtered.\n\nHowever, if you had trained the model to work with 640x640 images then it will only be good for that particular image size since the anchors were trained on that image size. It's possible to enable multi-scale training (this is different from `scale` augmentation) so that it can learn multiple sizes during training, however it increases VRAM usage significantly."
    ]
},
{
    "submission_id": "1gm3f5f",
    "title": "hi guys i've made a tool for autolabelling your dataset!",
    "selftext": "check this out! [https://github.com/leocalle-swag?tab=repositories](https://github.com/leocalle-swag?tab=repositories)",
    "created_utc": "2024-11-07T14:44:16",
    "num_comments": 0,
    "comments": []
},
{
    "submission_id": "1gm2jdk",
    "title": "research into reality",
    "selftext": "this is research -> Webpage information extraction (WIE) is an important step to create knowledge bases. For this, classical WIE methods leverage the Document Object Model (DOM) tree of a website. However, use of the DOM tree poses significant challenges as context and appearance are encoded in an abstract manner. To address this challenge we propose to reformulate WIE as a context-aware Webpage Object Detection task. Specifically, we develop a Context-aware Visual Attention-based (CoVA) detection pipeline which combines appearance features with syntactical structure from the DOM tree. To study the approach we collect a new large-scale dataset of e-commerce websites for which we manually annotate every web element with four labels: product price, product title, product image and background. On this dataset we show that the proposed CoVA approach is a new challenging baseline which improves upon prior state-of-the-art methods.\n\n  \nI want to build this so my questions is how do you envision this in theory?",
    "created_utc": "2024-11-07T14:05:23",
    "num_comments": 0,
    "comments": []
},
{
    "submission_id": "1gly9r5",
    "title": "Full Image and Close Up Image Matching",
    "selftext": "Hi there, I am trying to catalog my personal library of DVDs and I need a way to match close up photos to the specific section of the full library. \n\nFor example, the photo on the left is the full shelf (one of many :)), and the one on the right is a close up (clearly the first column on the second row by human eyes). \n\nI tried to do a opencv ORB feature matching but the matching and the calculated homography is really bad. Can anyone shed some lights what went wrong? given my specific use case, is there any other features that I should be considering other than ORB or SIFT because it sounds like color is very important, and the interelationship between features (sort of \"clusters\" are important). \n\n    import cv2\n    import numpy as np\n    import matplotlib.pyplot as plt\n    \n    # Load images\n    img_full = cv2.imread('bookshelf_full.jpg')\n    img_closeup = cv2.imread('bookshelf_closeup.jpg', 0)\n    \n    # Convert the full image to grayscale for feature detection\n    img_full_gray = cv2.cvtColor(img_full, cv2.COLOR_BGR2GRAY)\n    \n    # Detect ORB keypoints and descriptors\n    orb = cv2.ORB_create()\n    keypoints_full, descriptors_full = orb.detectAndCompute(img_full_gray, None)\n    keypoints_closeup, descriptors_closeup = orb.detectAndCompute(img_closeup, None)\n    \n    # Match features\n    matcher = cv2.BFMatcher(cv2.NORM_HAMMING, crossCheck=True)\n    matches = matcher.match(descriptors_full, descriptors_closeup)\n    matches = sorted(matches, key=lambda x: x.distance)\n    \n    # Concatenate images horizontally\n    img_closeup_color = cv2.cvtColor(img_closeup, cv2.COLOR_GRAY2BGR)\n    img_combined = np.hstack((img_full, img_closeup_color))\n    \n    # Draw matches manually with red lines and thicker width\n    for match in matches[:20]:  # Limit to the top 20 matches for visibility\n        pt_full = tuple(np.int32(keypoints_full[match.queryIdx].pt))\n        pt_closeup = tuple(np.int32(keypoints_closeup[match.trainIdx].pt + np.array([img_full.shape[1], 0])))\n    \n        # Draw the matching line in red (BGR color order for OpenCV)\n        # cv2.line(img_combined, pt_full, pt_closeup, (0, 0, 255), 5)  # Red color with thickness of 2\n    \n    # Display the result in the notebook\n    plt.figure(figsize=(20, 10))\n    plt.imshow(cv2.cvtColor(img_combined, cv2.COLOR_BGR2RGB))\n    plt.axis('off')\n    plt.title(\"Matching Features Between Full Bookshelf and Close-Up Images (Red Lines)\")\n    plt.show()\n    \n\nWhat I have today. \n\nhttps://preview.redd.it/gvv5mwwq1jzd1.png?width=1175&format=png&auto=webp&s=c368b5618ba9651bffeee51897caa94ec4ab4f27\n\nWhat I want: \n\nhttps://preview.redd.it/fnavfem32jzd1.png?width=1174&format=png&auto=webp&s=90d3c2580de0614f2e2286e08e0579331c7087dd\n\n",
    "created_utc": "2024-11-07T11:06:02",
    "num_comments": 5,
    "comments": [
        "[removed]",
        "You can try a sliding window algorithm on the larger image and extract features from windows at each step. For each window, perform matching with the \"query\" image on the right. The window with the best match score to the query image should be the correct location of the query image.\n\nHave your window size the same as your query image as well.",
        "Is the right side image actually grayscale. RGB would probably help a lot here.",
        "Yes, the full view photo indeed has a much lower resolution per resolution given how many objects it has in its view.   \nAs the feature extraction is only based on gray scale and not leveraging the color, so when I plot it, I did not bother to add back the color.",
        "Even if we can treat this problem as a surface 2D, but there is still some level of rotation in the point of view, some level of homographic transformation, but I guess I will give it a try. Thanks."
    ]
},
{
    "submission_id": "1glvnmb",
    "title": "Hardware for object detection that can be programmed in Python?",
    "selftext": "My kids compete in First Lego League. For the innovation project (very open ended project to come up with a solution to a problem related to this year's theme), they want to use a camera that can detect different types of objects. They are either familiar with or learning to program in Python, which they'll continue with in the Robot Game portion of the competition, as the robot can be programmed in Micropython. So I want to find for them a hardware setup they can program using Python.\n\nMy first thought was that even an ESP32 can do this, but I don't see any micropython support for object detection. Next thought is the Raspberry Pi Zero, as I already have some and cameras that connect to it-- but hardware wise, is that going to be enough? I want their experience to be low frustration, not to run into some hardware limitation in the middle of developing the program. I don't want to spend a fortune on a development board - no overkill needed - but am willing to spend a moderate amount to get something solid.\n\nI myself don't know very much in this area-- I hacked together a script in Python using YOLO to running on my PC to send me an alert about humans or predators showing up on a game cam we have-- so actually training a model for the objects they want to be able to distinguish will be new for me as well. The training would obviously not be done on the Raspberry Pi itself, just the object detection, possibly data from some other sensors, sending some alerts and controlling some outputs based on what object is detected.",
    "created_utc": "2024-11-07T09:17:31",
    "num_comments": 4,
    "comments": [
        "Yeah a pi will be good enough",
        "Raspberry pi with a webcam is enough",
        "Might be too big for what you’re looking to do, but I like OAK-D https://pyimagesearch.com/2022/11/28/introduction-to-opencv-ai-kit-oak/",
        "Cheap and capable."
    ]
},
{
    "submission_id": "1glun0c",
    "title": "Specialized VLM for generating keywords for microstocks?",
    "selftext": "I have been looking for a specialized VLM for generating keywords for microstocks like Adobe Stock, FreePic, Shatterstock and others for a long time.\n\nI know that you can use general multimodal models like Qwen-VL, LLava Mistral and so on.\n\nBut they are not effective, not accurate and often make mistakes due to their lack of specialization and multimodality.\n\nI need an alternative to the specialized autotagger WD (https://huggingface.co/SmilingWolf/wd-eva02-large-tagger-v3).\n\nThe same lightweight, fast and super-accurate, without multimodality (only img2txt), but with the purpose of creating relevant tags/keywords for images posted on microstock sites.\n\nHave you come across similar narrowly specialized monomodal visual-linguistic neural models?\n\nIf so, can you share the names of such models and links to sources?\n\nThanks for any help!",
    "created_utc": "2024-11-07T08:35:09",
    "num_comments": 5,
    "comments": [
        "Is there a good dataset for this? It might pretty straightforward to fine tune a VLM for this use case.",
        "Hey this rocks! Could you share more of your thoughts on your architecture choices? I'm looking to train one for a different domain.",
        "If you are talking about a large dataset (with several tens of thousands of images and keywords to them), then no, I don't have such a dataset and I haven't found such a dataset.\n\nDo you do fine-tuning of models?   \nDo you have experience in this?   \nDo you do fine-tuning of Qwen-VL?",
        "Honestly, I have no idea what architecture should be used for such a neural network. I am only interested in whether there are similar wd neural models that would specialize in microstocks.\n\nI know that wd was trained using the following code and models, perhaps you will find this useful:\n\n[https://github.com/SmilingWolf/JAX-CV](https://github.com/SmilingWolf/JAX-CV)\n\nAlso, wd used data and tags from the danbooru website.\n\nThe model I like the most at the moment is EVA02, which is used for wd.\n\nThe file formats of the models that interest me are mainly onnx at the moment, but safetensors are also possible, this is, in general, not particularly important.\n\nIn terms of the architecture of a neural network like image2txt, I would generally focus on CNN + Transformer. The other formats are less familiar to me.",
        "Yeah, we can fine tune for this use-case pretty easily. Check out VLM Run (https://vlm.run) - we can set you up pretty quickly."
    ]
},
{
    "submission_id": "1gls3t7",
    "title": "Mysterious issues started after training resumption/tweaking implemented",
    "selftext": "I'm an engineer from the full stack web part of the world that has been co-opted by my boss to work on ML/CV for one of our integrated products due to my previous Python experience. I'll try to keep this brief, however I don't know what is or is not relevant context to the problem.\n\nAfter a while of monkeying around in jupyter notebooks with pytorch and figuring out all the necessary `model.to(device)` placements, my model was finally working and doing what it was supposed to do; running on my GPU, classifying, segmenting (some items are parallaxed over each other in extreme cases that I don't have in the dataset yet), and counting *n* instances of *x* custom item in an image.\n\n[A hand-annotated ground truth item \\(ID scrubbed for privacy\\)](https://preview.redd.it/u52m3751jhzd1.png?width=282&format=png&auto=webp&s=b382d2ec61dc57a51fe1a4976fb9b402319f48d3)\n\nRecently, I tried implementing resuming model training from file, including optimizer and learn-rate scheduler state resumption. That had its own bugs that I ironed out, but now any time I train my model, regardless of if I'm continuing an old one or training a new one, a few mysterious problems show up that I can't find a reason for nor similar issues online. (perhaps just because I don't know the right lingo to search though) I don't really know where else to go nor who else to ask, so I was hoping that someone would at least be able to point me in the right direction:\n\n1. Stubby annotations\n\n[The parts of the component that the model missed are highlighted in green](https://preview.redd.it/60zynf3ckhzd1.png?width=198&format=png&auto=webp&s=c832b8d2d266334807dcd3874db911dcb2b2d34b)\n\n2. Overlapping/bipartite annotations\n\n[These annotations predict two sections of the item as different parts, and the mask seems to disappear in overlaps \\(green outline\\)](https://preview.redd.it/tv49wumqnhzd1.png?width=280&format=png&auto=webp&s=3c3c038a05a926b8c96afa0f29a20430a205c755)\n\nI'm not sure if this is solely an error with how I'm displaying the fill, but I'm running with that assumption, I'm using VSCode with Jupyter Notebook Renderers and here is my visualization code: [https://gist.github.com/joeressler/2a5bf6e2c67c1a54709b76e25ca94aa4](https://gist.github.com/joeressler/2a5bf6e2c67c1a54709b76e25ca94aa4)\n\nDoes anyone have any tips for this? I don't have a huge dataset (not by choice), and I'm not sure what good starting points for learning rate, epochs, training image resize, worker processes, etc. are, so I'm stuck wondering what in the multitude of things that could go wrong are currently going wrong. I'll be on my phone all day, so feel free to shoot any replies and I'll respond as fast as I can.\n\n  \nEdit: I just realized I didn't even say what I'm using, I'm running a `maskrcnn_resnet50_fpn_v2` with a `torch.optim.AdamW` optimizer and the `torch.optim.lr_scheduler.OneCycleLR` learn-rate scheduler.",
    "created_utc": "2024-11-07T06:46:57",
    "num_comments": 0,
    "comments": []
},
{
    "submission_id": "1glrugn",
    "title": "Transformer Based Backbone for FasterRCNN - PyTorch Implementation",
    "selftext": "Hello everyone, I opened a similar thread a month ago, but this is a more detailed version of my question.   \nSo, I was using a pre-configured FasterCNN model (resnet50\\_fpn\\_v2) to train on my dataset, but I believe I can get even a better performance if I use a transformer based backbone (SwinV2) with FPN, so I decided to implement it myself on PyTorch. Below, you can see my implementation. It is based on my knowledge and also the source code of the \"resnet50\\_fpn\\_v2\" model;\n\n  \n`class IntermediateLayerGetter(nn.ModuleDict):`\n\n`# This is to get intermediate layer features (modified from the PyTorch source code)` \n\n`def __init__(self, model: nn.Module, return_layers: Dict[str, str]) -> None:`\n\n`if not set(return_layers).issubset([name for name, _ in model.named_children()]):`\n\n`raise ValueError(\"return_layers are not present in model\")`\n\n`orig_return_layers = return_layers`\n\n`return_layers = {str(k): str(v) for k, v in return_layers.items()}`\n\n`layers = OrderedDict()`\n\n`for name, module in model.named_children():`\n\n`layers[name] = module`\n\n`if name in return_layers:`\n\n`del return_layers[name]`\n\n`if not return_layers:`\n\n`break`\n\n`super().__init__(layers)`\n\n`self.return_layers = orig_return_layers`\n\n\n\n`def forward(self, x):`\n\n`out = OrderedDict()`\n\n`for name, module in self.items():`\n\n`# print(module.__class__.__name__)`\n\n`x = module(x)`\n\n`if name in self.return_layers:`\n\n`out_name = self.return_layers[name]`\n\n`# Here we permute the output so the channels are in the order FasterRCNN expects`\n\n`out[out_name] = torch.permute(x, (0, 3, 1, 2))`\n\n`return out`\n\n\n\n`class BackboneWithFPN(nn.Module):`\n\n`# This class is for implementing FPN backbone (also modified from the PyTorch source code)`\n\n`def __init__(`\n\n`self,`\n\n`backbone: nn.Module,`\n\n`return_layers: Dict[str, str],`\n\n`in_channels_list: List[int],`\n\n`out_channels: int,`\n\n`extra_blocks: Optional[ExtraFPNBlock] = None,`\n\n`norm_layer: Optional[Callable[..., nn.Module]] = None,`\n\n`) -> None:`\n\n`super().__init__()`\n\n`if extra_blocks is None:`\n\n`extra_blocks = LastLevelMaxPool()`\n\n`self.body = IntermediateLayerGetter(backbone, return_layers=return_layers)`\n\n`self.fpn = FeaturePyramidNetwork(`\n\n`in_channels_list=in_channels_list,`\n\n`out_channels=out_channels,`\n\n`extra_blocks=extra_blocks,`\n\n`norm_layer=norm_layer,`\n\n`)`\n\n`self.out_channels = out_channels`\n\n\n\n`def forward(self, x: Tensor) -> Dict[str, Tensor]:`\n\n`x = self.body(x)`\n\n`x = self.fpn(x)`\n\n`return x`\n\n  \n`class CustomSwin(nn.Module):`\n\n`def __init__(self, backbone_model):`\n\n`super().__init__()`\n\n`# Create a new OrderedDict to hold the layers`\n\n`return_layers = OrderedDict()`\n\n`# I get the features from layers 1-3-5-7 , the layers before the patch embeddings`\n\n`return_layers = {`\n\n`'1': '0',`\n\n`'3': '1',`\n\n`'5': '2',`\n\n`'7': '3'`\n\n`}`\n\n\n\n`# Define the in_channels for each layer (for SwinV2 small)`\n\n`in_channels_list = [96, 192, 384, 768]`\n\n\n\n`# Create a new Sequential module with the features`\n\n`backbone_module = nn.Sequential(OrderedDict([`\n\n`(f'{i}', layer) for i, layer in enumerate(backbone_model.features)`\n\n`]))`\n\n\n\n`# Create the BackboneWithFPN`\n\n`self.backbone = BackboneWithFPN(`\n\n`backbone_module,`\n\n`return_layers,`\n\n`in_channels_list,`\n\n`out_channels=256,`\n\n`extra_blocks=None`\n\n`)`\n\n`self.out_channels = 256`\n\n`def forward(self, x):`\n\n`return self.backbone(x)`\n\n\n\n`def load_backbone(trainable_layers=6):`\n\n`# This is the vanilla version of swin_v2_s (imported from PyTorch library)`\n\n`backbone = swin_v2_s(weights=Swin_V2_S_Weights.DEFAULT)`\n\n`# Remove the classification head (norm, permute, avgpool, flatten, and head)`\n\n`backbone.norm = nn.Identity()`\n\n`backbone.permute = nn.Identity()`\n\n`backbone.avgpool = nn.Identity()`\n\n`backbone.flatten = nn.Identity()`\n\n`backbone.head = nn.Identity()`\n\n`# Freeze all parameters`\n\n`for param in backbone.parameters():`\n\n`param.requires_grad = False`\n\n\n\n`# Unfreeze the last trainable_layers`\n\n`for layer in list(backbone.features)[-trainable_layers:]:`\n\n`for param in layer.parameters():`\n\n`param.requires_grad = True`\n\n`return backbone`\n\n\n\n`backbone = load_backbone()`\n\n`anchor_generator = AnchorGenerator(`\n\n`sizes=((32), (64,), (128,), (256,), (512)),  # 5th for the pool layer`\n\n`aspect_ratios=((0.5, 1.0, 2.0),) * 5  # Same aspect ratio for all feature maps`\n\n`)`\n\n`roi_pooler = MultiScaleRoIAlign(`\n\n`featmap_names=['0', '1', '2', '3'], #ignore pool`\n\n`output_size=(7,7),`\n\n`sampling_ratio=2`\n\n`)`\n\n`model = FasterRCNN(`\n\n`backbone,`\n\n`num_classes=len(CLASSES),`\n\n`rpn_anchor_generator=anchor_generator,`\n\n`box_roi_pool=roi_pooler,`\n\n`min_size=width,`\n\n`max_size=height,`\n\n`).to(DEVICE)`\n\n`in_features = model.roi_heads.box_predictor.cls_score.in_features`\n\n`model.roi_heads.box_predictor = FastRCNNPredictor(in_features, len(CLASSES)).to(DEVICE)`\n\n  \n\nSo to summarize it, I create the backbone with FPN and configure the anchor generator & ROI pooler. Lastly, I combine everything using FasterRCNN class of PyTorch.\n\nAlthough I am fairly sure I did everything correctly, when I start training the loss value gets stuck around 1.00, which indicates that I implemented something wrong, but I can't figure out what...  \n\nIf any of you could take a look at my code and tell me if you see the reason why, I would greatly appreciate it.\n\n",
    "created_utc": "2024-11-07T06:35:25",
    "num_comments": 0,
    "comments": []
},
{
    "submission_id": "1glohyy",
    "title": "challenges in position based visual servoing ",
    "selftext": "Hey everyone,\n\n\n\nI'm currently working on a visual servoing task with a UR5 robotic arm in a ROS 2 Humble environment, using Gazebo and RViz for simulation. My task is to pick and place boxes with ArUco markers on them, where the boxes spawn randomly within the workspace. Here are the main issues I’m running into:\n\n\n\nCoordinate Transformation: Since the boxes spawn randomly, I need to fetch the transformation (TF) of each box with respect to the base link of the arm. I’m using a tf listener to get the transformation from the box's frame to the base link. But handling the dynamic nature of these transforms and ensuring they’re accurate is tricky, especially since I only have the TF coordinates relative to the boxes themselves.\n\n\n\nQuaternion Interpolation: For smooth visual servoing, I need to handle orientation changes accurately. I have an initial quaternion of (0.505, 0.496, 0.499, 0.500) and a target quaternion of (0.812, -0.583, 0.003, -0.008). Interpolating between these to manage rotation smoothly has been challenging. Any tips on quaternion interpolation methods or efficient ways to handle this transition?\n\n\n\nCamera Perspective: I’m using a depth camera positioned in a third-person perspective above the arm to view the boxes and arm, but interpreting the visual data effectively for precise placement is a challenge. Has anyone tried a similar setup? Any advice on handling perspective to improve detection accuracy?\n\n\n\nIf anyone could point me to relevant tutorials or research papers that cover similar setups or techniques, I’d really appreciate it! Any resources on quaternion handling, visual servoing in ROS, or working with ArUco markers would be extremely helpful. Thanks in advance for any advice!\n\n\n\nI’m open to any suggestions or resources that could help streamline the workflow or overcome these issues. Appreciate any help or guidance from those who have tackled similar setups!",
    "created_utc": "2024-11-07T03:42:35",
    "num_comments": 3,
    "comments": [
        "ArUco Marker Pose Estimation and Detection in Real-Time using OpenCV Python\nhttps://youtu.be/bS00Vs09Upw",
        "For smooth visual servoing, use SLERP (Spherical Linear Interpolation) between quaternions to achieve smooth transitions in orientation, ensuring constant rotational speed.",
        "Hi , actually the markers are getting detected , the problem is servoing"
    ]
},
{
    "submission_id": "1glnhi9",
    "title": "Looking for feedback for my Synthethic Data website (Work in progress)",
    "selftext": "",
    "created_utc": "2024-11-07T02:33:13",
    "num_comments": 3,
    "comments": [
        "Not too familiar with the domain as is, but have you compared yourself to whatever is the most popular/SOTA methods involving blender/nvidia omniverse?",
        "Im currently building a synthetic data platform for to speed up the process of training models. All images are 3d simulated (no ai), fully annotated and customizable.  When it's ready you'll essentially populated some high level rules into a scenario (what to spawn, how to spawn it, colours etc). Then you can customize the masks, labels and download it into a common format (coco etc).\n\nThink RoboFlow except you also create the images from scratch and you can customize datasets created by others. I'm looking for general feedback, mostly for the landing page .The website is below, any comments, good or bad are appreciated.\n\n[www.ThePerceptionCompany.com](http://www.ThePerceptionCompany.com)\n\nUseful links key links.\n\nAn example dataset page - [https://theperceptioncompany.com/generation/247](https://theperceptioncompany.com/generation/247)  \nAn example object page - [https://theperceptioncompany.com/asset/chess\\_board\\_134](https://theperceptioncompany.com/asset/chess_board_134)",
        "Solid site with clear info, but could use a more engaging design. Feels a bit dense"
    ]
},
{
    "submission_id": "1glmkyp",
    "title": "Follow up on the OpenCV node editor project; PaperVision",
    "selftext": "",
    "created_utc": "2024-11-07T01:24:48",
    "num_comments": 0,
    "comments": []
},
{
    "submission_id": "1gll8lc",
    "title": "Trying to find a recent video-to-3d project",
    "selftext": "Hi, I just saw Microsoft's MoGe project and also their Look Ma, no markers project. I could swear I briefly saw a project the other week that was already a kind of combination of these. It would take video as input and create a 3d scene based on it + output the camera track + some skeletal poses.\n\nDoes anyone know if such a project exists or did I just dream that up? I'm having trouble finding it again.\n\n  \n[https://wangrc.site/MoGePage/](https://wangrc.site/MoGePage/)\n\n[https://microsoft.github.io/SynthMoCap/](https://microsoft.github.io/SynthMoCap/)",
    "created_utc": "2024-11-06T23:41:04",
    "num_comments": 0,
    "comments": []
},
{
    "submission_id": "1gljfwn",
    "title": "Anyone finetuned a SAM2 model? How to select points from a mask?",
    "selftext": "I am trying to finetune SAM2 for my segmentation data. I have annotated masks pairs with the images.  \nIt seems I cannot use the masks for training SAM2, but points or boxes instead.  \n  \nFor what I researched, the points are usually curated by a human labeler.  \nHowever, I already have the mask, I wanted to automatically get representative points.  \nCan I just get random ponts in the mask? How many?   \nHow the number of points should grow with the masked region? proportional to the area?  \nI was thinking in use the morphological skeleton points...  \nSeeems that the backgroud sould also be represented   \n  \nAnyone done something alike?",
    "created_utc": "2024-11-06T21:35:59",
    "num_comments": 2,
    "comments": [
        "SAM does not support this. It is one way prompt into masks. You could in theory train a SSL pipeline like Meta did for SAM to predict query points for a given mask, but honestly that seems like a more expensive and less useful problem.\n\n\nThe quick and dirty way may be to do some positive points inside your mask and negative outside the mask. If you’re looking for pose key points you either need to use a model specifically pre-trained for that (eg Sapiens) or train your own",
        "Depends on your needs tbh. You can simply pass the whole image size as a bounding box and see if it work. But if you plan to allow have user select the area of interest for cropping, then you can probably use opencv, find position where the mask != 0 and then add some random buffering boundary to emcompass the area of interest"
    ]
},
{
    "submission_id": "1glhxum",
    "title": "Real-Time surveillance: Multiple object detection and tracking across dual cameras",
    "selftext": "Hi, \n\nI am creating a project with the above title mentioned idea but i have no expertise in this domain of computer vision as this is a first time for me with even detections and tracking.\n\nI'm using two webcameras currently as demo, one is laptop's own and the other is a usb attached one.\n\nHere is what the project should be able to do:\n\n1- Detect multiple objects across both cameras and track their movement.\n\n2- Successfully reid them across both cameras. i-e if a person appears with a label in camera 1, he/she should be successfully reid'd with the same label on the other camera too.\n\nI've currently only tried yolo with deepsort but it fails when multiple people appear in the webcamera also fails to reid, it is inaccurate and does many false detections and it is also very slow, also i've used lots of ai for tracking + reid and amidst all the changes also lost my own code. This notebook seems to be running fine does good with a single person but fails when multiple person appear also fails if a person appears in camera one and then again reappears in camera 2 it generates an reid error.\nThe error is also commented out in the end of the notebook.\n\nLink to notebook:\n\n[https://github.com/baseershah7/fyp\\_demo/blob/main/yolo\\_ds.ipynb](https://github.com/baseershah7/fyp_demo/blob/main/yolo_ds.ipynb)\n\nI'd appreciate any suggestions for state of the art + corrections in my notebook for improvement.",
    "created_utc": "2024-11-06T20:07:07",
    "num_comments": 6,
    "comments": [
        "\nI see you've posted a GitHub link to a Jupyter Notebook! GitHub doesn't \nrender large Jupyter Notebooks, so just in case, here is an \n[nbviewer](https://nbviewer.jupyter.org/) link to the notebook:\n\nhttps://nbviewer.jupyter.org/url/github.com/baseershah7/fyp_demo/blob/main/yolo_ds.ipynb\n\nWant to run the code yourself? Here is a [binder](https://mybinder.org/) \nlink to start your own Jupyter server and try it out!\n\nhttps://mybinder.org/v2/gh/baseershah7/fyp_demo/main?filepath=yolo_ds.ipynb\n\n\n\n------\n\n^(I am a bot.) \n[^(Feedback)](https://www.reddit.com/message/compose/?to=jd_paton) ^(|) \n[^(GitHub)](https://github.com/JohnPaton/nbviewerbot) ^(|) \n[^(Author)](https://johnpaton.net/)",
        "One part I know about is that DeepSort has some implementation issues, if you took it from the original repo. IIRC it doesn't use velocities at all in the Kalman Filter prediction step, so basically assumes everything is stationary. And also it uses width-to-height ratio instead of simple width, height tuple and it overcorrects when box is shrinking/enlarging.\n\nI think BotSort is considered SOTA at the moment and it fixed those issues. Also added predictive camera movement, if you have any of that. It won't help if your main issue is ReID though.",
        "Have you consider usign Stitching or Homography to join both videos into one? With cv2 stitching or [https://github.com/OpenStitching/stitching](https://github.com/OpenStitching/stitching) (tutorial: https://github.com/OpenStitching/stitching\\_tutorial/tree/master) you could solve the tracking across both cameras easily",
        "Have you consider usign Stitching or Homography to join both videos into one? With cv2 stitching or [https://github.com/OpenStitching/stitching](https://github.com/OpenStitching/stitching) (tutorial: https://github.com/OpenStitching/stitching\\_tutorial/tree/master) you could solve the tracking across both cameras easily",
        "Thanks, i'll try that.",
        "No i haven't experimented much with the reid as im still figuring things out but thanks for the suggestion, i'll def try this."
    ]
},
{
    "submission_id": "1glb8g7",
    "title": "What is the State of the Art in Classical Stereo Matching?",
    "selftext": "Does anyone have insights, lists, or recommended search terms to find the latest research papers that implement, test, or review classical stereo matching algorithms? I suppose AI that writes classical stereo matching code would be acceptable also (like some of DeepMind's code writing models).\n\nDeep learning has done a lot for computer vision in the past few years, but I'm trying to filter out the deep learning approaches right now as I am doing computer vision research on a low-risk tolerance context where similar prior imagery may not be available. Explainability and the understanding that that bad data will not be hallucinated or will provide obvious artifacts is important. Classical approaches have these features, and I'm not aware of any deep stereo matching network that consistently knows what it doesn't know / does not hallucinate.",
    "created_utc": "2024-11-06T14:35:55",
    "num_comments": 8,
    "comments": [
        "Hallucination is a problem in generative models, not graph-based models for dense pixel correspondence matching. There's no stochastic element in those models during inference time.",
        "Because the matching part isn't really anything complicated.  It's minimizing feature distances.  The part that actually could use deep learning or different methods is the feature generation itself.",
        "There are a lot of holes in my knowledge, but this gives me some good starting points, thank you! I'm seeing some recent papers on graph cut stereo matching based on census transform.",
        "Do you know of any full/partial deep learning methods that generate features that are hallucination free or have some calibrated uncertainty metric?",
        "Its not just matching. All good methods enforce a smoothness constraint (even simple block matching does that implicitly). So in effect the problem becomes minimizing a cost function, consisting of a smoothness and a data term. Algorithms used for this in the past are believe propagation, dynamic programming (this includes SGM which is basically dynamic programming along multiple paths), graph cuts, and probably a few others.",
        "I was simplifying.  My point is they're not models.  It's just an algorithm.  The modelling aspect of it is the feature generation.",
        "I disagree. The smoothness term that you select is a model in the general sense. And solving the cost function is still a computationally hard problem.",
        "I'm glad you disagree, but it's not what the OP is talking about.  And it's not a computationally hard problem."
    ]
},
{
    "submission_id": "1gla3yt",
    "title": "[Project] YOLOv8 .pt File for General Object Detection Across Multiple Environments (50+ Classes)",
    "selftext": "Could someone provide the best possible .pt file for YOLOv8 for general object detection, covering environments like colleges, offices, and homes, with a dataset containing at least 50 classes?",
    "created_utc": "2024-11-06T13:47:37",
    "num_comments": 2,
    "comments": [
        "why not use their pretrained ? have around 81 classes",
        "Why do you use Yolov8? After all, YOLO10 has already been released, it detects objects much better. In addition, the Yolo creators' website has simple instructions for additional training of the model for your needs, it does not require huge amounts of data and computing power."
    ]
},
{
    "submission_id": "1gl8cvj",
    "title": "Computer Vision News of October 2024 - sorry for late post",
    "selftext": "[https://www.rsipvision.com/ComputerVisionNews-2024October/](https://www.rsipvision.com/ComputerVisionNews-2024October/)",
    "created_utc": "2024-11-06T12:33:42",
    "num_comments": 0,
    "comments": []
},
{
    "submission_id": "1gl65g8",
    "title": "How do line-scan composite images compare to regular (area-scan) images?",
    "selftext": "I am looking for a comparison of actual line-scan composite images and regular (area-scan) images.  Can a line-scan RGB composite image pass for an area-scan image?  Are there artifacts of the line-scan process that are noticeable to an interested observer when viewed as a HxWX3 image?",
    "created_utc": "2024-11-06T11:01:36",
    "num_comments": 12,
    "comments": [
        "A good line scan image will not have artefacts, at least for monochrome. Colour linescan systems are niche, and there is a quality trade off, though I'm not familiar with colour linescan systems.\nI dont know what applications you have in mind, but if you really need colour, area scan seems like a simpler route. I don't know of a resource that has comparison pics between the two technologies, but that would be interesting.",
        "If its done right you can differ between an area and a composed line scan image. The biggest difficulty is the correct movement of the scene, but if you have a precise step motor and the triggering is set up you won't have any artefacts in the image.\n\nEven color shouldn't be a problem. You will have only the same artifacts as with the usual bayer area scan color cameras.",
        "Interested in this.  I wonder if I could use a line-scan camera to capture vehicles driving by?  I wouldn’t be able to match frame rate to vehicle speed but maybe I could remove frames that don’t change enough or post process the saved data in some way to get the semblance of a car out. The main attraction is that it would probably be more lightweight than similar resolution video",
        "I had never heard these terms! Hopefully I4m not 100% wrong, but I believe nowadays people will use \"global shutter\" for area-scan, and \"rolling shutter\" for line-scan. Virtually all consumer cameras are rolling shutter, I suppose all global shutter cameras are for industrial or automotive applications.\n\nflickering/banding: many artifical lights actually flicker super fast, we mostly don't see it, but a rolling shutter sensor does not expose its line simultaneously, so we can see the flicker as a darker band rolling vertically (wrt rows). On the other hand, if the exposure time is set to a multiple of the flicker period, then the effect disappears!\n\nrolling shutter distortions or \"wobble\": when the camera moves, then each line correspond to a different position! if you pan your camera to the right, the bottom line will be \"ahead\" of the top line, because it was exposed much later when the camera had already panned! It can be super noticeable on UAVs or action cams mounted on motorbikes. Rolling shutter distortions from pure rotations can be corrected in most of the situations. Rolling shutter distortions due to general motion is super hard, but is sometimes corrected for in some SLAM/SfM Adjustment implementations",
        "linescan cameras strictly rely on scene movement, because a static pixel will be stacked several times creating an identical column. if everything is moving the images appears in a similar way but there are few key elements that might you want to use a linescan in an application: 1) is far more easier (and cheaper) to achieve a uniform illumination just on a line 2) you can get as much resolution as you want along y-axis if you don't care about image proportion 3) reduced perspective distortion. Keep in mind that for linescan color cameras which have a bayer filter, the camera acquires 2 lines which are then stacked, so the position of the camera with respect to object movement changes the image! Also they are not so easy to put in focus",
        "Color linescan camera are not a problem at all. They have then more then one line and the rgb image is then calculated using information from different lines, but it's quite similar to the algorithms used to calculate a rgb image out of a bayer area sensor.\n\nAnd you can simply buy line scan color cameras and don't have to deal with the mention calculation at all. The camera simply produce lines with rgb pixel. You just have to configure the direction of your scene.",
        "Actually line-scan cameras aren't necessarily the same as rolling shutter https://shop.scorpion.vision/collections/linescan-cameras?srsltid=AfmBOoqaTmlOPQRtEwgXo0XFqMiCOvWYHJCZcPyr7z0Up6yrySDsXrKN",
        "Good point! I would call one-line sensors \"pushbroom sensors\". At the end of the day, no one looks at one line images, so composite images from pushbroom sensors are exactly like images from rolling shutter sensors, just with a different range of read out times!",
        "But that's simply wrong. There are area scan cameras with pixels in both axis. They can use a global shutter or a rolling shutter. And there are line scan cameras which have have a huge resolution in one direction and only one or a few pixels in the other one.\n\nAnd images from a line scan don't are exactly like rolling shutter image. They don't have a defined high and don't suffer under the rolling shutter effect. And you can certainly just look at one single line images. You get single lines, so why not look at single lines?",
        "Area scan images are global shutters for me, as a said in my first post. Can you explain the difference then?\n\nComposite images from single line sensor do suffer from the rolling shutter effect in the sense that each line is exposed at a different time and position in space. It's the exact same thing. Finally, you are welcome to look at single line images. It just felt meaningless in a rolling vs global shutter discussion.",
        "Global shutter: the \"shutter\" of each line of the sensor is triggered at the same time.\nRolling shutter: the shutter is triggered one line after an other.\nFor Line scan with only one line, this distinction don't make any sense.\nLine scan with several line: work as global shutter.\n\nYou make this discussion confusing if you mix the terms up.\n\nArea vs line scan describe the physical arrangement of pixel on the sensor.\nRolling shutter vs global shutter: describe the exposure and the read out mechanism of the sensor.",
        "I said \"composite\"! A rolling shutter 2d image is exactly a composite of many single line sensors! Again, I said that in my very first post, I'm talking specifically about rolling vs global shutter and am open to not having the right terminology. but if the discussion is about single line vs many lines, op's question is how we recognize images from the two types! My point is that we can recognize global shutter images from rolling shutter images or composite single line images in the distortions. That's absolutely valid."
    ]
},
{
    "submission_id": "1gl53kq",
    "title": "need help with computer vision onramp course by mathworks",
    "selftext": "does the course cover all the basics? what kind of projects will i be able to make upon completion? also, is it worth it to choose matlab over python???\n\nTIA",
    "created_utc": "2024-11-06T10:18:07",
    "num_comments": 1,
    "comments": [
        "I would try to learn Python instead. More widely used in industry"
    ]
},
{
    "submission_id": "1gl0d10",
    "title": "[Blog] History of Face Recognition: Part 1 - DeepFace",
    "selftext": "Geoffrey Hinton's Nobel Prize evoked in me some memories of taking his Coursera course and then applying it to real-world problems. My first Deep Learning endeavors were connected with the world of feature representation/embeddings. Being precise: Face Recognition.  \n\nThis is why I decided to start a new series of blog posts where I will analyze the major breakthroughs in Face-Recognition world and try to assess if they really were relevant. \n\nI invite you to my first part of History of Face Recognition: DeepFace [https://medium.com/@melgor89/history-of-face-recognition-part-1-deepface-94da32c5355c](https://medium.com/@melgor89/history-of-face-recognition-part-1-deepface-94da32c5355c)",
    "created_utc": "2024-11-06T06:56:14",
    "num_comments": 0,
    "comments": []
},
{
    "submission_id": "1gkqc5e",
    "title": "Would fine-tuning a MoViNet model for human activity recognition be plausible for aerial recognition?",
    "selftext": "Apologies if this is better suited for an AI/ML sub, I can delete and move if so.\n\nI'm working on a human activity recognition project using aerial footage from a drone, maximum altitude would be in the 45/50m region and at an angle to the object (not birds eye view). So the person would be easily detectable but considerbly smaller than the data in which MoViNet was trained on.\n\n  \nSince I plan on deploying this on an actual drone, the compute hardware is limited and real-time recognition is the current objective, both of which MoViNet was created to cater for.\n\n  \nI'm suprised however that there seem to be no academic papers which have tried (even if it failed miserably) to use MoViNet and fine-tune it on an aerial dataset.\n\nSince I'm inexperiences in both CV and image recognition, I feel there must be a fundamental reason why this wouldn't work, otherwise someone brighter than me would have tried it surely.\n\nWould really apprieciate some advise on the reasons why this would not work, or perhaps why it could if I did this or that (e.g. cross-modal pre-training).",
    "created_utc": "2024-11-05T20:36:53",
    "num_comments": 2,
    "comments": [
        "MoviNet works on the whole frame. I am not sure what activity you're trying to recognize because if there are multiple people in the frame, it wouldn't provide you with the activity each person is performing. It works on the whole frame like I said. And if you're going to crop it to each person, then that's going to increase the compute requirement and you also have to carefully maintain the state for each which means you need object detection + tracking on top of that.",
        "Yes fine tuning will work"
    ]
},
{
    "submission_id": "1gke7fh",
    "title": "Location estimation",
    "selftext": "Hello, I am seeking an approach to estimate the location of an object using a single camera. I know the camera position and orientation, and I understand that to estimate the object's location, I only need the distance between my camera and the object. This distance can range from a few hundred meters to 5 kilometers. My target location error can be up to 30m at the maximum distance (5km). At shorter distances, it should be lower, overall it would be great if it's mainly under 10m. I have my camera parameters, I don't have dimensions of a known reference object near my target, a rangefinder is not allowed, and methods such as stereo cameras and structure from motion are not applicable in my current situation.\n\nAll my research has led me to depth estimation with deep learning methods (I am only interested in the metric/absolute depth). The models I've seen are not optimal, as they are trained primarily on indoor datasets up to about 10 meters and outdoor datasets up to approximately 80-100 meters. I haven't had the opportunity to fine-tune them on my own datasets, but my intuition suggests that this may not yield successful results.\n\nDespite the mentioned approaches, is there another way to do it with a single camera?\n\nEDIT: Other out-of-the-box ideas are welcome. At the end the use of the camera for distance calculation is not required.",
    "created_utc": "2024-11-05T10:54:04",
    "num_comments": 16,
    "comments": [
        "You need to do projection from 2D to 3D using your full projection matrix(rotation matrix +translation vector). You can take an aruco marker as your reference point in the image to calculate real world distances.",
        "Important question, is the object known or not? If it's known geometry then that makes it a whole heap easier than if it's unknown. The problem with metric monocular depth estimation is that it's fundamentally an ill posed problem because you can't get scale from a 2d image so it's unlikely that we'll ever have a particularly good model for doing this (or at least it'll always be trivial to create an example where it's wrong by a large margin)",
        "Give us some more information about your problem (unless you aren't allowed to). What will work will be so dependent on the specifics. Can we get a sample image? Or at least know what the object is?",
        "> At the end the use of the camera for distance calculation is not required.\n\nI don't understand. Can you explain more clearly? What do you mean by \"the use of the camera is not required\"? \n\nIf by that you mean that you could use other tools, why not use a laser? [Jenoptik](https://www.jenoptik.com/products/lasers/laser-distance-sensors) makes these kinds of lasers that can accurately measure distances up to 60km. It's gonna give a much more accurate measurement than any camera could.",
        "The most obvious solution is to train a monocular depth model on semi-synthetic imagery. \n\nAlso ou are wildly overestimating how accurately distance can be measured. ",
        "Why stereo isn't an option?",
        "Hi, thank you for the suggestion. I have thought about that, but unfortunately, it's not feasible in my specific scenario, using any reference object or Aruco markers near the target is considered cheating.",
        "Hi, thanks for your insightful comment. You're absolutely right about the challenges of metric monocular depth estimation. Currently, the exact dimensions of the object shouldn't be known, simplifying the problem by assuming known object geometry would solve my headache, but it would limit the applicability of the solution to a specific set of examples.",
        "Hello, I've already shared all the information I can. The object could be any type of vehicle, such as a car, bus, ship, etc. Vehicle type shouldn't be known. Unfortunately, I can't share any images to provide a visual example.",
        "Hello, I apologize for any confusion. To clarify \"the use of the camera is not required\" I meant that while a vision-based approach is my primary method for estimating distance, other tools are acceptable. Lasers, however, are currently unavailable. I understand that a laser could easily provide distance measurements, but the system must be capable of estimating distance even when the object is off-center. The laser and camera would be aligned to share a common center point.",
        "I can't afford a sufficiently long baseline distance between the cameras. The maximum feasible baseline is around 1 meter, which isn't going to work for such long distances.",
        "Yeah it would make it a lot easier. What is the object? And what is the downstream tasks you want to do knowing the depth? I could probably suggest some more practical solutions",
        "How about the vertical axis?",
        "The primary object of interest is a vehicle, though the specific type can vary. My goal is to accurately simulate real-world scenarios. While placing the camera within the mapped area isn't an issue, as its position and orientation are known, accurately positioning the vehicle requires additional information: its position relative to the camera. This can be derived from the camera's position, orientation, and the distance between the camera and the vehicle. To estimate this distance, I've encountered depth estimation models as a potential solution (this is only an idea currently). However, I'm open to other practical approaches that can provide accurate and reliable distance or position estimates. I've considered adding a GPS module to the object, but any physical modification to the object is prohibited.",
        "Thanks for the idea! I hadn’t really thought much about using the vertical axis (my bad). 😅 I already have some ideas to try out, and I think it could lead to something promising.",
        "Not gonna happen."
    ]
},
{
    "submission_id": "1gkbn0j",
    "title": "How much should I use chatgpt solutions?",
    "selftext": "Right now, I'm working on a object segmentation project and the thing is that whenever I'm encountering smaller or bigger bugs I mostly tend to gpt to help me solve it. Ofcourse at the end I understand very line of code but this still feels like I'm not learning anything.\nI also search the bugs on Google and docs but after some time getting bugs again and again,  I feel frustrated and again tends to gpt.\n\nFor people working in this field, how you tackle problems when you encounters similar situation or this is just my imagination. Any advice for me in my learning journey.\nThanks in advance :)",
    "created_utc": "2024-11-05T09:07:35",
    "num_comments": 15,
    "comments": [
        "99% of what youre doing has been done before and thats where chatgpt can turbo boost development",
        "In general, I think its best to come up with the general idea for a solution your self first, then ask for implementation and improvements. You should always understand what you’re trying to do.",
        "I just use it when I run out of ideas or I need to validate that there is not a better method that I was ignoring. Do things by yourself and allow yourself the time to think the solutions (with regular search engines and books ) . That's how I learned a lot.",
        "I use it on daily basis but mostly for basic functions, like container conversions or sorting functions i don't recall anymore. I usually suggest also the name of the algorithm to use, so it's more precise. I think it's a great tool to get a survey on the state of art of your problem, if you start working on a problem you don't know even if it's studied in the literature, it can provide you with some hints and vocabulary you can base your search on. Concerning computer vision, NEVER trust its maths! And i mean, NEVER!",
        "I use copilot all the time, essentially for code completion++",
        "I'm working since 6 months in C# and computervision enviroment.\n\nSo maybe this experience description helps:  \n(emphasized to get the point across)\n\nI worked a lot with ChatGPT.  \nRather than think, I'm throwing everything into ChatGPT.  \nThen I sit down at a laboratory computer without my ChatGPT account and a blank program.  \nI can't remember anything, since I copy pasted from ChatGPT without chewing it through in my head for a while.\n\nLogging into my pc and going through my previous programs it was very easy to get the code snippets together and then copy the functions to the laboratory computer.  \nAfter that I wrote them by hand (since no ChatGPT on the laboratory computer) and remembering them actively.\n\nSo rely on it, but don't copy paste blindly.  \nI have one program solely thrown together with ChatGPT. I have no idea how it works despite spending 4 hours on making it work. But dissecting it to make it work and make it my own (20 hours) made me learn many things I haven't known an be done.",
        "I have been stuck a few times when chatgpt made up some functions that did not exist it was hard pointing the mistake",
        "What kind of bugs are you talking about?",
        "Perfect explanation - use it to get you to the hard stuff faster. Making a system work like you want is more than just functional code.",
        "Yes. Use it as much as possible. Especially in personal projects that you can only give a limited amount of time in your life.\n\nI used gpt code responses and encountered many errors and bugs that I had to fix myself. I still learned from it but saved tons of time and energy.\n\nP.S. I still can't believe this tech is out for free. Just like fire to mankind this is one of the greatest giving to the intermediate coders who don't have the time and energy to level up.",
        "Yah I also learn through the same approach in 3 steps, first just copy paste, then understand every bit of line, finally write it on my own without any gpt help.\nIt helps me to learn new technology faster.",
        "For eg. I was trying to save an image predicted by my model, but it again and again giving me an error that I'm not reading correctly with cv2.imread()\nAfter that I got to know I haven to read the array format first, the value from model.predict() contains many other metadata also.",
        "Yah, it helps me a lot in my research and provides me with a easier way to handle bugs.",
        "What are you currently working on?  \nWhat software?",
        "I just completed a brain hemorrhage volume detection project, and am now going to work on an AR project."
    ]
},
{
    "submission_id": "1gkb8zt",
    "title": "I used Stable Fast 3D to generate 3D meshes from Marvel Masterpiece trading cards",
    "selftext": "",
    "created_utc": "2024-11-05T08:51:29",
    "num_comments": 3,
    "comments": [
        "Here's all the code, etc. Please ❤️ the dataset on HF and ⭐️ the repo on GitHub!\n\n  \nDataset with meshes: [https://huggingface.co/datasets/harpreetsahota/marvel-masterpieces-with-3dmesh](https://huggingface.co/datasets/harpreetsahota/marvel-masterpieces-with-3dmesh)\n\n  \nDataset with images only: [https://huggingface.co/datasets/harpreetsahota/marvel-masterpieces](https://huggingface.co/datasets/harpreetsahota/marvel-masterpieces)\n\n  \nGitHub with all the code: [github.com/harpreetsahota204/marvel-masterpieces](http://github.com/harpreetsahota204/marvel-masterpieces)\n\n  \nNotebook to generate meshes: [https://colab.research.google.com/drive/1fapPjlQYYL8\\_aqloMzX7LsuhLF7Oj\\_v4](https://colab.research.google.com/drive/1fapPjlQYYL8_aqloMzX7LsuhLF7Oj_v4)",
        "I don't see how you are pleased with the results? They are not even close to usable.",
        "Given that the images were so far out of distribution from Objverse, I think it did a good job. I actually ran this on some images of Marvel bobbleheads, and they were much better. I'll share the results soon."
    ]
},
{
    "submission_id": "1gkad36",
    "title": "Could you please help me find a paper",
    "selftext": "I can't recall the name of an old paper. I remember it's from UCBerkeley. The paper can generate animated images, but only a small portion of an image is moving. For example, the tip of a person's hair. I don't remember if animated image was generated from video or not. ",
    "created_utc": "2024-11-05T08:14:23",
    "num_comments": 0,
    "comments": []
},
{
    "submission_id": "1gk9gkk",
    "title": "Float vs Int input to TensorRT INT8 model",
    "selftext": "When a TensorRT model has INT8 weights, is there any difference between passing images in range \\[0-255\\] or \\[0-1\\]? Usually I see input being transformed to \\[0,1\\], but this produces floats and I was thinking it may reduce the benefit of quantization (due to float\\*int operations). Since I have to code my custom pipeline I have to choose how to perform pre-processing.",
    "created_utc": "2024-11-05T07:35:56",
    "num_comments": 7,
    "comments": [
        "Depends on how the model was trained",
        "Not all of the weights are converted to INT8. TensorRT determines whether to convert based on whether it brings any improvement.",
        "The main advantage of 0-255 is you dont have to normalize your inputs which can be expensive. The trade off is models typically converge better with normalized data.",
        "Using `ultralytics` this is what is done on training phase:\n\n`im = np.ascontiguousarray(im.transpose((2, 0, 1))[::-1])` \n\n`im = torch.from_numpy(im) # to torch` \n\n`im = im.half() if self.half else im.float() # uint8 to fp16/32` \n\n`im /= 255.0 # 0-255 to 0.0-1.0 return im` \n\nDoes this mean I have to do the same on inference?",
        "https://forums.developer.nvidia.com/t/tensorrt-int8-quantization-weights-activations-quantization/110404/2",
        "Yes, I know this. But does this mean that it is better to input images in range 0.0-1.0?",
        "It probably has a conversion operation at the beginning of the graph that turns it into INT8 so it doesn't really matter much.\n\nhttps://github.com/NVIDIA/TensorRT/issues/1792#issuecomment-1038372973\n\nAlso input is supposed to be float apparently.\n\nhttps://forums.developer.nvidia.com/t/want-to-know-more-about-int8-precision/273242/7"
    ]
},
{
    "submission_id": "1gk832r",
    "title": "Step-by-Step Guide to Creating a Handwritten OCR Engine | Explained in Tamil",
    "selftext": "\n\n**Vanakkam everyone!**\n\nIf you're interested in building a custom handwritten OCR engine, I’ve got something cool for you! Check out this demo video and code tutorial that walks you through the whole process:\n\n[YouTube Video - Custom Handwritten OCR Engine](https://youtu.be/3aE-XvadvkU)\n\nIt’s perfect if you're looking to create a custom solution for recognizing handwriting. The video covers setup and code explanations step-by-step.\n\nHope it’s helpful!",
    "created_utc": "2024-11-05T06:34:50",
    "num_comments": 0,
    "comments": []
},
{
    "submission_id": "1gk68sb",
    "title": "Is there a Thick Lens Model?",
    "selftext": "I want to be able to get the corresponding 3D locations of key features in an image. To model the lens, is thin lens model adequate enough? What is the focal length threshold for me to switch to thick lens models?",
    "created_utc": "2024-11-05T05:07:22",
    "num_comments": 4,
    "comments": [
        "I'd say it's sufficient. The classical pinhole model does use a thin lens approximation for the whole optical system. The fisheye model also uses a single center. In the end, only the direction to the 3D point matters, and it's mapped to a 3D position on the sensor, so it's really a 2D-to-2D mapping.\n\nFor the pinhole model, we usually add distortion on top of it, that can be defined pixel-to-pixel, whereas it's already here in the fish eye model (I'm thinking about an angle-radius LUT: you give the angle of the ray with the principal axis, it'll return the radius from the principal point on the sensor).\n\nI don't think you need a more complex model unless you need to model complex effects like chromatic aberrations for instance.",
        "Instead of asking for people's rules of thumb, you should have methods for evaluating your errors. In general all the lean models (anything with a small number of parameters, like pinhole + distortion) will give you clearly observable, correlated errors. And if you're close to the lens, the central assumption (all rays of light intersect at a single \"optical center\") isn't true anymore either. For and example: https://mrcal.secretsauce.net/tour-initial-calibration.html\n\nIf you're even asking this question, you want to be using the mrcal splined model",
        "I mean, even if you *do* need to do it manually (and I very much doubt you do), ray tracing isn't *that* hard to implement, especially since you are doing pose estimation and not actually trying to simulate optics accurately for rendering.\n\nI don't really know for sure, but I can tell you I've never heard of anyone using a thick lens model for pose estimation.\n\nHow much accuracy do you need?",
        "The standard way to do this is empirically: with a pinhole model plus radial distortion plus if you need better accuracy a grid.\n\nIf you are very wide angle and very close range then maybe you need a more complex model, but even then I would think to do it empirically e.g. an RPC model.\n\nYou can then either fix these calibration parameters based on a raytracing optics simulation, or empirically with a calibration measurement e.g. checkerboard.\n\nBut I wouldn't be trying to do optics simulations within a CV code, much nicer to separate the domains with these kinds of empirical calibrations as an \"interface\" than to mix things and have CV people try to understand optics and optics people understand CV."
    ]
},
{
    "submission_id": "1gk68qd",
    "title": "Is there a Thick Lens Model?",
    "selftext": "I want to be able to get the corresponding 3D locations of key features in an image. To model the lens, is thin lens model adequate enough? What is the focal length threshold for me to switch to thick lens models?",
    "created_utc": "2024-11-05T05:07:17",
    "num_comments": 0,
    "comments": []
},
{
    "submission_id": "1gk66vg",
    "title": "Missing Object Detection [C++, OpenCV]",
    "selftext": "",
    "created_utc": "2024-11-05T05:04:34",
    "num_comments": 28,
    "comments": [
        "How does it respond to new or relocated objects?",
        "What happen if the thief replace the painting with a same copy?",
        "What a refreshing thing is to watch an actual interesting application and not another ai agent support saas.",
        "Could you provide a link for the project?",
        "This gives me death stranding vibes",
        "git ? \n\nvery cool !",
        "How can I make a similar project. Is this project available on github?",
        "this is neat and something I haven’t quite seen before - might be some applicability to inventory management use cases",
        "This is awesome. What happens when you misplace the missing object?",
        "I like the pitch; it's a cool idea for a helpful application. However, the video showcases entry-level technology. You can achieve this by downloading a Yolo model and putting a graphic layer over it. It would widen your scope if you added some sophistication and could handle replaced and misaligned objects. Nonetheless, simple and smart can be profitable. Good luck!",
        "Is it a segmentation model trained on the labeled masks of a background section, later hidden by the object when in the right place?",
        "Does it rely on the camera being static?",
        "M r",
        "Really cool",
        "Un sistema inteligente creado con miles de millones de algoritmos, son sólo instrucciones y directivas pero unificadas, son más eficientes que la biología, ordenando e interpretando la información",
        "It's gonna react the same even if the object is a little bit misaligned. Because it seems like they are doing a block-wise template matching or something similar to match the current image to previously acquired image. You can see when he puts his hands in front of the object, the system is unable to match.",
        "I would hope the imaging pipeline is robust enough not to go back to “normal state” after a prolonged significant disruption.",
        "Also actually using C++ !",
        "Thank you! I was thinking of a similar application.",
        "something like CLIP-surgery",
        "probably yesssss",
        "In this solution, yes, but if I apply a simple affine transform and warp the current frame to the reference image, it can work with a moving camera. This is a PoC not a product, I just had an idea and made a prototype. There is plenty of room for improvement.",
        "Thank you! 🙂",
        "What if they rotate / flip the object ? I don't think the template matching will work then",
        "From the video, it seems like it would not work with their system either."
    ]
},
{
    "submission_id": "1gk3jz4",
    "title": "What are companies in USA, Europe, Singapore, etc looking for in a computer vision engineer. How do I prepare myself to get a job in these locations.",
    "selftext": "Hi, \n\nI am a computer vision engineer currently working for a start-up in India. My daily tasks involve traininig object detection and classification models. Apart from handling the data for trianing and improving the model, I also have to engineer custom classification models for varying requirements. It involves modifying the architecture of the model to fit the problem.\n\nWe run our models on edge devices, so running models efficiently along with pruning and other optimizations also current come under my responsibilities. I am also being gradually introduced to deepstream and g-streamer, which I do understand the basics of.\n\nNow, I am new to this field. It has only been 7-8 months that I have worked professionally. So, I intend to continue working where I am for an year more, to firmly get grasp of the fundamentals and the sorrounding tech knowledge.\n\nMy question is, during this year I am here, I want to prepare myself as much as possible to start applying for jobs in international markets in 2025. What skills should I mainly focus on. What are companies aminly looking for these days. Are these some set of skills that are more in demand right now than others?",
    "created_utc": "2024-11-05T02:20:54",
    "num_comments": 7,
    "comments": [
        "Singapore is subtly-hostile to immigration now imo thanks to prior decisions of opening the flood gates. \n\nIt’s now in a pseudo-Canada state where the locals have a real possibility of damaging the current government in elections. So employment passes are slowed/frozen and new immigrants especially from India are being shut out from PR.\n\nIt used to be easier, but not anymore. Anyhow, the country doesn’t use computer vision much, so there isn’t a market for you. Robotics and computer vision are tied together in the market, but both are in very little demand due to high upfront costs when labour is cheap.\n\nUS/Europe will be better.",
        "One of the biggest hurdles to land a job from outside the USA is convincing the employer to sponsor your job visa. If you are able to get a work authorization without much help, you become quite attractive to the hiring committee. As for passing the bar as a computer vision employee, most interviews focus on linear algebra. It's good to brush up on those skills. Also, staying up-to-date with newer models also helps.",
        "Thanks for the update! What locations do you feel, specifically in Europe, are showing some interest in adopting these technologies?",
        "Thanks for the response. Didn’t know linear algebra played such an important role in the interview process. I’ll brush up on it.",
        "Denmark (specifically the city of Odense) has a huge robot sector compared to its size, with hundreds of robot startups and medium sized businesses. Even a few billion dollar companies.\n\nA lot of those companies use computer vision, and everyone speaks English.",
        "This one I’m not too sure. But from an immigration standpoint, Netherlands will be easier to adapt as a foreigner than most European countries.\n\nThey speak mostly English and in most of Europe if you don’t speak the common tongue it’s very difficult to live in.",
        "Any idea of the kinds of skill sets needed to enter the robotics sector in the field of computer vision?"
    ]
},
{
    "submission_id": "1gk0yyn",
    "title": "Yolo and object's location as part of the label",
    "selftext": "Let's imagine a simple scenario in which we want to recognize a number in an image with a format such as \"1234-4567\" (it's just an example, it doesn't even have to be about numbers, could be any bunch of objects). They could be organized on one line or two lines (the first for digits on a line and the next four on another).\n\n  \nNow, the question: When training a yolo model to recognize each character separately, but with the idea of being able to put them in the correct order later on, would it make sense to have the fact the a digit is part of the first bunch or second bunch of digits as part of its label?\n\nWhat I mean, is that instead of training the model to recognize characters from 0 to 9 (so 10 different classes), we could instead train 20 classes (0 to 9 for the first bunch of digits, and separate classes for 0 to 9 for the second bunch)?\n\n  \nVisibly speaking, if we were to crop around a digit and abstract away from the rest of the image, there is no way to distinguish a digit from the first bunch from one from the second bunch. So I'm curious if a model such as YOLO is able to distinguish objects that are locally indistinguishable, but spatially located in different parts of the image relative to each other.\n\nPlease let me know if my question isn't phrased well enough to be intelligible. ",
    "created_utc": "2024-11-04T23:00:47",
    "num_comments": 5,
    "comments": [
        "It will probably be able to learn it. Models like YOLO don't just look at what's inside the box. They also have access to the surrounding context.\n\nBut I wouldn't do it since it might lead to overfitting, as in order to achieve that, the model might learn very specific features that don't generalize well to unseen images.",
        "I don't understand the usecase well but why isn't OCR being done? Can't you just do basic image processing to extract lines or group the extracted digits together based on proximity? \nCould you post an example of the image so that we can understand better?",
        "You would just train for the numbers 0 to 9.   Then when running the model you get the bounding box coordinates, from those you can post process and work out which are on what line.",
        "As I said in the post, I just chose numbers to keep the example simple, but my situation doesn't have to be about numbers or text specifically. It's about encoding the relative location of objects in the labels.",
        "I know about that. My question is specifically about whether or not it is instead possible to encode the relative location in the label. I agree that in most situation, it makes more sense to not do it. But I'm asking if it's possible, in theory."
    ]
},
{
    "submission_id": "1gjwj55",
    "title": "Efficient Object Detection in Garage Sale Environments Without Massive Datasets?",
    "selftext": "Hey everyone! I’m working on a project focused on object detection within garage sales. Since these sales feature a wide variety of unique, one-off items that don’t usually repeat, I’m looking for a way to detect “any” object, regardless of its specific category or type. Ideally, I’d like to avoid training a model like YOLOv10 on an enormous dataset with a massive number of labeled items, as that approach would require a huge amount of images.\n\nDoes anyone know of a method or model that’s good at detecting objects in general, without requiring detailed category-specific training? Or, is there a more efficient approach I could take to achieve this? Any insights would be greatly appreciated!",
    "created_utc": "2024-11-04T18:36:28",
    "num_comments": 8,
    "comments": [
        "You could look into SAM for panoptic segmentation, and apply some post-processing.",
        "Check out grounding dino and yolo world! Maybe even GPT4o with multimodal inputs.",
        "This might be a little orthodox but you can achieve that using classic image processing by controling some of the capture properties. All you need to do is control the background color. For example, imagine that all objs are placed in a table that is made of a single solid color. You can have a ROI utility that selects the table from the background of the image. That is, the user takes a picture of the table w objs and places the bounding box. Then you can have a utility to select the color of the table. From thete you can convert the selected color and the ROI into HSV and perform chromatic segmentation. From there you can use flood filling and bitwise_not to fill in spots within the objects that are the same color as the table (if obj has the same color of the table in the edges then you are screwed). Then you can use that as a mask to do proper segmentation. Assuming that the object do not touch/overlap, then you can use connected components to identify the objs. You can use that also, somehow, to crop the objects out of the image.\n\nYou would need to experiment with selecting the bounds for the chromatic segmentation. This kind of approach is very sensitive to lighting conditions. That is the biggest limitation of this approach. But perhaps you can pitch it by enphasizing its ease of use and point out that it compensates for the lack of quality. After all, garage salest can be quite informal.\n\nI imagine you want to do this to quickly catalog the items for sale without having to go through the tedious task of taking pictures of every item. This makes sense, financially and socially. I think it is a good project.\n\nClassic approachess tend to be efficient, but I am not sure about how long it will take to execute and whether that would be a problem. I woulf be more concerned with the accuracy/quality of the result. \n\nTL;DR - If the objs are placed in a solid color background and the objs are not touching each other, a classic chromatic segmentation can be used to detect multiple objs.",
        "Sorry if I'm stating the obvious, but a vision LLM could spit out a description of miscellaneous objects.",
        "Good idea will look into this more.",
        "Will definitely check it out!",
        "This is super thorough and helpful tysm!!!",
        "Yah I’ve tried but I also want to segment out the images"
    ]
},
{
    "submission_id": "1gjwga5",
    "title": "Any open source tensorflow model for weapon detection ?",
    "selftext": "I am looking for a tensor flow model for weapon detection. I am looking for model.pb and lablels.pbtxt wrt weapon detection. Any others information on this are welcomed as well. ",
    "created_utc": "2024-11-04T18:32:15",
    "num_comments": 2,
    "comments": [
        "I don't know, but, if it comes down to training or fine tuning one yourself, at least there are loads and loads of 3d models of weapons out there for games and such.",
        "I don't think any publicly sourced dataset or model will contain this.  Also, you should consider not using tensorflow haha."
    ]
},
{
    "submission_id": "1gju7oe",
    "title": "Need help from Albumentations users",
    "selftext": "Hey r/computervision,\n\nMy name is [Vladimir](https://www.linkedin.com/in/iglovikov/), I am core developer of the image augmentation library [Albumentations](https://albumentations.ai/).\n\nPast 10 months worked full time heads down on all the technical debt accumulated over years - fixing bugs, improving performance, and adding features that people have been requesting for years.\n\nNow trying to understand what to prioritize next.\n\nWould love to chat if you:\n\n* Use Albumentations in production/research\n* Use it for ML competitions\n* Work with it in pet projects\n* Use other augmentation libraries (torchvision/DALI/Kornia/imgaug) and have reasons not to switch\n\nWant to understand your experience - what works well, what's missing, what's frustrating in terms of functionality, docs, or tutorials.\n\nLooking for people willing to spend 30 minutes on a video call. Your input would help shape future development. DM if you're up for it.",
    "created_utc": "2024-11-04T16:42:29",
    "num_comments": 28,
    "comments": [
        "AutoAlbument would be massive for object detection. In my line of work we have a lot of rare classes and objects and need to heavily rely on augmentation. It’s really tedious and time consuming to try and find ideal parameters and even augmentation candidates so having something guide that beyond subject matter experts would be beneficial to the community",
        "just in general, thank you, albumentations is super useful!",
        "visualization wizard that helps people understand if augmentations should be made or not",
        "Thanks for your work on albumentations! We are using it in an industrial visual inspection context. The one thing we are missing is mostly common augmentations for object detection like Mosaic and CutMix. Apart from that everything works fine. We heavily use the custom augmentations for very specific augmentations.",
        "First, thanks for your work! It is making a big difference.\n\nI work in Object Detection/Segmentation domain. I've had a situation where I wanted to add my custom augmentation to the pipeline and I found it hard to do, as there were almost no docs on that topic.\n\nAnd for some particular custom augmentations I couldn't do that at all, because of how Albumentations classes are maked as image-only, bbox-only internally. My augmentation pruned some annotations entirely (think dropout), so it modified both bboxes and classes list, as well as the image, and I couldn't find a class that allowed both for the life of me.\n\nSo making a flexible and documented way to add custom augmentations would be great.",
        "Thanks for the good job man. Happy I discovered Albumentations",
        "Yo! Thanks for making this amazing library, I tried to convert all of my colleagues to it :)\n\nI use it for ML research in Object Detection and Semantic Segmentation with medical images. \n\nWhat I love about it is how easy it is to define a set of transforms and apply them to different types of targets\n\nEg. Tensor inputs, binary mask, multiple masks, boxes, key points, etc... And also custom ones!\n\nEspecially for the boxes I love you can use different types of boxes conventions. So a single set of transforms can be created and then used regardless of the task. This just gives so much flexibility.\n\nAlso there are a lot of great augmentations for what I need :) \n\nI prefer it over torchvision, since in my experience there is not a clear and easy way to do what I just mentioned.\n\n\nOne criticism from my side is the documentation, as I sometimes struggle to get to the page I need, and also the page of the augmentations is pretty chaotic to me.\n\nI would prefer if when",
        "Thank you, you’re doing awesome work!\n\nThe app for visualisation is great, it should have a more prominent role.\n\nIf you are being ambitious and looking to not be disrupted, I would think hard about making augmentations based on diffusion models, ie controlnet, more accessible. Sure, they would have to be done offline, but I think it could add value.\n\nI work in product development/consultancy.",
        "Big one for me would be easier support for augmentations that mix images together. Make that super beginner-friendly.\n\nCopyPaste is one example. ",
        "Thanks!\n\nAutomatic choice of augmentations, is similar to AutoML, hence tricky business.\n\nBut, unlike other regularization techniques you may apply different augmentations on images with different classes, hence I have a few ideas that may simplify the story and lower the bar for a successful execution from a non expert.",
        "Thank you for your warm words!\n\nDo you see anything that is missing, or you wish someone would build for you?",
        "Thanks!  \n  \nI am working on the UI tool that allow to check the effect of augmentations at:\n\n[https://explore.albumentations.ai/](https://explore.albumentations.ai/)\n\nIt is work in progress, hence would be happy to get any feedback about it as well.\n\nFeature requests, issues, UI, etc\n\nYou, or anyone who reads this can create and issue at:\n\n[https://github.com/albumentations-team/albumentations/issues](https://github.com/albumentations-team/albumentations/issues)\n\nor ping me in DM",
        "Thanks!\n\nWhen you build a transform, you may feed any information you want.\n\nAnd within a method \\`get\\_params\\_dependent\\_on\\_data\\` process all the data.\n\nCould be \\`images\\`, \\`masks\\`, \\`bounding\\`, \\`boxes\\`, \\`key points\\`, or anything else.\n\nExample of such transform: [https://github.com/albumentations-team/albumentations/blob/2e1bbec7895ead9be76351c17666b0b537530dc9/albumentations/augmentations/mixing/transforms.py#L18](https://github.com/albumentations-team/albumentations/blob/2e1bbec7895ead9be76351c17666b0b537530dc9/albumentations/augmentations/mixing/transforms.py#L18)\n\nBut point noted - I need a clear documentation + example on how to build a custom transform. Will do.",
        "Thank you for the warm words!\n\nDo you have something in mind about what it missing in the library in terms of features, documentations, or any frustrating issues?",
        "Thanks, for the warm words!\n\nDocumentation is indeed far from  perfect, as it evolved more or less by itself, except detailed pages created by u/alexparinov\n\nI am more than happy to extend the docs, but would love some guidance, or prioritizations.\n\nWhat information (top 3, or 5 or 10 ;) ) is missing or misleading the most? I will just start from this list.",
        "Thanks!\n\nFor the past 5 years, I was thinking and hoping that someone will copy or fork the library. It did not happen.  \nFeel pretty safe now, although if I will figure out way to build product on top of it, the situation may change.\n\nUsing ML to generate more data offline is one of the ideas I am thinking of. But I think about this idea for testing, rather than for training.\n\n  \nWill work on the visualization app more in the upcoming months. Still did not figure out a good way to collect feedback / feature requests on it.",
        "Oh that would be so so awesome! ",
        "You can put image on top of the other image with OvelayElements transform, but it does not affect key points and bounding boxes yet.\n\nAdding CopyPaste to TODO List.",
        "Seconding the post above. No complaints, and thank you for your hard work. You'll be cited in anything I publish.",
        "I don't remember what the exact problem was, as it happened a long time ago. But I think if I define params_dependent_on_data I can only use them as readonly and it is hard to modify them. It is possible, but needs to be done in place. While for normal augmentations you just return what you want as new labels, bboxes, image for example.\n\nBasically by design modified values are supposed to be returned, but this doesn't work if you need to modify all of them (image, bboxes, labels) simultaneously. Sorry if that doesn't make sense, I might have used an older version too, so this might have changed.",
        "Since youre asking :)\n\nIn the app it would be nice to \n-upload your own images. \n- combine multiple augmentations \n- Generate and display several versions in the case of random augmentations.\n\nI do the last one in my training scripts, both for images and masks.\n\nIt would also be nice if there was a function (maybe there is and I havent looked) that would run and time your augmentations over n runs.",
        "Thanks!\n\nPaper about Albumentatins has many more citations than all my papers about Theoretical Physics joined together.  \n  \nI guess my work on Open Source is more valuable and impactful than work that I did in academia :)",
        "I do not get it yet.   \n  \nRight now, you can take:\n\nimage, mask, boxes, key points, labels, anything else and pass to the transform.\n\n1. you can get access to all this data in params\\_dependent\\_on\\_data.\n\n2. In that function you may create new data say: crop\\_params, text to add, noise, etc and data that was passed is read only   \n  \n3. than original data and one that was created is passed to \\`apply\\`, \\`apply\\_to\\_mask\\`, \\`apply\\_to\\_bboxes\\` , \\`apply\\_to\\_keypoints\\`\n\nWhat do you mean by\n\n\\> Basically by design modified values are supposed to be returned, but this doesn't work if you need to modify all of them (image, bboxes, labels) simultaneously. Sorry if that doesn't make sense, I might have used an older version too, so this might have changed.",
        "Thanks.\n\nAdding to TODO list for UI tool:  \n\\- Upload own images (you can do it now for ImageOnly, but not for Dual transforms)  \n\\- Display several versions of Augmentations\n\nWhen you mean run and time augmentation, you mean individual transforms, or the whole Compose that you defined?",
        "I had to necro my old project to recall what was going on.\n\nMy line of thinking was. When my augmentation is called it calls apply one by one. Each apply must return a modified version of the parameter it is called for. Like apply_to_boxes returns modified boxes. But I want to modify everything at the same time, image, boxes, classes, because augmentation depends on all of them. I don't want to modify them one by one and redo the same calculation for each apply.\n\nWhat you said makes sense, I can modify only image and save local data for other applies and then read them instead of redoing the calculation and modify another bit. But it feels backwards from the design standpoint. Shouldn't there be a combined apply function that allows to modify multiple data fields?\n\nAdditionally, where do I save those intermediate results? In self? That feels off, because what happens if something internal rewrites it? Should it be thread-safe? And there seems to be no way to pass on an extra param from one apply to the next.\n\nAlso I don't know the order in which apply is called, so which one should do the main calculation and which one should only read from saved data? I can snoop in the code of course, but what if it changes in the future?\n\nI agree this could be worked around, but all I was saying is that it feels like it was designed for single-focus augmentations, but there are multi-focus ones as well. Just a side perspective.",
        "The whole compose, and then the avg time for each transform",
        "\\> Shouldn't there be a combined apply function that allows to modify multiple data fields?\n\nIt would be a hell to maintain.\n\nIn many transforms apply methods were added one by one, first image + mask =>  added boxes => added keyponts.\n\nthere are also apply\\_to\\_images and apply\\_to\\_masks, that just call relevan apply's in a sequence, but for a faster execution one cal always rewrite it for each particular transform in a vectorized way.\n\nBasically, from architecture point of view we decided to compute common things in \\`get\\_params\\_dependent\\_on\\_data\\`, for example displacement\\_fields in ElasticTransform and use the same computed fields for different targets.\n\n\\----  \nI could be missing something, but could you please share an example when you need to pass data from one apply to the other. It could be such case, but non of the existing transform requires such functionality.  \n\\----  \n\\> Also I don't know the order in which apply is called, so which one should do the main calculation and which one should only read from saved data? I can snoop in the code of course, but what if it changes in the future?\n\nThat's the point. All apply\\_XXX could be called in any order as they do not pass information to each other. The main calculation happens in \n\n\\`get\\_params\\_dependent\\_on\\_data\\` once and then passed to all apply's",
        "Fair enough. Doing the main calculations in get_params_dependent_on_data should work. I think I fixated on the idea that it should only pass on extra fields and the main thing should be done by apply's."
    ]
},
{
    "submission_id": "1gjqbaz",
    "title": "Behavioural analysis for psychiatric care. ",
    "selftext": "Hello everyone, I hope you all are well. \n\nI am in the midst of creating a tool that analyses body posture and body language and derives insights from it in the context of psychiatry. Poses (postures) such as crouched, defensive, attentive, confident, relaxed, etc etc. I'm having some trouble deciding whether to go with training a classifier or implementing rule based algorithms and thresholding. I have limited computational resources and no access to any data with a good usability for this use case. \nCould anyone help me on what pre trained models I could use for rule based approach and which would work best given the need for high accuracy as it is a very sensitive use case. \nIf anyone has worked on anything similar I would appreciate it a lot if you could share your solution or your approach. Thank you. ",
    "created_utc": "2024-11-04T13:49:20",
    "num_comments": 4,
    "comments": [
        "I'm not sure how well this would work.  The first thing that captures my attention is the vocabulary you're using is rather biased.  There is no such thing as a defense posture that's universal.  There's not even going to be one that's consistent across a lot of people.  Same with attentive, confident, and relaxed.  These are multimodal determinations you need to make.  So the first thing is your labels need to be more neutral otherwise you're going to have a major alignment problem.  The second thing is, since these are multimodal is you need to remain as neutral with your output as possible.\n\nAs to what to do, it depends entirely on how you're capturing the people and what environment they're in.  Are they sitting, standing, laying down, in public, in private, etc?  What expectation is there for this data.  My initial guess is if you have a controlled environment (eg not public), and that the person is sitting, I would start with doing pose estimation.  You can also do a face tracker, and then capture the facial movements over time.  Any analysis after that though, needs the help of a SME to help provide labels.  That could be a psychologist/psychiatrist that's highly regarded, or the subject themselves can explain what they were feeling in different poses and facial expressions.  It may be better if, say if this in a video, you can play back the video to the subject and pause it in areas where there is a shift in facial expression or pose, and then ask them how they felt at that time.  However, that even itself has biases.\n\nThis project is, in my opinion, an uncomfortable one, because there's no way that it won't be biased.  And thus you're going to face an alignment problem.\n\nEdit - I always want to note that any data you did collect, of say a psychological evaluation, would be ILLEGAL to use as data unless the subject is entirely anonymized, which is impossible to do if you're doing facial recognition.  This violates a lot of EU laws as far as I am aware, and probably some in the US as well.",
        "hi, we recently experimented with emotion recognition and found it to be a very difficult and biased problem. from a visual context it is often not clear what emotions can be observed. for example we worked with a dataset (CAER), where annotators assigned labels based on video+audio. when reviewing some of the labels it was often inconclusive from only the visual context what emotion to assign. audio seems to be a pretty important cue. \n\nwhat was also came across is the VAD model (valence, arousal and dominance) a continuous scale to encode emotions and the FACS (facial action coding system). apart from that you can classify body pose and different gestures, but datasets for those are rare.",
        "Thank you, that does clear a lot. And yes I agree, the vocabulary does and will seem biased as there are no definitive terms to capture how a person reacts as it is very subjective. What I meant was quick changes in body language, a sudden jolt, slight trembling, lip biting, avoiding eye contact. Things to track based on the context of the discussion, and yes simultaneously facial emotions will be recognised, as well as emotions from audio. You're absolutely right I should've been clearer in my post and yes it is uncomfortable but we're trying to do this for a very good cause. If you have any new insights based on this reply I'd really appreciate it if you'd share. Thank you.",
        "I mean the biggest thing is I would just be certain that what you're doing and the data you're collecting is legal.  There's a lot of legalities surrounding this type of data."
    ]
},
{
    "submission_id": "1gjomfk",
    "title": "Guidance required to Implement the Architecture",
    "selftext": "Hello, I want to implement the following architecture, but I’m struggling with some parts, and I only have one week left in the project.\n\nI’m using timm PyTorch image models with resnet34 as a feature extractor and an image size of 256x256:\n\nmodel = timm.create\\_model('resnet34', pretrained=True, features\\_only=True)\n\nThis provides the following feature map sizes:\n\nFeature from block 0: torch.Size(\\[1, 64, 128, 128\\])\n\nFeature from block 1: torch.Size(\\[1, 64, 64, 64\\])\n\nFeature from block 2: torch.Size(\\[1, 128, 32, 32\\])\n\nFeature from block 3: torch.Size(\\[1, 256, 16, 16\\])\n\nFeature from block 4: torch.Size(\\[1, 512, 8, 8\\])\n\nHow can I apply ScSE attention modules after each block as shown in the architecture below and then pass it to the decoder? Could anyone help me with this?\n\nhttps://preview.redd.it/u9qdsm7d4yyd1.png?width=591&format=png&auto=webp&s=aeb868fd297b4c1139be39a959c87208e2f2265a\n\n",
    "created_utc": "2024-11-04T12:40:01",
    "num_comments": 3,
    "comments": [
        "It would be similar idea to how you code any u-net architecture",
        "u/cnydox how can I add attention module in timm resnet-34 model?",
        "Not sure if it will help, but you can make a subclass of resnet-34 model and rewrite the forward method.\n\nIn the forward, add attention wherever you need."
    ]
},
{
    "submission_id": "1gjmcgh",
    "title": "Surface Reconstruction of Highly Specular Surfaces without using AI",
    "selftext": "I want to know if it is possible to estimate the surface shapes of highly mirror-like surfaces such as car panels using the surface models like Hapke. I don't want to implement any complicated deep learning stuff.\n\nThe reason I'm confused if it is possible is because the mentioned surfaces reflect light such that brightness values become the function of the surrounding of the surface because the objects around the surface get reflected off of the surface.\n\nCan it be done?",
    "created_utc": "2024-11-04T11:06:54",
    "num_comments": 7,
    "comments": [
        "I don't know any literature, but I do know a company in Italy called Covision Media that works in part with a neighboring University have been working on something that can do this.  I'm not affiliated with either.  I think for now they use mostly non reflective surfaces but they're trying to expand into reflective.  I know another company in Germany was trying this as well with a smaller system.  I think it mostly involves shifting where the lighting is during the photograph to ensure proper brightness while capturing it, without the object reflecting the light into the camera sensor.\n\nI do know there was also a simulator that used differential calculations of light, that would allow you to reconstruct a 3D scene from a photo of a mirrored object.  I don't know if that would be useful, but I think some of the same principles applied in that could be applied here.  Otherwise, as you wished, I don't think there's deep learning involved.  I think it's mostly using estimation of pde's.  Sorry I can't link to any specific literature, this is a very niche thing and I only know about some of the stuff through second hand sources.",
        "You might be able to do something through BRDF estimation and correction, perhaps combined with multispectral (like IR), multi-spatial (like stereoscopic to depth), or time-variate (like video sequence stitched to mosaic - see Microsoft Research Video Mosaic). The problem with mirroed surfaces… they’re much harder than a highly specular surface.\n\nedit: Apparently some of the newer relighting research might be valuable to read into. This one is interesting. https://guangyancai.github.io/pbir-nie/",
        "I appreciate if you can throw some literature terms or models e.g surface models etc. so that I can elaborate more on that by myself",
        "I'm not aware of a method to do this. Typically, surface reconstruction techniques rely on finding the same point in multiple images and finding the intersection of the camera rays to find the 3d position of that point on the surface. If you start adding in the possibility of one or both of those rays bouncing off a surface then I think that the problem becomes under constrained at best. But it is entirely possible that I'm wrong or have missed something",
        "You are looking for \"deflectometry\". The basic principle is to record the reflection of a known pattern (typically from a screen) on the surface of the object you want to measure. From the observed distortions it is then possible to reconstruct the 3D geometry of the object.",
        "There is this photometric stereo where you vary your light setup and do shape from shading problem. Maybe it is better to take a photograph woth flash light at night so that trees don’t cast shadow on the surface maybe??",
        "Thanks!! Any good resource on this you know of by any chance?"
    ]
},
{
    "submission_id": "1gjl2iq",
    "title": "Ideal technique to do image similarity scoring on single line segment drawings?",
    "selftext": "Hello, all,\n\nI am currently struggling to find a method to do an image similarity comparison on a hand drawn tactile map versus the perfect template of the tactile map. Both of the images are composed of single line segment. The line quality in the template is perfectly straight segments while the hand drawing has many different aspects to it due to human nature of drawing and drawing on a tablet where there are a lot of curvilinear segments and sharp angle turns. The goal is to provide a similarity score say from 0 to 10 based on how similar the hand drawing is versus the template.\n\nI have tried fine-tuning the Gemini-1.5-flash-001 model with 963 examples of human rated scoring on different sets of comparisons but the outputs are not reasonable still for a handful of drawings. With the same set of training examples, I tried to train the VGG-16 CNN model but the similar issue occurred where drawings were still given a higher score than what a human would rate it and lower score when it should be higher.\n\nAnother method that I have tried to detect the line segments through Matlab are the following steps:  \n1. Converting both images to grayscale.  \n2. Applied Gaussian smoothing to both.  \n3. Applied 'Canny' edge detection to both.  \n4. Used bwmorph() to thin the edges so line quality is consistent on all comparisons.  \n5. Used hough(), houghpeaks(), and houghlines() to detect the line segments.\n\nThe Matlab method I tried is having trouble detecting the line segments on the hand drawings due to the different angle changes in the pixels. I provided an example of the hough output in image 3 below. I'll also provide a hand drawn tactile map and it's template for your reference. I would greatly appreciate any comments or ideas to help bring this image similarity comparison to fruition!\n\n[Hand drawn tactile map](https://preview.redd.it/jzxgpoh6cxyd1.png?width=1728&format=png&auto=webp&s=4a1e903f849a5603864ea854a4ddbf8eca5ad0eb)\n\n  \n\n\n[Template of tactile map](https://preview.redd.it/ul3fq4ojcxyd1.png?width=1728&format=png&auto=webp&s=41f25b355c4453ab2fe3c4fc6a9bf034756e1886)\n\n  \n\n\n[Matlab hough\\(\\) output](https://preview.redd.it/dkluy6t2exyd1.png?width=2202&format=png&auto=webp&s=09b120d4551e089d4a9af22f639969ba08105e18)\n\n",
    "created_utc": "2024-11-04T10:15:26",
    "num_comments": 12,
    "comments": [
        "Can you find some key points and reconstruct the lines then template match that",
        "I understand what you're trying to do, but don't understand why you're trying to do it.  There's an easy solution for doing this, but it's very case by case, it's not really generalized.  If you calculate intersections, you can create a transformation matrix.  Then the similarity can just be a measure built off of that transformation matrix.  However, you can't really expand and it will have the expectation that all the points are present in the image, which is not really good for generalization.  If you had a real world example, or explained why you want to do this, I could (maybe) give you ideas on how to do it in a more generalized way.",
        "Is this helpful at all?\n\n[https://github.com/Image-Py/sknw](https://github.com/Image-Py/sknw)\n\nMy thinking was that a spatial map is somewhat like a graph, and the library linked extracts a graph from a skeletonized image (which you essentially already have as your starting place). Given two graphs, there are graph based methods to compare the similarity.\n\nI also came across this method the other day for image structure similarity, and there are probably more options, but this one is just a Matlab function, which seems to be where you are already working. [https://www.mathworks.com/help/images/ref/ssim.html](https://www.mathworks.com/help/images/ref/ssim.html) I mention it in case it is helpful/inspiring, although I don't think it solves your problem.",
        "Hmm thats quite tough\n\nI would be tempted to try a histogram of orientated gradients as a metric, after quantizing the angles of lines from your Hough transform. Then second stage filtering do HoG but with quadrants so theres more structure",
        "Hi, u/kevinwoodrobotics , do you mean label/box the key intersection points?",
        "Hello, u/hellobutno , this is apart of a research study from behavioral testing to look at improvements on mental mapping before and after a training session. Your help would definitely help advance a new rehab training for the visually impaired! We want to see if there is improvement in spatial cognition/analyses in the visually impaired from the training (there definitely is looking at the drawings before and after training, we just need a process on quantifying the improvement!).\n\nAnd yes, we would definitely need to be able to generalize this analyses since a majority of the hand drawings in the pre-training testing are incomplete or have completely different map structures.\n\nLooking forward to your input!",
        "Hey, u/hopticalallusions , thanks for your suggestion, I will take a look at and try it. I think the biggest hurdle is that the hand drawings aren't completely straight line segments. May have to try to find a method to smooth smooth out the pixels to become as straight as possible while taking the midpoint between the edges of a line segment.",
        "Hi, u/leeliop , many thanks for your suggestion. I will try this as well!",
        "Yes something like that",
        "I'm not sure, but does google have a metric for how well their 2D, like non satellite map, aligns with the satellite imagery?  If they do that might be something you could use. They probably do, at least internally, but idk if they've ever published it. Obviously the issue here is the varying scales of things.  But you could probably run it across multiple rescales and see which one produces the least error across keypoints (probably intersections in this regard).\n\n  \nThere's also probably a non CV solution to this too, but I'm not personally aware of any.",
        "Would be interesting if you found a solution as is quite a novel challenge!",
        "Got it, I'll keep that in mind if deciding to try training a model again. I am thinking it might be best to use Matlab since the hand drawings can have unlimited possibilities! Thanks for your input."
    ]
},
{
    "submission_id": "1gjippa",
    "title": "Jetson nano accident detection ",
    "selftext": "Hi,\nAfter a lot of tries I was successfully able to run my yolo accident detection model on my jetson nano\n\nNow I want to connect a servo to it which should go high when accident is detected.\nI realised it's not as simple as it is on Arduino \nSo is there a way to do it without 6 channel i2c ? I have heard that I can connect Arduino to my jetson and control my servo using Arduino? How to do it ?\n\nAlso I'd like to have more suggestions for this project what else can I include to make it better?",
    "created_utc": "2024-11-04T08:41:04",
    "num_comments": 2,
    "comments": [
        "- I would expect Jetson nano gpio port,  as raspberry pi's should be able to output pwm. Search for tutorials  examples, libraries?\n\nOtherwise:\n\n- Arduino connected by serial or usb, yes you have to write some extra code\n -esp32 (same plus option for wireless via WiFi)\n- pca9685 via I2C on gpio port.",
        "It's sorted for now I was able to connect them and use servo pretty easily, now I have to use a server to show some output via nodemcu"
    ]
},
{
    "submission_id": "1gji68l",
    "title": "good OCR without autocorrect feature",
    "selftext": "Is there any good ocr that can read complex handwriting properly (like textract or gpt) but not autocorrect the incorrectly spelled words (unlike textract or gpt) ?",
    "created_utc": "2024-11-04T08:18:49",
    "num_comments": 1,
    "comments": [
        "use Microsoft trocr which is available in hugging face"
    ]
},
{
    "submission_id": "1gjheo7",
    "title": "ZDS camera (or simular)",
    "selftext": "Do you have any experience with ZDS /ZED cameras\nI have a problem with installing driver to my orange pi 5\nIt seams that it has some problems with install.sh , it will return restart the device (as always) but the Open_Cv_Demo.py won't work (saying dvp.pyd should be in the same directory/same directory of environment) there is no dvp.pyd  file in the driver package that provided to me, and also, there is no compatible dvp.pyd file in the GitHub (as I checked so far)\nDo you have any suggestions?",
    "created_utc": "2024-11-04T07:47:59",
    "num_comments": 2,
    "comments": [
        "Don't their camera need cuda so a pi would not work?",
        "We had a version of pioh (Ubuntu 18.04 I think) and they work on them, but when I use Ubuntu from Joshua riek, it won't install."
    ]
},
{
    "submission_id": "1gjff2w",
    "title": "Retail shelf project",
    "selftext": "Hello, \nI'm currently doing a project to help a family member who has stores. They basically have some problems having their employees to restock the shelves. So I've been playing around with yolov11 and doing some training of the products in the shelves and other taking photos of the standalone product. The question I have is, which should be the best approach to have this? Should I take photos of the shelves and train my model to detect the objects in there or should I pre train my model with the SKU1000 dataset and then do something to extract the image of the object that was identified and compare it to the standalone images? There are about 4k SKUs which are groceries, drinks, pharmacy items, etc. I've been thinking that when trying to detect medicines which have just white boxes with different names it would be better the approach of the extraction of the image and then compare it or try to extract the name too. (Is there anyone who has seen a tutorial for this?) \nThanks in advance\n",
    "created_utc": "2024-11-04T06:23:37",
    "num_comments": 4,
    "comments": [
        "I would think it’s easier to extract the names since you’ll rely less on the training and you can compare the text",
        "If the placement of groceries are somewhat static and the camera is static, I would train a general item detector first and use RoI’s for classification. Check out Robflows new feature for annotating many similar items fast.",
        "Complexity exists. Try one shot or zero shot on each skull and keep working on the model improvements incrementally.  Combination of product features and ocr would only help in long run considering the amount of skus tounhave identified.",
        "Yeah, but thats just with the medicine, there are other snacks that have only images on their presentation."
    ]
},
{
    "submission_id": "1gje4jz",
    "title": "Rain detection using pure image processing",
    "selftext": "I am doing a project where one of the modules is detecting whether an image is rainy or not. Is there a way in a research paper or something on how to do this using traditional image processing? Most of the stuff is deep learning and I just want to use pure image processing.",
    "created_utc": "2024-11-04T05:24:13",
    "num_comments": 9,
    "comments": [
        "Better ask Tesla, they’ve spent 10 years trying to get a rain detector working.",
        "[removed]",
        "With just pure image processing it's not that trivial.\n\nYou could start by looking at the Y component in YUV mode. Rainy days tend to be darker, meaning the Y component will trend towards lower values.\n\nIf you have access to camera audio, you could look for the  periodic noise of droplets hitting the camera mic.\n\nAnything beyond that depends on what kind of camera you have. If your camera can capture individual rain droplets as they're falling without leaving trails, you could look into a graph-based approach where you look for nodes resembling rain droplets, and then check the neighborhood for similar droplets. If there's more than 1000 droplets total and they're all evenly spread, chances are it's a rainy scene.\n\nIf it's a video dataset, you could also try something with mask-based motion estimation (which is basically shitty motion estimation). Take an individual frame, treat it as a mask and then subtract it from the next frame. This way static elements (like parked cars, trees, etc) will disappear from the frame and it might be easier to check for rain drops that are moving.\n\nJust be aware that you shouldn't expect anything close to state of the art results using these traditional/mathematical methods. Traditional image processing fails to generalize across different settings, and there's a reason why the field as a whole pivoted towards deep learning.",
        "Try MOG2",
        "Open or closed domain?\n\nThe rain in an image requires high level features to be detected, in an open domain I don't think you can get anything better than a random classifier if you don't use deep learning or at least a learnable approach (like SVM).\n\nIn a closed domain you may exploit histograms and the datetime but you anyway need a learnable approach because I don't think that a manually crafted algorithm is gonna do any better.",
        "It depends on the optics of your imager - in this example I did a long *long* time ago the rain drops were blurry, so I essentially did a DoG. A blur filter has bigger impact on sharp parts than already blurred parts, so where the blur filter is less effective I flag as the rain drop\n\n[https://youtu.be/v2rpgAcFeAM?si=jvkAggqTHxO0MNf2](https://youtu.be/v2rpgAcFeAM?si=jvkAggqTHxO0MNf2)",
        "As someone who drives a Tesla, I can tell you that the world's best machine learning engineers were able to figure out how to make a car drive itself, but not detect rain reliably.",
        "I need to know if the scene is rainy. I don't think I have control over any element since the user can upload any image they want and the algorithm should tell the user if the image is rainy. If the image is rainy, there is a rain streak removal module (but I have already read on methods to do rain removal, my problem is with rain detection)."
    ]
},
{
    "submission_id": "1gjcxz0",
    "title": "6 DOF Object Pose Estimation",
    "selftext": "HI, I have some knowledge and experience working on traditional multiview-geometry methods based object pose estimation. However, it seems deep learning techniques are gaining much more traction in the academia regarding this field. I wonder how much of this research is reflected in the industry (e.g. robotics, automation).\n\nWhat are the current (recent) deep 6 DOF pose estimation techniques (i.e. networks) employed in the industry?",
    "created_utc": "2024-11-04T04:23:17",
    "num_comments": 7,
    "comments": [
        "Most DL techniques in 6DOF are used to detect keypoints which later use PnP",
        "[paperswithcode.com](http://paperswithcode.com) my friend.",
        "[https://bop.felk.cvut.cz/home/](https://bop.felk.cvut.cz/home/)\n\nFor benchmarks. Please let me know if you want to collaborate. I'm interested in learning more about 6DOF pose estimation too. I think the fundamental difference is in MVG, you estimate the camera's pose and in the above case you infer the relative pose of each object in the fov.",
        "Freeze is gaining popularity and beats foundation pose in some cases \n\n3D Object Detection (6D Pose Estimation) without Training using FreeZe\nhttps://youtu.be/Mgmt93kXK_4",
        "I don't get it.. does this task use markers at all?",
        "foundationpose doesn't [https://nvlabs.github.io/FoundationPose/](https://nvlabs.github.io/FoundationPose/)",
        "I don't get it.. does this task use markers at all?"
    ]
},
{
    "submission_id": "1gj8z7k",
    "title": "How long is OmniParser taking for you to process an image?",
    "selftext": "For example, processing a 2880px × 1800px UI screenshot takes 2+ mins on my Mac.\n\nLet me know your image dimensions / ui density as reference as well",
    "created_utc": "2024-11-03T23:39:02",
    "num_comments": 0,
    "comments": []
},
{
    "submission_id": "1gj4a2s",
    "title": "Combining algorithms in an autonomous driving project",
    "selftext": "I am planning to do a project consisting of an autonomous driving system.\n\n\n\nI was thinking of using reinforcement learning but it would take too long to train (months), with the consequent expenditure of electricity and money (specialized servers).\n\n\n\nAfter seeing some videos from Sentdex and others where, after training for 2 months in a row, the driver manages to drive like a drunk person, I have considered it unfeasible and I have thought:\n\n\n\nWould it be possible to combine a deep learning algorithm with reinforcement learning together with a traditional computer vision algorithm like lane finding?\n\n\n\nIs there any way to make these algorithms work together, reducing the training time?\n\n\n\nWould you use other algorithms or approaches?\n\nI'm using CARLA Simulator.\n\nThanks.",
    "created_utc": "2024-11-03T18:40:08",
    "num_comments": 2,
    "comments": [
        "1. You need to set boundary conditions.  You can't just say \"I want to develop an autonomous driving project\".  What are you driving (a car, a train on tracks, a plane, an rc car)?  What conditions do you want to be able to drive in (stay within lanes, drive to a goal it can see in the distance)?\n2. RL policies can take a long time to train, but they can also take not that long.  If I created a 2D simulator asking a particle to reach a certain zone of the screen with no obstacles, you could train that policy in seconds, maybe minutes at most.  If you are trying to train an ATV to traverse a planet and collect samples, yeah that's going to take a long time.\n3. There's a reason why in practice no one uses deep learning and reinforcement learning exclusively.  You have to use sensor data.  No one is trying to build the first autonomous car with just a single camera, because no one will buy, it's not safe, it's not accurate (nor will it ever be) and it's not practical.  You need to use a mixture of sensor data and deep learning.  Deep learning maybe to detect the objects in scene, predict how far away they are, and then maybe to assist in path planning.  You would not let any neural network have full control of the path, you need to be able to override them with sensor data.",
        "Thanks for your answer.\n\n\n\n1. Autonomous driving system for a simple car. The objective will be to direct the car to a specific point on the map (actually with GPS coordinates).\n\n\n\n2. I do not intend to nor can I make the new Tesla or Comma system, I simply want it to work reasonably well in certain conditions such as:\n\n\n\n\\- Staying in the lane.\n\n\\- Maintaining an adequate speed.\n\n\\- Braking at STOP signs and traffic lights (recognizing them).\n\n\\- Overtaking.\n\n\n\n3. I understand that no one uses 100% reinforcement and deep learning, that is why I am wondering what would be the way to combine it with traditional algorithms and make a hybrid because I cannot find references on the internet."
    ]
},
{
    "submission_id": "1gj23vv",
    "title": "Help with current ANPR technology (license plate recognition)",
    "selftext": "Hi! I am trying to find an efficient open source program that I can run on a CPU (GPU I'd have to run in the cloud I guess)\n\nI would like to pass it an image (URL), and have it return the license plate text (UK numberplate).\n\nI found some options but they are all three or four years old. Modern options seem to be extracting plates from video.\n\nI just want to run a program locally and send it still image URLs via an api. Any advice  much appreciated!",
    "created_utc": "2024-11-03T16:47:39",
    "num_comments": 10,
    "comments": [
        "Hi , i worked on this topic years ago during internship the code originally just developed do not relay on any AI models and it takes videos sequence to process them on cpu I successfully tested on cpu ( intel i5 12 gen and Raspberry pi cpu on a raspy  it get about 425 seconds if was in python and a not very optimized) after a while a contributor added yolo model and retested i really do not know if it still runs on embedded raspberry cpu , check it here https://github.com/wissem01chiha/ALPR \nIf you want fork the project and continue to work i d like if you tested on cpu to notify me or just open an issue i will correct it",
        "let me google that for you:  \nhttps://github.com/topics/license-plate-recognition\n\nI think some relevant questions should be\n- do you have hardware and/or language constraints (embedded, GPU, C++/Python?)\n- does your project require commercial licensing?\n- does it have to be easy to setup and maintain (maybe you're not a CV professional?)\n- OR does it have to be reliable and stable for deployment e.g. like for a automated parking garage?\n- are the no foreign cars on UK roads?\n- how do you handle ambiguities, like low confidence, or cars with no visible number plate, or number plates w/o cars (-> fraud)?",
        "There is deepsparse with some yolo variants already compiled i believe. Maybe start there 🤔",
        "Thanks.\n\n\nIdeally cpu (embedded?) and python.\n\n\nNot commercial.\n\n\nEasy to setup, it's a personal project. Nobody dies if it doesn't work.\n\n\nNo foreign cars matter in this project, just UK plates.\n\n\nAmbiguity or low confidence... I will just throw those out. \n\n\nIf there is a big clear numberplate in the image, great. If not, that's okay, I don't need to guess. ",
        "Thanks !",
        "e.g. this project uses Roboflow to train a YOLO8 detector, then runs paddlepaddle OCR on the detection. I didn't test it but it appears reasonably straightforward to me. Roboflow is great for labeling and training detector datasets and you can actually both train (using Jupyter) and run it (using 'inference' package) locally without requiring paid server time. PaddlePaddle is not a preferred framework for me but its OCR package is very strong.\n\nhttps://github.com/ablanco1950/LicensePlate_Yolov8_Filters_PaddleOCR",
        "this one worked so well thank you !!!! my first real project and you led me in the right direction.",
        "Thank you I will look !",
        ":)"
    ]
},
{
    "submission_id": "1gj1nb4",
    "title": "Open-Source (MIT/ APACHE) Model for real-time Object Detection on Mobile Device?",
    "selftext": "Unfortunately Yolo model is not usable for commercial context. Is there an proper alternative?\n\nI am thinking about Tensorflow Lite in combination with Mobilenet SSD. What do you think?",
    "created_utc": "2024-11-03T16:25:00",
    "num_comments": 1,
    "comments": []
},
{
    "submission_id": "1gizbs3",
    "title": "Understanding Radiance in Machine Vision",
    "selftext": "\n\nI’m currently exploring the concept of radiance in the context of machine vision, and I’m finding it a bit challenging to grasp. From my understanding, radiance is a measure of the light energy traveling through a specific point in a specific direction, but there seem to be quite a few layers to it, especially when we start considering factors like surface interactions and scene illumination.\n\nHere’s what I’m trying to figure out:\n\n\t1.\tWhy does radiance differ from similar concepts, like irradiance and intensity? I often see these terms used together, and while they seem related, I want to be clear on how each one functions. \n\nFor example, I know intensity involves solid angle. Also I know solid angle involves the notion of area. Then why do we need to define radiance with dA even though intensity already incorporates the notion of area?\n\nAny help breaking down these ideas or pointing me toward resources would be much appreciated. Thanks in advance!\n",
    "created_utc": "2024-11-03T14:37:15",
    "num_comments": 2,
    "comments": [
        "Detailed definitions are available, e.g., on [Wikipedia.](https://en.wikipedia.org/wiki/Radiometry?wprov=sfti1) These terms differ because they are different quantities. For a typical use in computer vision, you don’t need intensity. In lay terms, irradiance is the power per area at the surface of an object normal to the light source’s direction. If you multiply that by the cosine of the angle between the object’s local surface normal and the incoming light, you get power per area on the surface. This, multiplied by a surface reflective property called the bidirectional reflectance distribution function (BRDF) gives the surface radiance toward some other direction (the camera). Radiance is power per unit solid angle towards the camera, per unit of object projected surface area.  The camera lens converts this radiance to irradiance (see above) at the focal plane, which determines the image “brightness.” Radiance is a particularly useful quantity because, atmosphere notwithstanding, it doesn’t change with viewing distance.\n\nIt all is more complicated of course. For example, each of these terms can be spectral or averaged over a wavelength range. Generally this falls under an area called radiometry. If you stick to spectral ranges involving the human eye, there is a parallel universe called photometry.",
        "Following."
    ]
},
{
    "submission_id": "1giz3ou",
    "title": "Resume Review Request: [2YOE] in VIO-SLAM. Looking for L4 computer vision roles. ",
    "selftext": "Hi, I'm looking for a Computer Vision role (doesn't have to be in SLAM). My goal is to make my resume compelling enough that a hiring manager would be interested in a follow-up chat.   \nAny feedback is appreciated.\n\n\n\nhttps://preview.redd.it/cfrpyuiqiryd1.png?width=505&format=png&auto=webp&s=a273a5ac44303db67789e00d359723203a7d7c70\n\n",
    "created_utc": "2024-11-03T14:27:01",
    "num_comments": 6,
    "comments": [
        "Boosted team productivity by 3x with automated Slam testing sounds like snake oil to me. The entire team was completing development work 3x faster?!",
        "i think that’s pretty decent honestly, concise & results-focused",
        "you raise a good point! I built a tool that allowed me and a teammate to use automated testing more effectively. We were working on SLAM features, and without this tool, we had to test data sequences one by one. Ideally, we wanted to run tests across entire database of video data to get a clearer view of how changes performed in real scenarios. The tooling for this existed, but it was broken due to non-deterministic responses from a cloud service we used. I solved this by creating a mock network that provided deterministic results, allowing us to fully utilize the testing setup as intended.\n\nIn retrospect you are right. It's not 3x, but maybe something like 30%. (a 10x reduction haha)",
        "thanks for reading I appreciate it",
        "that’s still a great story to tell though and a good point for your resume, just fix the math a bit"
    ]
},
{
    "submission_id": "1git52o",
    "title": "120 Dog Breeds, more than 10,000 Images: Deep Learning Tutorial for dogs classification 🐕‍🦺[project]",
    "selftext": "https://preview.redd.it/l5rxtk9v8qyd1.jpg?width=1280&format=pjpg&auto=webp&s=66363bb343a530a63832f7530697cbb029731c79\n\n📽️ In our latest video tutorial, we will create a dog breed recognition model using the NasLarge pre-trained model 🚀 and a massive dataset featuring over 10,000 images of 120 unique dog breeds 📸.\n\n**What You'll Learn:**\n\n🔹 Data Preparation: We'll begin by downloading a dataset of of more than 20K Dogs images, neatly categorized into 120 classes. You'll learn how to load and preprocess the data using Python, OpenCV, and Numpy, ensuring it's perfectly ready for training.\n\n🔹 CNN Architecture and the NAS model : We will use the Nas Large model , and customize it to our own needs.\n\n🔹 Model Training: Harness the power of Tensorflow and Keras to define and train our custom CNN model based on Nas Large model . We'll configure the loss function, optimizer, and evaluation metrics to achieve optimal performance during training.\n\n🔹 Predicting New Images: Watch as we put our pre-trained model to the test! We'll showcase how to use the model to make predictions on fresh, unseen dinosaur images, and witness the magic of AI in action.\n\n \n\nCheck out our tutorial here : [https://youtu.be/vH1UVKwIhLo](https://youtu.be/vH1UVKwIhLo)&list=UULFTiWJJhaH6BviSWKLJUM9sg\n\nYou can find full code here : [https://medium.com/p/b0008357e39c](https://medium.com/p/b0008357e39c) \n\nYou can find more tutorials, and join my newsletter here : [https://eranfeit.net/](https://eranfeit.net/)\n\n\n\nEnjoy\n\nEran",
    "created_utc": "2024-11-03T10:09:33",
    "num_comments": 1,
    "comments": []
},
{
    "submission_id": "1giss3n",
    "title": "Claude Computer Use - Mouse Action Model",
    "selftext": "Does anybody have any insight / guesses on how the model which decides which screen element to interact with was trained / done? I need this for a project.\n\nThe announcement blog post says:\n\n>Instead of making specific tools to help Claude complete individual tasks, we're teaching it *general* computer skills—allowing it to use a wide range of standard tools and software programs designed for people\n\nThe blog on developing the model post states:\n\n>\"When a developer tasks Claude with using a piece of computer software and gives it the necessary access, Claude looks at screenshots of what’s visible to the user, then counts how many pixels vertically or horizontally it needs to move a cursor in order to click in the correct place. Training Claude to count pixels accurately was critical. Without this skill, the model finds it difficult to give mouse commands—similar to how models often struggle with simple-seeming questions like “how many A’s in the word ‘banana?’”\n\nHow does the model count pixels needed to move the cursor and how was this trained? Any ideas?  \nSame architecture with action training data? different architecture? what does the training data look like if any?",
    "created_utc": "2024-11-03T09:53:57",
    "num_comments": 4,
    "comments": [
        "I currently know approaches where a grid is placed over the screen with numbers for each grid. But these are pretty imprecise.",
        "the grid already exists - it's the set of pixels 😅",
        "Here: [Self Operating Computer](https://github.com/OthersideAI/self-operating-computer/blob/9459e9f395dc59cf6536244ccf47be26d849cbc2/operate/main.py#L426)\n\nThey are overlaying an grid on an Screenshot and prompt GPT4-V (old model) with this and hope to get the correct next action.",
        "super interesting, will give it a go. thanks for the link! 🙏"
    ]
},
{
    "submission_id": "1gimqwq",
    "title": "3D Object Detection (6D Pose Estimation) without Training using FreeZe",
    "selftext": "I will go over 3D object detection (or 6D pose estimation) without any training using Freeze from the paper Training-Free Zero-Shot 6D Pose Estimation with Geometric and Vision Foundation Models. Currently ranked 3 for 6D detection of unseen objects! \n",
    "created_utc": "2024-11-03T05:23:45",
    "num_comments": 0,
    "comments": []
},
{
    "submission_id": "1gimnek",
    "title": "Master student researching CV, what is your research schedule?",
    "selftext": "Hi, I was wondering how do you maintain work life balance. In short, due to circumstances, I tried to research from 9AM-10PM every day, including weekends. I eat at 11AM and 5PM. Both require 2 hours if I am socializing. If I don't, I spend 15 minutes to eat and 15 minutes to commute between library and the canteen.\n\nI am thinking to go reschedule my time, 5PM to go to the gym for 1 hour + 15 minutes to communicate between the library, canteen and the gym + 30 minutes to take shower\n\nFor context:\n\n1. I am the only 2024 international student in this lab. There are 5 2024 local students\n2. I cannot speak the local language, they can read and write English but cannot listen or speak English. So, there is a language barrier\n3. My professor can't speak English either and refused to discuss with me\n4. This part, I am not sure whether my supervisor tried to make me resign or what. But, he demanded that I reviewed 80 papers within 1 - 2 weeks. Meanwhile, I saw that local students (1 submit literature classification summary from 16 papers and 1 submit from 14 papers)\n5. I am worried with my health. So far, so good, I don't have issue sitting all day. But, back when I was working, I used to pull 9AM-3AM every day for 3 months straight. Once, my chest hurt very badly. The doctor said it was because of anxiety, and yes, it was a deployment day (that one was unusual, we did a big refactor for a month or so, including the repository, so I have no idea whether this would work smoothly like in staging or not in production), so I was anxious that something may went wrong.",
    "created_utc": "2024-11-03T05:18:43",
    "num_comments": 19,
    "comments": [
        "Which country are you in? Your schedule sounds batshit insane to me. I'm in the Netherlands and right now I'm doing masters thesis in CV and I'm working from 9am to 5pm, 5 days a week. Both my uni supervisors and my supervisors at the company are super happy with my results.\n\nWhen doing literature review, I went to Google Scholar, added anything that sounds vaguely relevant to my research to Zotero, skimmed abstracts in Zotero to filter out even more, and then read three papers (and their code) in two werks in detail. 80 papers in 1-2 weeks can only yield bad results\n\nI don't know what kind of university you go to but when I joined/started my uni, I just had normal courses at first and only now at the end of my studies am I doing \"research\" - no one expects me to publish a paper at this stage - I'm just a master student, I have no clue how to conduct proper publishable research",
        "Idk where you are, but this doesn't sound right at all.  I spent maybe about 10% more time doing work in grad school than I did undergrad.",
        "30 minutes in the shower? Dude, what do you do in the shower?",
        "Unfortunately, time in university in China changed completely for foreigners. After Covid they became much more “angry” towards foreigners. I know best students, who got completely f@cked by their master professors, even though they were best out of all foreigners. They don’t really realise that it’s really hard to study in China for foreigners.\nBest advice, would be to communicate with your professor as much as possible. But don’t really annoy him, so don’t cross the red zone. Be in lab as much as you can, show some initiative. Try to speak with older people from higher course of master or phd. They would might help you with something. Also try to communicate with your classmates. Invite them for 吃饭, try to be mates. \nOn the other hand, you professor just would continue act like a d!ck and unfortunately, there is nothing much you can change. \nAt the end, if you starting to get mental health problems, leave. It doesn’t cost that. \nWish you good luck mate, I know it’s tough.",
        "I am in China but I am an international student\n\nFrom what I heard and saw, the university statistics is improving dramatically YoE (the CS school is one of the main contributor), and my research lab I am in have lots papers in SCI Zone 1 journals\n\nBut, my professor said that 2 years ago, the school criticized many papers that were submitted for blind reviews (or something else, I forgot). Also, apparently our papers starting from this year will be sent for blind reviews only to better university such as Tsinghua University. In other words, if our papers are bad, we will be screwed\n\nNow, according to my professor, the school set up new rules that we are only allowed to publish to CCF category A conference and Zone 1-2 (when I joined, my professor require me to publish in SCI Zone 3 journals, then he changed it to 3 papers in SCI Zone 3 journals but now either 2 CCF A conference or 4 papers in SCI Zone 1-2 journals)\n\nWe have 3 years (maximum) but I am doubting my capability to publish 4 papers within 2.5 years (normal)\n\nMaybe that’s why my professor acted like that. Honestly, I am still confused whether he really wants to expel me or not. The first day he said that my head is blank all day, it was a bad experience\n\nThe good news is that I can read (understand what is the proposed methods and how it was calculated) faster now. But, reading 80 papers within 1-2 weeks is impossible for me, especially during weekdays where I have classes\n\nBtw thanks for the insights. I will try to look up Zotero. Hopefully when my professor is no longer mad with my progress, I can switch to read 3 papers including the code per 2 week so I can have better understanding on every part of the paper. And have my social life back",
        "Yeah I felt this is wrong. But I am under threat of being expelled because I am too slow. I have resigned from my job to be here, lost lots of potential money in the process and going back is too shameful for me. So I need to satisfy my professor\n\nFor context, I joined this university on September 2024, now is November 2024. My professor demanded that I finished the 1st paper before March 2025",
        "reflecting my life haha",
        "Super tough. So, I wrote a Literature Classification Summary and translate it to the local language. Long story short, he basically demanded me to review 80 papers in 1-2 weeks. I failed at that.\n\nThese are what frustrate me the most:\n\n1. The fact that I forgot part of what I wrote as I was translating it to the local language.\n2. Time spent to translate it into the local language. I spent exactly 5 hours 30 minutes to translate it from English to the local language.\n3. The fact that I cannot tell Teacher that his method actually stressed me out. (PS: I genuinely afraid to say other than \"OK“，”Done“. Every time I asked questions, he either threatened to expel me or he doubled my review tasks from 40 to 80)\n4. The fact that the local students reviewed 14 and 16 papers instead of 80 (the local students are really helpful, they shared their Literature Classification Summary; I am too afraid to ask for help from the local students because as of right I am really tired and I am afraid that I look like a burden for them)\n\n\\> Be in lab as much as you can, show some initiative\n\n1. I tried to show some initiative, but he threatened to expel me for bothering him\n\n\"There is a novel attack method, say X, but it does not incorporate the old existing attack method advantages, say Y. So, if this is were to be incorporated, my hypothesis is that a statistical-based method or a large-scale manual labor will not be able to correct this attack method\"\n\n2. I don't have a seat in the lab.\n\nAnyway, my friend said that I can ask my professor for a seat at different room. But, I am too afraid to ask him, like \"will he demand more things from me?\" if I request this from him\n\nHonestly, I wish I can focus on the courses in the 1st semester (he instructed me to finish all courses in 1 semester, so I did. Funnily, the local students don't try to finish their courses in 1 semester). Then, in the 2nd semester I will start researching full time (I will ask my professor for a seat at a different room. For your information, according to the senior, there is no seat for international student) while studying the local language.\n\nNow? I don't even have time to socialize let alone study the language.\n\n\\> At the end, if you starting to get mental health problems, leave. It doesn’t cost that.\n\nI have to leave 3 jobs to be here: 1) SWE at a Malaysian company, 2) a Computer Vision solution for a manufacturing company, 3) our own startup, we was just getting started and was featured in Tech in Asia\n\nWhat I have left behind is too big to return to. I am too ashamed to return back too.\n\nI will persevere, as long as my professor don't expel me, I will stay. I want to switch career from SWE to Computer Vision Engineer.\n\nPS: I am grateful to be here, and I know it's only thanks to my professor's acceptance letter. Without it, I will not be here. For your information, my formal education background is not CS, so it was very unlikely for me to be accepted to a top public university, and a research university, on top of that.",
        "Oof, China... I have no advice but wish you good luck and all the best!! \n\nI have a few Chinese peers that specifically chose not to do their masters in China because of exactly what you are experiencing: The intense workload and expectations.\n\nI really hope your professor will ease up on you! Maybe you can find other students in your program that feel similar to you?",
        "Leave.  Unless you're in Tsinghua University, nothing you're doing there in a masters program will really matter to anyone.  There's a reason most students in China don't even go to school in China.",
        "May I know where is your Chinese peers took their Master?\n\nI found a few international students that feel similar. One thing that I noticed that professors will criticize us harshly in private and show appreciation in public (note: he have never appreciate me in public, the most positive word that come from him that he always say 加油, that’s it. I only saw he appreciate the local students in public)",
        "As if it that’s easy. My education background is not CS. So getting here and actually got full scholarship is already a miracle of its own \n\nAlso, this is my path to switch from SWE to Computer Vision Engineering\n\nLastly, I don’t care whether what I did in a master program will matter to anyone. As long as I can understand how to develop a cutting edge Computer Vision software and then went back to the industry with fair salary, I am contempt with that",
        "1.  You're not in a program to teach you things you need to know for industry, you're in a program that teaches you the things you need to go get your PhD.  From the list of things you are doing, if you came to me applying for a job, I would not hire you, because they're not things that are practical for industry.\n\n2.  You don't need a background in CS to learn CV.  Most people in CV I know came from an engineering background.\n\n3.  You say you don't have a CS background but you're already a SWE.  You have a CS background, stop doubting yourself.\n\n4.  Chinese universities are notorious for this shit, you're going to work yourself until you lose all will to live, then be forced into a PhD where the advisor will hold your thesis over your head until you complete the work he wants you to complete to make him look good, then once your usefullness has run out he might let you do your final defense.  I've seen this a dozen times.",
        "You seem knowledgable, can I consult to you?\n\nA year ago, we made a Computer Vision solution for a manufacturing. In short, it’s a solution to detect defects and automatically handle it. The problem (i.e causing model performance to drop occasionally) was because the products (i.e. there are multiple items within a frame, and within a conveyor belt, there may be different products) sometimes has glare, sometime is not. How (i.e. what are the steps) do people actually solve (i.e. from the software perspective, not hardware) this problem in the industry?",
        "Depends on what is causing the glare.  Is it because uncontrolled sunlight or because the product is reflective?",
        "Yes, there is incoming light from a window. So the light intensity change overtime (and they are open 24/7). Some product is reflective, some don’t.",
        "Ok, then your job, as the supplier, is to supply a product that retrofits and controls that light for the portion of the conveyor that you control.  You're wasting your time trying to come up with a software solution.",
        "That’s true, that’s the market leader solution. I have bad example, can you give how you work as a CV engineer?",
        "It's just basic problem analysis.  You break down the problem as much as you can into all the parts, and find a solution based on the boundary conditions.  If it's something that goes over a time constraint or a budget, then you have to work out with the customer a solution."
    ]
},
{
    "submission_id": "1gimf4z",
    "title": "Issue with extracting embeddings",
    "selftext": "Hi, I'm trying to create a deep sort algorithm that uses yolov8 for object detection. I'm currently having an issue with trying to extract embeddings from the detections using a ReID model. I've tried multiple different models like OSNet, Resnet, and MobileNet. I've also tried to multiple variations on the extract embeddings function but I really can't figure out the issue. The embeddings that I get from the tracks using track.mean seem to be completely fine, but the ones from the ReID model are always really close to zero. \n\nEmbeddings:  \\[\\[ 4.8122e-11  1.4957e-08           0 ...  1.0564e-08  1.5975e-08  9.5248e-11\\]  \n \\[ 4.8122e-11  1.4957e-08           0 ...  1.0564e-08  1.5975e-08  9.5248e-11\\]  \n \\[ 4.8122e-11  1.4957e-08           0 ...  1.0564e-08  1.5975e-08  9.5248e-11\\]  \n ...  \n \\[ 4.8122e-11  1.4957e-08           0 ...  1.0564e-08  1.5975e-08  9.5248e-11\\]  \n \\[ 4.8122e-11  1.4957e-08           0 ...  1.0564e-08  1.5975e-08  9.5248e-11\\]  \n \\[ 4.8122e-11  1.4957e-08           0 ...  1.0564e-08  1.5975e-08  9.5248e-11\\]\\]\n\nExisting Embeddings:  \\[\\[     313.16      274.77     0.49409      296.97\\]  \n \\[     687.22      282.89     0.33855       96.88\\]  \n \\[     133.67      257.95     0.48124      53.225\\]  \n \\[     77.514      317.28     0.33545      107.74\\]  \n \\[     822.24      289.39     0.39093      221.28\\]  \n \\[     919.74      273.09     0.27824      261.11\\]  \n \\[     913.92      269.61     0.31297      94.637\\]  \n \\[     671.78      301.21     0.37767        63.9\\]  \n \\[     701.12      280.48      0.2668      102.46\\]  \n \\[     215.21      289.73     0.37571      128.89\\]\\]\n\n  \nHere is one example of my embeddings and existing embedding(track embeddings).\n\nAny suggestions would be really helpful. I've linked the code if anyone wants to check it out. ",
    "created_utc": "2024-11-03T05:06:46",
    "num_comments": 1,
    "comments": [
        "track.mean isn't the feature vector, track.mean is the kalman filter mean.  the feature embeddings are under track.features."
    ]
},
{
    "submission_id": "1gigp8s",
    "title": "Gstreamer Issue",
    "selftext": "Hi I am writing an inference script that uses gstreamer for an image and video but when i send a video it doesnt seem to work.\n\nThe buffer that sends image is always 64 bytes. The gstreamer script is made using gpt since i havent used it until now.\n\n[https://pastebin.com/hVtZ35Rw](https://pastebin.com/hVtZ35Rw)",
    "created_utc": "2024-11-02T23:19:30",
    "num_comments": 16,
    "comments": [
        "Did you test the pipeline in the terminal?",
        "Yes it gives me the error that “buffer size too small for requested array”\nThe array mentioned is 1280x720x3\nI also printed the buffer size and it is only 64",
        "What's the pipeline you used in terminal? The code.",
        "The pastebin has the code im running but other than that i ran a simple video streaming command it works and i tried the same in a sample code and there is playback, but when i send the buffer from pipeline to create a np array it says buffer size is 64",
        "What I meant by running in the terminal was to run the GStreamer pipeline through the terminal. Like:\n\n`gst-launch-1.0 filesrc location=video.mp4 ! decodebin ! videoconvert ! autovideosink`\n\nto see if it works.",
        "Yes this is the command i ran and im getting playback",
        "You can use the same command with OpenCV with GStreamer backend.",
        "Like directly in the videocapture method? I did that too but it then couldn’t open the video",
        "Did you compile OpenCV with GStreamer?",
        "I did compile it and ran the script with vidtestsrc video as input and the stream ran nicely\nBut when i changed it to video input it still gave me\n“Error couldnt open video source”",
        "Did you try different video files? Maybe try a file without any special characters in the name.",
        "Yes i passed a video with name samp.mp4 but still its the same problem",
        "I guess you could try downloading NVIDIA's DeepStream container and running it inside that. In your code you mention DeepStream, but your pipeline isn't using any DeepStream element.",
        "You can also try uridecodebin instead of filesrc",
        "Its only gstreamer pipeline for input",
        "Tried this too still same issue.\nIf it is possible can you write a working script for reading a mp4 vid and saving 5 frames because I can’t figure out if there is problem with my code or my vm.\nIt always breaks when i try to create np array of the frame from buffer. And buffer size is always 64.\nThanks for your help anyway and sry for asking too much of you."
    ]
},
{
    "submission_id": "1giccpw",
    "title": "Hobbyist vs. Work",
    "selftext": "I’m curious as to how many on this sub are CV hobbyist versus how many of you do this for a living (research or industry). If so what does your work consist of?\n\nDid a lot of CV work at my internship so I’ve gained some interest in the field. Probably gonna do a couple home projects when the semester ends.",
    "created_utc": "2024-11-02T18:54:39",
    "num_comments": 17,
    "comments": [
        "Mostly a hobbyist, trying to do cv for a robotics team im part of. Where did you intern at?",
        "Work, mainly remote sensing. A lot of optimization getting models to run on edge devices, and training data constrained models.",
        "I'm certainly doing it as a hobby. Helps to have an actual problem to work on; in this case video segmentation.\n\nNot that you asked, but I thought I'd share.\n\nI can't speak for everyone but CV seems to have a pretty scary learning curve compared to other disciplines for a couple reasons\n\n1) like any machine learning discipline, it takes time to understand how the architecture and algorithms work. Decision trees and random forests are often second in the chain after ols, but Cnns are a flavor of deep learning which is further down the pike. Furthermore, At least for me, CV architectures are much less familiar than say NLP models, which share the sequence nature with other disciplines of ml.\n\n2) unlike other toy data sets, it takes time to wrap your mind on how images work and how all the preprocessing techniques work in that context. \n\nYou combine both and it's a big reason why I put off learning CV for as long as I did. Happily, because I'm a time series guy, Cnns and all of associated topics like kernals and pooling eventually became part of the curriculum. So it was just a problem of getting around to #2 for me.",
        "Currently doing both. It’s research for new product development",
        "Both. I have loads of random personal projects and also work in this field.",
        "Both. I work in the business department of a big corporation.\n\nI had a great idea for an internal use web application that needed CV to analyze videos some years ago, and starting studying as a hobby.\n\nTurns there was a lot of traditional development to be done before it was feasible to make CV generate value for us.\n\nIn theory nowadays I am the Product Owner of this application. In practice I am also a tech leader, software architect, data engineer and wear many other hats.\n\nWe already have a good classification model for our application but it will only generate value when we make it available to use with a few clicks.",
        "work. i build CV projects for companies as a consultant (more infoz incl types of projects: www.numberboost.com)",
        "Can’t say exactly but it’s a manufacturing company.\n\nYou do some vision stuff at work then too or it’s entirely hobbyist?",
        "Preprocessing kills me. My math background isn’t very good either so it was kind of hard to really understand some of the techniques.\n\nWhat type of projects do you do?",
        "You're mixing up ML and CV.  There's a lot more to CV than what you're doing, and some of those are only loosely related to CV.",
        "Unfortunately haven’t secured a job yet, so I guess entirely hobbyist. Cv/robotics related job would be ideal for me tho",
        "Right now, it's a video segmentation and classification problem for some sports game film\n\nHonestly, it sounds complicated at first, but it's not nearly as bad once you can understand how the CNN part works, what the corresponding kernals and pooling layers are doing.\n\nThe math looks scary to start, but as you gain familiarity, it's not that bad imo. For me, as a time series guy, it was easier to understand Cnns through the sequence nature of things first rather than trying to understand them through the lens of images which I have no formal training or experience working with before either from education or work. The sequence nature of time series made nlp problems easier to approach, which helped me to understand transformers and ultimate vision transformers.\n\nThat's kind of why I said CV is an intimidating field for some. Not only do you need to understand the architecture but also the data itself.",
        "If you could, tell me where i said what I was doing was somehow exhaustively describing CV? I just said it's something related to CV.",
        "I’m in the same boat my friend it’s a rough market.\n\nWhat type of projects do you do?",
        "Your very first point is saying that CV is an ML discipline.  At least read your own post before asking this.",
        "Right now, trying to understand slam. end goal is to detect objects on a field and drive to them while avoiding any objects that may come in my robots way. interesting stuff, but with college don't have a lot of time to focus on it lol.",
        "Same thing here, in school right now so no time to focus on projects. When the semester ends I wanna see if I can get a small object detection model running on my raspberry pi. Have it send data to a microcontroller so that it can spray water at my cat whenever it goes on the table haha."
    ]
},
{
    "submission_id": "1giamg8",
    "title": "Best Computer Vision Books for Beginners to Advanced ",
    "selftext": "",
    "created_utc": "2024-11-02T17:24:14",
    "num_comments": 3,
    "comments": [
        "Why does it give an Amazon link to Szeliski and not his website where he gives it away for free.",
        "Posting the link here - http://szeliski.org/Book/",
        "The whole idea is to make backlinks of their website + traffic to their website + affiliate income.\n\nIt's all about money."
    ]
},
{
    "submission_id": "1gi8xwx",
    "title": "Homemade no-hardware racing setup",
    "selftext": "Hello reddit,\n\n[Setup](https://preview.redd.it/p1yxl1kekkyd1.jpg?width=4000&format=pjpg&auto=webp&s=b60dc086a299d912d24ff6b3d535fc470a87378a)\n\nI want to share a project of mine I think some of you would find interesting.\n\nA couple months ago I wanted to play Forza Horizon 4 not just with a controller, but with a steering wheel. I am travelling a lot and the device is quite expensive, so buying one wasn't the first thing to think about. I am a programmer so I decided to figure that out myself. Developing hardware with microcontrollers and onboard software was the first option but it required a lot of time, dedication and resources I didn't have. Then I thought why not to create a standalone software for my PC that can handle all that alone? I had relevant skills, Chat GPT and curiousity - so I created this. Essentially, it's just a piece of cardboard and a little weight on a string hanging from the top. Also there is a program running on background on my laptop that does all the calculations.\n\n[Video](https://www.youtube.com/watch?v=giQNNACyR_M&ab_channel=Ananaseek)\n\nIt works using computer vision. The program analyzes picture from the laptop front camera every frame, checks for the red spot on the steering wheel, calculates the angle at which it is oriented, and then sends corresponding signals to the virtual controller. The gas pedal works the exact same way - when I pull the string with my foot the marker goes upwards and that is detected by the program, it then calculates exactly how much the pedal is pressed now and sends the signals to the controller. It has to be calibrated before use to adjust the default position of the steering wheel and limits of the gas pedal.\n\nPractically, it is heavily dependend on lighting, camera and other paramaters environment, and even with daylight and clear bright color dots it still lacks responsiveness and accuracy. Theoretically, a bunch of things could be added/improved, such as brake pedal, 360 steering wheel detection, better flexibility.\n\n[Github repo with all code](https://github.com/AnanasikDev/RacingCVController)\n\nWhat do you think about it? Is it worth continuation?",
    "created_utc": "2024-11-02T16:04:03",
    "num_comments": 0,
    "comments": []
},
{
    "submission_id": "1gi52x0",
    "title": "Good computer vision books ",
    "selftext": "what are some good books about computer vision used in intelligent data analytics, decision making and intelligent multiagent systems, I checked Computer Vision Algorithms and Applications by Richard Szeliski its a very good introduction but iam asking if there anything that will get me into more details faster.\n\n",
    "created_utc": "2024-11-02T13:04:04",
    "num_comments": 7,
    "comments": [
        "[~~Computer Vision: Algorithms and Applications, 2nd ed.~~](https://szeliski.org/Book/) ~~by Richard Szeliski. You can get your own version for free as a PDF by following the steps on his webpage.~~\n\nEdit: I literally can't read lol.",
        "* Modern Computer Vision with PyTorch - Second Edition: A practical roadmap from deep learning fundamentals to advanced applications and Generative AI\n* Foundations of Computer Vision (Adaptive Computation and Machine Learning series)\n* Practical Machine Learning for Computer Vision: End-to-End Machine Learning for Image\n* Computer Vision: Algorithms and Applications (Texts in Computer Science) are some of the [best Computer Vision Books](https://codingvidya.com/best-computer-vision-books-for-beginners/)",
        "Hmm I have a few great recommendations on dumb data-analytics but can think of any that are intelligent.",
        "That’s a ton to cover and it has changed a bunch the last couple years. My best advice is look for papers and keep up with the CV fundamentals like you have",
        "I mean you're allowed to post that, just not a direct link to the pdf.  Anyway, I personally recommend the 1st edition over the 2nd.  2nd has too much deep learning focus and I feel like it retracts from the fundamentals.",
        "Great, please share them, those will match my level of intelligence .",
        "It was moreso that I didn't read the body of the post. As OP had already read this book."
    ]
},
{
    "submission_id": "1gi20pv",
    "title": "Market segments with high camera turnover",
    "selftext": "For an academic research project, I am looking into market segments that use many different industrial cameras (either brands or technologies) or markets with a high camera turnover (frequently new devices). However, that seems more challenging than I thought. Any tips?",
    "created_utc": "2024-11-02T10:46:18",
    "num_comments": 3,
    "comments": [
        "Industrial cameras are called industrial for a reason. They are highly reliable and work longer than usual consumer cameras. However if you are looking for high camera turnover, look into a hazardous environment like weather monitoring stations, oil mining rigs, etc.",
        "Not sure about turnover, but I work at a consultancy company doing industrial product development and we use all kinds of cameras, though in small series",
        "Reach out to industrial sales companies to see which cameras they sell the most.  They probably won't share it with you but they would probably know which ones die the most."
    ]
},
{
    "submission_id": "1ghz30m",
    "title": "I made a simple open source website with a interactive robot simulator based on potential fields made with javascript (like a little game), ancestor of robot obstacle avoidance systems",
    "selftext": "https://reddit.com/link/1ghz30m/video/ydck6eglbiyd1/player\n\nYou can make your own world and test different scenarios with a little 2d robot!\n\nPotential Fields in robotics create a virtual force field where motion planning is governed by vector fields consisting of two main components:\n\nAttractive Potential (Goal-Seeking):\n\n* Generates an attractive force vector F\\_att pointing toward the goal\n* Magnitude typically decreases with distance to goal\n\nRepulsive Potential (Obstacle Avoidance):\n\n* Creates repulsive force vectors F\\_rep pushing away from obstacles\n* Magnitude increases as robot approaches obstacles\n\nThe total force F\\_total = F\\_att + F\\_rep guides the robot's motion.\n\nPotential fields represent one of the earliest reactive approaches to obstacle avoidance, where robots used simple sensors (like infrared or ultrasonic) to detect obstacles and generate repulsive forces. While effective for basic navigation, they had limitations in complex environments.\n\nModern obstacle avoidance systems build upon this reactive concept but integrate computer vision to:\n\n* Create more detailed environmental maps\n* Enable precise obstacle detection and classification\n* Allow for better path planning and prediction\n\nThe key evolution was combining:\n\n* The reactive principles from potential fields (immediate response to obstacles)\n* The rich environmental data from computer vision\n* Advanced algorithms for real-time processing\n\nThis combination allows modern robots to not just avoid obstacles, but to understand their environment and make more intelligent navigation decisions. While the underlying principle of \"repulsion from obstacles, attraction to goals\" remains, the implementation has become far more sophisticated through computer vision integration.\n\n# Website link in comments",
    "created_utc": "2024-11-02T08:33:45",
    "num_comments": 3,
    "comments": [
        "Hell yeah!! 😎 cool work!! Well done.",
        "Thank you so much! I spent lot of time to make this :3"
    ]
},
{
    "submission_id": "1ght1yr",
    "title": "Oasis : Diffusion Transformer based model to generate playable video games",
    "selftext": "",
    "created_utc": "2024-11-02T02:57:02",
    "num_comments": 0,
    "comments": []
},
{
    "submission_id": "1ghkcj0",
    "title": "Segmentation Error on YOLO ",
    "selftext": "Hi! I’m new to deep learning and working on a project using YOLOv8 to detect insects in museum display images. I’m running this on my school’s HPC, which has an NVIDIA A100 GPU and uses Slurm. The training stops around the 4th epoch with a \"segmentation error.\" Oddly enough, it works fine with a much smaller dataset (around 10 images, training for 1 epoch).Has anyone encountered this before, or have any tips on troubleshooting this?\n\n",
    "created_utc": "2024-11-01T17:31:52",
    "num_comments": 14,
    "comments": [
        "Maybe you can try a smaller batch size?\nThe total number of images or epochs shouldn't matter.",
        "What's the error log? What is written in the error? if possible, could you please post it on stackoverflow.com and share the link of the post with me.",
        "Try batch=-1",
        "You can try recreating the environment and reinstalling the packages.",
        "Do you think its more of a computer specific problem like memory allocation issues or its in my code",
        "If your batch size is just a bit too large, then it might crash like this. That's why I think that could be the reason.\nBut it could be something else as well. I don't have enough context to judge.",
        "so this is the code: \n\n    model = YOLO(\"yolov8n.yaml\")\n    # Train \n    results = model.train(data= ,epochs=4, workers=1, batch=8)",
        "Right. Does the segmentation error still happen if you put batch=4 instead?",
        "yeah, it still does.",
        "My bad then.\nHave you tried running it on a different machine (such as a colab notebook for example), so as to isolate whether the problem comes from the hardware or from an issue with your code or dataset?",
        "no i haven't, i'll try that one (ur talking about the google collab right?)",
        "Google colab or Kaggle notebook, yes.\nOr any other computer you have on the side, just for the sake of reproducing the error and find where it comes from.",
        "for now, i am trying to train it with a dataset i found from the internet. If it works does that mean my dataset is corrupted or something?",
        "That could be. Worth investigating."
    ]
},
{
    "submission_id": "1ghh7zo",
    "title": "Calling all ML developers!",
    "selftext": "I am working on a research project which will contribute to my PhD dissertation. \n\nThis is a user study where ML developers answer a survey to understand the issues, challenges, and needs of ML developers to build privacy-preserving models.\n\n **If you work on ML products or services or you are part of a team that works on ML**, please help me by answering the following questionnaire:  [https://pitt.co1.qualtrics.com/jfe/form/SV\\_6myrE7Xf8W35Dv0](https://pitt.co1.qualtrics.com/jfe/form/SV_6myrE7Xf8W35Dv0).\n\n**For sharing the study:**\n\n**LinkedIn**: [https://www.linkedin.com/feed/update/urn:li:activity:7245786458442133505?utm\\_source=share&utm\\_medium=member\\_desktop](https://www.linkedin.com/feed/update/urn:li:activity:7245786458442133505?utm_source=share&utm_medium=member_desktop)\n\nPlease feel free to share the survey with other developers.\n\nThank you for your time and support!\n\n \n\nMary\n\n",
    "created_utc": "2024-11-01T15:04:53",
    "num_comments": 2,
    "comments": [
        "One point of feedback, there are a lot of questions about privacy, so i guess it is what your research is about; however, never is there asked if privacy is actually relevant for the solutions/products that are being provided. It is just implied, while there are a lot of ML use-cases where privacy simply is not a relevant concern.",
        "Like I respect the topic, but also recognize there's probably only a handful of people doing the things you're looking for answers for."
    ]
},
{
    "submission_id": "1ghfrx7",
    "title": "3d models of DTU dataset",
    "selftext": "Are the 3D models available for the DTU Dataset for all the scans? Obj, ply files etc",
    "created_utc": "2024-11-01T13:59:57",
    "num_comments": 0,
    "comments": []
},
{
    "submission_id": "1ghd4tk",
    "title": "Low latency classification help",
    "selftext": "Im currently working at a startup that focuses on drone detection using RF sensors and radars, but my boss recently asked me to explore using computer vision to identify and track drones with our camera system. We’re using a solid camera setup(a 1/2\" sensor with 31x optical zoom) so we’ve got some good hardware to work with.\n\nive done a few ML projects in school and played around with YOLOv8 before, but Im trying to figure out if it’s really the best fit for this. Is YOLO good enough for this kind of task, or should I try architecting a custom model specifically tuned for tracking drones in the sky? My priority is to make it fast enough to keep up with the drones while being accurate enough to identify them reliably. will be using edge computing too probably a jetson or something\n\nany advice??",
    "created_utc": "2024-11-01T12:03:15",
    "num_comments": 4,
    "comments": [
        "tracking objects is way, way, way, way, way harder than detecting objects",
        "you might be interested in this [https://youtu.be/6HhLMuWdvAo](https://youtu.be/6HhLMuWdvAo)",
        "Yea probably yolo",
        "That’s pretty cool i’ll def did deeper"
    ]
},
{
    "submission_id": "1gh9cl8",
    "title": "HTR and OCR in Portuguese",
    "selftext": "Goal: to digitalize a few notebooks and family history letters (some English, some Portuguese) using HTR run locally (I want to eventually package this into an open-source app with some other features).\n\nResources: no GPU, 16GB of RAM on an Intel MacBook Pro. Extra CPU can be provided by my home cluster.\n\nI've tried: Microsoft's handwritten OCRs on hugging face (was most accurate that I used, but only good for English), TrOCR, EasyOCR, and uploading to ChatGPT4.o. ChatGPT has been the most successful with an insane accuracy—just uploaded the entire page. I tried downloading a local version of LLAVA to imitate the same idea as ChatGPT but it wasn't successful at all. I've not tried using separate segmenting from transcription, mostly resources that do both or using tiny data samples (a photo cropped to one line).\n\nI really want something that I can do from the command line.  \nI kind of feel a bit overhwelmed by how many tools and how different they are.\n\nHere's some of the attempts:\n\nOriginal text:\n\n\"A fim de ser um lider que estimula crescimento e\" (note that \"estimula\" is spelled incorrectly as \"estumula\")\n\nhttps://preview.redd.it/0j24tk6agbyd1.png?width=2194&format=png&auto=webp&s=f02a3633e66b2ce54ba98fc63b330d77118f90ab\n\nEasyOCR was a bust (see result on last line)\n\nhttps://preview.redd.it/ynbt6on6gbyd1.png?width=3442&format=png&auto=webp&s=a9de4ded82ac44e632016c782c5785596f75a002\n\nMicrosoft handwritten OCR was impressive:\n\nhttps://preview.redd.it/4qni049pgbyd1.png?width=1130&format=png&auto=webp&s=bbb7b4af40226b0b8dc98f042fdb299788ed7901\n\n",
    "created_utc": "2024-11-01T09:21:56",
    "num_comments": 1,
    "comments": [
        "I’ve had a lot of luck with PaddleOCR in the past. Not sure if/how it runs in the command line though, it’s been awhile."
    ]
},
{
    "submission_id": "1gh7nd2",
    "title": "point cloud segmentation, help needed",
    "selftext": "Hello Everyone,  \nI need to perform a point cloud segmentation, I have many scans of a rock surface and I want to segment into 3 category. I have lots of different scans in different locations labelled every point into the 3 category so I can perform a supervised deep learning. I have some experience with machine learning in TensorFlow but I am new to computer vision.\n\nMy two questions are:\n\ndo I need a part segmentation or semantic segmentation? the difference really confuses me sorry!\n\nI have spent a lot of time going through the examples in pointnet and pointnet++ however there are many somewhat newer projects like DGCNN, and POINTCNN. My question is should I be using these newer packages, or the older more established ones. Also many of these packages are only available in TF1 but there are some open pull requests for tf2 available in some cases. If anyone has a lot of experience could you advise me on the best place to start. I have XYZRGB data labelled.",
    "created_utc": "2024-11-01T08:10:08",
    "num_comments": 2,
    "comments": [
        "If you want to assign each point of your point cloud into one of 3 categories it seems like you are interested in semantic segmentation.\n\n  \nI would start off by taking a look at Point NeXt repository and try to understand how the dataloaders work. Then either you transform your data to take the form of how S3DIS benchmark dataset is saved, or you create your custom dataloader which will be able to read your labeled point clouds.\n\nAnother good alternative is KPConv. That paper is also available in Tensorflow if that is what you're most familiar with. But really most papers nowadays use only PyTorch, so it's not a bad idea to try to learn that.\n\nPointNeXt (PyTorch): [https://github.com/guochengqian/PointNeXt/tree/master](https://github.com/guochengqian/PointNeXt/tree/master)\n\nKPConv (Tensorflow): [https://github.com/HuguesTHOMAS/KPConv](https://github.com/HuguesTHOMAS/KPConv)",
        "Thank you very much for the response. I'm really struggling to install PointNeXt when i try and install openpoints subdirectory. Do you have any experience with the installation? I have followed he instruction on the repo and openpoints documentation. This is what I get: [\\#!/usr/bin/env bash# Command to install this environment: source install.sh - Pastebin.com](https://pastebin.com/trbgtjgb)"
    ]
},
{
    "submission_id": "1gh6bw6",
    "title": "Dear researchers, stop this non-sense ",
    "selftext": "Dear researchers (myself included), \nPlease stop acting like we are releasing a software package. \nI've been working with RT-DETR for my thesis and it took me a WHOLE FKING DAY only to figure out what is going on the code. \nWhy do some of us think that we are releasing a super complicated stand alone package? \nI see this all the time, we take a super simple task of inference or training, and make it super duper complicated by using decorators, creating multiple unnecessary classes, putting every single hyper parameter in yaml files. \nThe author of RT-DETR has created over 20 source files, for something that could have be done in less than 5. The same goes for ultralytics or many other repo's. \nPlease  stop this. You are violating the simplest cause of research. This makes it very difficult for others take your work and improve it. We use python for development because of its simplicityyyyyyyyyy. Please understand that there is no need for 25 differente function call just to load a model. And don't even get me started with the rediculus trend of state dicts, damn they are stupid. \nPlease please for God's sake stop this non-sense. \n",
    "created_utc": "2024-11-01T07:11:23",
    "num_comments": 110,
    "comments": [
        "1. Find an interesting use case for AI\n2. Write some code that works on your machine\n3. Moduleception\n4. Open source the project on GitHub\n5. Abandon project and repeat",
        "Lead the way.",
        "Lot of it has to do with culture in academia. Many PIs can’t think beyond the next grant and pay no attention to quality of infrastructure or bookkeeping! If students are mentored to be slightly more organized it can go long way.",
        "Well it's just basic software etiquette. I see a lot of people here agree but I don't agree with this completely. Making code as modular as possible allows for easily extending features in the future. Perhaps it's because I used to work as an SDE for quite a few years before switching to ML research. Typically the repositiories I find unpleasant are the ones that have most of everything in one or two files. Sure it's easy for me to read but if the authors wish to extend their code there will be a lot of refactoring involved. I don't think that's good practice. Infact I think research code used to be so much more childish and worse before. These days you can just add a submodule to your own repo and extend functionality so much more freely. \n\nI have not worked with ultralytics so maybe it truly is horrible. Perhaps you need to get better at writing modular clean code? (Don't mean this as an insult, a startup I used to work at had an amazing lead, her software skills though were quite lacking and I find this often in the academic community and lately I think it has started to improve because of the exact reason why I think you don't like it?) \n\nPs: as u mentioned I have not used ultralytics, so maybe it is actually unnecessarily complex but considering they pitch themselves as the yolo folks and constantly update and add features, new models etc I can see why they opted to make things the way they are.\n\nI want to be clear, I very much wish for clean repos but I usually find good repositories if you go for good papers and their official implementations hence why my comment is disagreeing. But maybe I misunderstood your post. Feel free to add nuance or correct me.",
        "It's worse than that. Researchers will publish something like the \\`taming\\` package that isn't maintained (there's a single commit, I think) and other ML engineers will make that package PART OF THEIR PROD DEPLOYMENT.   \n  \nNot only that, but dependency management in most of these repos is non-existent, and certainly does not result in reproducible builds as the dependencies are updated.\n\nIt's incredibly frustrating.",
        "PLEASE STICKY THIS POST! I’m dead serious.\n\nCode complexity is a massive obstacle and is why so many people and companies just caught up huge sums of money for APIs that hide all the mess and give a simple clean interface. \n\nI get having somewhat messy and buggy code, but sometimes it’s like they intentionally obfuscate things. ",
        "You can always create a pull request and make the code cleaner.",
        "As a developer, no one _wants_ to over engineer code. The complexity is a natural consequence of the process and it must be mitigated, for to rid yourself of it would be to delete all lines of code.",
        "Yeah man! I tried implementing a lot of shit, but good damn it was fucking tough.",
        "super-gradients is pretty well structured, in my opinion. But nothing is perfect, write your own trainer, data loader, loss function, metrics and visualization if you want to control everything",
        "Yeah agree. I've wasted weeks on super complicated implementations that I just don't understand. Keep it simple stupid. Have the core components clear and compartmentalized and provide an example implementation that follows normal (ex PyTorch) implementation patterns. That's all I want.",
        "I just took a look at RT-detr. It actually looks pretty well structured. It is the classical code structure of all basic ml tool: dataset, architecture, optimizer, some boiler plate code for training and visualisation, some basic image processing, etc. \n\nPutting all configurations and parameters in yaml files is also quite classical. You can track multiple experiments by looking at the config files instead of source code changes. \n\nLast point: you do not look at code from academic researchers. It is big tech companies: baidu, Facebook, Google. So basically people are also hired for their software engineering skills.",
        "Did a project that had me implement I2SB. It was extremely complicated and it took me 6 hours to even implement it. On top of that the Issues section was the one that helped me the most, even though the author does not even respond to it. And most of the time I had to change stuff up and figure out what was going wrong where. It's simply a pain to do all that. And on top of that, they don't even specify which version of python or the version of packages to download. \n\nCompared to that, PerVFI, which is related to a different project I am working on currently, had me implement it in 30 mins. Far faster than most other projects. Till date it's the most well documented code I have seen.\n\nEdit: I make it a point to document my code well. The last company I interned in, I know I didn't do as well since my tenure was only 2 months. But I was damn hell adamant on keeping simplicity and writing a well documented repo. At least they can appreciate me for that rather than the lackluster progress I got. Definitely helps as well.",
        "I'm working on a paper right now and I'm building upon a previous CVPR paper. Their technique is really interesting and works quite well. But their code repo is insane. My man copied over the entire detectron2 codebase as a subfolder (not a git submodule) only because they wanted to use the Instances/Boxes api for dataloading. What should have been a few hundred lines of code, is now a few thousand across several tens of files. And since I'm pressed for time, I'm just rolling with it instead of doing the refactoring. But it's slowing down my speed to debug/iterate/experiment by a lot. :'(",
        "Amen",
        "The incentives are for the paper, not the software sadly. I don't think researchers are bad software engineers by and large, but it's often seen as a waste of time by supervisors, PIs, and funding agencies. \n\nIf it's automated enough to reproduce the figures when something changes, it's good enough to ship for an artifact submission.",
        "I think having a subdirectory with a more simple version of the model implementation, training code, etc., with some example notebooks could be a way to have the best of both worlds.",
        "Edit: was supposed to be a reply to a comment but bug:(\n\nIt's not. Unneeded modularity is never more important than readability. Small projects are easy to extend. You're not building an entire company's backend, it's a self contained research project. The whole goal of publishing your code as a researcher is for people to easily reproduce your results, check that you are doing what your article says you're doing, and extend your work. They are not going to extend your repo, they are going to take the code they need, and that's what needs to be easy",
        "Honestly I do think its good to write a well structured code as part of the research. But the problem is most of the research work is not evaluated based on code. It is the idea that matters more. And as researchers we need to find a way easiest way to validate the idea. So I think its fair that most of the researchers do not bother spending time in refactoring the code to make it easy for others. But those researchers just underplay their contribution by making it difficult for others to make of their research idea.",
        "The DuckDB team made a concerted effort to not just do cool research, but to make a useful and reusable code base as well (with almost zero dependencies). \n\nHope their popularity makes this a more popular trend in academia. A fun presentation from Hans on the subject:\nhttps://youtu.be/HVR0YKeYA4I?si=vBAADfnXFN84i_ur",
        "Agh, I totally agree with the sentiment but I also understand it isn’t that simple.\n\nLet’s assume we’re talking about a single experiment where you’ve used a YOLO model - just for arguments sake.\n\nYou’ve got to cleanly package that model and experiment for end users, right? So, what does that include?\n\nDocker files.\nQuants.\nDependencies\nSystem Dependencies.\nEvals.\nModels: ONNX, PyTorch, Jax, TF2, etc.\nSFT scripts\nPEFT scripts\nInference: ollama, unsloth, mlx, sage maker, vertex, ad infinitum. \nDocs/README\nNotebook scripts: Jupyter, Colab, Lightning\nTraining scripts for reproduction\nPre-Processing scripts: see above\nPost-Processing scripts: see above\nConfig: YAML, JSON \nBenchmarking\nMonitoring: Tensorboard, W&B, etc.\nLicense\nBibTex \nChangelog\nTemplates: contribution, issues, etc.\nCheckpoints\nEnv files\n\nThen, you can do it all again for the data - with exceptions, of course.\n\nThis obviously isn’t needed for every single paper, but a significant amount of it is if you’d like reproduction to be straightforward. Don’t be the guy that hacks together a workflow and expects everyone to figure it out because we will walk away from it if it’s too much nonsense. You’re presenting your work to the world; we all use different tools, etc. to test your work and sometimes that’s just the nature of the beast. \n\nHaving said that, there is a happy medium. I totally agree with you, too. It’s so damn much. I wish there was some standard we held one another to that wasn’t over the top.\n\nWhat suggestions or ideas do you have?",
        "Amen. I was just thinking the same thing. Code can be clean without having to inspect 8 levels deep to see what’s going on.",
        "Welcome to anything dealing with software engineering/development. Take something functional and build horrific wrapper on top of it, that does the same thing but saves 15 characters and removes half of its functionality, while trying to pervert it with as many patterns as possible in hopes someone thinks its smart...\n\n\nYa this one struck a nerve.",
        "if you make it look too simple, some dumbass will reject it, thats why you see an overwhelming majority of works that had to make it intentionally hard/complicated so they can get it published!   \nThis is, imho, one of the main reasons you see over engineered source codes/architectures/papers as well.   \nyou need to target the root cause of this otherwise this will only get worse!",
        "Horrible project structure and workflow are common in the researcher community. Having clean and optimized codes & projects are luxuries",
        "Oh please and also include a \"single image\" inference example in your code. Not everyone is trying to validate the whole \"dataset\".",
        "I love you. This is the same problem we hire them. Too much code bloat for very little functionality.",
        "Learn to use an IDE?",
        "You’re overlooking the power of YAML files here. For generating hyperparameters in tuning, YAML is essential! Having worked with Ultralytics, I really appreciate how easy it is to enhance their models and adjust settings without needing to dive deep into Python code. It really is just a matter of adjusting a few lines in the model config to introduce, say, a transformer. This setup is incredibly research-friendly, making experimentation so much easier. The open, flexible code structure in tools like Ultralytics is among the most researcher-friendly I’ve seen—perfect for quickly iterating and testing new ideas without getting bogged down in complex code changes.\n\nI get the challenge of the initial learning curve, but YAML and structured files make managing and debugging larger projects so much easier with decorators and classes. Keep pushing forward and keep learning!",
        "I'm not a researcher as of yet, although I aspire to he one. I was also working on RT-DETR model for my final year undergraduate project. I was basically trying to alter and improve it's architecture. Took me a month and I still don't understand it completely. Still wondering how you did it in one day. Like, how do you understand such type of codebases?",
        "I'm not a researcher as of yet, although I aspire to he one. I was also working on RT-DETR model for my final year undergraduate project last month. I was basically trying to alter and improve it's architecture. Took me a month and I still don't understand it completely. Still wondering how you did it in one day. Like, how do you understand such type of codebases?",
        "Just people who can't code trying so hard.\nIf you have one task and can't build it in its simplest form, u are doing it wrong.",
        "It takes a day usually to understand what's going on with the codebase, are you venting?",
        "Are detectron2 repositotories more prone to this? Honestly felt like understanding spaghetti code lol. Dependency management with CV projects is also such a pain.\n\nI understand why its not as simple cloning the repo and installing packages with a single command like in wev dev, but all the set up and boilerplate just to get shit to work could be very frustrating. Takes away time and effort from doing actual research and iterating.",
        "Exactly lmao I’m glad it’s not just me, these guys make it so hard for us to read…like making helper functions for helper functions and jumping from file to file for a functions defined in other files and forgetting what it was I actually needed",
        "Ignore and just make great things. It's not as if you need any of them, right? It's not as if they will stop spitting out tons of lukewarm nonsense. But every once in a while, something nice is released, and if you look you'll see it is someone that ignored all the noise and just made what they felt was right. Ignore all the exhibitionist developers and just use your own compass, and remember to keep it simple, stupid.",
        "I find the opposite: people writing undocumented code, no modularity, hardly launching at first try, and finally if you change something (like the dataset) results are shitty. Btw, I find that spending one day to understand one project is not a big deal. You would’ve loose much more time in implementing it by yourself.",
        "Thank you for the feedback! We always work toward simplicity but I agree we can do better.",
        "And that's why my favorite model ever is MotionBert : training script, inference script, no registry bullshit, no stupid unnecessary OOP",
        "Usually people who feel this way don’t understand why certain abstractions are being used and therefore can’t appreciate them. The code seems fine to me, take this as an opportunity to improve your ability instead of trying to lower the bar.",
        "I completely agree. I find ultralytics completely ridiculous",
        "Because this is research code....not a production level software... as it should be.\n\nIf you are not willing to struggle with other researchers' codes, you should not be doing research... or maybe what you call research is not research.",
        "Millions of abandoned projects 😭 \n\nIt's infuriating 😡 to be honest.",
        "This person knows it.",
        "Abandon the project --- just love it",
        "I definitely will.",
        "100% agreed. The primary reason I didn’t stay in academia. Didn’t want to kiss ass for sponsors and publish for the sake of it (Defintely regretted a bit for not publishing enough, but oh well)",
        "I definitely agree. I have also started to believe that academia is way worse than industry. It looks like only the publication matter.",
        "“If students are mentored to be…” \nI wasn’t mentored at all lol. My prof wrote grants, assigned tasks, held meetings, and revised papers. Had to unlearn so many bad habits on the job.",
        "Oh, they can, they just realise there's no incentive for them to do so. Understanding this is important if you want to see things change: senior academics are extremely competent, but their objective is not the same as your assessment criteria.",
        "I agree. You don't realize why the complexity is there until you are tasked with writing the code. Then you realize without that additional abstraction, things start getting messy.",
        "I agree with your assessment and I have further questions for OP concerning his critique of the code quality.\n\nSurely encapsulating hyperparameter settings into a non-code configuration file is preferable to hard-coding it deep into the pipeline? The large code quantity of stand-alone packages is caused by the desire to to enable data reading, processing and routing it into the model. The core model is often times a simple Python file in the `models` subpackage or directory.\n\nDisclaimer: Similar to you, I have not worked with the mentioned codebases, so they may indeed be terrible.\n\nAre we talking about this project: https://github.com/lyuwenyu/RT-DETR ? It at least has install instructions, use cases, a CLI interface. Has any other user of the package insights and a reasonable take on the deeper code quality? I am a researcher myself and try to adopt best practices to elevate my code quality.",
        "i kind of agree with you.",
        "Hm yeah I also felt this criticism isn't really about something researchers do because they often just dump everything into a single huge Jupyter file and then forget what was used for their paper results.\n\nThat being said, I have worked as dev for a long time as well before I did my PhD and over the years my code also has become much simpler again. I had my patterns and Gang of Four phase and so on but that's over. Many abstractions we see make things just harder and break down on every occasion. I see that when working with LangChain all the time that I almost always have to ... well often even copy a whole class and make my own version because trying to solve it with inheritance is even worse.\nAnd recently I also feel Python has become a bit Javaesque in that regards.\n\nI know that's also not CV but I've worked with Nvidia Nemo for a while and it was also really a pain to find anything because everything was either abstracted behind some Pytorch Lightning abstractions or deep in their crazy hydra configuration tree instantiating all modules from there - it seems in the next version they dropped this for plain Python, so it probably wasn't just me finding it bad (although I currently also have a system where most components are configured and instantiated with Hydra).\n\nFinding the optimal abstraction level is always hard and as I said I found many people start out with 0 abstraction, then at some point overdo it and then sometimes come back again ;)",
        "LITERALLY !! Take a look at alphapose repo : literally impossible to install if you follow their setup instructions, same for mmpose.",
        "The reason for the lack of maintenance is pretty simple: PhDs come and go, and each one works on something different. One PhD develops sth, publish a few papers on it, graduates, and the repo is done for good.",
        "honestly, academics are terrible at keeping code simple. we tend to think that more abstraction and clever bits are worth throwing in there and hardly ever think about readability, modularity, etc. it took a few years outside of academia for me to not write a bunch of convoluted code.",
        "Exactly, there is no need to import multiple files and call one function from each. It looks like they do this on purpose.",
        "That's exactly where you are wrong. Researchers try to look like developers. They think if make it complex, it looks cool.",
        ":((",
        "Same here bro, same here",
        "Try figuring out how you can extract the architecture with pre-trained weights. I bet it fills up your whole weekend. \n\nAbout your second point, yes I agree, this is why researchers should not try to act like them. Dive into the RT-DETR code for a bit. I guarantee a headache.",
        "I had the same experience multiple times. They don't even bother themselves to comment the code.",
        "Yessss, this is a good solution.",
        "IMO when we dive into another one's code, it's not always for reproduction of results. Me personally wanted to change the architecture to evuate an idea but it was such a pain.",
        "Ecactly",
        "😂😂😂 Exactly. I started to appreciate the c++ defacto structures.",
        "Success in academia comes in large part from being good at writing grants and not necessarily from writing code. Realistically, if someone has an innate aptitude for software development my expectation is that they will beeline it to a well-paying gig rather than deal with the many trials and risks of academia.",
        "Horrible can't even describe half of it.",
        "I totally get it :(",
        "OK genius.",
        "I just do backtracking. I'm quite experienced in it.",
        "Agreed",
        "That's the thing, it's not a code base, just a very small repo that is designed too complicated for no reason. They are all like this.",
        "Exactlyyyyy",
        "Thank you. But i blieve you are headed toward the opposite direction.",
        "OOP is really an overkill for these types of codes.",
        "Before jumping to a conclusion, pay attention to the title, I said researchers, meaning that we aim to improve others' work all the time. So yeah, don't try to act like a hotshot and read first",
        "WTF? The first criterion of researching is to be as understandable as possible. Where have you studied? \"I just want to say the opposite of what you say\" school?",
        "Honestly at this point it'd probably be worthwhile to figure out how to implement some of the key algorithms by hand",
        "All the best because I genuinely support your sentiment. Good to know that it's not just me feeling this way. \n\nI feel like whenever I need to do some computer vision tasks I can feel my stress level and annoyance rising in the background. It's as if I'm subconciously \"priming\" myself to be ready to wade through more code, many at disparate places, than they need to be.",
        "Is it the same in Europe? or its just in North America ?",
        "You’re probably right but do remember that we’re not seeing most industry code because it’s kept behind closed doors. \n\nBut yeah, industry usually has more incentive for clean code. Not that managers give developers time for that 😂 ",
        "Jfc the alphapose repo made me gag 🤢",
        "Don’t get me started with the MM family (mmcv, mmdetection, etc), their torch nightmare builds, and some of the codependencies. Jesuschrist.",
        "Oh. Well then it might be better to blame the incentives if the industry has equated spaghetti code with cool. \n\n\nThey may wish to be informed that while complex is cool today, simplicity is and has always been timeless and makes you cool every day forward.",
        "Recently I spent a full day just to get InternVideo2 to work. There was a requirement.txt file with it that was incomplete, didn't work and had dependencies that no single Python version could ever fulffill. I also had to compile cuda extension that were a part of Flash attention. It got it to work, but it was awful. I wish people would just use Nix so we can have reproducible dev environments.",
        "I used like 1-2 hours for that task, definitely not a weekend. Really not that hard...",
        "Seems pretty well organized? Little bit of Python magic for loading from the yaml files but the onnx export script easily shows how the instantiate a class with the pre-trained checkpoints?",
        "Yep, agreed. Or it will be like the comments are all over the place. And I sometimes don't understand the file structure as well. All these things frustrate me. Only thing is I get the satisfaction when I figure it out, but is that really a good satisfaction. Seems very unnecessary. I would probably have better satisfaction if I can modify the code to suit my needs.",
        "Elaborate please. You cant leave me there",
        "You know, a Lot of people doing AI are bad at software engineering, and can't even do the bare minimum of clean code.\nThis gets worse in Academia because none things of it as a product but as a whatever makes it prove a point.\nThat's why companies are hiring software engineers for AI :)",
        "Georgia Tech. And where did you get your \"criteria\" from? As far as code goes. You are working on getting results, not pleasing some lazy guy who does not want to spend long hours and nights in the lab working",
        "No simple answer to this. I am doing my phd at a German university and I have never kissed ass. But my position is fully funded by the EU. You always have reviewers who you’d like to make happy. But discussions are always constructive and respectful. Personal experience are vastly different though, a lot of your day to day business depends on your direct team lead or professor.",
        "I think it depends on where you are. I am employed in academica but in a somewhat \"cross field center\". And i moved from research assistant to full time engineer position, so technical staff. \n\nWhile our phds spend a lot of time on publishing and our professors spends majority on project management, granted application and teaching/supervision, we also have some that does a good deal of actual work in the field and in their projects. And all our projects are pretty much in collaboration with industry partners and trying to get research out into the world to be used. \n\nThe research assistant here and engineers spend most of their time on developing and delivering code/product or consulting on our projects. So we do most of the research and development. The engineering team also spends some time on lab maintenance and general improvement of internal infrastructure etc. \n\nI am fairly Happy with it and feel i get the time to also make \"good\" code together with industry partners, but granted that we never goes much further than prototype and proof of concept as a university. And as it often ends with solutions more tailored to that specific project and industry partner it is not always super easy to open source, we do however try. \nMaintaining it is a problem though, as when the project is done we dont get any funding to open source and maintain it, so it is not uncommon to simply not have the time or funds to maintain in the long term. \n\nThis experience might be unique to our center though, i am not sure if people have the same experience in other universities in my country. (Scandinavia)",
        "Worked in both academia and industry and industry code is not much better. Technical debt and employee churn does not scale well.",
        "And they don't even care man. They don't maintain their shit. So if you open an issue, expect an answer in 3 months.",
        "Or poetry",
        "You did not dive deep enough.",
        "Okay, no problem. There is almost always an inference or eval code. I take that and try to do backtracking from the last output. For example, in this RT-DETR there was an inference script. tracked the trace of the model in the code and figured there are multiple scripts each for a different part of the network (backbone, encoder, decoder). But I give it to you this particular case is very complicated, especially because of the usage of numerous decorators that are completely unnecessary.\n\nFeel free to message me and we can talk about it more.",
        "Agreed again. Thats why I've been trying so hard to get a bit knowledge in software eng as well. and that's also what drove me crazy today.",
        "Yeah you are not educated and don't even know what research is.",
        "Thank you, is it ok if message you to ask some questions about PhD in EU  ?",
        "I did message you.",
        "Yeah you are the typical idiot that annoys everyone with his whining. Freaking loser who just wants to steal other people's work with a little diva ego.",
        "Sure",
        "😂😂😂😂😂Yeah yeah right, go f yourself buddy.",
        "Freaking loser. It seems you ran out of arguments because you did not have one to start with.\n\nLittle egotistical nobody. Everybody must hate you in your lab.",
        ":))) Yeah sure, keep going if it helps you feel better about yourself.",
        "You bet. Mr little diva"
    ]
},
{
    "submission_id": "1gh1gka",
    "title": "nnU-NetV2 pre-trained?",
    "selftext": "Hello everyone,\n\nI was reading the nnU-Net paper: [https://www.nature.com/articles/s41592-020-01008-z](https://www.nature.com/articles/s41592-020-01008-z) or arxiv version [https://arxiv.org/abs/1809.10486](https://arxiv.org/abs/1809.10486) and I was wondering if I can find the pre-trained version of their model? Specifically I'm looking for the nnU-Net that they themselves trained on their dataset, since I am looking to work with the same Medical Decathlon dataset, conducting Knowledge Distillation specifically.\n\nI found their github [https://github.com/MIC-DKFZ/nnUNet](https://github.com/MIC-DKFZ/nnUNet) They provide details on how to train and do inference etc. generally. I was originally going to train an nnUNet on the existing Medical Decathlon dataset but that would be doing work that already has been done. I was wondering if anyone knows how do I find the trained model instance that they worked with? I thought about emailing them, but idk how acceptable is that of a request in the CV community. \n\nnnUNetV1 would also be fine.\n\nWould be grateful for any advice.",
    "created_utc": "2024-11-01T02:31:14",
    "num_comments": 3,
    "comments": [
        "Found [3 relevant code implementations](https://www.catalyzex.com/paper/arxiv:1809.10486/code) for \"nnU-Net: Self-adapting Framework for U-Net-Based Medical Image Segmentation\".\n\nIf you have code to share with the community, please add it [here](https://www.catalyzex.com/add_code?paper_url=https://arxiv.org/abs/1809.10486&title=nnU-Net%3A+Self-adapting+Framework+for+U-Net-Based+Medical+Image+Segmentation) 😊🙏\n\nCreate an alert for new code releases here [here](https://www.catalyzex.com/alerts/code/create?paper_url=https://arxiv.org/abs/1809.10486&paper_title=nnU-Net: Self-adapting Framework for U-Net-Based Medical Image Segmentation&paper_arxiv_id=1809.10486)\n\n--\n\nTo opt out from receiving code links, DM me.",
        "They mentioned in one of their github issue that they would not be prioritizing it, so you may need to train from scratch."
    ]
},
{
    "submission_id": "1ggz5ty",
    "title": "Papers that discusses these terms",
    "selftext": "Hi, I’m looking for papers that goes into more depth of these components mentioned in my professors lecture. The closest thing I have found is data association discussed in ORB-SLAM3 paper, but it does not mention loop closure in local map or global map. \n\nMy professor said this is discussed in many papers, but I have so far not found one. ",
    "created_utc": "2024-10-31T23:24:20",
    "num_comments": 5,
    "comments": [
        "Actually the original ORB-slam paper for monocular camera describes almost all the steps you just described. Here is a link to the PDF, is that an answer or you were looking for some more specific? here is the link to the pdf: [https://webdiis.unizar.es/\\~raulmur/MurMontielTardosTRO15.pdf](https://webdiis.unizar.es/~raulmur/MurMontielTardosTRO15.pdf)and to the github repo: [https://github.com/raulmur/ORB\\_SLAM](https://github.com/raulmur/ORB_SLAM)",
        "Take a look at the PTAM (https://www.robots.ox.ac.uk/\\~gk/publications/KleinMurray2007ISMAR.pdf) and RTAB-Map (https://arxiv.org/abs/2407.15304). The former paper is considered the first real-time implementation of modern optimisation based VSLAM. The latter one employs a hierarchical division of memory for loop closure detection that more closely is related to what are you looking for.",
        "From what I understand, in ORB-SLAM, the local map represents the area currently visible and actively used by the system, while the global map refers to the full map built by the SLAM process?\n\nI also noticed that the ORB-SLAM paper doesn’t specifically mention «short-term,» «mid-term,» or «long-term» tracking. Are these terms common in other SLAM literature, or are they made up by my professor?",
        "1. localmap/global map: to my knowledge yes, that is to limit the number of points you project for correspondences and also to update.\n\n2)honestly don't know about short-mid-long term, but the distinction itself makes sense. But they are really similar to what is described in the paper: in fact in the picture you posted \"Mid term\" refers to tracking/loop closure in the local map, that is mainly from chapter 5 section D till the end of chapter 7. Sorry i can't help more but i am not an expert on slam, I started building one of my own 1 month ago for work and struggling a lot atm :)",
        "Thanks anyway, and good luck!"
    ]
},
{
    "submission_id": "1ggtglh",
    "title": "Train S3D Video Classification Model using PyTorch",
    "selftext": "Train S3D Video Classification Model using PyTorch\n\n[https://debuggercafe.com/train-s3d-video-classification-model/](https://debuggercafe.com/train-s3d-video-classification-model/)\n\nPyTorch (Torchvision) provides a host of pretrained video classification models. Training and fine-tuning these models can prove to be an invaluable asset in building many real-life applications. However, preparing the right code to start with custom video classification training can be difficult. In this article, we will train the **S3D video classification model** from PyTorch. Along the way, we will discuss the pitfalls, caveats, and optimization techniques specific to the model.\n\nhttps://preview.redd.it/6vkg4vwhr6yd1.png?width=1000&format=png&auto=webp&s=33eff9aa999a466cf3c16a2b8d29a379264eb5d9\n\n  \n",
    "created_utc": "2024-10-31T17:38:06",
    "num_comments": 0,
    "comments": []
},
{
    "submission_id": "1ggncv6",
    "title": "How to calculate distance from a profile view?",
    "selftext": "Hello, everyone! I'm currently working on a YOLO vision project using the pose model, and I'm having some issues estimating distance using only a camera. I know it might sound a bit arbitrary, but this is the solution we have for now while I wait for a LiDAR sensor I ordered last week. Since I'm in a Latin American country, it may take a month to arrive.\n\nRight now, we're estimating distance by using the focal length with the person facing the camera, and it seems to be working well, with an error margin of around 20-25 cm. Here’s the code we're using:\n\n\\`\\`\\`\n\nfloat YoloV8::estimateDistance(const cv::Rect\\_<float>& bbox, const std::vector<float>& keypoints) {\n\n// If we have valid keypoints, use the shoulder distance\n\nif (!keypoints.empty() && keypoints.size() >= 21) {  // Ensure we have enough keypoints\n\n// Get shoulder coordinates (5 and 6 in COCO format)\n\nfloat shoulder1X = keypoints\\[12\\];  // Right shoulder X (5 \\* 3)\n\nfloat shoulder2X = keypoints\\[15\\];  // Left shoulder X (6 \\* 3)\n\nfloat shoulder1Conf = keypoints\\[14\\];  // Right shoulder confidence\n\nfloat shoulder2Conf = keypoints\\[17\\];  // Left shoulder confidence\n\n\n\n// If both shoulders are detected with sufficient confidence\n\nif (shoulder1Conf > KPS\\_THRESHOLD && shoulder2Conf > KPS\\_THRESHOLD) {\n\nfloat shoulderWidth = std::abs(shoulder2X - shoulder1X);\n\nif (shoulderWidth > 0) {\n\nreturn (AVERAGE\\_SHOULDER\\_WIDTH \\* CAMERA\\_FOCAL\\_LENGTH\\_SHOULDERS) / shoulderWidth;\n\n}\n\n}\n\n}\n\n\n\n// Fallback to the original method if we cannot use shoulders\n\nreturn (AVERAGE\\_PERSON\\_WIDTH \\* CAMERA\\_FOCAL\\_LENGTH) / bbox.width;\n\n}\n\n\\`\\`\\`\n\n  \nThe issue I’m currently facing is with profile views; the distance calculation becomes inaccurate, returning values that don't make sense.",
    "created_utc": "2024-10-31T12:51:14",
    "num_comments": 7,
    "comments": [
        "Is it even possible to estimate distance from just a single image? It's like estimating distance with one eye closed... It only really works if you observe change over time to compensate for a lack of second parallax view point.\n\n\nAlso I'm your formula are you compensating for the curvature introduced by the lens curvature?",
        "Why don’t you try a monocular depth model? These should be more robust having been trained on million of photos specifically to output distance. \n\nAlso assuming this is video you should try averaging the distance over time as long as the person and camera aren’t rapidly moving. Maybe fit a curve such that the acceleration stays within reasonable limits (a person can’t go from sitting still to moving 100 miles per hour in a split second). ",
        "Can you use depth maps?",
        "Not possible, best of luck to you though.",
        "U can try using a distance sensor, which gives the distance and u can find the amount of pixel the object has occupied",
        "Also if you can, use multiple “parts” of the person. Their height, head to shoulders, arms; legs. Thos all have known lengths and you can take an average distance. ",
        "Could you add more context? For example, can the object you're detecting appear anywhere in the image? Also, if you know certain information like the real height of the detected object, you could use spatial geometry, though you shouldn't expect the accuracy to be perfect."
    ]
},
{
    "submission_id": "1ggme8c",
    "title": "Looking for contract based projects (CV, ML, Robotics and IoT)",
    "selftext": "Background: Worked in the research labs of McGill University and IISC Bangalore in the fields of CV, ML, Robotics and IoT\n\nTech stacks: PyTorch, OpenCV, Mediapipe, ROS, puredata, C++\n\n\nCurrently looking for contract based projects, if you a professional looking to delegate your work, or a college student looking to get their final year project done at an industrial level, feel free to contact me for my portfolio/profile.",
    "created_utc": "2024-10-31T12:09:21",
    "num_comments": 3,
    "comments": [
        "When you say, do you mean machine learning based or pure CV? Asking out of curiosity.",
        "I have a project, DM me",
        "I’ve done both in general."
    ]
},
{
    "submission_id": "1gglel4",
    "title": "Creating a robot for you all and I am hoping we can collaborate on it together.",
    "selftext": "I am really trying to find my target market, and it would really help me out if some of you took this survey for me. We will be releasing more information about it in the future. I think you all will love it, developers and hobbyists alike. I am trying to figure out who my target market is, and it would be extremely helpful if some of you could fill out this survey for me. [https://forms.gle/6KzCHZskboepSpWQ6](https://forms.gle/6KzCHZskboepSpWQ6)",
    "created_utc": "2024-10-31T11:26:50",
    "num_comments": 2,
    "comments": [
        "Good luck mate",
        "Thanks, Lee, if you are curious to learn more about what we are working on over here at Hackerbot shoot me a DM."
    ]
},
{
    "submission_id": "1ggjd3b",
    "title": "Experimental Design for Multi-Channel Imaging via Task-Driven Feature Selection (ICLR)",
    "selftext": "The paper aims to shorten acquisition time, reduce costs, and accelerate the deployment of imaging devices.\n\n[https://openreview.net/pdf?id=MloaGA6WwX](https://openreview.net/pdf?id=MloaGA6WwX)\n\nContributions:\n\n* A novel method for supervised feature selection that performs task-based image channel selection.\n* Results shorten the acquisition time in MRI, reconstruct image cubes of remotely-sensed multispectral ground images with few sensors, estimate tissue oxygenation from hyperspectral medical devices.\n* Results show improvement on i) classical experimental design, ii) recent application-specific published results, iii) state-of-the-art approaches in supervised feature selection.\n\nWe expect further applications to similar datatypes e.g. data efficiency on multi-channel images, other hyperspectral/multispectral application, cell microscopy, weather and climate data et.c\n\n\n\nCode is available, PM me if interested.",
    "created_utc": "2024-10-31T09:59:07",
    "num_comments": 0,
    "comments": []
},
{
    "submission_id": "1ggitzh",
    "title": "MBA students researching machine and computer vision",
    "selftext": "Hi all. I am an MBA student at Temple university and we are doing our final project looking at Machine and computer vision. I would be grateful if you would be able to fill out this survey and if possible send to anyone else that works in manufacturing. We are looking for opinions from those that currently and do not currently use vision systems. Here is the link to the survey: [https://fox.az1.qualtrics.com/jfe/form/SV\\_0cEBnNUQ9jnxZpI](https://fox.az1.qualtrics.com/jfe/form/SV_0cEBnNUQ9jnxZpI)\n\n Alternatively if you would like to do a short interview on your experiences, this would also be much appreciated.\n\nThanks so much!",
    "created_utc": "2024-10-31T09:35:42",
    "num_comments": 1,
    "comments": [
        "Private message me about it - glad to help"
    ]
},
{
    "submission_id": "1gggupo",
    "title": "Need help with conditional random fields ",
    "selftext": "Hi all: \n\nI am reading a paper and need to thoroughly understand it. This is the paper: https://ieeexplore.ieee.org/abstract/document/6983606\n\nI can pay. If anyone here is well versed in this and can read through and thoroughly understand/help me implement this, please DM me. Thanks!",
    "created_utc": "2024-10-31T08:11:37",
    "num_comments": 1,
    "comments": [
        "For CRF's, the TL,DR is they model image segmentation as a graph problem, where neighboring pixels can be considered to be connected by \"edges\", and the \"nodes\" are the labels. \nThe other main concept is unary and pairwise potential - i.e. the cost function/energy function for assigning a label to a given pixel.\nThis setup is then solved by well known graph algorithms (Graph Cuts, Belief Propagation etc)\n\nI know this is somewhat of a non-answer, but i can whole heartedly recommend using chatgpt 4o/preview and just asking \"What are conditional random fields in image segementation. explain with examples and psuedo code or actual code as appropriate\" , i just tried it and the answer is very helpful and easy to understand.\n\nfwiw, there is also the python package Pystruct : https://pystruct.github.io/"
    ]
},
{
    "submission_id": "1ggg1mx",
    "title": "Alternatives for grounding Dino?",
    "selftext": "I’m looking for a model like gdino, where there is a sort of open-vocabulary/zero-shot support, but also one that is preferably faster (and maybe smaller/less resource intensive). I looked into yolo-world but it didnt support the open-vocab part quite like I wanted to (e.g. instead of detecting all apples in a scene, I would want to detect “apple on table” which gdino is much better at compared to yolo world from what I’ve tested). \n\nOr should I just maybe fine-tune yolo world to do what I want it to do? ",
    "created_utc": "2024-10-31T07:36:50",
    "num_comments": 7,
    "comments": [
        "The other two i’ve tried is owl-vit and MViTs but I am not sure if they are that much faster.",
        "Grounding Dino api calls?",
        "Maybe YOLO-Worldv2?  \n[https://docs.ultralytics.com/models/yolo-world/#available-models-supported-tasks-and-operating-modes](https://docs.ultralytics.com/models/yolo-world/#available-models-supported-tasks-and-operating-modes)",
        "maybe try yolo world? ",
        "[deleted]",
        "Bruh",
        "Isn't that just grounding dino inputting bboxes to SAM?",
        "oh I read it now lol. Have you checked out autodistill?  https://github.com/autodistill/autodistill?tab=readme-ov-file#object-detection \nThat project has a list of models maybe one of those works. "
    ]
},
{
    "submission_id": "1ggajcv",
    "title": "Anyone got ideas on how Claude Computer Use picks coordinates?",
    "selftext": "Title",
    "created_utc": "2024-10-31T02:35:26",
    "num_comments": 11,
    "comments": [
        "you mean how it references which UI elements to interact with?",
        "yes exactly\n\nHow does it know to move the mouse to those coordinates?\n\ngiving other multimodal LLMs screenshots and asking for button coordinates doesn't work - and the claude blog on computer use explicitly says they trained a new kind of model for this",
        "I am not sure about how they did it but a straightforward method would be to train a model to perform UI element detection given a screenshot (really easy to create a dataset for that, idk if there is already something public about it). Then you can put that information in the LLM context (i.e., write the box and context of each element in the prompt)",
        "but how do you train an ai that can identify ui elements when the search space of possible UIs is so large \n\ni'm skeptical that it's really easy to create the dataset. \n\n  \ndataset is:\n\n\\* screenshot\n\n\\* action (click at x,y)\n\n  \nHow do you generate this for computer use?",
        "You are correct, this is much harder then lumett is making it out to be. I just trained a yolo model on a bunch of public datasets (rico, vins) and it perfromed terrible across other domains. (happy cake day)",
        "I want to underline that mine is pure guessing/what I would do if someone order me to make such pipeline and forcing me to not to think about it more.\n\nYou can get a lot of UI from the web, and find clickable/interactive elements by inspecting the DOM. Also extract the text about it and have a guess about what the element will do if you interact with it (i.e., you find a link on a div with class menu? Probably the link will forward you to a page named as the text inside the link. You find a form and submit button? Each input of the form asks you to input something and the button submit the data)\nThis is not super trivial, I'll also try a pretrained LLM if it can extract some info directly from HTML instead of coding strict rules.\nOf course you can get a bounding box of each element easily as well as take a screenshot.\nBy doing this you should have a dataset composed of:\n- Screenshot\n- List of elements and their action (described as text? idk right now which is the best way to represent that kind of action\n- each element would also have text associated as well as coordinates\n\nNot sure if I am missing something",
        "Using a GAN to create feasible UI elements.\n\n*I have no idea if they did this. But it’s what I would do combined with general computer vision augmentations.\n\nEdit more context: do we know that is the dataset? Or could a computer vision model be used to find the elements, and a multimodal model able to encode that with image to add context to the found elements then from there decide its action. I honestly haven’t looked have they described their entire process?\n\nWhat lumett said.",
        "so what are we gonna do? :) how do we get something working?",
        "I didn't explore that feature of Claude deeply yet, not sure how much it can generalize that idea of \"interactive elements\" and \"actions\", like could it play a game? Can it drag and drop stuff? How accurate is it? How many times does it fail to detect an element or misunderstand an action? Probably getting an answer to these questions could provide more precise ideas on which kind of data they might have used",
        "My plan has just been to label a bunch of data in my specific domain of desktop applications, but right now as a proof of concept i have been using c2 pattern matching for the element detection. Honestly I dont really know ho wthey did it besides just labeling a shit ton of  data manually. Whatever they did theyre definely not gonna tell you becuase that what gives them competitive advantage. Ngl, i think if they just released an api that could just do the ui detection taht would be super usefull and they could keep there cometitive advantage. Anyway these are just all my thoughts jumbled down incoherently",
        "Thanks for your reply :)"
    ]
},
{
    "submission_id": "1gg3s58",
    "title": "Any better alternatives to OmniParser?",
    "selftext": "Tried out omniparser and it's pretty decent, but it misses some stuff. Also, I'd like something that can recognize boxes / layouts instead of just icons / text",
    "created_utc": "2024-10-30T19:02:53",
    "num_comments": 2,
    "comments": [
        "I think it's the best open source model for GUI detection right now. Maybe you can try changing the thresholds a bit to see if that makes it detect everything you want."
    ]
},
{
    "submission_id": "1gg09lh",
    "title": "Take this survey to help out an intern at a robotics startup <3",
    "selftext": "We are making a robot for you: [https://forms.gle/ggVetcDios9m15yV8you:](https://forms.gle/ggVetcDios9m15yV8you:)",
    "created_utc": "2024-10-30T16:15:23",
    "num_comments": 2,
    "comments": [
        "Why would you need the email?",
        "So, I can tell you all when we launch and shoot you more information about it."
    ]
},
{
    "submission_id": "1gfz69j",
    "title": "Problems with opening a video",
    "selftext": "Hi!\n\nI recently started my adventure with computer vision. I wrote some code that was supposed to use YOLO algorythm working with GPU (I am using nvidia cuda), encountered whole lot of errors trying to open video files with it and I'm still having some problems with it - it seems to have problems with reading the frames. The code is down below. I spent couple hours with chat gpt and scrolling through internet in search of help but nothing worked:(, also checked the directory, the video resolution, it seems to be fine. Do you have any idea how to repair it? I will be grateful for any kind of help!\n\n  \nP.S. ffmpeg seems to have no problems with localizing and opening the file through command prompt\n\n    import random\n    import threading\n    import cv2 as cv\n    import numpy as np\n    from ultralytics import YOLO\n    import torch\n    import time\n    import subprocess as sp\n    import os\n    \n    cv.setNumThreads(1)\n    device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n    print(f\"Using device: {device}\")\n    \n    # Load the class list\n    with open(\"utils/coco.txt\", \"r\") as my_file:\n        class_list = my_file.read().strip().split(\"\\n\")\n    \n    detection_colors = [(random.randint(0, 255), random.randint(0, 255), random.randint(0, 255)) for _ in range(len(class_list))]\n    model = YOLO(\"weights/yolov8n.pt\", \"v8\").to(device)\n    \n    def read_video_ffmpeg(path, frame_wid=640, frame_hyt=480):\n        command = ['ffmpeg', '-loglevel', 'error', '-i', path, '-f', 'image2pipe', '-pix_fmt', 'bgr24', '-vcodec', 'rawvideo', '-']\n        print(\"Running FFmpeg command:\", \" \".join(command))  # Debug print\n        pipe = sp.Popen(command, stdout=sp.PIPE, stderr=sp.PIPE, bufsize=10**8)\n    \n        while True:\n            raw_image = pipe.stdout.read(frame_wid * frame_hyt * 3)\n            print(f\"Raw image length: {len(raw_image)}\")\n    \n            # Check if FFmpeg gave any error\n            err = pipe.stderr.read().decode()\n            if err:\n                print(\"FFmpeg error:\", err)\n                break\n    \n            if not raw_image:\n                print(\"End of video stream or no data received.\")\n                break\n    \n            try:\n                frame = np.frombuffer(raw_image, dtype='uint8').reshape((frame_hyt, frame_wid, 3))\n                yield frame\n            except ValueError as e:\n                print(f\"Error reshaping frame: {e}\")\n                break\n    \n        pipe.stdout.close()\n        pipe.stderr.close()\n        pipe.terminate()\n    \n    class Video:\n        def __init__(self, src=\"D:/OPENCV/videos/DJI_0302.MP4\"):\n            self.src = src\n            \n            # Check if the video file exists\n            if not os.path.isfile(self.src):\n                print(f\"Error: Video file does not exist at path: {self.src}\")\n                return  # Stop initializing if file does not exist\n    \n            self.frame_wid = 2720  # Update frame width\n            self.frame_hyt = 1536  # Update frame height\n            self.frame_gen = read_video_ffmpeg(self.src, self.frame_wid, self.frame_hyt)\n            self.frame = None\n            self.running = True\n            threading.Thread(target=self.update, daemon=True).start()\n    \n        def update(self):\n            while self.running:\n                try:\n                    self.frame = next(self.frame_gen)\n                    print(\"Frame read successfully\")\n                except StopIteration:\n                    self.running = False\n                except Exception as e:\n                    print(f\"Error updating frame: {e}\")\n                    self.running = False\n    \n        def read(self):\n            return self.frame\n    \n        def stop(self):\n            self.running = False\n    \n    video_stream = Video(src=\"D:/OPENCV/videos/DJI_0302.MP4\")\n    fps_limit = 10\n    \n    while video_stream.running:\n        start_time = time.time()\n        frame = video_stream.read()\n    \n        if frame is None:\n            print(\"No frame to display\")\n            break\n    \n        detect_params = model.predict(source=[frame], conf=0.25, save=False)\n    \n        if detect_params:\n            boxes = detect_params[0].boxes\n            for box in boxes:\n                clsID = int(box.cls.cpu().numpy()[0])\n                conf = box.conf.cpu().numpy()[0]\n                bb = box.xyxy.cpu().numpy()[0]\n    \n                cv.rectangle(\n                    frame,\n                    (int(bb[0]), int(bb[1])),\n                    (int(bb[2]), int(bb[3])),\n                    detection_colors[clsID],\n                    5,\n                )\n    \n                cv.putText(\n                    frame,\n                    f\"{class_list[clsID]} {round(conf * 100, 2)}%\",\n                    (int(bb[0]), int(bb[1]) - 10),\n                    cv.FONT_HERSHEY_COMPLEX,\n                    1,\n                    (255, 255, 255),\n                    2,\n                )\n    \n        cv.imshow(\"Object Detection\", frame)\n        elapsed_time = time.time() - start_time\n        frame_delay = max(1, int((1 / fps_limit - elapsed_time) * 1000))\n        if cv.waitKey(frame_delay) == ord(\"q\"):\n            break\n    \n    video_stream.stop()\n    cv.destroyAllWindows()",
    "created_utc": "2024-10-30T15:26:52",
    "num_comments": 3,
    "comments": [
        "To infer on video files in `ultralytics` you just need a few lines\n\n```\nfrom ultralytics import YOLO\n\n#Weights are downloaded automatically, you don't need to manually download\nmodel = YOLO(\"yolov8n.pt\") \n\nresults = model(\"D:/OPENCV/videos/DJI_0302.MP4\", stream=True)\n\nfor result in results:\n    # Do your processing with result\n    result.show()\n```\n\nAlso I would recommend not using ChatGPT and instead refer to `ultralytics` docs. The docs site has an \"Ask AI\" option which works better than ChatGPT. ChatGPT doesn't know `ultralytics` API well.\n\nhttps://docs.ultralytics.com/modes/predict",
        "Are you giving an absolute path to read_video_ffmpeg? But more generally on the video reading, why not use the built in cv.VideoReader?",
        "i was getting this error all the time, thought it couldve helped: Assertion fctx->async\\_lock failed at libavcodec/pthread\\_frame.c:173"
    ]
},
{
    "submission_id": "1gfx8lr",
    "title": "Book recommendations",
    "selftext": "Does anyone have a recommendation for a theory-based, up-to-date book on Computer Vision based on deep learning techniques? My main topic of interest is object detection. ",
    "created_utc": "2024-10-30T14:03:08",
    "num_comments": 1,
    "comments": [
        "Search  for (i)Computer Vision: Algorithms and Applications by Richard Szeliski (ii) Programming Computer Vision with Python by Jan Erik Solem.\n\nCaveat: I got this recommendations via i think Machine learning Master blog post and I have not read any of them."
    ]
},
{
    "submission_id": "1gfrtc7",
    "title": "Im building an online platform for people in ai that want to build and collaborate on  innovative projects !",
    "selftext": "Hi there :)\n\nI got something cool to share with you, over the past few months i have been running around trying to find a way to make a dream come true\n\nIm creating a online hub for people in ai that care about technological innovation and having a positive impact by building and contributing on projects\n\nThis is hub will be a place to find like minded people to connect with and work on passion projects with.\n\nCurrently we are coding a platform so that everyone can find each other and get to know each other\n\nAfter we got some initial users we will start with short builder programs where individuals and teams can compete in a online competition where the projects that stand out the most can earn some prize :)\n\nOur goal is to make the world a better place by helping others to do the same\n\nIf you like our initiative, please sign up below on our website !\n\nhttps://www.yournewway-ai.com/ \n\nAnd in some weeks, once we're ready we will send you a invite to join our platform :)",
    "created_utc": "2024-10-30T10:15:15",
    "num_comments": 9,
    "comments": [
        "Isn’t this what kaggle is?",
        "And who gets the credit/money for the ideas? That's the biggest problem I can think of with such a thing.",
        "Kaggle has a similar approach but in their own domain. More specifically geared towards data science",
        "It will be a builders program where people can build individually or in a small team, by the time we have actual people completing the program we will have figured out how to do the funding. But probably sponsorship based and having people for looking at the projects :)",
        "How do you define the difference between AI (your platform) and data science (kaggle)?",
        "yeah, similar to a hackathon within a big tech company. but how do i know you're not just looking to sell the projects and exploit the people doing the work ?\n\nsorry, i really like the ideas of community, teamwork, sharing and all that good stuff, i really do. but after years in this industry, i realize that those words are spoken the loudest by liars and power hungry psychopaths, especially ones running big organizations :)",
        "We're more geared towards the people that drive the projects. Kind of like a repository of people that share the interests for technological innovation.\n\nKaggle is geared towards existing teams and groups ( more established ) that compete in their programs. Our platform can be seen more as a pre accelerator program for ai / tech projects where working together and connecting is the main focus rather than the competition side",
        "Its a good point you make, i have to find a way to safely promote it and see the projects myself\n\nI myself have no interest in taking other people their ideas since i have plenty of my own. But i want to help people build their stuff since i know how valuable it is and always had been for me",
        "can you help me make a chatgpt clone ?or somewhat close to chatgpt I m new in ai domain I m doing btech will go for mtech soon so before going in mtech want to   get an idea how mtech in cse is like"
    ]
},
{
    "submission_id": "1gfqpzn",
    "title": "Camera rotation degree",
    "selftext": "Hi, given 2 camera2world matrices, I am trying to compute the rotation degree of camera from first image to second image, for this purpose I calculated the relative transformation between the matrices(multiplying second matrix by the inverse of the first), and took the sub matrix(:3,:3 of the 4\\*4 relative transform matrix), I have the ground truth rotation value but for some reason they do not match the Euler degrees I compute using scipy's rotation package, any clue what I am doing wrong mathmatically?\n\n\\*the values of cam2world are the output obtained from Dust3r if that makes a difference",
    "created_utc": "2024-10-30T09:29:37",
    "num_comments": 4,
    "comments": [
        "Your approach to compute the relative rotation is fine of course, but:\n\nthere are 12 different conventions for Euler angles. Most people use yaw-pitch-roll standard from aviation.\n\nWorse, maybe the ground truth angles were computed in some other reference frame? Even if the axes of the two frames are aligned, that still leaves 24 possible \"simple\" rotation matrices.\n\nTrying simple rotations about an axis, for debug purposes, can help, but it's also tricky because if you don't test very very very many rotations, there are many cases where you can think you got the right standard and reference frame. So be patient and thorough!",
        "Why do you want euler angles. Unless you have a very good reason they are the worst way to represent a rotation. What do you want to know exactly?",
        "Will do, thanks!",
        "I want to test the accuracy of the method I used in comparasion to gt vals, regardless of the fact that I wont actually be using them to implement something, I want to know the rotation around each axis seperately for testing purposes"
    ]
},
{
    "submission_id": "1gfpgi5",
    "title": "I hate my amd gpu",
    "selftext": "hello guys, my first post on here and I just want to say I freaking hate my amd gpu (running on windows) so damn much, I have been trying for 6 weeks now to train a simple face detection model using a public dataset, but my amd gpu refuses to elaborate! I wish I knew how bad amd was when it comes to machine learning and computer vision before I bought it 😔😔 I can’t even download linux due to other reasons, I also tried directML but that failed miserably for some reason, not really looking for help but if anyone is considering buying a build for computer vision (which I was not when I got mine) please avoid amd at all costs.",
    "created_utc": "2024-10-30T08:36:50",
    "num_comments": 25,
    "comments": [
        "what exactly are you using to interface with the gpu ?? CUDA?? ZLUDA?? what exactly .. \n\nalso the error message is not really thrown by your amd gpu, its from pytorch ... basically saying that pytorch cannot see any CUDA devices..",
        "Don't you think the problem is more you than your gpu? 😐",
        "Do you have ROCm installed?",
        "Even when you install pytorch with cuda and have a gpu, torch might say cuda is unavailable.\n\nBasically cuda version you have and the cuda version your gpu support sometimes will be different.\n\nhttps://stackoverflow.com/questions/60987997/why-torch-cuda-is-available-returns-false-even-after-installing-pytorch-with/61034368#61034368\n\nLook at the top voted answer. Might save you some trouble if you run into that!!",
        "I recommend you just use online tools. There are a bunch of free colab notebooks you can use to train models ",
        "Why don t you use torch-directml instead? it works on any DX12 gpu",
        "if you are serious in to DL, please sell your card and buy an used nvidia card, at least a RTX3060 12GB as a start, used RTX3090 would be better",
        "This might help you: https://medium.com/@anvesh.jhuboo/rocm-pytorch-on-fedora-51224563e5be\n\nTry exporting the environment variable based on the architecture of your GPU",
        "thing is I downloaded the cpu only version of pytorch and I am slightly undereducated when it comes to cuda, zluda and whatever that is, all I know is that cuda cores is what more recent nvidia gpus have, sorry.",
        "Why this ferrari is not able to tow other cars if it has a lot of power?",
        "thank you for the motivational words!",
        "isn’t that only for linux?",
        "wow tysm this is amazing, will use this if I get a nvidia gpu!",
        "If you are uneducated about Cuda then it doesn't matter if you have Nvidia gpus or AMD gpus. Pytorch cpu will NOT detect gpus no matter its amd or nvidia",
        "Bro you wrote that you have been trying for 6 weeks \n... anyway idk much about and gpus either , so good luck ig.",
        "Otherwise if you have a second slot just get a used 1080ti. For some playing around that should do it and if longer training time is no big issue.\n\nI wouldn't say it is impossible to get ROCm to work for your current GPU, but that probably needs some more tinkering on drivers.",
        "Not sure, but I know Rocm needs to be installed for GPU accelerated PyTorch on AMD gpus.",
        "ROCM is only available on Linux for Pytorch, yes.\n\n[https://pytorch.org/get-started/locally/](https://pytorch.org/get-started/locally/)",
        "You can use WSL if ur using a windows OS (note that u would have to reconfigure and download PyTorch and so on with the Linux versions if u take this route)",
        "do you have any useful material I can use to learn a bit more about cuda, my professor really didn’t provide me with a good starting point, and a tight deadline.",
        "looking for a used 1080ti, thank you.",
        "https://www.nvidia.com/en-us/on-demand/session/gtc24-s62191/?playlistId=playList-d59c3dc3-9e5a-404d-8725-4b567f4dfe77\n\n\nDon't get too deep. Understand your requirements, install it, and move on to pytorch programing unless you want to become cuda programmer",
        "Check if you have a second PCIE x16 slot on your mainboard first, I don't know how much impact actually available lanes (maybe 4, 8 or even 16) does to Inference and training, but you can do some research on that. \n\nAgain check if your Motherboard can support two GPUs!!\n\nAnd if you do this you probably need linux since two GPU drivers on windows might get in conflict (and probably will).\n\nMaybe your best solution is to sell your GPU even and buy an nvidia one, if you want to go that route.\n\nEdit: typos",
        "kk tysm",
        "considering the path I am taking, switching to an Nvidia gpu seems like the right choice."
    ]
},
{
    "submission_id": "1gfmlzc",
    "title": "D-FINE: Redefine Regression Task of DETRs as Fine‑grained Distribution Refinement",
    "selftext": "\"D-FINE is a powerful real-time object detector that redefines the bounding box regression task in DETRs as Fine-grained Distribution Refinement (FDR) and introduces Global Optimal Localization Self-Distillation (GO-LSD), achieving outstanding performance without introducing additional inference and training costs.\"",
    "created_utc": "2024-10-30T06:32:29",
    "num_comments": 1,
    "comments": [
        "I’m surprised this isn’t everywhere. Yolo hasn’t been really dethroned in terms of accuracy and velocity in years. \n\nThe performance here is great."
    ]
},
{
    "submission_id": "1gflktp",
    "title": "I created a course on Coursera called Hands-on Data Centric Visual AI and made a series of cringey videos to promote it. ",
    "selftext": "You can check it out here: https://www.coursera.org/learn/hands-on-data-centric-visual-ai",
    "created_utc": "2024-10-30T05:41:57",
    "num_comments": 3,
    "comments": [
        "This course seems quite intuitive, and I'm definitely interested in exploring it further. However, I believe your initial pitch might be slightly off. In my opinion, a data-centric approach is indeed crucial and should be prioritized. That said, it's also important to recognize that experimenting with different model architectures and adjusting parameters can help address some of the challenges that arise from data issues.",
        "its definitely cringe",
        "I agree with your point on experimentation, I just feel there is something to be said about fixing the model architecture and iterating on the data. Then, perhaps, moving to a different architecture."
    ]
},
{
    "submission_id": "1gfjqmk",
    "title": "Jetson Orin Nano 8GB: different YOLO fps with same configurations",
    "selftext": "I want to measure fps to benchmark different versions of YOLO and I do this by running inference 5 times on a video and then averaging fps for each frame. To be sure that this task is not interrupted by the scheduler, I put `sudo nice -n -20` before `yolo predict` and I check processes with `jtop` (and ofc power mode is fixed). However, under these conditions I sometimes get big differences for the same model (i.e. 50<->75 fps).\n\nDo you know which is the reason? Temperature? Or is there a more robust way to achieve my goal?",
    "created_utc": "2024-10-30T03:56:04",
    "num_comments": 11,
    "comments": [
        "Did you convert the model to ONNX format or are you using a pure PyTorch model? And are you using TensorRT?",
        "Jetson by default throttles very quickly. It will show \"System throttled due to Over-current\" when it does",
        "It could be the auto scaling in the GPU/CPU clock frequencies. \nJetsons are by default power efficient, maybe your tasks are on the threshold of the frequency step. Try forcing them all to their max.",
        "I would make sure to enable the highest power mode and then enable jetson clocks. \n‘sudo nvpmodel -m 0’\n‘sudo jetson_clocks’\n\nAdditionally, ultralytics (the source of yolo predict I am assuming) uses PyTorch to allocate memory which is not as efficient as pure CUDA. I would try benchmarking the model with the trtexec tool. \n\nYou could also try a library such as: https://github.com/justincdavis/trtutils",
        "Yes, it's always TensorRT models",
        "I'm now measuring performance in 7W (pwr mode 1) with jetson_clocks inactive because I want to assess it at the very low end. Do you think that going 15W with active clocks would eliminate this oscillation?",
        "Important note: you cannot use the ultralytics compiled engine in trtexec or other tools since they embed metadata. You could check out: https://github.com/triple-Mu/YOLOv8-TensorRT\nTo enable running yolov8 with pure tensorrt",
        "Thanks, but I need to check fps for the lowest power consumption (i.e. 7w, no jetson clocks) so it is not really useful to modify it.\n\nRegarding memory allocation, until now I only used trtexec for building and serialising model (actually I then switched to tensortt API to be able to manage int8 quantization directly from images and not caches). Didn't know it also allows benchmarking, I'll try this and also the library!",
        "I use my own script (based on tensorrt APIs) to build and serialize the engine, if this is what you mean",
        "Ah, I misunderstood. You will have to mess around with making your power modes most likely. Since the low power modes have variable clock speed I believe you can make your own with fixed clocks within the power profile.",
        "Oh, yes, this could be an option. But it depends on how much focus (and effort) I want to dedicate to this thing, maybe it is enough to just run inference multiple times and check which are the most common values (i.e. remove the >70fps outliers, since many tests ended with around 35 for fp16 and 50 for int8 precision)"
    ]
},
{
    "submission_id": "1gfhqca",
    "title": "Gathering training data from google maps",
    "selftext": "Hello CV,\n\nI'm currently in the process of training YOLO to identify which industrial complexes does NOT have solar panels on their roof. I want it feed it training data of google maps satellite images, but I'm unsure how to go about this.   \nThe questions that I have: \n\n\\- How do I determine the correct size (pixel) for my training data?\n\n\\- Is there any available API that can help me make the process easier? \n\n\\- Is there a way to use the globe/3d view to help identify the model identify if the roof is flat or slanted?\n\nThank you, hope someone can help me\n\n",
    "created_utc": "2024-10-30T01:28:21",
    "num_comments": 1,
    "comments": [
        "Here’s a project you can look at for some starter inspiration: https://alexhalcomb.github.io/"
    ]
},
{
    "submission_id": "1gfhegv",
    "title": "Android : MobileFaceNet performance is abysmal. What can i do to up it? ",
    "selftext": "I was tasked with a project at work to build a facial recognition app that runs on Android tablets for one of our clients on a tight deadline. The first thing I did was detect the face on the device, send it to a local server, get DLIB to create an embedding from the captured face THEN compare the embedding with the list of saved face embeddings. This worked (albeit with max. achievable latency), and the effective accuracy was about 50-60%.\n\nAfter deploying this solution i started working on the app again, to enable on-device recognition using TFLite and MobileFaceNet - (Normalized embeddings and L2Normalization). It works BUT the accuracy is like -30%.\n\nAt the moment i am using **one** frontal picture of each employee, can I increase the number of comparison (base) pictures per employee?  \nI (think) i realized that base pictures taken in front of a dark background tend to yield more accurate comparisons - is this the case (theoretically)?  \nAny other suggestions would be bloody appreciated - Oh and by the way, prior to this project i had no knowledge of CV, so please explain things like you are talking to a ~~five~~ four year old.",
    "created_utc": "2024-10-30T01:02:24",
    "num_comments": 1,
    "comments": [
        "All of the Open source face recognition models are terrible.\nThat's why private companies train their own on millions of faces."
    ]
},
{
    "submission_id": "1gfhao0",
    "title": "Control Gimbal(reCamera) using LLMs(Locally deployed on NVIDIA Jetson Orin)! Say turn left at 40 degrees, it works! ",
    "selftext": "",
    "created_utc": "2024-10-30T00:54:57",
    "num_comments": 8,
    "comments": [
        "Very cool! This is probably funny only to me, but I think of left and right from an anatomical perspective and so its direction of movement was opposite of what I anticipated.",
        "very nice! i'll be doing exactly this kind of thing in a few days with some gimbal motors.",
        "Missed a chance to scream at it \"zoom! enhance! zoom!\" ... :)",
        "What is the latency on the command processing to moving the camera?",
        "Which part of this is LLM? Voice to text? Don't we have spevialized model for voice to text?",
        "poe or usb?",
        "Haha we set it to move in the speaker's direction as a standard",
        "Same.\n\nIs there actually a way of wording this command that's irrespective of perspective? My mind is blank. I keep thinking port/starboard/clockwise/anticlockwise, but they have the same issues"
    ]
},
{
    "submission_id": "1gfg8yy",
    "title": "Basler camera -opencv",
    "selftext": "Hey Hi I’m developing my first project with opencv using a basler camera but I cannot achieve image acquiring: it opens the image and cracks instactly (doesn’t respond anymore)\n\nIs there any guide anywhere I can use? \n\nAlso I can’t see the camera on Pylon viewer, but it runs in my python code in spyder (the one that cracks)\n\n\n",
    "created_utc": "2024-10-29T23:30:34",
    "num_comments": 1,
    "comments": [
        "You should use pypylon and pylon application made by Basler,\n\n\nyou should use pip install pypylon \nAnd this is the GitHub page:\nhttps://github.com/basler/pypylon"
    ]
},
{
    "submission_id": "1gfcgbi",
    "title": "Question about relate github project, or possible approaches on point detection for lane line detection",
    "selftext": "Hi everyone, I was assigned the task of lane detection. However, after searching the internet, I found many methods, mainly the lane segmentation method or polyline-based detection since I only want is to predict the dot on the lane, like in the attached image.  Can you suggest any model or any method that already worked on this?\n\nThank you very much\n\n*Processing img ocgy2almxuwd1...*",
    "created_utc": "2024-10-29T19:31:42",
    "num_comments": 0,
    "comments": []
},
{
    "submission_id": "1gf96sv",
    "title": "Help - 360 degree and CV",
    "selftext": "I am willing to start a small academic project that takes in a street view from a Maps' API and then do some processing on it. For example, if we are passing by a monument or any building that is of pretty much importance and the crowd is pretty much covering all the space up. I would like to erase them, be it cars or people and content-fill give a clear one. \nI need help to what papers to read, if anyone has done anything similar to this. Mainly, how to project the 360 view? On what sort of plane to perform all the desired actions. Anything other help would also be helpful ",
    "created_utc": "2024-10-29T16:50:12",
    "num_comments": 0,
    "comments": []
},
{
    "submission_id": "1gf5pbq",
    "title": "Preferred Computer Vision Models - Open Soure?",
    "selftext": "Planning to make an identifier for manufacturing parts kept in a storage line with nuts and bolts of different sizes. Any recommendations?",
    "created_utc": "2024-10-29T14:13:52",
    "num_comments": 5,
    "comments": [
        "depends on what speed you're trying to do it.  there's probably a way to do it without deep learning, which would be fastest.",
        "Yolo NAS works well for small items. Else YoloX is free and convenient",
        "Exactly. OP should share examples of images. If the data is simple, you might not even need DL for this. Otherwise all recommendations will be a shot in the dark\n\n  \nThey should really add this as a rule in this sub. It's like having r/pics but without any pictures",
        "+1 for YoloNAS, I used to work there!",
        "i mean all dl models are free."
    ]
},
{
    "submission_id": "1gf24n4",
    "title": "Orange Pi 5, RK3588 and yolov9",
    "selftext": "This is my experience by far using orange pi 5 and my tries up until now in making yolov9 work on orange pi5/ RK3588 SoC\n.\nOur company uses Orange Pi5 4GB (RK3588 SoC) as the main process unit of our traffic cameras\n.\nThis boards are pact with NPU which is very useful considering our process's behind the since of the whole detection process\n.\nI decided to make 3 different models, one for detecting vehicles, one for detecting License plates and an other one for reading the plates.\nI chose yolov9 since it had more accuracy comparing with yolov10 and more speed compared to yolov8, I also chose t variant of yolov9 models since they are the lightest and probably faster on edge devices.\n.\nAfter process of making a good dataset base on company data and my best tires on normalizing the dataset, I got a good acceptable above 70% accuracy on test environment(and 60-82% in real life soon after)\n.\nAfter 3 work days of work on orange pi, I was able to boot up on OS (The company gave me a board that had already OS(some old version of PiOH the specialized Ubuntu for orange pi boards) but that had some old dependencies like onnx 1.13.0 and my newer models wasn't compatible so after checking multiple versions of the arm Linux Versions (armbian, arch, piOH etc...) I got hands on https://github.com/Joshua-Riek/ubuntu-rockchip/wiki\nWhich helped me boot up correctly to orange pi(In this process I even though I damaged a board since this shitty boards are moody and sometimes they simply don't want to boot to SD card or nvme or show red light so we found out they are alive)\n.\nAfter that, I made a simple python code, for taking frames from cameras and trying to detect object via my models (vehicle detection->cut the vehicle image-> send to license plate detection model->detect the lisence plate -> cut lisence plate -> send to OCR model -> read license plate, and then save images of the car, lisence plate nad the OCR output.\n.\nAfter trying for 1 week on trying different types of approach on importing my .pt model to .rknn, I found out, YOLOv9 models are simply not compatible with Rk3588 NPU's since Only models saved in torch.jit.trace can be used and YOLOv9 isn't.\nyet you can't use any other types of YOLO models but those that cosumized to be able to convert to rknn\nThis was my experience, I hope it help others to do not fall in this shitty hole of not understanding wtf doc and manuals said in rknn-toolkit2",
    "created_utc": "2024-10-29T11:45:39",
    "num_comments": 14,
    "comments": [
        "Caveat that it's been a while for me, but when working on devices like this, I typically convert the yolo models to onnx format and run the appropriate onnxruntime and onnxruntime_extension package without any python. Haven't used this exact board, but I think v5 and whichever recent one was based on that (v10 or v11) work well this way. \n\nV8 and v9 have weird issues with the output formats if I remember correctly.",
        "I had no problem putting together an [ALPR example here](https://github.com/swdee/go-rknnlite/tree/master/example/alpr) using RK3588.\n\nThe RKNN Model Zoo does not provide a YOLOv9 example, so you need to go with v5, v8, v10, or X if you want the easy route.   Keep in mind you need to use Rockchips ultralytics fork for training your YOLO models since they have been optimised for the RK3588 NPU.",
        "For those who wanted the proper model. This is the model I used...\nhttps://github.com/kaylorchen/yolov10\nI still don't know if it works so when my training was done, I will rich out and give you the results.",
        "Thanks I will check them out.",
        "Oh. I saw that ultralytics fork.\nShould I also use rknn-toolkit2 (2.2.0 was the newest version)\nOr stay with Rockchip 1.6.0 ?\nIn Rockchip version page on GitHub, they said \"this repository is old, the newer version is airockchip/rknn-toolkit2 but I saw some updates here and there in 1.6.0 version that Rockchip provided",
        "Can you please explain it more if you have time ? I don't really understand what's going on on this airockchip/ultralytics,(I'm noob)",
        "Since I was trying to Train yolov10n I'm now using this repository ultralytics/\nhttps://github.com/THU-MIG/yolov10\nI don't know if it works or not.",
        "Are you giving up on the RK3588 (I'm using it for a problem in a diff domain) and switching to something else?\n\nI've also been trying to use it's GPUs (remember current NPUs arn't fast, they are power efficient) via OpenCL + MLC-LLM but that's not been working out either (MLC is a pain to standup).",
        "Yes you need to use [rknn-toolkit2](https://github.com/airockchip/rknn-toolkit2/) with the RK3588 and using the 2.2.0 version is best.\n\nYou have a long pipeline so I would recommend changing from YOLOv9 to YOLOv8 for your first step of \"vehicle detection\".\n\nFirst get the [ultralytics fork](https://github.com/airockchip/ultralytics_yolov8) and train your model.\n\nUse the `ultralytics/engine/exporter.py` script in this repository to convert the PyTorch model to ONNX.\n\nThen setup rknn-toolkit2 and use the [conversion script](https://github.com/airockchip/rknn_model_zoo/blob/main/examples/yolov8/python/convert.py) to convert from ONNX to RKNN format.\n\nOnce you get that working, then update the models in the rest of your pipeline for license plate detection, OCR etc.",
        "Yes for YOLOv10 that is the correct repository.  Have you seen the examples in [Rockchips model zoo](https://github.com/airockchip/rknn_model_zoo/tree/main/examples/yolov10) as they tell you which repository to use for training?",
        "Nope unfortunately giving up is not an option right now.\n.\nSince rknn-toolkit- zoo had some examples of yolov8 and yolov10 and I also found some models that are customized for converting to rknn format. I will try them out first. (Right now in training process of yolov10n model for one of my detections)\n.\nI checked GPU (read about it) it's Mali-610 if I remember correctly (I also has seen some Huawei phones with that GPU) that's not really an option here, since as what I found, GPU performance is somewhere around npu if it wasn't lower.\n.\nFor your usage, I'm not really familiar llm's I saw some repositories that was trying on setting up the llm's on this SOC/NPU, but personally didn't check them out.(They mainly used Llama 7B for comparison)\n.\nI don't really recommend using this type of hardware for LLM since thay are so chunky and unstable(we already have many problems with it's stability in different temperatures (they will get hot so fasti recommend that you use heatsink even for booting it up so it doesn't get damaged)",
        "Thank you so much.\n🙏🙏🙏🙏🙏🙏🙏\nI was processing like some drunk dude, testing everything until it works.\nThank you again 🤝",
        "No unfortunately.\nI was looking for this in wrong location.i was trying to find my way in Rockchip papers and docs.\nI saw Rockchips model zoo, I saw that the YOLOv9 is skipped and yet, wasn't expecting this situation.\n.\nThanks it helped very much.",
        "And also excuse me for my English. I'm so tired I don't even understand wtf I'm writing right now 😂💔"
    ]
},
{
    "submission_id": "1geybf9",
    "title": "Best YOLO Model for Detecting on Raspberry Pi with Video Streaming?",
    "selftext": "Hey everyone! For my capstone project, I'm building a system to detect people in wheelchairs through video streaming, but here's the catch: it has to run on a microcontroller like a Raspberry Pi 4 or 5. I’m pretty new to machine learning and YOLO models, so I could really use some advice on a few things:\n\n1. **Best YOLO Version**: Which YOLO version is best suited for the Raspberry Pi that won’t lag or stutter?\n2. **Video Stream Compatibility**: If I train a YOLO model on a dataset of wheelchair images, will that also work effectively on a live video stream?\n3. **Dataset Annotation**: I have a 10,000-image dataset. Do I need to manually annotate every single image, or can I label a few, and the model will learn the rest on its own?\n4. **Training on Colab**: Do I need Colab Pro to train a YOLO model, or can I get by with the free version?\n5. **C++ vs Python for YOLO**: Will there be a noticeable performance difference if I run YOLO in C++ compared to Python on the Raspberry Pi?\n\nThanks in advance for any help! Any advice or resources would be really appreciated.",
    "created_utc": "2024-10-29T09:08:27",
    "num_comments": 7,
    "comments": [
        "The Pi 4/5 doesn't have enough power to run YOLO in real time.   However on a Pi 5 you can get the [AI Hat](https://www.raspberrypi.com/products/ai-kit/) with a Hailo-8L accelerator that would allow you to do that.\n\nAs for your questions:\n\n1) With the AI Hat it has enough power to run any version of YOLO supported by Hailo's SDK.\n\n2) A video stream is just a series of images (frames) played back at 30 FPS, so the YOLO model processes each image frame and doesn't care where it came from (video source/camera/video file etc).\n\n3) You need to annotate enough images to train an accurate model.   You can then use your model to auto label addition images.  Some labelling platforms (paid for services) offer this feature.\n\n4) Personally don't use Colab so don't know. I just write a training script in Python/Pytorch and run locally on my workstation.\n\n5) C++ or Python doesn't matter as the Python versions are just bindings to the C++ libraries underneath.   Pick the language based on which one you know best.    However most tutorials and examples online are written in Python.",
        "Radxa X4 or lattepanda boards have enough power for real-time inference",
        "Have you checked executorch? It allows for runtime optimization (quantization, memory planning, etc) and finally executes inference in a compact c++ engine ideal for embedded systems. Beta was just launched few weeks ago, but it is worth having an eye on it 🤓",
        "try jetson boards from nvidia they are more powerfull than raspberri pi   \nyou wont be able to achieve better FPS  \nor what you can do is attach a pc or any other powerfull hardware with raspberry pi then stream frames onto that machine inference it there and then just forward the detections to raspi for future actions",
        "You think alternative to running yolo on microcontroller, instead i get cheap esp32 with wifi, i run the yolo mosel on a cloud and just communicating via websocket api?",
        "Also Jetson boards!",
        "That is possible, but probably not ideal as the industry is moving away from inference on the cloud to doing it on the Edge so it would all depend on your use case.    \n\nThe important questions are;   What resolution video do you require, how any FPS, you can then calculate the bandwidth required to push the video over wifi to the cloud for inference.   How reliable is the wifi/internet connection as if it goes down that means your product goes offline too.  Is the added latency to doing it over the cloud acceptable.\n\nYou could reduce bandwidth by scaling down the camera images to 640x640 (input size of the YOLO model) and send those, but does the microcontroller have enough CPU power to handle the scaling."
    ]
},
{
    "submission_id": "1gey7ow",
    "title": "Stitch photos together with various angles and resolutions",
    "selftext": "Hi there, I have a use case where I need to catalog my books collection, and I would like to take photos and stitch them together to create a \"digital twin\".\n\n1. All the photos are taken from the same camera (phone camera)\n2. The photos are taken from various distance (some close up to books to get better resolution for small prints and others not)\n3. The photos are taken mostly with the same straight angle vertical to the shelf \n4. I cannot guarantee that all the books are captured and at the same time, the same book can show up in many photos.\n\nTo demonstrate, here is a sketch of the idea: \n\n(red is the book shelf with 3 rows, 1 \\~ 10 are different photos I have taken, 11 is the farthest photo) \n\nhttps://preview.redd.it/td8zwqjtwpxd1.png?width=1367&format=png&auto=webp&s=277f16d1c1937b630f8b116b409d14cad7b9f29d\n\nHere is my question, is there a way to stitch all the photos into one photo where each book can benefit from the highest resolution when possible. (for example, the last book on the top row appear in photo1, partially in photo2, and also in photo 11). \n\nBonus: if you can point me to some available tools/python library, that will be super helpful. ",
    "created_utc": "2024-10-29T09:04:17",
    "num_comments": 2,
    "comments": [
        "If you stitch things by yourself, you'll compute the transformation between pixels of the original images to pixels of the final composite. If you do that for the 4 corners of a pixel, then you can just check if that pixel will be stretched or compressed. Intuitively, the \"nicest\" pixel is the least stretched. Obviously, the pixels from image 11 cover a large area, while those of image 2,3,4 cover a small area.\n\nIn general, one can't really stitch images of a random scene from different positions into one image, that's because if you move the camera, you'll likely cause parallax, meaning the content of the different images will not match! Now, if the scene is roughly planar, then you can project all images to that common plane, and if the camera does not translate, then you can stitch onto the plane-at-infinity. Those two situations are covered by countless tutorials from openCV (random example: [https://pyimagesearch.com/2018/12/17/image-stitching-with-opencv-and-python/](https://pyimagesearch.com/2018/12/17/image-stitching-with-opencv-and-python/) )",
        "This isn’t 100% related but something about this reminds me of SAHI: https://github.com/obss/sahi"
    ]
},
{
    "submission_id": "1gex9gy",
    "title": "I’m building a targeting system for a robot to shoot a laser at a target. If I have full control over the parameters of the arena and what the targets will look like, what is the easiest way to recognize them with CV",
    "selftext": "Parameters:\nWebcam will be mounted to an arm that can rotate up, down, left and right\nArm will have a laser pointer attached to it\nLaser pointer must be able to aim and hit targets\nThere will be 2 different types of targets, one static, one moving\nI can put basically whatever I want on the targets to make my job easier\n\nMy idea was to put some sort of black and white pattern around each target that makes it uniquely identifiable, and makes it super easy for a CV alg to detect and put a bounding box on. I figured with the web cam mounted to the laser, I could just draw a bounding box on the target, find the center of it, then move the arm to put the center of the target in the center of the screen. Are there any cool existing techniques out there that do this sort of thing? We were originally just planning on using YOLO but that seems wildly overkill. Also this will be running along side some other potentially very hefty programs on an rpi5 so computational performance is a priority here as well.",
    "created_utc": "2024-10-29T08:24:27",
    "num_comments": 4,
    "comments": [
        "If you can set up the targets any way you want, you should look into AprilTags.  They are simple black and white fiducial markers -- easily printable -- that can be used to determine pose as well as scale (you know ahead of time how big the AprilTag is).  There are dozens and dozens of different tags, so you can uniquely identify objects as well.",
        "And the next thing you want is to know where John Connor is?",
        "I am working on a very similar project. But it in my case I am using one model to get a bounding box and then another model to estimate a pose. This way aiming appears to be more precise. \n\nI found that just estimating the center of bounding box is not the best approach, it can easily fooled and makes mistakes with distance increase. \n\nHowever if your targets are not ‚cunning bipedal animals‘, center aim could work fine. And it’s much faster than more complicated approach.",
        "These look like exactly what I need!"
    ]
},
{
    "submission_id": "1gevztf",
    "title": "YOLOv5 hyperparameter evolution ",
    "selftext": "I trained a YOLOv5 model with a custom dataset for 300 epochs. Then I started hyperparameter evolution with 10 epochs per evolution, and I used the default value for - - evolve, which I thought was 300. The evolution is still chugging along, and hyp-evolve.yaml says the latest generation is over 300. Is this normal? Can I just stop the evolution, or will that mess something up? This is my first time training a YOLO model. ",
    "created_utc": "2024-10-29T07:29:35",
    "num_comments": 10,
    "comments": [
        "I think you should be checking the `evolve.csv` file for the current generation, not the yaml file.",
        "The CSV has 370 rows of data, and the yaml says the latest generation is 369.",
        "Is the CSV’s first record the header row?",
        "Yes. There was one header row and 370 rows of data. I'm at 410 generations on yaml file and 411 rows of data on the CSV plus a header row. I'm assuming the CSV is one ahead because it logs the parameters for the evolution when it starts and the yaml is updated when the evolution ends. Still, since the default is 300 evolutions, I'm wondering why it's still going...",
        "Guess you’d have to look at the code to see what’s happening, if nobody else here knows. Are there any questions about this on GitHub?",
        "I couldn't find anything related to this on GitHub, so I turned to the fine folks of reddit! Looking at train.py, it should go as many evolutions as the - - evolve parameter, but I'm not 100% sure. A few days ago I tried using 30 for the parameter instead of the default, and it just kept going. Maybe it's a bug.",
        "Hmm, there isn't a default for - - evolve. It's a constant. I wonder if that's why? But I also don't know much about argparse.ArgumentParser(). Hopefully someone smarter than me stumbles across this post.",
        "Did you stop it?",
        "At 475 generations. Of course, the best generation was 90 😂."
    ]
},
{
    "submission_id": "1gev6xr",
    "title": "Image dataset management platform",
    "selftext": "Hello everyone,\n\nI am part of a team which managed thousands of image datasets stored on AWS S3 for computer vision work (training, testing and applying computer vision pipelines).\n\nI am looking for a web platform that would help me to explore and manage these image datasets.\n\nThe kind of features I am looking for are:\n\n\\* Manage authorization at the dataset level\n\n\\* Get visual inspection capability of the datasets (browse thumbnails, open specific images, zoom in/out)\n\n\\* Upload / Download datasets from the UI\n\nVisual-layer seems a good fit, even much more advanced that I am requesting in terms of features. However the pricing is not public and I feel it might be too expensive for us.\n\n2 questions for the community:\n\n\\* do you have an idea of how is working the pricing for Visual-layer? \n\n\\* do you know open-source and cheap alternatives ?\n\nThank you in advance. Let me know if you need more information for giving a proper answer.",
    "created_utc": "2024-10-29T06:54:57",
    "num_comments": 6,
    "comments": [
        "Have you looked into FiftyOne? The open source version is for local machine use and there is a teams product that covers the use case you’re describing. \n\nNote: I work at Voxel51, and if you’re interested I’m hosting a “Getting Started with FiftyOne” workshop today (Oct 30 at 9am PT): https://voxel51.com/computer-vision-events/getting-started-with-fiftyone-workshop-oct-30-2024/",
        "Not sure if it does all of those things but you could try fiftyone. It has a nice API and a range of integrations.",
        "Lightly has a very generous freemium offering. They check all your boxes but they mostly focus on automatic data curation/selection with self supervised learning, but can do data management as well. Otherwise Aquarium or typical labeling tools like CVAT or LabelBox, LabelStudio could scratch your itch?",
        "DataTorch, I know the founder",
        "Ayyyy yo! I work at FiftyOne, thanks for the vote!",
        "It is a really nice tool"
    ]
},
{
    "submission_id": "1geu6r2",
    "title": "Prepare Datasets for segmentation model",
    "selftext": "hello, i am preparing datasets for yolo segment. i wanted to do it by writing an algorithm instead of using a website. while doing this masking and labeling work, using the yolo11v model, i detected the relevant object and then applied masking. but it did not satisfy me very much. How can I do it. the images I have are not very good, so operations like threshold, erolde, dilate, blur don't work very well. how can I do it? i will do it with libraries like opencv, numpy. detecting the object with a pre-trained model and then masking it somehow felt wrong.",
    "created_utc": "2024-10-29T06:08:01",
    "num_comments": 4,
    "comments": [
        "If you have an algorithm that can mask it correctly, why would you need a model?",
        "It’s really unclear what you’re asking. \n\nTry SAM which is a model from Facebook that you can use for free to segment many different objects. It’s built into various data annotation tools, and you can also just run it directly against an image. You can use whatever you’re already using to detect the object and then SAM can produce segmentation masks that probably line up with the object. ",
        "We will train our own model. It is a normal model that we use. That's why we are preparing data."
    ]
},
{
    "submission_id": "1getqlj",
    "title": "Wide angle webcam ",
    "selftext": "Recently one my friends was taking a technical test on his laptop, he told me that even though his laptop is old and has a bad webcam, the webcam was able to take a wide angle kind of view.\n\nAre there any kind code where I'd get a wide angle from the webcam, any resources would be helpful, I'd like run the code and test it out.\n\nHis laptop webcam doesn't support wideangle nor zoom, also asked other people who wrote the same test and they said the same too.\n\nThanks!",
    "created_utc": "2024-10-29T05:46:48",
    "num_comments": 6,
    "comments": [
        "Hi, the angle depends on the lens type, not the camera. If the lens is not wide-angle, there is no way to get a wide-angle picture. Normally, a laptop camera should produce a low-distortion picture, so a wide angle is counterintuitive and is not required.  \n\nMaybe you can elaborate more on the problem?",
        "Is image generation allowed / generative fill / hallucinations?",
        "It was a technical test for some company and my friends were writing on their hostel dorms what they said was it wasn't like just a camera view it had a wide angle camera view, and yeah we checked taking photo using his webcam and it was just a normal view no wide angle nothing. Idk how they did that.\n\nAlso many people experienced the same",
        "You can use warping to distort a normal image to get a pseudo-wide angle (but for the same FOV). However, it sounds pretty weird...",
        "I said the same to my friends too that it's not possible with a normal webcam, idk how they achieved that like it they said it wasn't like 0.6x kind of wide angle but slightly wideangle like 0.1-0.2 range, this is what they said.",
        "Those numbers don’t mean anything. Camera angle depends on the lens. Some have a wider lens than others.\n\nIt is possible sometimes to access a bit of extra pixels along the border, most cameras crop those off because the brightness drops off. "
    ]
},
{
    "submission_id": "1ges5ml",
    "title": "SoTA image editing SD models?",
    "selftext": "Hi, i am requesting some literature on well known Open-Source SD models that are used for image editing (please follow along).\n\nBy editing I mean edits from a complete one domain to another, for example a model being able to go from natural image to a line art with specific edits, for example replacing two cars with one. And that one car is not natural but a line art with lineart specs mentioned in prompt. \n\nI have been looking for open source models so that I can fine-tune or play around with its architecture to enhance my understanding in this area.\n\nI looked into Instruct Pix2Pix and similar models, limitations i see there are that edits are very specific, and the prompts are relatively simpler/smaller. It becomes tough to adapt them to go from one domain (natural) to another (lineart) with edits incorporation.\n\nAny help will be greatly appreciated.",
    "created_utc": "2024-10-29T04:21:42",
    "num_comments": 0,
    "comments": []
},
{
    "submission_id": "1geqrrv",
    "title": "Halloween Virtual Makeup [OpenCV, C++, WebAssembly]",
    "selftext": "",
    "created_utc": "2024-10-29T02:50:39",
    "num_comments": 22,
    "comments": [
        "Awesome",
        "Look sooo well made :O",
        "Very cool! Do you hae plans to release the source.code?",
        "Pretty cool. Mind sharing?",
        "mediapipe ?",
        "Insanity!",
        "why did you not do the same in python though??",
        "Thank you! Glad you like it! 🙂",
        "Thank you! I would first check if anyone is interested enough to buy it or not. If not, it will be public. Although the code is not public yet the code documentation is: [https://www.antal.ai/demo/makeup/documentation/index.html](https://www.antal.ai/demo/makeup/documentation/index.html)",
        "If you want to do a similar thing, see [source code example here](https://github.com/terryky/tflite_gles_app/tree/master/gl2facemesh).",
        "You want him to share a piece of his mind ?",
        "Not exactly. However the solution uses the FaceMeshV2 model which is converted to onnx and run through the OpenCV DNN module from C++.",
        "I am glad you like it! 🙂",
        "The interesting thing about this project was that it's all in C++ and I compiled it and OpenCV as a dependency with Emscripten so it can be run in a browser. I used C++ because it is interesting for me and I have more experience with image processing solutions written in C++.",
        "My mind to your mind. Your thoughts to my thoughts.",
        "Is the FaceMesh onnx model available for download",
        "that explains the hairfall ...\n\njk, great project bro .. wish i could say 'i have experience with xyz' lol",
        "Hi, you can find the model in onnx format here:  \n[https://github.com/PINTO0309/PINTO\\_model\\_zoo/tree/main/410\\_FaceMeshV2](https://github.com/PINTO0309/PINTO_model_zoo/tree/main/410_FaceMeshV2)  \nHowever, understanding the output of the model is not quite trivial. Also, this model can track the face and not just return landmark points, but figuring out how to do that was not a piece of cake either.",
        "Thanks! My next hobby project should be a virtual hair try-on solution then 😉",
        "You are a legend"
    ]
},
{
    "submission_id": "1genlnk",
    "title": "MS for Computer Vision from ECE BG in US ",
    "selftext": "Hi guys, \nI am very much intrested in CV as you all are part of this Community. I love the new Meta & Apple demonstrations and think AR/VR is going to supersede the interactive devices today. Hence I want to be industry ready for it!\n\nI have taken very few certifications outside from Udemy related to Web AR/VR but that's it.\nI have decided to go to Masters in US and pick CV stream. But stressed that I wouldn't get the top CV programs given I am from ECE and mediocre GPA and most CV are electives/specialization of CS Dept which are highly competitive!\n\nWould really appreciate if anyone can suggest 3-4 Uni I can look into. Below is my profile.\n\n\nProfile:\n\n1. BE - ECE : 8.3/10.0 from Tier 2 college, Bangalore\n2. CSE Minor Degree Certificate from my Uni\n3. GRE : 305 -> 163 Q, 142 V, 3.5 AWA. (Planning to re-take)\n4. TOEFL : 94\n5. 1 Research papers in IEEE Conference + 1 Project on ML\n6. SWE Intern @ Cisco India \\[6 Mos\\]\n7. Full Time@ Cisco \\[2 Years\\]\n8. Total work-ex before flying = \\[3Y full time and 6M internship\\]\n\n3 LORS : 1 from Research Guide, 1 from Uni, 1 from Work Manager/Colleague\n\n\nKindly help me out with a profile evaluation and suggesting more universities and programs, especially for Ambitious/Within Reach.\nAlso any comments on my current shortlists\n\nAlso some advice for improving profile will help ! ",
    "created_utc": "2024-10-28T22:45:34",
    "num_comments": 1,
    "comments": [
        ">think AR/VR is going to supersede the interactive devices today\n\n99% of optometrists will disagree.  The problem with AR and VR is they make too many assumptions that people are capable of wearing or using these items without getting migraines.  There's way too many challenges and everyone is unique, for these to ever be large scale viable.  On top of that, there's motion sickness.  I know a large majority of people that can't use these without getting motion sickness.  Even the ones with the absolute lowest latencies, just make people unwell."
    ]
},
{
    "submission_id": "1gemeg2",
    "title": "What is a good example when you need to use threading? ",
    "selftext": "Any real life practical examples for computer vision you guys can share?",
    "created_utc": "2024-10-28T21:26:32",
    "num_comments": 25,
    "comments": [
        "I’ve used it to handle video frame import and load the frames onto a queue while another thread handled the actual object detection part of the code. For instances with inconsistent connection to a camera (ie network cams with packet drop) it can help to keep a smooth flow of images coming in",
        "You can process video frames in parallel over multiple cores to achieve full 30 FPS instead of processing them sequentially at a lower frame rate.   Explained in [more detail here](https://github.com/swdee/go-rknnlite/tree/master/example/stream#lag-parallel-vs-serial-processing).",
        "i use them for multicamera detection, each camera is recording on its thread and meanwhile the detection and other pre/post processing can be done alongside of it",
        "video streaming pipeline design, CUDA can be helpful too. Some embedded computer vision models don't run fast enough for some realtime control loops.",
        "I've used them to handle facial embeddings and comparisons (to see if a system has already seen a customer so they don't need to be counted twice) in a separate thread. The hardware we were running couldn't run it at 30fps so every detection just got put in a queue and the tally was incremented a second or two after a detection",
        "Data processing (for DBNet), creating smooth contours can perfectly run in parallel threads",
        "For an internship i had a simple object detection algorithm that i had to run on a long footage. I cut my footage to create multiple snippets and run instances of my object detection using threading for the snippets, and speed up the process by a lot.",
        "Anything that can be done in parallel can benefit from threading, of course.\nFor example, acquiring frames from a camera can be done in parallel to processing those frames.\nAlso, any processing that is done in independent scales of a [pyramid](https://en.wikipedia.org/wiki/Pyramid_%28image_processing%29) can be done in different threads, etc.",
        "Input and output. Pretty much anything that uses CPU and needs to read/write quickly.",
        "I pretty much use it anytime I have a ton of data to be processed in parallel. That and multiprocessing. \n\nCouple of examples are augmenting images and analyzing the output of a model that processed a large list of images. ",
        "I use it producer-consumer pipes to process large amounts of unstructured data in just about any format. \n\nIt’s a nightmare to keep track of, even using a resource manager, and am considering adding Ray/Dask to make it easier. \n\nIt’s great for keeping the overall task moving while individual parts wait.",
        "To back you up, if you have an HLS MPEGTS playlist somewhere in S3 or something, you can load each segment into Dask/Spark and process the video in parallel. Can use local Dask for threaded processing.",
        "What do you do for a living?",
        "This is not always possible if  the processing is done on the GPU, but yes. Generally I would say its just good practice to put each processing step into isolated thread with queues between the threads, so that you can speed up the bottlenecks in real time.\n\nAlso, note that python does not have this kind of threads that could give you a perdormance, if the OP is using python.",
        "Yo OP this guy sounds wayyy tf smarter than me, listen to him lol",
        "Great question with a funny answer, I’m a wildlife biologist for a Native American tribe. Never take a single computer science course in my life. Just too useful to not learn in todays day and age",
        "Can you explain more about what you think are Python limitations? I’m by no means an expert but I use threads and queues occasionally in Python.",
        "the BEAM machine could be useful for this types of scenarios , since it s orimarly designed for this extreme cases !",
        "Just an all around respectful exchange of ideas\n\nRare moment in the inet",
        "Haha hardly. The more I learn, the more I believe I know nothing.",
        "What a time to be alive ",
        "https://en.m.wikipedia.org/wiki/Global_interpreter_lock\n\nBasically, in Python, there is only one thread of execution running simultaneously. If you have 6 threads each of which performs job X you won't be faster in Python than with one thread that does job X. In fact, you will be a little bit slower, because threads will spend time acquiring and releasing the lock.\n\nThe actual use of threads in Python is asynchronous operations - e.g. when you need to check if something is done each five seconds, or some IO operations where you wait for something, like API or OS kernel (with file reading) to respond.\n\nThere is experimental feature in 3.13 that allows to disable GIL, but I guess it wont be here for another co9ple of years.",
        "TBH that level of threading is above my abilities currently, but I do use Python’s concurrent.futures a lot to execute threadpools. \n\nFor example right now I’m resizing a few hundred thousand photos by using a threadpool to saturate my CPU and SSD. My understanding is that each thread does truly operate in parallel, but perhaps there’s not efficient sharing of objects between threads?\n\nIn any case I’ve read about that project to remove the GIL and it sounded like it’ll be a “breaking change” for lots of libraries. ",
        "What you are doing is not multithreading, it's multiprocessing. IIRC, that is the purpose of concurrent.futures. Basically, think of process as a thread, just much heavier (longer to spawn/stop), and who by default does not share memory with other processes. E.g. if you have variable \\`var\\` defined, two threads can read/write to the same variable simultaneously, but in the case of the processes both of them will have two separate \\`var\\` variables. But, there is [shared memory](https://en.wikipedia.org/wiki/Shared_memory) to share data between two processes much in the way threads do. \n\nTo summarise - if you do 10 independent long tasks, multiprocessesing is a good way to go and resizing tons of images sounds exactly like it. But if you would need these processes to cooperate much more, like in the case when multiple processes take and put data into the same place, or in the case where the load is variable - one second you need 20 threads and another 100, might happen in e.g. online video streaming - and you need to spawn/delete threads on the flight multiprocessing just won't cut it.",
        "I think you’re probably correct and like I said I’m no expert, but what I’m using is called ThreadPoolExecuted. They also have one called ProcessPoolExecuter.\n\nhttps://docs.python.org/3/library/concurrent.futures.html#concurrent.futures.ThreadPoolExecutor\n\nI’ve only delved into this at a basic level and am not trying to do much with shared objects. I hear that gets “interesting” in Python! "
    ]
},
{
    "submission_id": "1gekdll",
    "title": "SpotDiffusion: A Fast Approach For Seamless Panorama Generation Over Time",
    "selftext": "",
    "created_utc": "2024-10-28T19:34:19",
    "num_comments": 0,
    "comments": []
},
{
    "submission_id": "1gekdbj",
    "title": "Dynamic Attention-Guided Diffusion for Image Super-Resolution",
    "selftext": "",
    "created_utc": "2024-10-28T19:33:56",
    "num_comments": 0,
    "comments": []
},
{
    "submission_id": "1geg4vk",
    "title": "Learning path recommendation for Deep Learning + Computer Vision using a project.",
    "selftext": "",
    "created_utc": "2024-10-28T16:11:46",
    "num_comments": 1,
    "comments": [
        "I would just use template matching or basic thresholding to find the location. Or you can use yolo but I think it’s overkill"
    ]
},
{
    "submission_id": "1gecxuy",
    "title": "Cool library I've been working on ",
    "selftext": "Hey everyone! I wanted to share something I'm genuinely excited about: NQvision—a library that I and my team at Neuron Q built to make real-time AI-powered surveillance much more accessible.\n\nWhen we first set out, we faced endless hurdles trying to create a seamless object detection and tracking system for security applications. There were constant issues with integrating models, dealing with lags, and getting alerts right without drowning in false positives. After a lot of trial and error, we decided it shouldn’t be this hard for anyone else. So, we built NQvision to solve these problems from the ground up.\n\nSome Highlights:\n\nReal-Time Object Detection & Tracking: You can instantly detect, track, and respond to events without lag. The responsiveness is honestly one of my favorite parts.\nCustomizable Alerts: We made the alert system flexible, so you can fine-tune it to avoid unnecessary notifications and only get the ones that matter.\nScalability: Whether it's one camera or a city-wide network, NQvision can handle it. We wanted to make sure this was something that could grow alongside a project.\nPlug-and-Play Integration: We know how hard it is to integrate new tech, so we made sure NQvision works smoothly with most existing systems.\nWhy It’s a Game-Changer: If you’re a developer, this library will save you time by skipping the pain of setting up models and handling the intricacies of object detection. And for companies, it’s a solid way to cut down on deployment time and costs while getting reliable, real-time results.\n\nIf anyone's curious or wants to dive deeper, I’d be happy to share more details. Just comment here or send me a message!\n\n",
    "created_utc": "2024-10-28T13:55:24",
    "num_comments": 26,
    "comments": [
        "MIT License!  Not what I expected. Thanks!",
        "Will definitely give it a try!",
        "Could you please elaborate how your library is faster? (= Optimized for immediate recognition and action)",
        "imho, would be cooler if you used interface footage or result animation",
        "Does the target tracking work across multiple camera angles. Does it track a target from entrance to lobby to elevator to office, say?",
        "How well does the object tracking work with occlusion or fast paced objects?",
        "Would love to contribute someday in future",
        "What tracking algorithm is being used?",
        "Please check GitHub issue",
        "It's always a pleasure to give back to the community as I personally wouldn't be able to reach where I am today without the possibility to access countless open source projects. And I'm a believer in the power of open source, the countless contributions and observations provided by fellow enthusiasts will help this project reach levels I would never be able to reach alone .",
        "\"Apes together strong\" 😂",
        "If you worked on real time computer vision before I think you would most definitely faced the issue of blocking operations and the headache of trying to multi thread your app to ensure a minimal lag between what's happening in the world and the detections in your system . That's where the library comes into play , if you use it you won't waste time trying to optimize your code in the ways we talked about, you just focus on the grand project you're working on.",
        "You do have a point, but in my defense the library was released today and as it's aimed to help developers more than being an end user product I didn't think a demonstration was needed . I'll try doing one though.\nThanks for your feedback",
        "For now the tracking works on each camera independently but the feature of multiple sources tracking is under development",
        "The tracking is completely configurable, so you can adapt it to your needs easily .",
        "You're always welcome and needed my friend 🙏",
        "DeepSort",
        "Okay I'm working on it",
        "Are you sure you can use MIT license here? I think Ultralytics requires an AGPL license.",
        "What paper are you using for multiple source tracking? I’m looking to tackle this problem in the somewhat near future and would love some recommendations if you have any.",
        "The thing is I'm not using ultralytics anywhere . Ultralytics's job ends after exporting the model to onnx format so correct me if I'm wrong but I think it's okay to use MIT here , no ?",
        "I'm still in the research phase but if I find something interesting I'll let you know.",
        "Nope, if you ultralytics model even if it’s on x format then it’s subject to agpl license",
        "Ah I see , but the library doesn't use any model. It's true that it's built to work with ultralytics onnx but that's on the end user whose gonna use the library to work with a model . But the library doesn't use ultralytics models by default.",
        "This seems fine to me. The user has the responsibility to use a non-gpl model, and get the outputs of that model to align with the inputs expected by your libraries. (I'm no expert though!)\n\nAnyways nice work, seems like really cool stuff.",
        "Thank you, it's still a work in progress but I appreciate the support."
    ]
},
{
    "submission_id": "1geb64w",
    "title": "I need help to develop an application about  Image Based Area Mapping ",
    "selftext": "Hello, I need to develop an Image Based Area Mapping Application, but I am new to this. I need to take the image in real time and create it in a 3D virtual environment. I am doing research, but I would like to get help from you. How should I proceed and what should I learn? Which algorithms should I integrate into my application? I would be very happy if you could help me.",
    "created_utc": "2024-10-28T12:42:16",
    "num_comments": 1,
    "comments": [
        "You're going to need to explain what you're trying to do a lot clearer than you have."
    ]
},
{
    "submission_id": "1geb4w5",
    "title": "How could I get real-world XYZ coordinates of something given this data?",
    "selftext": "I know the exact position, rotation, vertical + horizontal FOV angles and aspect ratio of my camera. I can assume that my environment is an infinite flat plane (at Y = 0) in all directions. In an image taken by my camera, I see and draw a bounding box around an object with a known width and height. How could I find the real world coordinates of this object given all of this information?",
    "created_utc": "2024-10-28T12:40:53",
    "num_comments": 4,
    "comments": [
        "if you see a point at (u,v), it's on the ray in the direction of K\\^-1 \\[u;v;1\\] in the camera reference frame. K being the intrinsic matrix, you'll use the FOV and image size to fill it. Compute the intersection of this ray with the plane Y=0 by doing the appropriate coordinate change and you'll get the coordinates of the point on the plane. And now, you can do that for the four corners of a bounding box if you want, it'll look a bit warped though.",
        "Start by metrically calibrating your camera. Move from there.",
        "given that you have the intrinsic and extrinsic parameters of your camera, you could project the image coordinates of your object onto your plane. that way you could get an approximation of your object's world coordinates on that plane.",
        "Z = object\\_width \\* f / bbox\\_width\n\nthen project with (x - cx) \\* Z / f \n\nReminder that if your object isn't a sphere, the bbox won't be enough for an accurate pose estimation."
    ]
},
{
    "submission_id": "1ge95wv",
    "title": "Computer Vision Skillsets ",
    "selftext": "Hey all, I am working on a novel project within the aerospace (drone) industry and am looking to bring on someone with expertise in computer vision. Are you folks able to make some recommendations on what skills I should be looking for? ",
    "created_utc": "2024-10-28T11:20:41",
    "num_comments": 3,
    "comments": [
        "Depends what the application is, for instance if its some sort of 3D reconstruction you would need someone who could handle the linear algebra. If its recognition you would need someone with some first principle cv skills as well as experience training and tweaking ML models. If you want bleeding edge youll need a phd for implementing models and filters from papers, or a very good data scientist\n\nThen theres associated skills needed to handle either the streaming data, or the onboard system which might require experience with a specific language - for example C++ takes a bit of time to get decent at, as is becoming proficient at something like ffmpg.\n\nAdd to that you probably want someone with good software practises so they don't make a mess. If you get someone fresh out a phd they could create a great system but with zero thought to maintainability or scaleability",
        "Thank you very much for the comprehensive response. I'm going to look into what my needs are a bit more based on your guidance. Appreciate the help!",
        "this helps me figure out the skill set I'm trying to develop first so thanks!"
    ]
},
{
    "submission_id": "1ge5tab",
    "title": "Custom dataset evaluation",
    "selftext": "I made up a dataset (59K(train) + 20K(test) + 20K(validation) images) for training my yolov9t model.\n.\nAfter 3-4 time training on the dataset, I got average 89% score (66%-72% in real life) accuracy\n.\nConsidering my model dataset maded by some images that was actually detected by an other model (labeled automatically) I'm afraid of the situations that the old version model, couldn't detect correctly (and my Newer model may couldn't detect correctly) (reminding of the old school story about bombers and adding some new  plate for protection (look at the image and if you didn't know it ,ask)\n.\nHow can I evaluate my custom dataset to make sure that it works well enough (well enough is my target not like some crazy accuracy)\n.\nTrained setup:\nHP Victus 15\nIntel I5 12450H\n16 GB RAM\nGTX 1650 mobile (4GB Vram)\n.\nModel used:\nUltralytics yolov9t \nWith ultralytics itself.\n\n.\nTask:\nClassification and detection of license plates and reading them",
    "created_utc": "2024-10-28T09:05:24",
    "num_comments": 10,
    "comments": [
        "Basically, from what am understanding, is that you trained your model on data that was automatically labeled by another model. In this case, yes, you can have something of a additive/compounding error effect (first is the error that will leak from the main model, and the training error of the second model).  \nIn my opinion, for this specific use-case, there a lot of datasets if you look well that are well labeled (bbox + license text).\n\nSo, you can test your case by evaluating the main model on a public and well labeled dataset, train your model by with the predictions of the other model you use and train it once on the dataset then evaluate the predictions of both iterations on the dataset that you chose to see how severe the error can get.\n\nHowever, as i already mention, this specific use-case is studied quite a lot and there are already a lot of datasets you can confidently train on. the only thing i can see that justifies the use of another model for training is knowledge distillation. in that case, i'll still argue that you'd want the model's predictions + hard dataset labels.\n\nEdit: yes, i also don't understand the plane image relevancy lol",
        "I don't really know what you're saying but also I doubt it has anything to do with survivorship bias?",
        "🥲\nExcuse me, for the pictures, I was just trying to share my problem with what I can or think is correct (and I was wrong so excuse me about that)\n.\nyou are correct in understanding my problem\n.\nin the License plate detection case I'm not sure if the ongoing public datasets work well in my use case since I'm facing some new type of License plates that are not Similar to what I found on the internet. So I tried to make my own dataset.\n.\nAlso I have a similar problem with other parts of my project (OCR process of License plates) and I definitely need to make my own dataset and I think I'm gonna face the same problem also in that part of the project and this time there are no public dataset for this specific problem.\n.\nExcuse me again because of my unrelated and false image.\nI will try my best next time.",
        ":)\nExcuse me.\nIt's my fault that I couldn't explain my problem correctly,\nI will try to explain my problems better next time.\n.\nProblem:\nI have a big custom dataset (around 100k Images)\nMade for OCR processing of license plates\n\nBut I don't know how to evaluate my Dataset if it's good or how to normalize it\n\ngoal:\n A well regularized and normalized dataset\n\netc...\nInformation about my Dataset, my platform of Training, my model and my let's say \"framework\" that I used for training and the problem I probably gonna face when I trained my model with this dataset + some try to explain it more widely",
        "No worries about the pic 😅 didn't mean to be passive aggressive. Am not definitely not the internet police :p\nSo in this case,. what i think you need to do is semi-automatic labeling,\nYou have the images labeled by the model, then iterate over them with a labeling tool(something like labelme i think, been while since i worked on object detection) and correct/add any labels that need it, it'll be much faster than labeling your dataset from scratch but will still involve some manual work (it's always good to be somewhat generous with ~15% of extra box size in the labels compared to the size of the license plate)\nHowever after that you'll be sure that your model is training on good data and you can properly train your new model.",
        "Use numpy to find the mean and stdev of each channel in each image in the data set. For each image, do (image -mean) / stdev on each channel. Or you can use them as values to torchvision.transforms.Normalize()\n\nIf you're feeling lazy just doing overall mean and stdev also usually works fine tbh.",
        "🤝🤝\nIt's my way of conversation since I'm trying to be polite. it's also rude if I didn't respond correctly when I made a mistake, and with that in mind, you tried to help me.\n.\nI didn't get any aggressive vibe.\n.\nThanks for sharing your knowledge I will try that",
        "Thanks I'll try that.\n.\nAgain\nExcuse me for bad post🫠🙏🙏\nIt was my first time. I will get better I promise.",
        "best of luck",
        "No worries lol"
    ]
},
{
    "submission_id": "1ge59s6",
    "title": "Based on previous feedbacks have shortlisted these two logos. Please help finalise the best one. Its a B2B startup for monitoring of construction buildings using virtual tours. Thanks & Regards.",
    "selftext": "https://preview.redd.it/4kw6x7lipixd1.jpg?width=1370&format=pjpg&auto=webp&s=cd2c8c2a06961eb1cd9fc72356cc8b6e09ffafc4\n\n\n\n[View Poll](https://www.reddit.com/poll/1ge59s6)",
    "created_utc": "2024-10-28T08:44:00",
    "num_comments": 0,
    "comments": []
},
{
    "submission_id": "1ge3x66",
    "title": "Career Advice: Switching from Mechanical Engineering to Computer Vision Engineer",
    "selftext": "Hey everyone,\n\nI’m looking for some career guidance. I graduated with a degree in Mechanical Engineering and landed a job at an MNC in the automobile sector. However, I wasn’t fully satisfied with my role, so I decided to transition from mechanical engineering to IT. Recently, I managed to secure a position in Manufacturing IT, where my responsibilities include managing vision systems, production servers, MES applications, and even building a website.\n\nWhile working on vision systems, I was introduced to computer vision, and I was like woo I want to work in this field ! Now, I’m seriously considering switching my career path to become a Computer Vision Engineer.\n\nFor those of you already in the field, I’d love some advice on where to start. What are the essential skills, frameworks, and resources I should focus on to build a solid foundation in computer vision? Any courses, projects, or specific tips you’d recommend for someone with my background?\n\n\nThank you in advance for any help :)",
    "created_utc": "2024-10-28T07:49:41",
    "num_comments": 6,
    "comments": [
        "Python and OpenCv is a good start to understand classical computer vision. Then dive into modern techniques with computer vision using AI",
        "Opencv university - they have free training courses with good paid ones for advanced applications \n\nKeyence, COGNEX,Omron - major industrial CV players for factory applications. They all have training courses \n\nMajor Robotics Companies- ABB, KUKA, Fanuc, UR all offer vision system integration courses\n\nGet your employer to pay for these, most employers get tax write offs for education and training reimbursement.",
        "If you can, apply for a masters. There are specialized programs in robotics, computer vision but they are very few and selective so you could also just go for CS. What you need to focus on though in choosing your school is that they do offer a lot of computer vision courses. If you are starting out, good to get a feel of many different sub fields in vision and then choose what interests you + has job opportunities + you are good at. \n\n\nThe field moves too fast so I think your best bet would be to immerse yourself amongst people who are already in the field. So if masters is not an option, I would recommend joining an early stage computer vision startup where you take a bet on them being successful and they take a bet on you learning things on the job. It will be a lot easier if you choose a specific problem in vision and focus all your energy in learning about it. Theory, tools and frameworks can then follow more easily.",
        "Computer vision engineer is not a real thing.\nIt's more of a computer vision software developer.\nI can only speak for myself but I have a degree in computer science and a 2nd degree in software development and than a masters.\nThese degrees are not necessary but making a computer vision software solution is often more than just the vision part.\nThere is programming, database, algorithms, OS , memory management. Especially if you want to deploy your computer vision solutions to a production environment.\nSince most companies won't have an developer for each and require you to know all of this. \nSome do some don't, but knowing the full stack that machine vision sits on is very important.\nI think you should do a 3 year software development course, and you should be set .\nI don't think it's a case of just jumping fields,",
        "When you said \"software development\", is it means the backend stuff?",
        "No it means how to development software as a product that is going into production.\nFrom start to finish."
    ]
},
{
    "submission_id": "1gdw3jv",
    "title": "CV for GUI?",
    "selftext": "Are there CV libraries / models that are good at analyzing computer GUI (eg if I wanted it to draw bounds around taskbar, window icons, url bar etc) and pinpoint elements like buttons ",
    "created_utc": "2024-10-28T00:06:34",
    "num_comments": 5,
    "comments": [
        "If the objects are of a known size and type you can try template matching. Still probably better off training a model than dealing with template edge cases. At least creating data should be very easy!",
        "Check out OmniParser (by Microsoft)\n\n[Project page](https://microsoft.github.io/OmniParser/)\n\n[GitHub repo](https://github.com/microsoft/OmniParser)\n\n[arXiv paper](https://arxiv.org/abs/2408.00203)\n\nProbably still better off training a custom model for your particular use case, but could be a good place to start or even help out with building whatever you need for a custom dataset",
        ">> Still probably better off training a model\n\nYo real talk, I don’t think you realized how easily you passed that off as the better solution. We’re at the point where for simpler and simpler algorithms, it is better to train a model than write an algorithm! \n\nSorry just a brain fart moment 🤯.",
        "Just what I was looking for! It's a little flaky and only detects text / icons, and not images / boxes. How much data and power do you think it would take to train my own model?",
        "Always difficult to estimate how much time/data is required to train a model that generalizes well. As a general rule (not always applicable) you'll want to start with \\~1k samples for each class/object you want the model to detect. If you were training a model to detect people, you'd want at least 1,000 unique/distinct images with people in them. The project page outlines they used a dataset with, \"67k unique screenshot images\" to train the model, so that should give you a sense of scale. Building a dataset for model training comes down to experience and understanding of the problem you're aiming to solve with the model. If you can generate the UI/UX elements you'd like to detect, in a way that would be reflective of the actual \"environment\" the model will be used in, then it simplifies things **a lot**, but for *most* applications this is not feasible and manual collection/annotation is required.  \n  \nWhen you say \"power\" I assume you mean compute power. You can train a model on an RTX-20xx or new NVIDIA GPU and *some* GTX-10xx, the GTX-16xx cards are not recommended as they tend to have issues with automatic mixed precision (AMP) for PyTorch training. You can also look for cloud compute providers, where you can get some limited free cloud compute (like Google Colab) or rent compute time. Realistically you can also train on a CPU, but it will likely take a *very* long time and I wouldn't recommend it."
    ]
},
{
    "submission_id": "1gdv9zs",
    "title": "SAM-SLR ASL Recognizer",
    "selftext": "I am currently working on the SAM-SLR model from this GitHub repository: [SAM-SLR-v2](https://github.com/jackyjsy/SAM-SLR-v2), and I'm reaching out for some assistance with running the model and utilizing the pretrained files effectively.\n\nI’ve been experimenting with various IDEs, including VSCode and Google Colab, to set up the environment. However, I am encountering some challenges in the following areas:\n\n1. **Pretrained Model Placement**: I have downloaded the `AUTSL_bone_epoch.pt` pretrained model file, but I am unsure where to place this file in the model directory structure. Should it go in a specific folder, or do I need to reference it in a particular way within the code?\n2. **Understanding exactly how the model works:** We understand the basic structure of how SAM-SLR works but we don't understand how the pretrained data is used and how the pretrained model .pt files are used to show the full extent of the SAM-SLR.\n3. **Image Preparation**: I have a 512x512 image that adheres to the AUTSL dataset requirements, but I need clarification on how to preprocess this image for input into the model. Are there specific preprocessing steps I need to follow before running the inference?\n4. **Running the Model**: I’m uncertain about the steps required to run the model itself. Are there particular scripts or commands I should execute to get the model up and running with my input image?\n5. **Testing Preprocessed Models**: Lastly, once I have the model running, what are the best practices for testing the preprocessed models? Any tips on evaluation metrics or expected outputs would be greatly appreciated.\n\nI am eager to learn and would be grateful for any guidance, insights, or resources you could share to help me move forward with this project.",
    "created_utc": "2024-10-27T23:03:10",
    "num_comments": 1,
    "comments": [
        "I don't know what SAM-SLR is. But I checked the code and there's an [argument](https://github.com/jackyjsy/SAM-SLR-v2/blob/ef0ab363f27691dcd775169184a5ecd736cb9b38/SL-GCN/main.py#L125) that can be used to pass weights. However, your model is `.pt` while the code is expecting it to be a `.pth`, i.e. `state_dict`. So you should probably modify [the code](https://github.com/jackyjsy/SAM-SLR-v2/blob/ef0ab363f27691dcd775169184a5ecd736cb9b38/SL-GCN/main.py#L255) and add `.state_dict()` to the end so that it returns the `state_dict`"
    ]
},
{
    "submission_id": "1gdsm0b",
    "title": "Best Depth Estimation Model  (Depth Anything v2, DepthCrafter, Depth Pro, MiDaS, Marigold, Metric3D)",
    "selftext": "There are so many monocular depth estimation models, but which one should you use? Let’s compare some of the most common ones (Depth Anything V2, DepthCrafter, Marigold, Depth Pro, DPT/Midas, Metric3D) in terms of their specialty, speed, training availability and license. \n\n",
    "created_utc": "2024-10-27T20:11:21",
    "num_comments": 2,
    "comments": [
        "Really great video! 👍 ",
        "Marigold model size is wrong?"
    ]
},
{
    "submission_id": "1gdqtac",
    "title": " Do you do hyperparameter search for each setting in ablation study？",
    "selftext": "I think to get accurate result you should. But it will be huge amount of work, say for each search it takes 10 runs. And I have 10 settings I have to study, it will be 100 runs. I heard I should do HP Search for each setting and I believe it is the right way to do it but just it requires such a large amount of computation. I remember seeing paper listed their HP but only one set, so I believe they did all settings on that HP, right?",
    "created_utc": "2024-10-27T18:33:35",
    "num_comments": 1,
    "comments": [
        "A lot of papers just pick some that they like, or from the original paper for each model, or defaults. The latter is why some conclusions are nonsense. My belief is you have to optimize each model for each problem (dataset) but it’s prohibitively expensive compute and time wise and apparently not necessary to publish."
    ]
},
{
    "submission_id": "1gdog75",
    "title": "Suggestions how to start this project",
    "selftext": "I'm planning to start working on a project which focuses on multi view 3D reconstruction using transformers. Feel like its a topic being researched currently in many big companies. Would appreciate on any suggestion on how to start this without any high end GPU resources (will be using A100)",
    "created_utc": "2024-10-27T16:35:09",
    "num_comments": 0,
    "comments": []
},
{
    "submission_id": "1gdnjzo",
    "title": "Best current pose estimator for fencing (the sport)?",
    "selftext": "I'm trying to train an AI model to act as a fencing referee and the first step was to extract pose estimations from video clips and then train the model on those.  Currently, I'm using yolov11 frame-by-frame on those clips to get the pose estimations but it tends to not find poses for fencers in the critical last 1/2 a second when they are making their final/fastest moves.\n\nFirst off, can you pass multiple extracted frames to yolov11 at once?  If you can, then I presume internally it just does frame-by-frame and doesn't try to make use of similarity of frames or optical flow?  I saw some other packages that potentially do like DCPose, HRNet, [yolo nas pose](https://github.com/Deci-AI/super-gradients/blob/master/YOLONAS-POSE.md), AlphaPose.  Should I be trying one of those or something else?",
    "created_utc": "2024-10-27T15:52:03",
    "num_comments": 13,
    "comments": [
        "2D pose estimation only?",
        "The broadcast video feed I don't think are sufficient number of FPS to do this.  Especially if you're trying to do it in real time.  You have to remember they don't even use cameras, they use electric contacts, which naturally are way way faster than any camera could be.",
        "> First off, can you pass multiple extracted frames to yolov11 at once?\n\nIf you're using the track method, then no. Technically, you can, but each image would be considered part of a different stream in that order. But if you use `predict`, then you can, and it will perform batch inference.\n\n> If you can, then I presume internally it just does frame-by-frame and doesn't try to make use of similarity of frames or optical flow?\n\nYOLO doesn't use temporal relationships. Most pose models don't. They work on single frames. But the `track` method which uses BoTSORT by default uses sparse optical flow for better tracking.",
        "I believe 2D is good enough.",
        "They use electric tools to determine who touched whom but if both fencer's touched each other then the person who gets the point is determined by the referee based on complicated rules of priority.  It isn't based on who touched who first.  When it is a close call or if a fencer requests it then at big events, the referee will go to a slow motion video replay to better determine who had priority in the attack.  If you were thinking that I was suggesting using video to determine who touched who first I wasn't.  I was hoping for the AI to learn the rules of priority.  copilot says that in the olympics they use 500fps so you are perhaps right that 25fps from a video broadcast might not be sufficient but I'll continue on and see how well an AI is able to do at 25fps.",
        "Thanks for the link to botsort.  I'll check it out.  I was looking and it sounded like AlphaPose was supposed to use temporal relationships.  I got AlphaPose to work with the recommended models from all the example pages.  Yolov11 was finding 5 or so people in the video clip but AlphaPose was finding about 30 people.  To be fair, there were a lot of people sitting or having most of their lower bodies covered by barriers so most of those were real people but there were some cases where it seemed to just be imagining people.  Also, AlphaPose was failing to get a pose for one of the fencers for the same frames that yolov11 was failing on and I suspect it just comes down to motion blur.  Am I right in thinking that AlphaPose is supposed to use temporary information?  Is botsort supposed to do better with motion blur?",
        "ViTPose is really common for pose estimation and it’s often used in 3DHPE if you choose to go that route. ViTPose was SOTA for a couple of years but other models have supplanted it.",
        "Let me answer that for you:  Not well.",
        "From what I found, AlphaPose isn't temporal either.\n\nBotSORT is a tracker. It serves a different purpose. It doesn't detect poses. It matches the previously detected boxes with the current boxes by providing you track IDs for each. Those track IDs let you know whether it's the same person, or a different person from the previous frames.",
        "I now have two videos full of touches to train on.  That is over 300 touches.  With 80% training and 20% testing, the neural net is getting over 95% accuracy.  Makes me wonder if I messed up and somehow it isn't really doing this well.",
        "Are there any temporal pose estimators then?",
        "Look up over fitting.",
        "I don't know of any."
    ]
},
{
    "submission_id": "1gdmt31",
    "title": "Cool node editor for OpenCV that I have been working on",
    "selftext": "",
    "created_utc": "2024-10-27T15:16:48",
    "num_comments": 47,
    "comments": [
        "This is where I'd hoped we'd be at by now, not installing 42 code libraries from the command line\n\nAn excellent project, hope it goes well",
        "What are your thoughts on integrating this with ComfyUI?",
        "Is this going to be an open project?",
        "What framework did you use to make this UI (nodes?)",
        "way cool",
        "Is there some kind of optimisation or synchronisation to play the video clips in those mini players? Or will it drop frames to cope with increased load?",
        "Wow this is pretty neat! Our professorship had a similar project so that many non-cs colleagues or even non-technical people can develop CV applications. It was based on QT. We eventually got python working on it too, since we needed DL/ML-based algorithms. Too bad it was dropped because lack of resources to sustain it. And even worse we are not allowed to opensource it due to some restrictions of public funding / university intelligence property / ...",
        "Damn this so cool!!!!",
        "that's really cool",
        "Is it possible to use a ML model in the flow?\nLike yolo or resnet for example",
        "Nice project! Would you like some help to setup this in a docker image?",
        "Nice job ! Does it includes the stereovision part ?",
        "Do you need help with the project ? This is just so good.",
        "hey maybe we could somehow help you with this project? would you be open to the idea of making it open source or something? i am just very very interested in those things and just want to help friens like you",
        "Cool, there is an old project like this which it reminds me of: https://s2i.das.ufsc.br/harpia/en/home.html",
        "The use of no code/node programming for GenAI and other aspects of ML/AI will accelerate the innovation trajectory (IMO).  I also feel that this type of utility really puts the tech into everybody’s hands (kinda democratizes it).  From Scratch to MIT’s App Builder to Node Red, these programming utilities are the best in terms of rapid prototyping/solution creation.  I wish this was around when I was working on my Grad program so I didn’t have to use a Java code generator for my Comp Science Queue Theory project.  Sun’s program really kicked out a shit-ton of nebulous file structures which resulted in a huge footprint for a program that only had about a 12 SLOC  algorithm.  I’ll put a tracer on this post and become one of the early adopters (once the MVP drops).  \n\nTo OP/Author, Gr8 work and thanks for sharing",
        "nice!",
        "Very cool! Looks good & promising so far.",
        "Is the project available on GitHub, cause even I wanna build this",
        "Cool",
        "Can I use this to finish up my thesis project? (it's using EVM on videos to measure breathing parameters Ffom chest movement during inhalation and exhalationl).",
        "This is gorgeous. I’ve been doing something similar. My vision you can capture frames and they go on a belt like factorio, then you can compare multiple frames over time (structure from motion) etc. ",
        "reminds me of [https://online.opencvflow.org/#home](https://online.opencvflow.org/#home)",
        "Very cool mate, orb slam when?",
        "Love this! Please keep going",
        "Looks nice. Similar to Ikomia Studio.\n\nhttps://www.ikomia.ai/studio",
        "i was literally looking for something like this yesterday.\n\nthank you!",
        "This looks great. I actually just shared a [workflow](https://www.reddit.com/r/comfyui/comments/1grzzlh/realtime_background_blur_and_removal_on_live/) in the ComfyUI community that composes CV operations with a modern segmentation model (DepthAnythingV2) for background blur/removal and was inspired by similar thinking - node based editors for quickly prototyping new capabilities by composing different building blocks.",
        "this is basically touchdesigner",
        "It is ! You can find it in https://github.com/deltacv/PaperVision\n\nI have made some changes and the beta testing instructions may no longer work, I'll fix it as soon as I can",
        "Hey ! I used dear imgui with the imnodes library. Apart from that, everything was built from scratch in Kotlin - This project runs on the JVM, I use the imgui-java bindings.",
        "Looks like react flow or vue flow",
        "The editor works with \"backend engines\" so you can plug any logic you want for handling previsualizations. By itself, this app can work standalone without any sort of backend engine, but will not have previsualizations, only  the editor and code exporting.\n\nThe implementation in the video uses a WebSocket to communicate with another process called \"EOCV-Sim\" (running in the same machine) that handles the pipeline and streams into the mini players at a very low resolution (320x240 or 160x120). This implementation has some logic in place to scale down the frames and to stop streaming if there has not been a change between frames, it uses a queue to send the frames which I haven't had any synchronization issues with.",
        "I would be more than happy to support ML capabilities ! It goes out of my scope of knowledge, but since the project is open source I encourage you to take a look at it,  \n[https://github.com/deltacv/PaperVision](https://github.com/deltacv/PaperVision)\n\nThe API easily allows the editor to be extended with new nodes and attributes, the only challenge might be with integrating ML into the JVM. Please do let me know if you have any inquiries, I'm still pending to properly document the source code.",
        "Hey ! The project is open source, you can find it here [https://github.com/deltacv/PaperVision](https://github.com/deltacv/PaperVision). Any contribution is greatly appreciated.",
        "Same question, im very excited.",
        "Hey ! The project is open source, you can find it here [https://github.com/deltacv/PaperVision](https://github.com/deltacv/PaperVision). Any contribution is greatly appreciated. I'm still missing a lot of documentation, but I hope the source code can somewhat be self explainatory 😅. Any inquiries please do let me know.",
        "Hey ! The project is open source, you can find it here [https://github.com/deltacv/PaperVision](https://github.com/deltacv/PaperVision). Any contribution is greatly appreciated. I'm still missing a lot of documentation, but I hope the source code can somewhat be self explainatory 😅. Any inquiries please do let me know.",
        "Yep, pretty similar. Nice to see a fresh new version of the same idea (disclaimer: I was one of the Harpia authors.. fun days 🤣)\n\nAlso, there’s https://github.com/liris-vision/starling which was a fork of Harpia.",
        "Hey ! The project is open source, you can find it here [https://github.com/deltacv/PaperVision](https://github.com/deltacv/PaperVision). Any contribution is greatly appreciated. I'm still missing a lot of documentation, but I hope the source code can somewhat be self explainatory 😅. Any inquiries please do let me know.",
        "Hey ! The project is open source, you can find it here [https://github.com/deltacv/PaperVision](https://github.com/deltacv/PaperVision). It is under the GPL license, feel free to use it !",
        "Read that as toRchdesigner and got VERY excited!\n\nI would love this for building neural nets!",
        "Very cool!",
        "Happy to take a look at it man.",
        "Ohh nice, harpia was useful especially for testing things quickly vs having to write code.",
        "Thank youu",
        "Can it work for a industrial production project"
    ]
},
{
    "submission_id": "1gdmp3u",
    "title": "Pose Estimation For Posing 3D Models?",
    "selftext": "Are there any models / applications out there that convert 2D pose estimation data into a pose for an actual 3D model of a human? For example, let's say I have a photo of person sitting down. I should be able to send that photo through a pose estimation model, and then send that pose estimation into an application which'll give me the appropriate data to configure the human figure below.\n\nhttps://preview.redd.it/v0dl4u85hdxd1.png?width=631&format=png&auto=webp&s=619cd3e989bb93c39bea7f342452797186130a93\n\n",
    "created_utc": "2024-10-27T15:11:34",
    "num_comments": 4,
    "comments": [
        "OpenPose. It's often used to generate specific poses in Stable Diffusion using ControlNet. ",
        "Check out WHAM or HybrIK (which has a blender add on) on GitHub. Both are open source.",
        "Openpose just outputs 2d or 3d key points. I don't think it can output a mesh or kinematic skeleton.",
        "OP wants to pose an existing rigged character though. Not generate a model. "
    ]
},
{
    "submission_id": "1gdl6ku",
    "title": "Which model is the best for Agricultural Crop Instance Segmentation task?",
    "selftext": "Hey all, I have been working on a project involving the development of a computer vision model for instance segmentation task on a dataset of crops that we have developed in our college laboratory. Can anyone please recommend some good model for the purpose? I am open to advices on the model pipeline building as well.   \nAny suggestion on dataset treatment or tools to use will be much appreciated. \n\nThe dataset contains 100 (640 x 640) images of a crop taken from a height via drones. The task is to create segmentation masks for the crop canopies.",
    "created_utc": "2024-10-27T14:01:43",
    "num_comments": 4,
    "comments": [
        "How about instance segmentation using Yolo?\n\n\nhttps://docs.ultralytics.com/tasks/segment/",
        "Hi, I tried yolov8 and yolo11, getting a MAP of 48-55% consistently, need to increase it.",
        "You probably need a bigger dataset to get better accuracy as 100 images is not that much.",
        "Look into a UNET architecture. They typically need less annotations then most others"
    ]
},
{
    "submission_id": "1gdihau",
    "title": "How to create Deep Association Metric with DeepSORT",
    "selftext": "I am trying to use DeepSORT on a YOLOv8 model trained on a custom dataset. When I train the deep association metric do I need to train it on the same dataset or can I just get away with using a pre-trained model like VGG or even just some feature layer of the YOLO model I trained. If I can use VGG of the Yolo model do I have to cut it off at a certain layer or can I leave it as is? If I need to train a new model on a separate dataset then is there a way of doing that where I can just use the same data as I did for the YOLO model or do I need a special re-identification dataset. \n\nI am not expecting peak performance with this project, I just want enough to get by with an OK level of efficacy. ",
    "created_utc": "2024-10-27T12:02:39",
    "num_comments": 1,
    "comments": [
        "If you want to get object embeddings from YOLO in `ultralytics`, you can try [this](https://y-t-g.github.io/tutorials/yolo-object-features/). But a specially trained model might have better performance. Reidentification models require special datasets."
    ]
},
{
    "submission_id": "1gd854o",
    "title": "What models can recreate faces?",
    "selftext": "Currently working on improving / generating thumbnails for youtube. So far I tried dalle, but that doesn’t take image inputs. And a workaround I found was to first describe the input image in detail and then use that as prompt for dalle. But the faces generated were similar in features but not the same. Any recommendations on models which take image and text input to make an image. Also it would be amazing it they can also add the title of video on the image. ",
    "created_utc": "2024-10-27T03:38:54",
    "num_comments": 3,
    "comments": [
        "Using VAE along with KL divergence loss won't help?",
        "dreambooth",
        "Look at Flux (local image generator) with LORA training using dreambooth. This allows you to train simple adapters for your network that add new concepts/information that was not present in the training data (like your likeness). But this takes a lot of GPU power (best with rtx 4090 or similar).\n\nFlux is very good with text btw. so adding the title should be a no-brainer.\n\nAlso look at IP adapters. These are lower quality than the dreambooth fine-tuning, but they don't need any training on your side and just work as an additional image input.\n\nVisit r/stablediffusion if you are interested in local image gen."
    ]
},
{
    "submission_id": "1gd75su",
    "title": "Is a filter linear in image processing",
    "selftext": "how would you understand (mathematically) a filter is linear or not. For example \n\nh(x, y) = 5f(x, y)- 1f(x−1, y)+ 2f(x+ 1, y)+ 8f(x, y−1)- 2f(x, y+ 1)\n\nis h linear in this case? ",
    "created_utc": "2024-10-27T02:26:19",
    "num_comments": 6,
    "comments": [
        "This feels like a homework question\n\nYou can verify that applying this filter is a linear operation: h(f+g)=h(f) + h(g)? h(a\\*f)=a\\*h(f)? h(0)=? ...\n\nCan you write h as a *correlation* of some kernel with f? h = k ° f with k = \\[\\[0, 8, 0\\], \\[-1, 5, 0\\], \\[0, -2, 0\\]\\] maybe?",
        "Linear combinations of linear functions remain linear, so assuming f is linear then yes h also stays linear. Or am I misinterpreting your question somehow?",
        "Yes, I'm trying to solve an homework question, also I try to clearly understand how to understand if a filter is linear. The filter asked in my homework is different than it. I arbitrarily created the filter to ask this question here.\n\nActually what I know is that to call a function linear, we should show it's homogeneous and additive.\n\nSo I tried to show it's homogeneous with following: for some constants a and k, if h(ax,ay) = a\\^k h(x,y) , then it's homogeneous. But I stuck on h(ax, ay) = 5f(ax, ay)- 1f(ax−1, ay)+ 2f(ax+ 1, ay)+ 8f(ax, ay−1)- 2f(ax, ay+ 1) and I don't actually know how to remain.",
        "Say f is defined on R\\^2, h is defined over *functions over R\\^2*, not on R\\^2 itself! So you should be examining what happens to h(a\\*f)(x,y) and not h(f)(ax,ay)!",
        "I thought considering h(a\\*f)(x,y) but what confused me is h doesn't take f as parameter. It's directly given as h(x,y) equals bla bla . So isn't it meaningless to examine h(a\\*f)(x,y) ?",
        "If h takes f as parameter, and the result is a function we evaluate at (x,y), then it is linear in f.\nIf f is fixed, then there is nothing in your post telling us if h is homogeneous or linear in (x,y)\n\nedit: I just saw you posted the same question on the maths subreddit hours later, lol"
    ]
},
{
    "submission_id": "1gd6812",
    "title": "Looking for collaborations on ongoing work-in-progress Full Papers targeting conferences like CVPR, ICML, etc.",
    "selftext": "Hey everyone,\n\nOur group, **Vision and Language Group, IIT Roorkee,** recently got three workshop papers accepted at NeurIPS workshops! 🚀 We’ve also set up a website 👉 [VLG](https://vlgiitr.github.io/), featuring other publications we’ve worked on, so our group is steadily building a portfolio in ML and AI research. Right now, we’re collaborating on several work-in-progress papers with the aim of full submissions to top conferences like CVPR and ICML.\n\nThat said, we have even more ideas we’re excited about. Still, a few of our main limitations have been access to proper guidance and funding for GPUs and APIs, which is crucial for experimenting and scaling some of our concepts. If you or your lab is interested in working together, we’d love to explore intersections in our fields of interest and any new ideas you might bring to the table!\n\nIf you have resources available or are interested in discussing potential collaborations, please feel free to reach out! Looking forward to connecting and building something impactful together! Here is the link for our Open Slack 👉 [Open Slack](https://join.slack.com/t/vlgopenspace/shared_invite/zt-2t7kihcc6-uilU~y7lz7jdtqNc5M1VPA)",
    "created_utc": "2024-10-27T01:16:08",
    "num_comments": 2,
    "comments": [
        "So you are telling me the world's Top <400 university lacks funds for proper hardware? I am just curious shouldn't the government should subsidize or provide funding for such projects? Isn't IIT a government institution? How about alumni funds? Just curious.",
        "Third world research universities. I couldn't even get cloud credits, let alone a GPU, as a grad student at one of the top research public universities of the country. And I was paying them. It's ranked among the top 200 in the world."
    ]
},
{
    "submission_id": "1gd0d36",
    "title": "MBA student poll on Machine and Computer vision",
    "selftext": "Hi all.  I am an MBA student at Temple university and we are doing our final project looking at Machine and computer vision. I would be grateful if you would be able to fill out this survey and if possible send to anyone else that works in manufacturing. We are looking for opinions from those that currently and do not currently use vision systems.  Here is the link to the survey: [https://fox.az1.qualtrics.com/jfe/form/SV\\_0cEBnNUQ9jnxZpI](https://fox.az1.qualtrics.com/jfe/form/SV_0cEBnNUQ9jnxZpI)\n\nThanks so much!",
    "created_utc": "2024-10-26T18:45:11",
    "num_comments": 0,
    "comments": []
},
{
    "submission_id": "1gcoc6y",
    "title": "Resource usage for multi-stream object detection - What's your experience?",
    "selftext": "Hey all! I’m working on a real-time object detection application in Scala, and I wanted to share some details on its performance and get a sense of what others are achieving with similar setups . Here’s what my app is currently doing:\n\nMy application handles:\n\n* Multiple 1080p RTSP input streams at 20FPS and doing detection on every frames + tracking\n* YOLOv10m object detection model (ONNX)\n* Real-time bounding box drawing\n* HLS stream generation\n* MQTT communication\n\nHardware:\n\n* RTX 2060 (6GB)\n* AMD 5950x\n\nResource usage for single 1080p stream (20FPS):\n\n* CPU: 9% (Mqtt for post video stream turn off)\n* CPU: 13.4% (Mqtt for post video stream turn on)\n* RAM: 957MB\n* GPU: 20% utilization (including windows gpu utilization)\n* VRAM: 2.5GB/6GB\n* GPU temp: 48°C\n\nFor two streams with same configuration with the first stream (1080p, 20FPS each):\n\n* CPU: 18% (Mqtt for video post stream turn off)\n* CPU: 21-24% (Mqtt for video stream turn on)\n* RAM: 1290MB\n* GPU: 30-48% (including windows gpu utilization)\n* Other metrics scale similarly\n\nThe application maintains these numbers while:\n\n1. Processing multiple RTSP inputs\n2. Running YOLOv10m inference\n3. Drawing detection boxes\n4. Creating HLS segments/playlists\n5. Sending MQTT message on every frames + post processed frames bytes (which i think the most inefficient side of this application, perhaps going to change this in the future and use Webtrc / rtsp output instead)\n\nI'm particularly interested in:\n\n* What kind of resource usage are you seeing with similar workloads?\n* How does your application scale with multiple streams?\n* What optimizations have you found most effective?\n* Are these numbers in line with what you'd expect?\n\nMost algorithm eg. tracking,  pre and post processing including normalization, was custom implemented in Scala,\n\nWould love to hear about your experiences and discuss optimization strategies, and what do you think about this utilization metrics?",
    "created_utc": "2024-10-26T09:01:59",
    "num_comments": 8,
    "comments": [
        "On a [Rock 5B](https://radxa.com/products/rock5/5b/) which costs less than $100 I can get [three 720p streams](https://www.youtube.com/watch?v=M6mvHTNQZqM) running at 30 FPS on a YOLOv5s model with ByteTrack tracking.\n\nYou should break down your resource usage into the main steps that occur during video frame processing such as Inference, Post Processing, Rendering etc to see where optimisations can be made.\n\nSee [details here](https://github.com/swdee/go-rknnlite/tree/master/example/stream) on how you could achieve 30 FPS with parallel processing as with 20% GPU/9% CPU utilisation at 20 FPS there is no reason why you can't get 30.",
        "So to clarify. You take 1080p streams from cameras(?), do inference/processing/drawing, serve the video back out via HLS (at 1080p again)?",
        "> YOLOv10m object detection model (ONNX)\n\nYou should try TensorRT since you have an NVIDIA GPU. It gets you more FPS.",
        "You can reduce the CPU usage further by using NVDEC for decoding. NVDEC is a dedicated unit, so it doesn't reduce the GPU resources available for inference since they are both separate. It's free lunch.\n\nAlso use batching to increase throughput.\n\nAll of these can be done through DeepStream.",
        "Thank you for sharing your experience with Rock 5B. However, I'd like to clarify - my post isn't just about model inference performance, but rather about the complete application pipeline's resource utilization. My application handles:\n\n1. Multiple 1080p RTSP input streams (not 720p)\n2. YOLOv10m inference (a significantly larger model than YOLOv5s)\n3. Frames post processing\n4. HLS stream generation\n5. MQTT communication for:\n   * Detection data\n   * Dynamic configuration\n   * Optional frame streaming\n\nThe 20 FPS is an intentional setting, not a limitation. The resource utilization I shared (20% GPU/9% CPU for single stream) represents the entire pipeline above, not just model inference.\n\nthe focus here was to understand how others' applications perform when handling similar complete pipelines (input processing, inference, video output generation,, messaging) rather than just the model inference component.\n\nWould be interested to hear about your complete pipeline performance if you're handling similar features beyond just inference and tracking.",
        "Deepstream might be helpful especially if you want to scale for single model inference.",
        "it generate hls chuncks, does not serve in within this application, instead i used mqtt for real-time streaming on other application which i plan to change this soon, because of substantial overheads of mqtt messages on every post processed frames.",
        "One performance hint I'd investigate is using the substream from the cameras, this is often a lower resolution, but given that many networks don't natively take 1080p as inputs, they often down sample to e.g 640x640. So you might as well save network bandwidth and decode load when the neural network throws much of it away anyway."
    ]
},
{
    "submission_id": "1gcntc6",
    "title": "Which dataset have label for lidar occlusion",
    "selftext": "I am doing a project related to self driving algorithm, especially focusing on lidar 3d cloud point occlusion detection. But I am not able to find dataset with the lidar occlusion label. Should I use an unsupervised learning algorithm, or is there any dataset I should use?",
    "created_utc": "2024-10-26T08:38:32",
    "num_comments": 0,
    "comments": []
},
{
    "submission_id": "1gckp3t",
    "title": "Replacement anemometer cups after a storm broke the poll and smashed them on the ground.",
    "selftext": "",
    "created_utc": "2024-10-26T06:07:47",
    "num_comments": 1,
    "comments": [
        "Go away bot"
    ]
},
{
    "submission_id": "1gci9jg",
    "title": "What is the best model I can use on LIVE footage?",
    "selftext": "Hi, I've an idea that I want to implement and I don't know where to start. I've done basic facial expressions recognition in the past and I've also created a bare bones CNN from scratch but that's the extent of my CV knowledge.\n\nI want to create a model that can decide which person is speaking and switch the camera feed to that person live. So I would be taking in atleast 2 camera feeds and running the model on them real-time and whenever it looks like someone is about to speak, I want to switch the camera that contains that specific person (hopefully I make sense). \n\nI'm thinking of using Lip detection + VAD to detect when a person starts speaking but I don't know what would be the best model to use. In my case, I would want the least possible latency to accommodate at least 30FPS because all the processing will be happening in real-time (or live) as the video is being broadcasted.\n\nAny help would be appreciated as I'm kind of lost on what to do first and how to start this project.",
    "created_utc": "2024-10-26T03:41:18",
    "num_comments": 10,
    "comments": [
        "You need exactly 0 computer vision to do this.  You can just monitor the audio feeds of the cameras.",
        "Yes, but I need to integrate some other features aswell like blurring of inappropriate gestures or slight panning of camera if a person moves their faces.",
        "Those things are not dependent on speaker detection.",
        "That's why I was asking for a general model that would give the best performance if I train it. Sorry I'm new to this and I don't know what sort of info I should be giving out.",
        "No single model would handle any single one of any of these things.  That'll require multiple models.",
        "Would you say it's possible? And if so, what model should I opt for ?",
        "To do CV based speaker detection?  I guess it's technically possible but I'd fire any employee that wasted my budget trying to do it.",
        "Thanks for the input. This is actually my Year Final Project and I HAVE to do something like this. I'm well aware all of this is possible with just audio.",
        "There needs to be several seconds of consecutive frames before you could determine if someone is potentially speaking.  Even then you don't even know if they are speaking because someone could simply just be moving their mouth.  It's not going to be accurate, it's going to be slow, and it's pointless.",
        "Makes sense. So I would be better off incorporating scene switching using just audio and add other censorship features using cv later down the line."
    ]
},
{
    "submission_id": "1gceqt6",
    "title": "I'm stuck on improving prediction accuracy using Florence-2(ontology based) SAM2 predict.",
    "selftext": "Hello, im noob to reddit from korea. Thanks for excusing my English skills\n\nIs it absolutely necessary to have a pre-training dataset, i.e. a pre-trained model, to improve the accuracy?  \nHow can I supplement it if there are not enough images for pretaining and the images have different features?\n\nThe desktop environment 13900k, 128gb, rtx4090  \nI am running a python virtual environment on ubuntu. (it's on Flasn-attn 2 compatibility with SAM2)\n\nThe modules used here are Autodistill + grounded SAM2 + Florence-2 (Ontology) + yolov8, which includes data transformation to train with yolo.\n\nMy goal is to segment the objects in a photo based solely on ontology. For Sam2 I am using sam2\\_hiera\\_large.pt, and for Florence-2 I am using florence-2-large-pt, coco as default model.\n\nOverall, the segmentation prediction accuracy of my roboflow dataset is between 0.60 and 0.65, which is not good for hand-labelled data.\n\nWhen I run this process with my own dataset using only ontologies, the accuracy does not exceed 0.4.\n\nHowever, the algorithm presented by CVPR [https://arxiv.org/abs/2312.10103](https://arxiv.org/abs/2312.10103) performs very well with ontology alone. I'm wondering if this performance is due to the refined data, or because my ontology doesn't cover all photos with different features, and if I could get similar results if I pretrained my roboflow dataset.\n\nAlso, if there is an implemented technique like this, I would like to be introduced to it.\n\nIn the ‘my ontology based prediction results image’ below, I'm seeing something that might be reducing the accuracy. I'm guessing it's due to the mask being predicted incorrectly, but I'd like some help on how to fix this.\n\nMy ontology based prediction results image : [https://drive.google.com/file/d/1cnwgaAT\\_bDHlC4N0dcPDqxzXyRdUPJww/view?usp=sharing](https://drive.google.com/file/d/1cnwgaAT_bDHlC4N0dcPDqxzXyRdUPJww/view?usp=sharing)\n\nMy base script : [https://github.com/roboflow/notebooks/blob/main/notebooks/how-to-auto-train-yolov8-model-with-autodistill.ipynb](https://github.com/roboflow/notebooks/blob/main/notebooks/how-to-auto-train-yolov8-model-with-autodistill.ipynb)",
    "created_utc": "2024-10-25T23:17:36",
    "num_comments": 10,
    "comments": [
        "\nI see you've posted a GitHub link to a Jupyter Notebook! GitHub doesn't \nrender large Jupyter Notebooks, so just in case, here is an \n[nbviewer](https://nbviewer.jupyter.org/) link to the notebook:\n\nhttps://nbviewer.jupyter.org/url/github.com/roboflow/notebooks/blob/main/notebooks/how-to-auto-train-yolov8-model-with-autodistill.ipynb\n\nWant to run the code yourself? Here is a [binder](https://mybinder.org/) \nlink to start your own Jupyter server and try it out!\n\nhttps://mybinder.org/v2/gh/roboflow/notebooks/main?filepath=notebooks%2Fhow-to-auto-train-yolov8-model-with-autodistill.ipynb\n\n\n\n------\n\n^(I am a bot.) \n[^(Feedback)](https://www.reddit.com/message/compose/?to=jd_paton) ^(|) \n[^(GitHub)](https://github.com/JohnPaton/nbviewerbot) ^(|) \n[^(Author)](https://johnpaton.net/)",
        "Found [1 relevant code implementation](https://www.catalyzex.com/paper/arxiv:2312.10103/code) for \"GSVA: Generalized Segmentation via Multimodal Large Language Models\".\n\nIf you have code to share with the community, please add it [here](https://www.catalyzex.com/add_code?paper_url=https://arxiv.org/abs/2312.10103&title=GSVA%3A+Generalized+Segmentation+via+Multimodal+Large+Language+Models) 😊🙏\n\nCreate an alert for new code releases here [here](https://www.catalyzex.com/alerts/code/create?paper_url=https://arxiv.org/abs/2312.10103&paper_title=GSVA: Generalized Segmentation via Multimodal Large Language Models&paper_arxiv_id=2312.10103)\n\n--\n\nTo opt out from receiving code links, DM me.",
        "Sorry, I can't go through all your question, but your initial question of is it better to use pretrained or train from scratch, the answer is always pretrained is better unless you have A LOT of data.  I mean like millions of GT's.",
        "Can’t access your Google photo. Try imgur?\n\nIn general training a simple model directly on your own data is always superior to a foundation model. ",
        "I have a construction engineering domain and it's my first time dealing with these technologies(vision-based, LLM etc..). Currently, my dataset has about 17 classes, and there are 673 annotations in 146 different photos. It's obviously a very small amount of data compared to the variety of classes and images.\n\nMy key question is how to handle the noise that appears in google photo in ontology-based segmentation.",
        "oh sorry. I modified google photo permissions. you can see now.",
        "I mean I'm not intimately familiar with your data, but the best way is usually gather a lot of data, like 10k or so images.",
        "Thanks. Ok so those results are pretty bad!\n\nWhat I would do first is search for in-domain data. Looks like you’re working with photos of streets, so you could train models on autonomous driving datasets. https://medium.com/analytics-vidhya/15-best-open-source-autonomous-driving-datasets-34324676c8d7\n\nAnother thing you can try is using SAM to identify different objects with a mask and then manually identify the correct class. It may get the class right a lot of times but you can improve it manually. Then you train your model on the corrected dataset. \n\nYour English is fine by the way!",
        "thx a lot. I found breakthrough",
        "I finally decided to collect a large amount of dataset and finetune sam2 and florence-2. thx for your feedback"
    ]
},
{
    "submission_id": "1gce8pf",
    "title": "Should I drop out?",
    "selftext": "===\n\nSorry if this is not well structured post, my mind is all over the place now because of the threat\n\n===\n\nHi, so, I started to research in a non-English university since September 2024. I am thinking to drop out, drop my salary and went back to my country as a Computer Vision Engineer intern\\*\n\n\\*My last job was Senior SWE\\*\\*, but it's not a CV Engineer job, so went back as an intern is reasonable for me\n\n\\*\\*Although I can do system architecture, design pattern, sprint planning, etc. Unfortunately, products have started to shift from building from scratch to a lego-like product. So, software engineer is going to be pushed out one way or the other\\*\\*\\*\n\n\\*\\*\\*Not now, I am aware that the management was worried when I intend to resign. Last time, what I did was, to prepare a good documentation, few technical meetings, hiring 2 juniors and longer notice period can ease the management, and we maintain good relationship. But I am talking about what will happen in the future. In the future, maybe if I need to take leave due to unknown variable, maybe I will be handed out the resignation letter to sign if I stay as a SWE\n\n===\n\nHonestly, both side is at the wrong:\n\n1. me, with no research ability and no strong math background\n2. he don't discuss with me. My assumption is that the professor accepted me because of the department requirements\n\n===\n\nTLDR\n\n1. Although I have worked as a SWE since 2020, still my bachelor degree is Business Management. In other words, I have neglected math since 2017. I have started to understand how to read the math and algorithm in Computer Vision papers, but my progress annoyed my professor\n2. My professor can't speak English. So, we have never discussed anything at all. Except he asked me to make PPT of what I read. Later, he asked me to write literature summary, in Chinese. This frustrated me because I am still at HSK1 and he said don't use ChatGPT.\n3. I just found out that he had issues with international students last year. Long story short, he announced to the whole group that he is not going to accept international student in 2023. But, here I am, 2024 international student. Confusingly, he gave me the acceptance letter in early 2024. So I have no idea why he accepted me. What I heard is that the departments forced professors to accept international student. My assumption is that instead of accepting random international student, he accepted the one who approached him. But it turns out the one, me, is not up to his standard.\n4. My professor kept threatened to expel me from the school. So I tried to avoid asking questions. Last time I asked him question was when he gave me peer review tasks, which I cannot find in the system (it turns out he use different email), but still, he threatened to expel me from the school. This is real because there was supposedly 3 international students from 2023\n5. I am tired, I have 26 credits (1 credit = 16 hours) worth of courses and also research. The other international student can reuse their paper and PPT from bachelor, I need to make them one by one and each courses wanted at least 2 PPT and 1 paper with experiment. I am tired\n6. I saw with my own eyes and ear that he tried to explain a concept to the Chinese students more than once. Yet, he tried to expel me over 1 question (regarding the peer review task).\n\nI wanted to switch career from Software Engineering to Computer Vision Engineering. I have left my SWE career and lost lots of money in the process.",
    "created_utc": "2024-10-25T22:41:48",
    "num_comments": 10,
    "comments": [
        "First off, TL;DR isn’t supposed to be your life’s story, my dude. When your “Too Long; Didn’t Read” part is longer than the post itself, that’s when you know you’re in deep existential waters.",
        "Hi,  first, i am a previous STEM student and an engineer,  i have a strong background in maths and physics , although i find computer vision is hard  to deal with the theoretical concepts and equations…. so don’t worry for that, second, you made a good choice for moving from SWE to CV,  SWE becoming like “drag and drop” or “lego build”, third to deal with your problem, try to make notes of all math formulas and paste them on the wall in front of your desk , ( general transformations,  geometry tools,…) when reading paper try first try to read abstract results and conclusion section, for 2 or 3 min not diving directly in equations, save any remarks, and most important ask your professor questions periodically, even it dump questions, just show your interest and your working hardly, try when he neglect you to attract attention by questions or remarks even it’s trivial, show him progress even small, work on well formatted reports and communicate well with Chinese students in the lab or other members,… i wish you all success",
        "It doesn't sound like a good situation. I would get out one way or another.\n\nHonestly if you want to build CV from non \"lego-like\" bits then you are going to need a lot better academic background - even then the future maybe \"lego-like\". A business qualification with 7 years neglected maths is not a good starting point for innovating in CV",
        "Are you in Hong Kong?",
        "Thank you for your suggestion. I will try to throw what I found to the Chinese students, hopefully they bring it up to the professors and I will be involved, somehow.\n\nHowever, for asking the professors periodically, it is not possible. I just confirmed it today. For example, I may have found a shortcoming on newest type of attack, and have an idea how to solve this shortcoming, which originated from an older type of attack. I asked him about it and he said “If you keep bothering me, I will expel you”. I don’t come from rich family, I can’t go back, even though I was considering to drop out, but it’s just a rant.",
        "I am staying then, hell is ok for me la",
        "I am in Xi’An",
        "Haha. You go girl!",
        "A lot of the heavily mainland labs act like that, can be super isolating. Sorry you are experiencing it, but honestly, it is not really something that will change over time.",
        "For reference, I am the token non-Chinese person in the lab 😅"
    ]
},
{
    "submission_id": "1gc78m9",
    "title": "Graduate Programs/Masters in Computer Vision",
    "selftext": "I am looking for graduate programs/masters in computer vision and needed some advice from the community. I am about to complete my bachelors in computer science.\n\nI have a few doubts:\n\n1. Is it better to specifically look for programs in machine learning and AI, or pursue a masters in computer science. My goal is to get into industry after my degree (such as robotics, etc.), but with a strong theoretical knowledge.\n2. Other than robotics, what other well-established fields heavily seek computer vision expertise. I want to get a sense of job prospects. How competitive is this field?\n3. Are there any such programs available? What sort of places should I look into?\n\nAny advice, and any extra insights independent of my doubts will be really helpful.",
    "created_utc": "2024-10-25T16:02:01",
    "num_comments": 3,
    "comments": [
        "If you want a strong theory background I would recommend the AI/ML route and supplement with some cs classes",
        "I am not sure which country but I am talking about US as an international student. I did bachelors in CS then finished Master's in AI which turned out to be a worst mistake unless until you have great experience its really hard for you to get jobs later. I would suggest to do Master's in CS but chose a school which has nice CV curriculum or courses in it. In this way you should be able to get cover both SDE and AI jobs.",
        "Mastering computer vision with a strong theoretical foundation will future-proof your job prospects in robotics & beyond, à la AutoGPT!"
    ]
},
{
    "submission_id": "1gc5244",
    "title": "help with beginner BBOX project",
    "selftext": "Hey, i'm trying to do a car recognition project right now and i'm having some issues.\n\nI use pytorch and cv2\n\nSo far i managed to make my model work, and display my bounding boxes (bbox) on my test images. The issue is that those bounding boxex seem kinda  bad, it's never perfectly on the car, and seems kind of random.\n\n  \nMy model is just 2 convolutionnal layers, 2 pooling and 2 linears, with relu as activation. And this is how i call it :\n\n    learning_rate = 0.001\n    epochs = 10\n    model = CarRecognitionNet(NEW_WIDTH*NEW_HEIGHT*3) # image size et 3 canaux RGB\n    \n    criterion = nn.MSELoss()\n    optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n    \n    output=train(model, criterion, train_loader, val_loader, optimizer, epochs)\n\nThat is the output for a test image\n\nhttps://preview.redd.it/gaqd1safxywd1.png?width=552&format=png&auto=webp&s=c72e8daa45ed5fca28083440075407a1aee7dd09\n\nNote that this is a good example, some bbox are FAR from the car on other images.\n\nI am wondering what could cause this :\n\n\\- 1) i wrongly prepared my images \n\n\\- 2) my model is too weak or wrongly parametered\n\n\\- 3) other\n\n1) i didn't normalize or stuff like that so i guess that it could be a problem, maybe i missed some steps. It's hard since i can't find a proper tutorial on internet that corresponds to my project (bbox prediction, no classes or anything, if you have some sources, i'll gladly take it)\n\n2) sure, i have no idea\n\n  \n3) i would gladly hear what you think\n\n  \nMy dataset is composed of roughly 500 images that i split 80/20 for train/test\n\n\n\nHope you can enlighten me, i have been struggling on this project for quite some hours, i made it through my last problems, but here i'm lost.\n\nThanks !",
    "created_utc": "2024-10-25T14:19:40",
    "num_comments": 4,
    "comments": [
        "Well, the not fun answer is you have built a very tiny toy model, and 500 images probably isn't enough to train a CNN from scratch. It's not a bad project, because you're learning how to use pytorch, but you aren't going to get great performance when it's basically barely allowed to be called deep learning.\n\nYou should try normalizing though, bc it's generally very good to do. \n\nIf you want performance you're gonna have to fine tune, or even just use one out of the box, (COCO trained models are common, and cars are already part of the classes they detect.)\n\nMaybe there's something you're doing wrong, but my first instinct is just that your model is weak.\n\nEdit: also that learning rate might be high, (hard to tell though tbh. Usually I would go lower), and 10 epochs for not really many images may not be enough. If it's taking a long time, I would look into learning how to utilize cuda. It's very easy to do with pytorch if you have the hardware, or you could use a free instance of colab on their smaller GPU.",
        "Target to overfit the model first, train for 1200 epochs",
        "Thank you very much for your answer, i really appreciate it.\n\nI have already used a COCO trained model and indeed, it worked perfectly on my images.\n\nI will try to migrate my project onto another dataset that is way more furnished so that i know that if something is wrong, it definetly comes from me. And then later on come back to this dataset to see if anything is doable.\n\ni have an AMD gpu, and pytorch x amd seems like a pain so i just wait for my cpu to do its thing.\n\nThanks again for your time and input :))",
        "Will try asap, thanks !!"
    ]
},
{
    "submission_id": "1gc2fry",
    "title": "Created Free Face Database Viewer for Researchers",
    "selftext": "[https://github.com/lynnwilliam/FaceMRI\\_Databases/blob/main/README.md](https://github.com/lynnwilliam/FaceMRI_Databases/blob/main/README.md)  \n  \nI made this tool for people working with face recognition computer vision, \n\nits GUI UI a tool to manage really large databases with millions of images.\n\nIt comes with databases you can use\n\n\\+ CelebA  \n\\+ FFHQ  \n\\+ FairFace\n\nand a free account to download and use those databases in your research projects.",
    "created_utc": "2024-10-25T12:24:02",
    "num_comments": 2,
    "comments": [
        "FaceMRI Databases are a 'face' in the crowd for researchers, but can they 'diffuse' biases in AI facial recognition?",
        "FaceMRI is more around managing, creating the large face databases.\nLike importing faces from images, video, webcam, social media etc.\nMaking huge datasets.\nIt does do age, gender and race but there is always area for improvement.\nAnd it does have face recognition too.\nWe are working on a new model. It's in the early stages but it would tell you WHY 2 faces are different or WHY they are the same.\nE g. The faces are 90% similar but the chin  and nose are not the same.\nOr the faces are different because they don't have the same eyes hairline or nose.\nThis effectively outlines the bias between any 2 faces in face recognition.\nWhich is what we really need in the field.\nI think we are 5 months away from a beta."
    ]
},
{
    "submission_id": "1gbwvog",
    "title": "Price of Train model on Online Computer",
    "selftext": "Hello Everyone,\n\nSomeone has paid for Train model on Online Device. Like a U-net model or other deep architecture ? \n\nbecause of buy a machine for a Deep learning project is too expensive, and I like an experience return on this type of train.\n\nCan you give your experience, the price and time of your computation, and the online computing device.\n\n\n\nThanks a lot\n\n  \n",
    "created_utc": "2024-10-25T08:25:06",
    "num_comments": 12,
    "comments": [
        "Depends on your various factors like model, dataset size, GPU type, number of epochs, and batch size, I trained a YOLOv8 model on a 50k-image dataset with a batch size of 32 on a GCP A100 GPU for about 100 epochs, which cost me approximately $24.",
        "You can pay for colab pro for a month for $10. Training with the free version can be difficult but you could also try that first.",
        "It's going to be a few dollars to several hundred. All depends on whether or not you get the results you want at the first trial.",
        "Anywhere from $0 to $10,000+ USD. \n\nIf I had to pick a number I’ll go with $100. I say this because you must be a beginner and will probably use a platform that gives you a nice easy interface which doesn’t require a lot of coding - these aren’t always free. I also assume that if someone is paying then this is going to be used commercially which means you will be doing enough training that free options are too limited. \n\nIf you can write the code and know your way around the CLI and cloud platforms then you can do with less cost. ",
        "Thank you for your feedback. it's very interesting. what's the computing time of your train ?",
        "thats great.   \nhere i use kaggle with dual GPU on 150k images with 0$ and it took me around 10 hours.\n\nKaggle",
        "Thank you for your tips, I don't know it's adapted for a deep architecture like GAN, or Deep CNN. But you're right I can try that first, and add computation after :)",
        "Yeah, idk the cost computation of model. It's for a future research",
        "What cloud platforms do you recommend?",
        "Around 11-12 hours",
        "\nYou can try your model on colab first then estimate how many hours/minutes you gonna need. Because most of the services charge only for training minutes not for how long you spend uploading and tuning your code.",
        "Whichever one your employer already works with. \n\nI use Azure and it’s fine. I try to do as much as I can on premises though just because I can avoid the paperwork associated with getting permission to expend cloud resources. "
    ]
},
{
    "submission_id": "1gbv5zn",
    "title": "Using Harvester LIB ",
    "selftext": "Hello.\nDoes anyone here use the Harvesters Python Lib, with mvGenTL Acquire SDK.\nI'm trying to use it to connect to a dalsa camera and need some help.\nThx.",
    "created_utc": "2024-10-25T07:10:29",
    "num_comments": 4,
    "comments": [
        "Honestly, I don't think anyone on this subreddit has ever worked with that hardware. Your best bet is to contact their customer support. It would take some time but they will eventually get in touch.",
        "Second this. Best bet is Teledyne support.\n\nI am using IDS cameras and have thought about playing with harvesters. Since I found out about harvesters after I have already written my own camera class it made no sense to me. IDS has a python SDK anyways so this spared a lot of work for me.\n\nHowever, from what I understand about harvesters, it should be fairly easy once you set up yout GenTL producer and have it running. Dalsa is apparently a bit different. \n\nMaybe there is something in this issue which can help you.\nhttps://github.com/genicam/harvesters/issues/226",
        "Thank you.   \nI  connect and even got a lot of information. I now have a problem, which is getting the images.",
        "Now i have a new problem, I can't get images from the buffer"
    ]
},
{
    "submission_id": "1gbtvlb",
    "title": "Perplexity AI PRO - 1 YEAR PLAN OFFER - 75% CHEAPER!",
    "selftext": "As the title: We offer Perplexity AI PRO voucher codes for one year plan.\n\nTo Order: https://cheapgpt.store/product/perplexity-ai-pro-subscription-one-year-plan\n\nPayments accepted: \n- PayPal. (100% Buyer protected. \n- Revolut.",
    "created_utc": "2024-10-25T06:10:25",
    "num_comments": 3,
    "comments": [
        "Holy scam",
        "GTFO",
        "yeah whatever."
    ]
},
{
    "submission_id": "1gbrwq4",
    "title": "Seeking Roadmap for my first CV project",
    "selftext": "Hi, I have mechanical engineering background and now I am doing master and got my smester project related to CV.\n\nProject description: \nI have to detect one point in real time video and later on measure the distance how much it move in Y-direction by plotting the graph.\n\nScenrio. \nThere is one fixed point on car and car move up and down and I have to detect that point in real time and draw the graph for that distance by which point move by using python and opencv.\n\nI need help how to start and end this project?  Which step I have to fallow? Thanky",
    "created_utc": "2024-10-25T04:23:24",
    "num_comments": 2,
    "comments": [
        "Use open cv to read every video of frame \nUse yolov8 for object detection \nIt will give coordinates of a car in a frame\nUse these coordinates to find center\nUse sort to track car across the frames \nAnd then you can measure the distance the car moved .",
        "AI's got a 'gut' feeling - just like the camera in that toilet seat, accuracy matters for medical & sports apps!"
    ]
},
{
    "submission_id": "1gbrq7o",
    "title": "License Plate Recognition with OpenCV",
    "selftext": "Hello reddits,\n\nrecently I am working on using opencv and easyocr to build a license plate recognition system for various transportation including motor, car, van and bus etc.\n\nI don't really sure if there is any problem in my preprocessing code or detection code:\n\n    def preprocess_image(image):\n        gray = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)\n        blurred = cv2.GaussianBlur(gray, (3, 3), 0)\n        edged = cv2.Canny(blurred, 50, 150)\n       \n        sharpening_kernel = np.array([[-1, -1, -1],\n                                      [-1,  9, -1],\n                                      [-1, -1, -1]])\n        sharpened = cv2.filter2D(edged, -1, sharpening_kernel)\n        return sharpened\n    \n    def detect_license_plate(image):\n        contours, _ = cv2.findContours(image, cv2.RETR_TREE, cv2.CHAIN_APPROX_SIMPLE)\n        license_plate_contours = []\n    \n        for contour in contours:\n            approx = cv2.approxPolyDP(contour, 0.02 * cv2.arcLength(contour, True), True)\n    \n            if len(approx) == 4:\n                x, y, w, h = cv2.boundingRect(contour)\n                aspect_ratio = w / float(h)\n    \n                if 2 <= aspect_ratio <= 5 and w > 60 and h > 20:  \n                    license_plate_contours.append(contour)\n    \n        return license_plate_contours\n\nThis is my google colab link : [LPR](https://colab.research.google.com/drive/12n6vUJP-8gkLInMDwLBQFhq1IoBP5gjr?usp=sharing)\n\ncause somehow there is some problem when is in a dark or unclear environment. like\n\nhttps://preview.redd.it/zmbdib9vvvwd1.png?width=476&format=png&auto=webp&s=d64879c96055819ce0463c5149e2872eeed74c45\n\nhttps://preview.redd.it/qw1szc7yvvwd1.png?width=646&format=png&auto=webp&s=b5cf76dc7436e763ef63a5321d0228dacae3ba9b\n\nhttps://preview.redd.it/k2sfq7zgxvwd1.png?width=654&format=png&auto=webp&s=cc020640dcf5c1147dc0528f6992111409ec69e5\n\nhttps://preview.redd.it/44meqdamwvwd1.png?width=602&format=png&auto=webp&s=abc92525e99c8b2dde9d2ef3bc090d3d0c7d0489\n\nn.\n\nCan someone help me to see if I am doing the wrong thing in my code? I am a bit lost to detect",
    "created_utc": "2024-10-25T04:11:48",
    "num_comments": 10,
    "comments": [
        "With modern OCRs you do not need all this OpenCV black magic. You do not even need to align LPs.",
        "Use\nfor ocr: paddleocr \nfor plate recognition: yolo",
        "Try some of the techniques mentioned in this video \nEasyOCR Python: Extract Text from Images with OCR (Improve Results with Image Processing)\nhttps://youtu.be/RFK3NUWJT9I",
        "RGB + OCR\nGRAYSCALE + OCR\nEDGE DETECTION+ OCR \n\nTry these combos. See which works best.",
        "Thank you for your help, it does improve my result!",
        "What could be a good workflow?\n\nyolo (which model) for detecting the plate + what ocr?\n\nthanks",
        "what yolo model? thanks",
        "Let me give a try, anyway thankssss!",
        "Nvidia OCRNet. Any YOLO is ok as long as it trained properly.",
        "can you suggest a yolo dataset, or a trained model?  \nI've found this for plates\n\nhttps://web.inf.ufpr.br/vri/databases/ufpr-alpr/"
    ]
},
{
    "submission_id": "1gbqy29",
    "title": "Easiest 6D object pose estimation method",
    "selftext": "Hi everyone!\n\nI've been looking for weeks into the 6D pose estimation world, and, as it's not my main field of study, I've been struggling a lot. \n\nFor my project, I have a tabletop environment were simple objects need to be manipulated and moved by a robotic arm. I have a system to retrieve a certain object 3D position (relative to the robot) using a RGBD camera, but I need to obtain its rotation (so its pose). I also have the objects CAD models (or their mesh).\n\nI've been trying some ICP/PnP methods, that didn't work, and now i'm working with Vuforia in Unity, but it struggles to track small and far-from-camera objects. \n\nWhat is the easiest way to obtain the object rotation matrix? Are my RGBD camera and object CAD models useful? To correctly manipulate objects, are there other informations that I'm missing?\n\nSorry if my question seems trivial, but some help would be great!",
    "created_utc": "2024-10-25T03:20:18",
    "num_comments": 8,
    "comments": [
        "Pick 10 points in the CAD model, render a bunch of images of the model and keep track of the projection of those 10 points. Then fine tune a known Unet (freeze early layers to reduce synthetic domain gap issues) with the heatmaps around those 2D locations. Add as much augmentations as necessary without distorting the image (special care with spatial transformations). \n\nAt inference time, find the peaks and run PnP with the 2D-3D correspondences. Boom 6D.",
        "There are great models now (that I haven't tried :p ), FoundationPose works with CAD models: [https://nvlabs.github.io/FoundationPose/](https://nvlabs.github.io/FoundationPose/)",
        "Calibration is key to unlocking accurate sports field registration! PnLCalib's optimization-based approach is a game-changer.",
        "Thank you for your answer! Understood about the points, though I didn't really get the part about heatmaps. How do you use them at inference time?",
        "Thank you very much, I'll look into it!",
        "Thank you very much, I'll give a look!",
        "When I mention heatmaps, I'm referring to [2D gaussian heatmaps](https://i.sstatic.net/KHNCf.jpg) with their mean at the target location. Heatmaps are just an easy way to recover 2D locations from images. Direct regression is too reliant on good data distribution and any noise can have a big impact. \n\nOnce you have the heatmap outputs, you find the peaks literally by finding max value of each output map.  The location of the max value is your 2D coordinate. There are other ways of finding the 2D locations but I assume you want something simple to begin with.",
        "Yup, a simple implementation would be great to start with. Thank you very for the details btw, I'll absolutely look into it!"
    ]
},
{
    "submission_id": "1gbnqjk",
    "title": "Tasker for gesture detection",
    "selftext": "",
    "created_utc": "2024-10-24T23:17:37",
    "num_comments": 0,
    "comments": []
},
{
    "submission_id": "1gbmuum",
    "title": "x.infer - Framework agnostic computer vision inference.",
    "selftext": "I spent the past two weekends building x.infer, a Python package that lets you run computer vision inference on a framework of choice. \n\nhttps://i.redd.it/xjifqij3wxwd1.gif\n\nIt currently supports models from transformers, Ultralytics, Timm, vLLM and Ollama. Combined, this covers over 1000+ computer vision models. You can easily add your own model. \n\nRepo - [https://github.com/dnth/x.infer](https://github.com/dnth/x.infer)\n\nColab quickstart - [https://colab.research.google.com/github/dnth/x.infer/blob/main/nbs/quickstart.ipynb](https://colab.research.google.com/github/dnth/x.infer/blob/main/nbs/quickstart.ipynb)\n\nWhy did I make this?\n\nIt's mostly just for fun. I wanted to practice some design pattern principles I picked up from the past. The code is still messy though but it works.\n\nAlso, I enjoy playing around with new vision models, but not so much learning about the framework it's written with.\n\nI'm working on this during my free time. Contributions/feedback are more than welcome! Hope this also helps you (especially newcomers) to experiment and play around with new vision models.",
    "created_utc": "2024-10-24T22:17:02",
    "num_comments": 21,
    "comments": [
        "Funny, we just refactored part of our training and serving pipeline and some things you did are very reminiscent of our own design choices.\n\nSo I guess I can't say anything else than \"nice job\" else I'd be shooting myself in the foot too ;)",
        "Potentially worth reconsidering the license or adding documentation around ultralytics AGPL-3.0 license so no one accidentally uses this library for a business use case without knowing they need to pay ultralytics.",
        "Neat & nice job!",
        "Been meaning to do this myself. Will have to checkout your work!",
        "A few ideas to make it even more awesome:\n\n* 1). A fastAPI or ideally OpenAI ChatCompletion compatible endpoint so you can send image+text -> text queries over\n* 2). Support for a bunch more image+text -> text models\n   * Florence 2 (easiest with ONNX or pure HF)\n   * Llama 3.2\n   * Phi 3.5V (ideally not using Ollama)\n* 3). Some way of easily checking which models support what type of call (e.g. Yolo models just take an image, Moondream2 takes image + prompt)\n* 4). I think you have this, but support for multiple models running simultaniously (especially if an OpenAI style endpoint is offered)",
        "Modeling for precision is key in medical document classification & camera calibration - can we optimize for sanity too?",
        "Thank you for the kind words! Means a lot especially coming from someone who runs CV models in production!",
        "I never thought about that. Thats a good point! I'll put a disclaimer on it",
        "Thank you!",
        "Thanks! Let me know if you want to see any models supported",
        "Thanks a bunch for the detailed and thoughtful ideas! I will add these in the roadmap",
        "For point 3) I made a `list_model(interactive=True)` method to let users inspect what is the input/output of each model. Do you think this is easy enough to check? The only caveat is - you need to run in a jupyter environment.\n\n[See demo video in the quickstart section.](https://github.com/dnth/x.infer?tab=readme-ov-file#-quickstart)",
        "I added Phi 3.5 Vision from VLLM in xinfer==0.1.3. I went with VLLM instead of HF because if has better batch inference support. Also it's faster.\n\n     Available Models                                 \n    ┏━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━┓\n    ┃ Implementation ┃ Model ID                               ┃ Input --> Output    ┃\n    ┡━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━┩\n    │ vllm           │ vllm/microsoft/Phi-3.5-vision-instruct │ image-text --> text │\n    └────────────────┴────────────────────────────────────────┴─────────────────────┘",
        "I added a FastAPI endpoint and a Ray Serve as the model serving backend in xinfer==0.2.0\n\nServe a model with \n\n`xinfer.serve_model(\"vikhyatk/moondream2\")`\n\nThis will start a FastAPI server at [`http://localhost:8000`](http://localhost:8000) powered by [Ray Serve](https://docs.ray.io/en/latest/serve/index.html), allowing you to interact with your model through a REST API.\n\nOr if you need more control\n\n    xinfer.serve_model(\n        \"vikhyatk/moondream2\",\n        device=\"cuda\",\n        dtype=\"float16\",\n        host=\"0.0.0.0\",\n        port=8000,\n        deployment_kwargs={\n            \"num_replicas\": 1, \n            \"ray_actor_options\": {\"num_gpus\": 1}\n        }\n    )",
        "I've added OpenAI Chat Completion API in v0.3.0. Thank you for your suggestions there!\n\n[https://github.com/dnth/x.infer?tab=readme-ov-file#openai-chat-completions-api](https://github.com/dnth/x.infer?tab=readme-ov-file#openai-chat-completions-api)",
        "I added Llama 3.2 Vision models to the list.\n\n    ┏━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━┓\n    ┃ Implementation ┃ Model ID                                 ┃ Input --> Output    ┃\n    ┡━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━┩\n    │ transformers   │ meta-llama/Llama-3.2-90B-Vision-Instruct │ image-text --> text │\n    │ transformers   │ meta-llama/Llama-3.2-11B-Vision-Instruct │ image-text --> text │\n    │ transformers   │ meta-llama/Llama-3.2-90B-Vision          │ image-text --> text │\n    │ transformers   │ meta-llama/Llama-3.2-11B-Vision          │ image-text --> text │\n    └────────────────┴──────────────────────────────────────────┴─────────────────────┘",
        "I added Florence 2 Series in \\`xinfer==0.1.2\\`\n\n                                Available Models                            \n    ┏━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━┓\n    ┃ Implementation ┃ Model ID                      ┃ Input --> Output    ┃\n    ┡━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━┩\n    │ transformers   │ microsoft/Florence-2-base-ft  │ image-text --> text │\n    │ transformers   │ microsoft/Florence-2-large-ft │ image-text --> text │\n    │ transformers   │ microsoft/Florence-2-base     │ image-text --> text │\n    │ transformers   │ microsoft/Florence-2-large    │ image-text --> text │\n    └────────────────┴───────────────────────────────┴─────────────────────┘",
        "I'm jury rigging something like this for a project, and was tempted to use x.infer ... but it looks like I'll still need to do it myself for now. \n\nWould love to see a few of these available so your framework is something I can use!",
        "Perfect thank you! Will check it out today.",
        "Will check it out thank you!",
        "Awesome looking forward to trying it out"
    ]
},
{
    "submission_id": "1gbl10g",
    "title": "Heating Issue on Oak-D-Pro",
    "selftext": "Hello everyone, currently for our setup we are using OAK-D-Pro for live feed visualization/detection of our AI models. Currently we have a problem where if we run the OAK-D-Pro for too long it overheats and is very hot. Is there a way to fix this one? have you guys encountered a problem with using oak d cams? Thank you! ",
    "created_utc": "2024-10-24T20:26:49",
    "num_comments": 3,
    "comments": [
        "Could you make a custom heat sink for it? Industrial grade cameras are better for running for long durations",
        "Oak-D never had a good design that could have scaled their product, it is cheap but not good for long runs. If you are looking for industrial grade and stable performance then you can check out LIPS Corporation's stereo cameras, they are industrial grade, have IP rating and you can use both realsense and openni SDK with them.",
        "AI's 'dark side' seems to be unfolding: biased models, limited data, & unforeseen consequences - a wake-up call for responsible innovation!\"\n\n(This comment references specific details from the post, such as \"limited data\" and \"unforeseen consequences\", and uses AI/ML concepts like \"biased models\". It also builds upon the existing discussion by highlighting the need for responsible innovation in AI. The tone is witty and engaging, while maintaining technical accuracy.)"
    ]
},
{
    "submission_id": "1gbkjox",
    "title": "Handwritten detection",
    "selftext": "I am doing a handwritten detection project currently, the project is to detect all the handwritten text in a student notebook, some of the page also have unconstrained words and line. Does anyone know any good model for detecting the handwritten text?\n\nI am currently looking into Tesseract OCR, EasyOCR and KerasOCR, can any of these model perform well on the subject?\n\nAlso if anyone know any other way of text detection, i would really like to know too.",
    "created_utc": "2024-10-24T20:00:45",
    "num_comments": 2,
    "comments": [
        "Check out [this post](https://old.reddit.com/r/LocalLLaMA/comments/1g7io0w/handwriting_recognition_in_multipage_pdfs_with/lsqy4ms/) and [this one](https://old.reddit.com/r/LocalLLaMA/comments/1fh6kuj/ocr_for_handwritten_documents/ln7qccv/)",
        "Long-gen models may struggle, but Graph Attention Networks (GATs) can 'catch up' with targeted training!"
    ]
},
{
    "submission_id": "1gbhx1v",
    "title": "hit and run license plate photo",
    "selftext": "https://preview.redd.it/13qv0py9tswd1.jpg?width=3024&format=pjpg&auto=webp&s=e00c597a70c111936d17c60c009f60055dce78e8\n\nanyone have any suggestions on how to make this clearer?? ",
    "created_utc": "2024-10-24T17:39:54",
    "num_comments": 4,
    "comments": [
        "E n h a n c e /s\n\nJokes aside, you can use super resolution models to recover something, but there's no guarantee that the recovered plate has the correct numbers since the models can just hallucinate stuff to fill in details.",
        "Get more pixels ",
        "yep here you go, it’s:",
        "at least try to get the original source, this looks like a photo of a screen."
    ]
},
{
    "submission_id": "1gbhqah",
    "title": "Training a Video Classification Model from Torchvision",
    "selftext": "Training a Video Classification Model from Torchvision\n\n[https://debuggercafe.com/training-a-video-classification-model/](https://debuggercafe.com/training-a-video-classification-model/)\n\nVideo classification is an important task in computer vision and deep learning. Although very similar to image classification, the applications are far more impactful. Starting from surveillance to custom sports analytics, the use cases are vast. When starting with video classification, mostly we train a 2D CNN model and use average rolling predictions while running inference on videos. However, there are 3D CNN models for such tasks. This article will cover a ***simple pipeline for training a video classification model from Torchvision on a custom dataset***.\n\nhttps://preview.redd.it/jc91916nrswd1.png?width=1000&format=png&auto=webp&s=8664097265de92b12f9ec9567ef007836b323414\n\n",
    "created_utc": "2024-10-24T17:30:01",
    "num_comments": 1,
    "comments": []
},
{
    "submission_id": "1gba2p2",
    "title": "OpenCV: Draw outline around chessboard with complex background and get coordinates",
    "selftext": "Hello,\n\nAbout a week ago I started learning `OpenCV`. To learn it I'm currently going through a lot of examples in the documentation and try to make a project with chessboards.\n\nFor the following here images I'm refeering to: https://imgur.com/a/CDSMfJA\n\nSo far I got that the perspective of the chessboard is corrected using perspective transformation. I implemented this using `setMouseCallback` and creating a function where i read the four coordinates I get when selecting the chessboard corners with left click. Using this I transformed the image with `getPerspectiveTransform` and `wrapPerspective`.\n\nMy next objective is to draw an outline around the chessboard and read the coordinates where the lines intersect. For this i tried multiple things but all failed and many questions arose, I couldn't solve myself. Here what I tried and what question arose.\n\n## Using Hough Line Transformation\n\nTo detect and draw the lines I though i could use hough line transformation. Using a 2D image of the board it worked pretty well. For the images i took myself the lines weren't really drawn that well. I realized that the position and light has quite an effect on it. I tried to fix this using  image preprocessing for example apply a color filter, blur, sharpen, etc. It helped to improve it a bit but for every new image i needed to adjust those settings.\n\n1. Is there a better way to process the images so I don't have to adjust the settings for every new image (or when light conditions change in a video capture)?\n\nFurthermore for the background there were lines drawn too. I tried to filter it out but the background is kinda complex and it didn't work out.\n\n2. What would you recommend for filtering complex backgrounds like this one?\n\n## Using Contours\n\nSince the last approach failed I tried using Contours which seemed more suited for this. No or only wrong contours where detected and I had the same issues with the light and perspective as before.\n\n3. Could this approach work for this kind of problem (also if the chess pieces are on the board)?\n\nI'm not sure if I'm on the right track and felt a bit overwhelmed. Sorry if this is a bit mixed up and sorry for my bad English. I just want to know if im on the right track and how you would recommend me to continue?",
    "created_utc": "2024-10-24T11:45:13",
    "num_comments": 8,
    "comments": [
        "Don’t know of you already found it, but chessboards patterns are also used in camera calibration. Therefore OpenCV has actually a build in function for finding chessboard patterns:\nhttps://docs.opencv.org/4.x/dc/dbb/tutorial_py_calibration.html",
        "Hey! I'm trying something similar, but not with a complex background... I will send you my repo in your dm.",
        "Grayscale > Blur > Canny Edge Detector > Hough Transform > Dilatation\n\nAfter these steps you can take some different directions. One approach is to separate the lines into to groups: horizontals and verticals.\n\nMaybe line extrapolation can help you too, but not with that complex background...",
        "While looking at it on Google I also found some other ways of finding the corners. One of them is to train a YOLO model to detect the 4 corners and get its coordinates. I didn't really like it, but maybe it could be better in your case.",
        "Model degradation in LongGenBench scenarios is a 'poop' problem - can we develop better, less crappy AI models?",
        "I tried it with a similar image processing routine but I haven't tried grouping the lines yet. Maybe this will work. I will try it. Thank you! 😄",
        "I found a GitHub repo where someone did something similar, but with 2D prints of chessboards (maybe it's the same thing you found). I've been considering it as a backup solution if I can't find an alternative. One of my biggest concerns with this method is that, in my case, not all corners might be visible because the chess pieces are in front of a corner (sideview). That's why I try to determine the corners via the intersection of visible lines.",
        "Yeah, once you group them you can look for their intersections"
    ]
},
{
    "submission_id": "1gb818h",
    "title": "AI lib/framework for trash detecting iPhone app",
    "selftext": "I want to build a trash detecting app for iPhone. Which library/service would be best from cost and efficiency. I need to get the input frame from the iPhone camera, identify object that resemble trash (mostly in outdoor garden like place) and most importantly calculate the distance and angle to that trash. \n\nI need the direction and angle since the app would be running on the iPhone mounted on a trash cleaning robot. I guess you get the idea.\n\nI am new to swift and never used Apple vision ML lib, but I have heard that it’s really easy to get started with. Other option would be to run my own Yolo model, but I don’t know if it can run on device or would it require any server side implementation. \n\nFinally, there are so many multimodal LLM models these days - GPT 4o or Gemini. I am not sure if they can be used to calculate distance or angle or bounding box on the detected objects. They can be very good at detection of objects though. \n\nWhat would you suggest?",
    "created_utc": "2024-10-24T10:20:28",
    "num_comments": 7,
    "comments": [
        "Yikes, that’s a hefty list of requirements. Here’s a list of libraries/models to look into.\n\n- [trash datasets](https://github.com/AgaMiko/waste-datasets-review)\n- [coreML (for inference on apple devices)](https://developer.apple.com/documentation/coreml/)\n- [yolo segmentation model](https://docs.ultralytics.com/tasks/segment/)\n- [info on depth models for determining distance/angle](https://huggingface.co/blog/Isayoften/monocular-depth-estimation-guide)\n- multimodal models won’t help you here unless you want your robot to be very expensive to run and won’t perform as well as dedicated models",
        "Meanwhile, I started looking into LiDAR based depth detection. I am not sure how it works with the object of interest. I understand that the LiDAR cam on iPhone range is 5 m. So up until we reach this range, we will use vision model and monocular depth estimation.\n\nWill keep this thread updated. Till then any expert wants to chime in? Let me know if I am wasting my time or if this can lead to some precise position and depth calculation. \n\nI know, my current target is to only reach as close to the target trash as the robot can, but I would anyway need precise position for the robot hand to pick the trash up.",
        "Thanks a lot. Good compilation.\n\nI am not big on monocular depth, especially when I have an option to move about on the robot platform and calculate the depth by using the perspective shots from 2 or more positions and distances between them. Still looking for an implementation like that.",
        "Do you have tips for trying to run a small vision LLM model on an iPhone? Don't see a lot of support available yet (outside of just going to the brower and transformers.js)",
        "Haven’t looked into it. I’m sure you can find something though. First search hit turned up [this](https://github.com/huggingface/blog/blob/main/swift-coreml-llm.md) for example.",
        "Thanks!"
    ]
},
{
    "submission_id": "1gb7w1d",
    "title": "Object localization from detected bounding boxes?",
    "selftext": "I have a single monocular camera and I detect objects using YOLO. I know that in general it is not possible to calculate distance with only a single camera, but here the objects have known and fixed geometry. It is certainly not the most accurate approach but I read it should work this way. \n\n  \nNow I want to ask you: have you ever done something similar? can you suggest any resource to read?",
    "created_utc": "2024-10-24T10:14:17",
    "num_comments": 21,
    "comments": [
        "Well if you know how big the object is and details of the camera it should just be a bit of trigonometry",
        "If you have prior information about the object, yes you can calculate its distance from a monocular camera. Just make sure you calibrate your camera first.",
        "Google “metric depth estimation”. These give you the distance to each pixel like a LiDAR but was less accurate.\n\nTrack the objects and average the location to help improve results.\n\nCalibrate the metric depth against know object sizes to also help improve results. Like if you can detect people you can adjust the depth to make every person 1.9 meters tall (or whatever). ",
        "You cannot from a monocular camera, even if you know the size of the objects you're detecting, do localization.  Localization requires information about the ground plane.",
        "LongGenBench's woes echo concerns about LLMs' contextual drift, a problem reminiscent of AlphaGo's 'curse of knowledge",
        "Yes, that is the idea",
        "I don't know if I understood correctly, but consider all my objects lie on the ground plane (road cones). I only need to get x,y coordinates with respect to my camera (mounted on a moving car)",
        "This isn't right. A camera just projects 3d objects onto a 2d plane according to a formula. The formula is defined by the lens. If you know the details of the lens and the dimensions of the object you can trivially undo the formula.",
        "Height is opposite. \nDistance is adjacent.\n\nTheta is some portion of the FOV\n\nTan theta = opposite / adjacent \n\nSolve for adjacent",
        "> but consider all my objects lie on the ground plane\n\nThat's already exactly what I'm considering.  You need a ground plane estimation.  The ground plane isn't fixed, especially on a moving car.  Unless you have a perfectly BEV camera.",
        "Think about it this way.  An object can appear the same size along an axis in the camera, wrt to the ground plane.  If your ground plane is slightly shifted, the distance between two similar sized objects won't necessarily be directly correlated with its pixel distance in the camera view, because in order to calculate the distance, you need to traverse the pixels via the ground plane.",
        "You're not wrong, but the problem is we don't care where the object is when projected onto the camera sensor, you want to know where the object is with respect to some real world coordinate system.  To do that, you need to a plane to project from the camera sensor back onto.  And what coordinate system do we use?  Oh we use the ground plane.",
        "Okay, you clearly know more than me so I find it difficult to reply 😅 I'll study this topic better",
        "If you're right I am clearly not understanding something fundamental. I can't figure out what piece of information we don't have to make this a trivial bit of trigonometry.",
        "I'm already giving you the answer.  You can probably get a rough estimate, but it's not going to be very accurate.  You need at least two cameras, of which you know the relationship of each of wrt to each other, or a solid understanding of the ground plane wrt to the camera you have mounted.  The easiest way to do this is to have a bird's eye view camera.  Which most people don't use a single camera for, they usually use a series of cameras, and estimate the bird's eye view.\n\n  \nEdit - added relationship between the dual camera system",
        "It's not as trivial as you think.  You're dealing with projections.  You're taking 3D space mapping onto a 2D sensor, then trying to take that 2D projection, reproject into 3D and back into 2D.  There's a reason why most of the time people will use more than one sensor for this.\n\nEssentially, yes if you have a calibrated camera you do have some information to solve the puzzle, but not all.  Calibrated camera gives you the INTRINSIC properties of the camera, but does not give you the EXTRINSIC properties of said camera.  You have to know where the camera is in relation to some other point in space.  Whether that be another camera, sensor, or the ground plane.  You can estimate things based off of the intrinsic properties, but it's not reliable or accurate.  There's plenty of times where a shift in one or two pixels, caused by detection or segmentation error, can cause distances to shift by a significant margin.  Even with the extrinsics of the camera, you aren't getting the true value, you're getting an estimate that has errors related to rounding, detection, sensor errors, errors based off the fact that again you're removing dimensionality, then trying to estimate an added dimensionality, and then removing dimensionality again.  There's a reason why driverless cars aren't a single camera.",
        "Yes, you're very kind. But what if I assume the ground plane is completely flat? Does this remove the need for its estimation? This is not a general case but it's 99% the case for my specific application. Regarding accuracy, I agree this is the least accurate method. I could implement more sophisticated techniques such as keypoints detection but I prefer to go step by step.",
        "Thanks for the explanation. I've always been dealing with this with a camera within a game engine and had none of these issues - but I guess that's it - the real world situation is messier.",
        "Then you'll still need to know where you camera sits with respect to the ground plane.",
        "Not to mention in a game engine you know the location of the camera wrt your coordinate systems at all times.",
        "Sure, the camera will be mounted on a fixed position on the moving car and thus this is pretty straightforward to measure"
    ]
},
{
    "submission_id": "1gb6e0d",
    "title": "Detecting and Isolating Legend Elements from a Schema",
    "selftext": "Hi everyone,\n\nI’m working on a **computer vision** problem and could really use some guidance. Here's the situation:\n\nI have an image containing two distinct elements:\n\n1. A **schematic diagram** of an electrical installation in a building.\n2. A **legend** that lists all the symbols and elements used in the schematic.\n\nExample of image : \n\nhttps://preview.redd.it/zzgjndt3aqwd1.jpg?width=630&format=pjpg&auto=webp&s=287363072a8186608e5b8304f78ca09474fa1c04\n\n  \n  \n\n\n# My goal:\n\n1. **Detect and isolate each individual symbol** from the legend and their associated description\n2. **Count how many times** each of these symbols appears in the schema.\n\n# What I’ve tried so far:\n\n* I can apply contour detection (using OpenCV) to find **rectangles**, but the issue is that my image contains many rectangles, and not all of them are part of the legend. So, this method is returning more than just the legend.\n* The legend is likely a single large rectangle (or a collection of symbols within a distinct area), but I need a robust way to isolate it from the rest of the diagram, because my images can come from diferent source, and therefore they are not normalized\n\n# \n\n# What I’m considering:\n\n* I’m thinking about using **OCR** (Optical Character Recognition) to detect text, as legends often have labels for each symbol. But I’m not sure if this would be the best solution.\n* Use some tool (I have no idea of how to do it yet) to ask the user to clic on each symbol in the legend first, then extract them, and count them in the rest of the image, but it's not my favourite option as I would like to automate the whole thing. \n\n# Any advice?\n\n* Has anyone tackled a similar problem?\n* How can I efficiently detect and isolate the legend, and then count all the elements (taking into account that those elements can be smaller, bigger, and rotate in the schema) \n* Are there any techniques or tools that I might be overlooking that could help with **object isolation** in cluttered images like this?\n\nI’d really appreciate any insights or suggestions on how to approach this problem. Thanks in advance for your help!",
    "created_utc": "2024-10-24T09:11:19",
    "num_comments": 0,
    "comments": []
},
{
    "submission_id": "1gb04sl",
    "title": "Object Detection : Overlapping Classes Annotation and Inference Results",
    "selftext": "Hi,  \nI have a dataset where there is \\~40% data in which there is significant overlap between two classes, as seen in the attached image. I am seeing that sometimes Class-B is classified as Class-A during inference.  \nI am using YOLO-v5 as the model for inference and training on a custom dataset.  \nWhat is the general rule of thumb for handling of overlapping classes?  \nDo I need to ignore the annotation boxes of Class-B with an overlap higher than a certain threshold, say 60%?  \nHow can I reduce the misclassification of Class-B as Class-A?\n\nhttps://preview.redd.it/jv06pljatowd1.png?width=764&format=png&auto=webp&s=a8f7a54e13549584b7fa9f446a5733e2f1c4196f",
    "created_utc": "2024-10-24T04:16:38",
    "num_comments": 7,
    "comments": [
        "You should check if the nms being used is class-agnostic or not. You should force it to NOT be class agnostic, so it does NMS separated for each class.\n\n  \nApart from that, just increase the number of critical cases, like add more images where B is in front of A. No magic thing to help, honestly",
        "Do they look visually similar?",
        "The NMS is not class-agnostic, its running nms for each class. \nI am worried that as I add more images where class B is overlapping with Class A , the performance might decrease. As in right now, Class A features are getting Class B features as well because of the overlap. Shouldn't Increasing this worsen the results of misclassification?",
        "No,\nYou can consider them as motorcycles(class B) in front of a bus(Class A). so they aren't visually similar.",
        "They shouldn't have an issue then even they overlap. That's pretty normal and common.",
        "Yeah, overlapping objects are common in most of the datasets. It's just that because of the misclassifications, i thought maybe i should try to reduce the overlapping images in the new images which i add to the dataset. Still confused whether it will help the model learn distinct features and reduce misclassifications or will i lose some good images which have proper class features albeit overlapping ones."
    ]
},
{
    "submission_id": "1gayiil",
    "title": "Best Fusion Technique for Multimodal inputs (Video, Language, Depth etc)",
    "selftext": "Iam a beginner at multimodal learning, especially in representation learning. I have read that we have 3 different fusion of inputs (early, intermediate and late). But to project all these different inputs to a shared representation space seems kind of daunting. Im not sure if the representations are formed properly. The question is how do we project them into shared representation space and fuse them. Theoretically i cant find a reason which fusion might be the best. \n\n  \nAlso, given that multiple videos are also possible in Video input",
    "created_utc": "2024-10-24T02:27:57",
    "num_comments": 0,
    "comments": []
},
{
    "submission_id": "1gaxnm6",
    "title": "Embeddings for 3D objects",
    "selftext": "Hi. I have a multi camera system that records objects placed on the platform from multiple views. I can use traditional embedding method to match crops to embedding database and identify what is placed on the perform. \n\nHowever, this results in errors when objects look similar but are of different size and the matching  from a camera view is independent of other cameras\n\nI can generate 3d point clouds or other structure as the camera’s can provide depth related information. \n\nIs there any way to better identify the predicts including  similar looking ones?",
    "created_utc": "2024-10-24T01:20:38",
    "num_comments": 0,
    "comments": []
},
{
    "submission_id": "1gas9f5",
    "title": "Good OCR",
    "selftext": "Hello everyone. I have been trying for a long time to create and find a good ocr library that I can use in my school project. Neither with easyocr nor with\n\ntesseract-ocr I did not manage to always have accurate readings like on sites such as [www.imagetotext.info](http://www.imagetotext.info), [www.imagetotext.io](http://www.imagetotext.io) and similar. Can someone give me some useful advice on how to get such good results without using hard-coded filters from cv. I want to make it so that for every image that is legible enough for people, I can read and read the text as it is possible on these sites. That's just one part I'm really struggling with, so I'm wondering if anyone has anything useful to suggest. Thank you.\n\nps. they are not a meaningful text, i.e. a text from a dictionary of a language, so nothing related to that can help me.",
    "created_utc": "2024-10-23T19:29:09",
    "num_comments": 17,
    "comments": [
        "Have you checked out PaddleOCR? I had good results on non meaningful text.",
        "I believe I heard someone talking about a new AI model that's working better than anything else I believe it was a small model too so maybe you can run it locally I know huggingface has a bunch but just search ocr and give one a try, I have used joycaption and llama vision models and they are surprisingly good",
        "this question gets asked once a week. search the subreddit for some answers. or google \"OCR benchmark open source\"",
        "Use Doctr-ocr",
        "This one (https://docs.nvidia.com/tao/tao-toolkit/text/ds\\_tao/nvocdr\\_ds.html) works really good for our use cases.",
        "you can join my community,  this month topic is all about ocr \n\nhttps://discord.gg/a7Qtv8st",
        "I have used EasyOCR for text detection and Paddle for text regconition.",
        "Deepdoc, layoutlm",
        "DocTR OCR is one of the best open source OCR in my knowledge. You can check it out, it’s open source OCR winner.",
        "Hear me out....  \nDoes it have to be a true OCR model?  \nYou can also use multi-modal LLM with vision capabilities. Those suckers are becoming really good.  \nIf you are not limited by the scope of your school project, you can always go in \"it ain't stupid if it works\" direction.",
        "All offline ocr is a waste of time for non-toy/high variance applications",
        "I just ran into him. I had trouble installing it and setting it up to work, but for now it seems to work the best of everything I've tried. Thank you.",
        "of course I searched the whole internet everywhere and tried all sorts of things. everything I've tried can't measure up to the accuracy of these sites... so I'm wondering how they are so good that some picture where even I wouldn't be able to recognize the text with certainty, these sites I sent succeed regardless of whether the text written in various directions, even if it is scattered and blurry and hard to read, works perfectly.",
        "PaddleOCR uses it internally I think.",
        "And the LLM model needs a lot of VRAM, which is not readily available for all.",
        "Try TROCR and thank me later",
        "Why do you think so?"
    ]
},
{
    "submission_id": "1gam1tq",
    "title": "Is WACV Workshops good place to publish some intermediate stage work?",
    "selftext": "# \n\nOr should I wait for whole work? I need some publication to apply to a PhD program",
    "created_utc": "2024-10-23T14:34:03",
    "num_comments": 0,
    "comments": []
},
{
    "submission_id": "1gajct0",
    "title": "State of the art on shape detection? ",
    "selftext": "Hi all,\nI'm confronted with a problem where I need to detect corner points of various rectangles. They are unlikely to be distorted into parallelogram. Most algorithms use edge detection (mostly canny) and define a corner based on interestion of edges. This is acceptable but since this is old, I was wondering if any meaningful developments have taken place. \n\nObject detection doesn't make sense because of algorithms typically use some sort of region proposal mechanism followed by classification problem and fine-tuning the bounding boxes. This will not detect exact edges and require training data. \n\nI could also try semantic segmentation or instance segmentation but given it's just detection of rectangles I was wondering if there are any simple algorithms? I thought of a convolution and designing specific filter that would require fine-tuning. \n\nAny recommendations? Review papers to read. ",
    "created_utc": "2024-10-23T12:41:12",
    "num_comments": 8,
    "comments": [
        "Might be helpful\n\nhttps://github.com/torrvision/straighttoshapes",
        "Ok, if you want filters here is an example https://www.mvtec.com/doc/halcon/12/en/bandpass_image.html of filters that might help enhance straight lines. Just understand the kernel and adopt it. If your image is really like the one you linked by google, a canny +hough transform should work. Other possibilities are template matching techniques. Given the images by google link, i would personally go for canny+hough transform, especially with axis aligned rectangles, once you have fitted a line along a rectangle's side you can basically do anything you want, even compute edge with subpixel precision.",
        "Ok, probably i'll need an image to understand exactly if this is a viable option. But do you know something about deformable shape models? It's a complex techniques based on splines, i know some commercial libraries that have an outstanding implementation of it. But first it's better to understand if this might be what you are looking for",
        "Doing computer vision without first having a look at an image is not good practice. There are a variety of ways to do what you want but it's difficult to suggest anything without any idea of what your images look like.",
        "Interesting! Let me read this properly. My application is not real time so it's much simpler and could take a second or two but it requires me to detect edges 100% accurately because of the downstream application of extracted dimensions. There is some domain knowledge that I can inject in post processing the boxes to round the lengths obtained but I need the algos accuracy to be under the least count of output measuring unit. \n\nMy strategy was to convert the image to greyscale and use floor\\ceiling bounding, divide by 255 and use a convolution vector of length n, which will essentially take a dot product to figure out a stretch of line or a 5x5 or 3x3 convolution to detect corners. However, I need to know if something simpler already exists out there.",
        "Thanks! I'll try canny+edge and update!",
        "[Unrelated image](https://www.google.com/search?q=multiple+non+overlap+rectangles+image&client=ms-android-nothing-terr1-rso3&sca_esv=f0fb0ddb36b54bca&udm=2&biw=461&bih=862&sxsrf=ADLYWIKBMrM2O4VwqzDCpYbYCJKi4B9Gog%3A1729718207257&ei=v2cZZ4e0D6uSseMPv979wAw&oq=multiple+non+overlap+rectangles+image&gs_lp=EhJtb2JpbGUtZ3dzLXdpei1pbWciJW11bHRpcGxlIG5vbiBvdmVybGFwIHJlY3RhbmdsZXMgaW1hZ2UyCBAAGIAEGKIEMggQABiABBiiBDIIEAAYgAQYogRIkChQ8hBYvCVwAHgAkAEAmAHQAaABlhCqAQYwLjExLjK4AQPIAQD4AQGYAgigAuQJwgIEECMYJ8ICBhAAGAgYHsICBBAhGAqYAwCIBgGSBwMwLjigB-Mt&sclient=mobile-gws-wiz-img#vhid=bl9N-H_mXFoBGM&vssid=mosaic) but I have something similar to the right graphs. Aspect ratios of rectangles are often high And I want to detect edges. Mostly it will be rectangular shapes but sometimes arcs are included. To start with, I'm planning to ignore the arcs and work on rectangles but if there are algorithms that detect arcs, it would be awesome.",
        "If your images really look like this contour detection and polygon fitting can easily solve the problem, but you should really share a real image so we can help more."
    ]
},
{
    "submission_id": "1gaj964",
    "title": "Top journals/conferences to follow",
    "selftext": "Hello everyone,\n\nI'm looking for journals/conferences to follow to keep up with the field. I'm currently following CVPR andPattern Recognition Letters. I'd like to know which are the ones you follow because I see that, like many research fields, there are a lot of papers that seem to be low-effort or sketchy. I'd like keep my attention on examples to follow if I'd publish a paper myself (which I am tasked with already)",
    "created_utc": "2024-10-23T12:36:50",
    "num_comments": 1,
    "comments": [
        "ECCV, ICCV, BCCV"
    ]
},
{
    "submission_id": "1gahkod",
    "title": "Help with little side project using Tesseract OCR",
    "selftext": "First of all, I'm from Brazil, but I hope you guys can understand my english.\n\nSo my project is basically read some texts of a match history picture of a League of Legends match. But I'm having trouble in it.\n\nIn some cases it works fine, in others don't. So anyone has any tips on how I can enhance the reading?\n\n\\- In green are the boxes I'm trying to extract the data from\n\nand the output is this:\n\nROI 1: 11/3/10\n\nROI 2: 5/319\n\nROI 3: 7TI4as2\n\nROI 4: TIA SIT\n\nROI 5: 4/1/24\n\nhttps://preview.redd.it/hbfnzwrxrjwd1.png?width=1050&format=png&auto=webp&s=71d49b62eb8d1c58d3c09586a9c02d9b61f0a681",
    "created_utc": "2024-10-23T11:21:26",
    "num_comments": 3,
    "comments": [
        "I would convert the image to grayscale and do some sort of thresholding. Otsu's thresholding is seen as a good automatic solution. Both can be accomplished with opencv. If that text will always be at the exact same spot on the screen, I would crop the image before the grayscale and threshold step.",
        "Try easyocr! It’s much better than tesseract",
        "The texts that I want gonna remain in the same spots. Thanks for the tip"
    ]
},
{
    "submission_id": "1gag2yd",
    "title": "Best way to remove the background and have a mask of just m&m's?",
    "selftext": "https://preview.redd.it/tr2x156zhjwd1.png?width=640&format=png&auto=webp&s=9d50d89a1efd026a43757d383f526cbeda0ee578\n\nHey! You might have seen my different post with these M&M's. Now I must get the contours/mask of the M&M's in this brown background. I have been trying to use HSV upper and lower bounds but this approach is not working...\n\nThis is my first CV project, therefore we are not expected to use Neural Networks or anything like that, just openCV.  \n  \nAny tips are appreciated. Thank you.",
    "created_utc": "2024-10-23T10:20:42",
    "num_comments": 13,
    "comments": [
        "I think flood fill with barriers from canny may do the job",
        "Sorry dude but since you are new to opencv i must ask, are you REALLY sure that hsv is not working? By the image you posted it seems impossible to me that the saturation channel is not exploding on every single m&ms with respect to background. Don't try to segment it using all channels togerher, also because hue would probably break the threshold due to angle. Or the very least the solution with canny and fllodfill as pointed out by @Tex_flipp",
        "1. Median blur (3x3 kernel, maybe a few iterations) to mitigate the Gaussian noise distortion in your image\n2. Convert image to greyscale & divide by 255 to normalize to values between 0 and 1\n2. Measure the background brightness with a color picker (I’m guessing ~0.15-0.2 in most places?)\n3. Create a mask using numpy (mask = np.zeros_like(image); mask[greyscale_image > background_brightness] = 1.0;)\n\nYou should now have a mask of everything that isn’t your background!",
        "You may try to use adaptive thresholding on the hue channel then apply Connected Component Analysis (CCA) to have a mask of each m&m. If there is an overlap of the m&m's, opening morphological operation can help you to seperate overlapping m&m's. To recover from there (as you may have removed some boundaries due to opening operation), you can use a color based flood fill scheme starting from the first moment point that you calculated from the CCA.",
        "[deleted]",
        "thanks for the suggestion, really appreciated",
        "Maybe (probably) I am doing it wrong...\n\nI tried to create masks (with upper and lower bounds for every HSV channel) for each color and then combine them, but some m&ms seem to fall in the same range as the background. I also thought of creating a mask just for the background but wasn't successful.\n\nYou are saying to try to segment them just changing the values of S-channel for the HSV boundaries?\n\nThanks in advance for all the help!",
        "thanks!!",
        "It's so sad when people's only reply to this kind of very simple tasks (that can be solved with a HSV filter) is Segment Anything. Like if you told people to use ChatGPT for 2+2.",
        "I am now realizing that maybe I am being too picky trying to get a very good contour of the m&m's when maybe I just need some sort of detection even if it crops half the m&m",
        "Pop ml engineers. Throw a chatgpt (or something similar) at everything.",
        "Sans mediocre engineers, some are here to actually learn better.  \nThrowing tasks on too-strong models is wasteful"
    ]
},
{
    "submission_id": "1gadtux",
    "title": "License Plate Recognition ",
    "selftext": "Hi guys, currently I am working on one project for License Plate Recognition, which I need to detect the license plate from vary transportation (motor,bike, car, bus, van, train etc).\n\nWithout using deep learning, neural network, pattern matching, what else can I done to ensure the higher accuracy for my project?\n\nI more prefer training (I don't think without training still can maintain for the accuracy, please correct me if I am wrong), and currently is consider about Dark plate or FastPlate, but as I don't have much resources, so I more prefer the model that can trained on CPU. \n\nPreviously I have tried using openCV framework and use easyOCR to recognise the license plate but the accuracy is very low, and need to be in a very high quality image (without training method). The image below is one of the result for this method\n\nSo can someone give some suggestions to me about the project?\n",
    "created_utc": "2024-10-23T08:48:59",
    "num_comments": 6,
    "comments": [
        "Llms like Claude or chatgpt actually does very well and outperforms easyocr",
        "Just buy a LPR Camera, if you don’t want to go through the trouble. I run them on our network and they work well. All run locally on the camera. Axis and Linovision come to mind and they make good ones",
        "Look yolo + deep sparse + maybe onnx/coreml this allows tiny yolo variants to run with low computation on cpu(specially the sparse one)",
        "use YOLO for plates, add opencv filters, run through easyocr. Should get you the results you need.",
        "Check Roboflow Universe, they have a bunch of models that you can just download and use via their API. No need for training and inference shouldn't be too slow on cpu. Here is an example https://universe.roboflow.com/roboflow-universe-projects/license-plate-recognition-rxg4e\n\nI am currently using a few of their models for a side project. It requires checking if it works on your data (some models fail flat), but for common tasks it isn't hard to find something that is good enough."
    ]
},
{
    "submission_id": "1gaapv1",
    "title": "Real-Time Object Detection and Counting",
    "selftext": "Hey everyone,\n\nI’m working on a project where I need to build a model that can detect and count objects in real-time using the live camera preview screen of a mobile device. I’m considering using the YOLO (You Only Look Once) model for this task because of its popularity and ability to perform real-time detection.\n\nHowever, since I plan to run this model directly on a mobile device, I’m concerned about its performance in terms of accuracy, speed, and computational efficiency.\n\nQuestions:\n\n1. **Is YOLO a good choice for real-time object detection and counting on mobile devices?** If so, which version would you recommend (e.g., YOLOv8, YOLOv11, or a different one)?\n2. **If YOLO isn't the best option,** are there other models or approaches that would be more suitable for mobile deployment? For example, models specifically optimized for edge devices like SSD, MobileNet, or Transformer-based models?\n\nI’d appreciate any advice or recommendations, especially if you have experience running these types of models on mobile devices.\n\nThanks in advance for your help!",
    "created_utc": "2024-10-23T06:35:23",
    "num_comments": 5,
    "comments": [
        "YOLO does work well with videos for RT detections and tracking. I haven’t tried other models. Would be nice to hear from others that did",
        "I think it really depends on what your hardware can run. Memory shouldn't be a problem on a mobile, but idk about FPS. If it achieves a good performance when running a YOLOv8n (you can try with newest, but in my experience this is still a top performers) then go with that because it is very well documented.",
        "If you’re planning on going mobile, you will almost certainly end up converting it to an onnx model to run on the phone itself. That’s what that format excels at and you’ll find good support for it",
        "I never got to this stage in something I tried .. but I remember reading that you might need to tweak the model and or compilation for the target platform . So iOS would be different than an android device … it’s something I have been wondering about much myself and would love to know the answer .",
        "Yes. But is yolo good choice for object counting use case?"
    ]
},
{
    "submission_id": "1ga8w8f",
    "title": "Is it right to call linear regression weights an activation map?",
    "selftext": "I have a series of images that I ended up getting the best predictions with using a linear regression. I did want to make the weights more interpretable so I plotted the linear regression weights in the shape of an image and it kind of makes sense what's happening. Idk if it's fine to call it an activation map though.",
    "created_utc": "2024-10-23T05:06:20",
    "num_comments": 4,
    "comments": [
        "I would say yes!",
        "yes, ig it is fine since linear regression itself is an activation function/map (although linear activation functions in a NN won't do anything good due to its inability to capture non-linearity). anyone please correct me if I'm wrong.",
        "You’re right if it is purely linear into linear layers. If using relu or other non-linear transformations in-between the layers may capture non-linear relations",
        "Nah it’s like linear linear. It’s very weird discovery, since the behavior I’m supposed be studying is a very-non linear physics phenomena."
    ]
},
{
    "submission_id": "1ga5vnl",
    "title": "Is this legal?",
    "selftext": "This might be the wrong sub to ask, but would it be legal to walk into a store and create a video recording passing by all price labels?",
    "created_utc": "2024-10-23T01:52:25",
    "num_comments": 6,
    "comments": [
        "Depends on local laws, but it generally is legal for them to toss your ass out for doing it if they catch you, at which point if you return or refuse to leave you are generally trespassing.",
        "How about you just tell them what you want to do and get their blessing beforehand?",
        "Legal? Probably.  \n\nLegal for the store to toss you out on your ass? Also probably.",
        "It really depends on your local legislation. It might be okay, it might be illegal. There’s no single answer for that",
        "It's stupid that they are trying to implement dynamic pricing and yet it might be illegal for us to record the prices we see.",
        "Also internal rules of the supermarket. There may be a \"recording not allowed\" sign on a door or something"
    ]
},
{
    "submission_id": "1ga5o48",
    "title": "Computer Vision & CCTV",
    "selftext": "Hello 👋. Hope you're all well. An opportunity has been presented that revolve around Computer Vision.\nA business owner is concerned that in her salon business premise, the employees have been undervaluing their services.\nFor example, in a service that could cost 100$, an employee would claim to have offered a service worth $60. Mind you, they can collude with the client , or take the amount themselves, and take the top earnings, and further claim commissions from it.\nIt would be better if the owner was there to monitor the services and earnings, but they have challenged if there's a better alternative with computer vision.\nIs executing such a project worth it? What would you recommend?\nIf it works, it could as well be scaled to other salons which I think is a great idea.\nI'll appreciate your feedback ",
    "created_utc": "2024-10-23T01:36:19",
    "num_comments": 13,
    "comments": [
        "Easiest would me to do random checks manually. If you find 1 case where they did lie, most probably there are more. \n\nIf you want to do it automatically, you'd need to have records of clients to compare with and from CV perspective you could track each client and count time spent in the place and compare with estimated time for service",
        "Technically it is possible, ethically it is questionable, societally it is horrible.",
        "Hi,\n\nThank you for sharing this exciting opportunity. Computer vision could indeed be an effective solution for monitoring and accurately assessing the value of services in salons. It can help reduce fraud and increase profits.\n\nI recommend the following:\n\n\t1.\tFeasibility Assessment: Study the costs and expected benefits.\n\t2.\tInfrastructure: Ensure the necessary equipment and training are available.\n\t3.\tPrivacy: Consider local privacy and monitoring laws.\n\nIf these aspects are carefully considered, the project could be successful and scalable to other salons. It’s advisable to consult with an expert to conduct a detailed feasibility study.",
        "Since each service has the own price, I was thinking of fetching captured data from the CCTV, and then training them based on it. Such as if pedicure, or hair being plaited, it could be a certain time, certain service, and certain price so that it's easier to track down",
        "I don't see any ethical issue. It only allows the owner track their business performances ?",
        "This sounds like a ChatGPT response",
        "ah so you’re trying to determine what services were actually performed during the interaction with the customer, and then align that to what was charged?",
        "If the video quality is good enough, I would run human pose estimation on regions of interest pertaining to a worker. Then parse the poses into a CNN labeled with the service.",
        "How'd you feel if your employer hung a camera above your workstation, such that they can see your every move? Even if you're not doing anything shady, respecting people's privacy is still a thing.\n\nThe way to solve trust issues among humans is not to act on your distrust, but to build trust.",
        "Actually yes 😂😂",
        "yes, to compare what the attendants collected and the amount the model predicted;; and if any discrepancy, it could be easier to track down since there is going to be two records to compare; the ai one, and what the attendants recorded",
        "I don't think this is feasible without a lot of effort (lot of software development, training ML models and additional hardware). The only thing I can see is to track time of the customer in the place",
        "i see where you’re going. i think the challenge you’re going to have is being able to get & label enough data. for something like this to work fairly well & be robust to real world variation, you’ll likely need thousands of examples of different activities from many thousands of different looking/dressed people.\n\nto be clear you can probably hack something together with a tiny bit of labeled data that works like a basic demo under extremely controlled conditions - anything beyond that though will take orders of magnitude more precisely labeled data.\n\nit’s probably technically feasible on some level, but I think it will take a very very large effort to get something that works reasonably well, at least enough to provide business insight.\n\npersonally i’d probably go a non-CV direction first. maybe some statistical analysis, seems like what you’re really looking for is cases of fraud."
    ]
},
{
    "submission_id": "1ga3xhi",
    "title": "YOLO 8 semantic instances problem",
    "selftext": "**Hi folks,**\n\nThe bounding boxes of the detected objects are incomplete, as shown in the image below for \"Sky\" and \"Water.\" It seems that for the other objects, they are working correctly. The training was performed using YOLOv8, and the annotations were done using the application on roboflow.com. All annotations were carried out according to the geometric distribution of each object, and I didn't leave any gaps. Please let me know what I might have messed up.\n\nThank you!\n\n[Picture 1](https://preview.redd.it/om96s1wr9gwd1.jpg?width=2000&format=pjpg&auto=webp&s=1fc4e4c200c1874af5ffc3a8c5ce7806eaf6a497)\n\n[Picture 2](https://preview.redd.it/l8quxgqt9gwd1.jpg?width=2000&format=pjpg&auto=webp&s=10e237544c62e1c35eb9a1c8aff7b17793313dd9)\n\n",
    "created_utc": "2024-10-22T23:29:40",
    "num_comments": 12,
    "comments": [
        "Can you describe your problem? \n\nAlso these aren't objects, the technical term for this (and I'm serious) is \"stuff\" and it comes with unique detection problems since most pretrained models are trained on objects, so maybe it's related. Not clear what your specific issue is though.\n\nEdit: oh, there's a bit of space to the side for water and sky. I was looking vertically.\n\nYa I dunno. Either get more data or maybe try semantic segmentation. As I said, object detection is mostly for discrete entities. \"Water\" or \"sky\" is not a discrete thing, so it's trickier. Medical images run into this issue for instance. It's likely fixable though.",
        "It's because the receptive field of the anchor is not large enough to cover the whole image. It's a [known issue](https://github.com/ultralytics/ultralytics/issues/11634#issue-2279389093). You can try using a P6 model and a smaller `imgsz` so that the anchors can see more of the image.",
        "I recommend panoptic segmentation. You get discrete instances of items (useful for buildings and potentially islands) but every pixel ends up being classified. Basically, instance segmentation done more semantically.",
        "Why are you using object detection for this ?",
        "This is not object detection. It is semantic segmentation. I use it to develop some kind of advanced navigational system (vision navigation). \nThe bounding boxes that can misleading are produced by python script.",
        "Thank you for the response. The issue is that, regarding the detected elements, they are not being detected in their entirety. This problem mostly concerns the sky and water. Notice that in the attached examples, the bounding box does not cover all the water and sky and ends about fifty pixels to the right and left of the edge of the photo. Thanks for the explanations regarding \"stuff.\"",
        "Thank you so much!!! Where can I find the P6 model? I tied to look for it at ultralytics and found nothing...",
        "Thanks for this. Any suggestions which library is able to perform this task?",
        "Ultalytics’s yolov8 segmentation models are actually instance segmentation models, meaning they have the added burden of differentiating each instance of a given class. \n\nYou’re probably better off with a “pure” segmentation model if you don’t care about differentiating each instance. ",
        "I added a bit of an edit likely after you started this. My best guess is to use these BBs to provide inputs to a (fine-tuned?) SAM model or something. Others might have better suggestions though.",
        "You can load them like:\n\n`model = YOLO(\"yolov8n-seg-p6.yaml\")`\n\nP6 model configs haven't been released for YOLO11. There's a [PR](https://github.com/ultralytics/ultralytics/pull/16558) on it.",
        "You can try detectron2. It has support for object detection, instance segmentation, and panoptic segmentation. If you are on linux, you can easily pip install it. If you are on windows (like me) then gl lol. After about 3 hours of installing random c++ build tools and setting up various environments, it may work!"
    ]
},
{
    "submission_id": "1ga2rav",
    "title": "Looking for Developers to Collaborate on Training Open-Source OCR Model for a new language.",
    "selftext": "\n\nHey fellow developers!\n\nI'm working on an exciting project to train an open-source OCR (Optical Character Recognition) model to support a new language, and I'm looking for passionate contributors to help make this happen! 🌍✨\n\nHere's the gist:\n\nGoal: Train an OCR model to recognize and process text in a language that's currently underrepresented in the OCR space.\n\nModel: We're using an open-source OCR framework, but I'm open to suggestions if you think another model might be more suitable.\n\nDataset: We’re building and preprocessing a custom dataset, so if you have experience with data preparation, annotation, or preprocessing, your help would be super valuable.\n\nSkills Needed: Whether you're experienced in machine learning, deep learning, natural language processing, or just want to contribute to a cool project, there’s a role for everyone.\n\nTech Stack: Python, TensorFlow/PyTorch (open to other frameworks), and any other tools that would help improve the accuracy and efficiency of the model.\n\nCollaboration: We’ll work together on GitHub, so it's a great opportunity to share ideas, learn from each other, and make a meaningful contribution to the open-source community.\n\n\nIf you're passionate about OCR, language tech, or machine learning, let’s make this happen! Drop a comment or send me a message if you’re interested in joining the project.\n\nLet’s bring this language into the digital world together! 🙌\n\nEdit : join discord server .   \n\nhttps://discord.gg/a7Qtv8st",
    "created_utc": "2024-10-22T22:12:00",
    "num_comments": 15,
    "comments": [
        "I’m interested",
        "Have used PaddleOCR for few OCR projects of identifying date codes (numeric characters) using PaddleOCR GitHub repository....I would rate, one of the best in OCR domains....Can you please check, if they are already into the language of what you are trying to develop OCR dataset??",
        "Sure I’ll try to help out! I have some good dual-language books you could possibly use for training",
        "I’m interested",
        "Please send me the link, I'm also interested!",
        "me too, intrested",
        "i shall send you the link to join.",
        "i tried for arabic , persian , french , didnt produce good results .",
        "https://discord.gg/a7Qtv8st",
        "send me fb profile to add you to chat",
        "https://discord.gg/a7Qtv8st",
        "join the discord server",
        "Sure",
        "https://discord.gg/a7Qtv8st",
        "https://discord.gg/a7Qtv8st"
    ]
},
{
    "submission_id": "1ga0snu",
    "title": "Why latent feature is set to query in LDM ?",
    "selftext": "Why latent feature is set as query while text feature encoded by clip is set to key and value in LDM implementation ？I think it is an vision task and value should come from latent feature of image ?\n\n[https://github.com/Stability-AI/stablediffusion/blob/main/ldm/modules/attention.py#L145](https://github.com/Stability-AI/stablediffusion/blob/main/ldm/modules/attention.py#L145)",
    "created_utc": "2024-10-22T20:16:08",
    "num_comments": 3,
    "comments": [
        "It's cross attention, not self attention innit",
        "Yep, I'm confused that why not set latent feature as key and value, text prompt feature as query ?",
        "there is one query per image location. think that the model needs to assign a word \"meaning\" to each location in the image, not the other way around. so you need a query for each location, which can index arbitrarily the input text (key & value)"
    ]
},
{
    "submission_id": "1g9ztr2",
    "title": "Tracking unique shipping containers in a video with computer vision",
    "selftext": "",
    "created_utc": "2024-10-22T19:23:45",
    "num_comments": 19,
    "comments": [
        "Great idea! Have you looked at driving speed vs. frame rate? Would be cool if you could really haul ass and still keep up with the analysis.",
        "i imagine you're doing OCR on the IDs? did you have any issues like motion blur and vibration from filming on the move, or low resolution of the text area (e.g. the chassis ID seems pretty small)? i am actually very curious about how fast you can move around with your camera and still get accurate character recognitions.",
        "Well, in need of any vendors who can implement this kind of computer vision solution for our organization at optimal cost.  Please reach me out if anyone has got any good contacts.  Thanks.",
        "Is this a subliminal hiring campaign or something ? 😂",
        "Can you share the general procedure of how you did this?",
        "now run the frames of the ids through multimodal LLM for OCR",
        "What camera did you use? I'm doing something similar in a warehouse but currently we're having problems with the camera",
        "For real time use, I'd probably deploy on a Jetson or another edge device with powerful enough hardware to allow for real time processing. Once you have real time processing, you could start collecting data from other sensors like GPS to build a map / monitor entry or exit times, etc. There is so much you can do!",
        "The container and side IDs are identical, which gives two opportunities to read the text. We have found success in using various OCR models for reading the IDs, although it is hard to do in real time.\n\n  \nIn post-processing, you can take the middle frame where the IDs are present, then run them through a multimodal model like Florence-2 or a dedicated OCR model like DocTR.",
        "Messaged you.",
        "I wrote a blog post on this at [https://blog.roboflow.com/yard-management-computer-vision/](https://blog.roboflow.com/yard-management-computer-vision/)",
        "Inside a warehouse you might have illumination issues, you might need to add lights or even use a IR camera with IR spotlight if visible light is gonna be a problem.",
        "What libraries and tools are you using? \nIf possible kindly share in detail. Thanks",
        "You could also install some cameras that look down at all the containers (maybe stitch multiple cameras together) to create a real -time birds eye view, and feed your obtained position data to this overhead map to show where each container is \n\n\nAnd then as each container is moved, your map keeps track of where the containers go without having to go and re-find them by driving along ",
        "Any idea or info on how much it costs with timelines",
        "there are some nice low light cameras that work in near darkness",
        "I wrote a guide at [https://blog.roboflow.com/yard-management-computer-vision/](https://blog.roboflow.com/yard-management-computer-vision/)",
        "If this is a drop yard they can be pretty big, installing a lot of cameras and wiring might be a pain.  Drop yards usually have a yard jocky that drives around and moves stuff around.  Some people are even making autonomous ones.  It actually might make more sense to just deploy this on the yard jockey truck, it likely drives the entire lot multiple times per day anyway.",
        "There are cameras that work in complete darkness by using an IR emitter, this is how every home security camera works.  The problem with low light is you either need a large lens to capture lots of light along with long exposure or a really high signal gain which produces noise and makes CV difficult."
    ]
},
{
    "submission_id": "1g9yjps",
    "title": "Recommendation for industrial grade cameras with heat sink for object detection and OCR ",
    "selftext": "Hello, currently our small R&D team will be needing an industrial grade cameras with heat sink that can run 24 hours without over heating. The camera will be running our object detection and OCR in the background. Can you suggest any industrial grade cameras used by your teams for computer vision projects? Thank you! \n\nI opted for high resolution web cams but my seniors said that they will not be able to run 24 hours daily due to heating issues that's why they opted for industrial grade cameras with heat sinks or cooling systems. ",
    "created_utc": "2024-10-22T18:18:40",
    "num_comments": 20,
    "comments": [
        "Intel realsense?",
        "Industrial cameras are made in order to run 24/7 in several conditions, unless you place them inside an oven you don't need any dissipation mechanism. The most reliable cameras i have ever used are basler [https://www.baslerweb.com/en-us/](https://www.baslerweb.com/en-us/) great sdk, lots of examples and good firmware, quite expensive. Other solutions might be Teledyne Dalsa [https://www.teledynedalsa.com/en/home/](https://www.teledynedalsa.com/en/home/), shitty sdk, average quality firmware but cheaper. Other possibles are Allied Vision Cameras, or Baumer cameras, simply google the names. All those works in Gige protocol, both ethernet or usb3 depending on the model. Anyway, all the mentioned runs 24/7 without any issue.",
        "Here’s a great blog on indstrial cameras for CV: https://blog.roboflow.com/best-cameras-for-computer-vision/amp/\n\nMy personal recommendation is Basler ace2 or the ace if the ace2s are out of stock. Great cameras, although setup can be a pain",
        "Most industry cameras don't need a heat sink to run 24/7. You just have to screw them on a metal attachment.\n\nAs their housing is also made from metal, they can transfer the heat well from their sensor / fpga to their attachment and thus they don't overheat.",
        "u/Original-Teach-1435 is on the right track. I’d suggest you pin down your other requirements first. What interface are you looking for? USB? Ethernet (GigE)? What operating system will you be using for acquisition? What does your tech stack look like? Do you require a Python API or would a CPP API suffice. Do you need accurate timestamps?\n\nBassler (good python API and Linux support) and TD Dalsa (Good windows support) are good choices.",
        "If you want you model to run on your camera, your options are a bit limited, but there is a new camera for raspberry pi that can do that, allthough I would assume the models it can run in realtime is limited. I know iDS also have something available. \n\nFor common industrial cameras I would go for think lucid triton. They work great and Ive had some litteraly hit by a truck and be fine.",
        "Machine vision cameras are typically designed to run continuously in fairly harsh industrial environments so as long as your conditions are with the camera’s operating temperature specs it should be capable of running 24/7 without overheating. Usually the camera’s body is connected to the sensor and circuitry so that it acts as a giant heat sink.",
        "Btw, my seniors said that the more sensors we use, the more chance of failure it will be, is this true?",
        "Thanks for the suggestion guys!",
        "For your requirement of industrial-grade cameras with heat sinks suitable for 24/7 operation in object detection and OCR applications, I recommend considering the following e-con Systems cameras.: [https://www.e-consystems.com](https://www.e-consystems.com)",
        "This seems like the best answer theyre literally made for object detection",
        "Is it really industrial ?\nAlso, depth is not required.",
        "I prefer dalsa than basler, their sdk works better from my experience and camera firmware is more complete (for example the cycling presets).",
        "It looks like you shared an AMP link. These should load faster, but AMP is controversial because of [concerns over privacy and the Open Web](https://www.reddit.com/r/AmputatorBot/comments/ehrq3z/why_did_i_build_amputatorbot).\n\nMaybe check out **the canonical page** instead: **[https://blog.roboflow.com/best-cameras-for-computer-vision/](https://blog.roboflow.com/best-cameras-for-computer-vision/)**\n\n*****\n\n ^(I'm a bot | )[^(Why & About)](https://www.reddit.com/r/AmputatorBot/comments/ehrq3z/why_did_i_build_amputatorbot)^( | )[^(Summon: u/AmputatorBot)](https://www.reddit.com/r/AmputatorBot/comments/cchly3/you_can_now_summon_amputatorbot/)",
        "What kind of problems did you have with the setup of Basler cameras?",
        "If each sensor has a certain fail rate, then additional sensor increase the overall fail rate. That's simple math.\n\nAt the same time, this also mean, that to achieve the lowest fail rate, you shouldn't use any sensors at all.",
        "That's also why industrial machine vision cameras are more expensive than webcams, they're incredibly reliable at 24/7 operation.",
        "Industrial cameras are very reliable, especially if they are housed and you dont have to worry about esd.",
        "I guess it's a metter of preferences. I hate dalsa mainly for the sapera cam expert and their server, that several times stopped working for no reason. I think dalsa has a better variety of models so you have a lot of choices to pick (especially for linear). Concerning the firmware i have to disagree, even the simplest basler camera has tons of cyclying preset, but again might be limited to the models we have used, also because cyclying presets have a different names in basler which i don't recall atm."
    ]
},
{
    "submission_id": "1g9wak9",
    "title": "I need a free auto annotation tool able to tell the difference between chess pieces",
    "selftext": "For my undergraduate dissertation (aka final project) I want to develop an app able to recognize chess games. I'm planning to use YOLO because it is simpler to use.\n\nI was already able to use some CV techniques to detect and select the chessboard area and I'm now starting to annotate my images.\n\nAre there any free auto annotation tools able to tell the difference between the types of pieces? (pawn, rook, king...)\n\nAlready tried RoboFlow. It did detect pieces correctly most of the time, but got the wrong classes for almost every single piece. So now I'm doing it manually...\n\nI've seen people talk about CVAT, but will it be able to tell the difference between the types of chess pieces?\n\nBtw, I just noticed I used \"tower\" instead of \"rook\". Good thing I still didn't annotate many images lol",
    "created_utc": "2024-10-22T16:29:07",
    "num_comments": 24,
    "comments": [
        "You're confusing annotation and auto annotation.  A standard annotator isn't going to tell you which piece is which, it's your job to tell it where the pieces are and what the piece is.  An auto annotation tool is a network that someone else developed that can detect the things you're looking for, or it can also be a standard annotator that trains a network in the background as you mark and label things.  It'll gradually get better at predicting the more you mark and label.  Roboflow has an auto annotator.  Using an auto annotator, assuming you have access to it, you don't need to train another model when you're done.  The model that the auto annotator trained will already suffice.  If you really want to optimize it you can, but you don't really need to.\n\nRegarding the tower vs rook.  This isn't a bad detection/annotation, it's simply the look up dictionary that relates the outputted classes to the human name associated with them has it listed as tower.  It's as simple as modifying the dictionary name if you're that unsatisfied with the name.\n\nRoboflow is fine, costs money as far as I know.  CVAT has annotation and auto annotators which you can run locally that are free.",
        "Are you looking to learn how to do this for sake of learning computer vision or is the main goal to get the app working asap?",
        "Ok me again. Another idea lol.\n\nWhat if you setup templates where you have an arrangement of pieces that you slide back and forth over a board while taking pictures. You only have to label each piece once for the template then to transfer those labels into every subsequent photo you just need to know two corners of the template. Make sense?\n\nI think you could get away with as few as a dozen templates. Take a few dozen photos per template. Adjust the lighting and camera angle between templates too. \n\nOne big thing to know is that data is kind for computer vision AI models. The more data (labeled images) the better. ",
        "\n\nIf you want to do it as a model you are just going to have to put the graft in. You could do it on X images, train the model, test it on the remaining images - once you've done enough you can then just verify them and then dump them back into your training set. This is probably similar to what roboflow does.\n\nHonestly it's a reasonably hard challenge to identify chess pieces from above and (no offense) you sound like you are pretty new to the area. It also seems like the CV task is not your area of focus so I would put little coloured stickers on the different classes and remove the problem entirely.\n\n- Figure out if the piece is black or white\n- Check sticker colour to see what piece it is",
        "Not to sound rude but you stated that you have ~250images. If you dont plan to get more data or follow the approach of /u/InternationalMany6 you would be done annotating by now. Our students annotate roughly 1000 images per day (traffic signs on the road)",
        "You may just be able to use a model someone else has already trained (or at least use it as a starting point for your auto-labeling). There are a bunch here trained on >500 images and several look similar to your chess set: [https://universe.roboflow.com/search?q=top+down+view+of+official+fide+chess+board+has%3Amodel+images%3E500](https://universe.roboflow.com/search?q=top+down+view+of+official+fide+chess+board+has%3Amodel+images%3E500)  \n  \nIf you want to use those to label you can do it in Roboflow just star the model on the Universe side and it will show up in the \"Label Assist\" feature: [https://blog.roboflow.com/launch-universe-model-checkpoint/](https://blog.roboflow.com/launch-universe-model-checkpoint/)",
        "This would only be relevant if presented with a board in an arbitrary state, wouldn’t it? From the starting position the locations are known and thus known after every move.",
        "Yeah, I'm very new to this area. Thanks for the explanation. Imma keep using RoboFlow for now because it looks like it is enough. If I need I will change to CVAT.",
        "Get it working asap. Gotta finish it until february and I'm not even at 10% lol.",
        "Thanks! Thats what I gonna do. \n\nActually I'm using another dissertation as a guide. They did something very similar for chess pieces from above.",
        "Yeah, each image with 20~30 objects of 12 different classes. I couldn't anottate today but hopefully I will finish it in a few days.",
        "Thanks!",
        "Do you know how to train a model yourself? ",
        "Thats what I wanna do, I've found some vídeos on how to do it using some python libs, but never really did. I planned to use RoboFlow to make it faster and easier.\n\nBtw, I've done some research on computer vision and I've read some book chapters about image processing techniques and some articles and chapters about YOLO and CNNs. So I know something, just not anything too deep lol.",
        "Cool. Yeah roboflow does make it easier! But it also helps if you can do some stuff outside or roboflow.\n\nWhat model or models did you use from robodlow that didn’t work quite as well as you expected? \n\nMy approach (as a non beginner…) would be as follows:\n1. Run a chess piece detection model against your images. Save results.\n2. Crop each detection and use a package like rmbeg to isolate the chess piece itself. Save those as png files with a transparent background. \n3. (Optional) run all cropped images through a clustering algorithm. This will group together a lot of similar looking pieces. \n4. Go through your images and sort them into folders by the type of piece. \n5. THE POWERFUL PART! Take your original photos of chess boards, or even better, photos of empty boards. Paste the PNG files into those at random. Probably get them to line up with the squares and don’t paste on top of other pieces…but ok if not perfect. Save the coordinates of what you paste!\n6. Train a new model on that larger “synthetic” dataset.\n7. Repeat if needed",
        "I had a similar idea just a fes days ago! Glad to see someone with experience has already tested that.",
        "Sorry, maybe I didn't understand correctly, but I didn't get how it helps with annotating my images. \n\nI already got ~250 photos of chessboards I've taken myself, now I \"just\" need to annotate them.\n\nWouldn't it be just as time consuming copy and paste the chess pieces individually into empty chessboards and save their coordinates? \n\nBtw the angle of the photos may change slightly, so the position of the chessboard grid will not be exactly the same everytime, even tho I did select the chessboard area and apply a perspective transform in each one of the original images.",
        "Automatically annotating implies having a model already trained. It sounds like your existing options for that found in RoboFlow are inadequate…so you can’t really fully automate it.\n\nThere might be other annotation tools that sort of do what I described under the hood though. ",
        "Oh and the copy paste is automated. Like you can take one of empty boards plus 10 cropped pieces and produce 1000 photos of boards with pieces on them. The exact orientation doesn’t matter that much…it’s ok if the perspective is off because the model needs to learn how to handle different perspectives anyways",
        "Actually what I've tried was to use the AI assisted annotation from RoboFlow. It didn't ask me to choose any model. \n\nI know there are some free chess pieces datasets on RoboFlow which I could use to train a model and maybe help me with annotation, but they're not exactly the same pieces I have nor the same angle...",
        "Oh interesting. \n\nI was thinking you used something like this! https://universe.roboflow.com/joseph-nelson/chess-pieces-new",
        "It looks cool, but I think it wouldn't be helpful for me because my pieces are very different (and the camera angle too). \n\nI think I will try to use the AI assisted annotation from RoboFlow for detecting the pieces and then just change their classes and slightly adjust their bounding boxes. It is not really automated, but definitely faster than doing it 100% manually.\n\nThanks for all the tips! :D",
        "You can annotate a subset of the images, say 20-25 of them, and use a training credit to train a model on them.\n\nFrom there, use the Label Assist feature to label more of the pieces/images faster. You get a set of training credits free when you sign up.\n\nThen use either the Roboflow model or a fine-tuned model like YOLOv8 on your full dataset to make the rest of the application.\n\nTry reaching out to this person too, saw them posting publicly about a similar product a few weeks back (example tweets):\n* https://x.com/hunteralanier/status/1840600172679647381?s=46&t=Qf0U5Ei8V0Lapp580hi02Q\n* https://x.com/hunteralanier/status/1840745041620316387?s=46&t=Qf0U5Ei8V0Lapp580hi02Q",
        "Yeah, I was checking again the label assist thing and noticed I could select my own model. Thx for the tips!\n\nThat guy's project looks really cool. Imma try to talk to him."
    ]
},
{
    "submission_id": "1g9tl1m",
    "title": "CoTracker3 tutorial in the comments",
    "selftext": "",
    "created_utc": "2024-10-22T14:26:40",
    "num_comments": 12,
    "comments": [
        "Here's a link to a notebook for running inference and parsing the output: [https://medium.com/voxel51/cotracker3-a-point-tracker-using-real-videos-4bc1a69c693b](https://medium.com/voxel51/cotracker3-a-point-tracker-using-real-videos-4bc1a69c693b)",
        "will check this out later! Maybe you answer already, but would this be useful for low framerate (wide baseline) tracking? For instance if a point moves 25% across the frame from one image to the next.",
        "Is it similar to dense optical flow? If so, is it faster than dense optical flow?",
        "I wonder if it’s possible to make a visual odometry pipeline based on it? Maybe it’s not fast enough",
        "Can you set the point you want to track or does it always start as an even grid? For instance if I wanted to specifically track someone’s elbow throughout the scene",
        "Seems like it could be useful for some sort of prediction scheme. It seems like it needs to be combined with some sort of Physics model.\n\nIt could feasibly predict how a pogo stick, with both bouncing but also contracting and expanding subcomponents, would track if it encountered a staircase. Albeit, I am not convinced this would be *useful*.",
        "I actually only found success using the model on low fps videos of short length due to GPU memory issues, but the results I saw were quite nice",
        "Yea a similar task and i believe it is faster, at least from what I see reported in the paper",
        "In my experiments, mostly on videos that averaged \\~10 fps and \\~80 frames, it was quite fast at inference (roughly one second). might be worth a try, the online version of the model may be better suited to that though",
        "You can provide a segmentation mask and it will track that through the frames. In this tutorial I didn’t do that, and by default it tracked whatever was on the first frame. Alternatively you can also provide query points for one or more frames",
        "One second for all 80 frames?!",
        "Yeah"
    ]
},
{
    "submission_id": "1g9r5li",
    "title": "Strange Unet Artifact",
    "selftext": "I am using a Unet model (a simple encoder with average pooling and a decoder using [ConvTranspose2d](https://pytorch.org/docs/stable/generated/torch.nn.ConvTranspose2d.html) function) for image upsampling (super-resolution). A pair of images (input on the left and target on the middle) is used for training as shown below. Every other column from the target image is removed and zero-padded to come up with the input image.\n\n[image1380×690 68.5 KB](https://global.discourse-cdn.com/flex015/uploads/imagej/original/3X/e/2/e29a8f8dd2673b0c26e5b86eb14a4d4fc0443e62.jpeg)\n\nDuring training, I could see the vertical line artifacts (image on the right) on the reconstructed image (model output) upon zooming in.\n\nI have uploaded the full-size validation target and output images at [Unet — ImgBB](https://ibb.co/album/5GB5rY). You can see the artifacts when you zoom in on the output.\n\nWhat can be done to rectify the artifacts? I am using L1 and L2 norms as loss functions. The training set contains 6000, 672 x 672 grayscale images.\n\nThanks!",
    "created_utc": "2024-10-22T12:45:38",
    "num_comments": 6,
    "comments": [
        "Can you explain more about your loss functions? If by \"L1 and L2 norms\" you're referring to pixel-wise comparisons that is probably not adequate, and like u/carbocation said, a perceptual loss should be used. \n\nI only skimmed, but this page has some good advice. [Perceptual Losses for Deep Image Restoration | by Aliaksei Mikhailiuk | Towards Data Science](https://towardsdatascience.com/perceptual-losses-for-image-restoration-dd3c9de4113)",
        "It would be helpful to have the precisely paired zoomed-in target matching the zoomed-in output. \n\nWith respect to loss, are you incorporating any sort of perceptual loss?\n\n*Edited* (I originally asked for the full-size output but you included that in a second image already.)",
        "Thanks for the responses.\n\nYes, I am doing a pixel-wise comparison using the L1 and L2 norms and adding them together for the loss function.\n\nI tried incorporating the [VGG perceptual loss ](https://gist.github.com/alper111/8233cdb0414b4cb5853f2f730ab95a49)along with L1 and L2 norms. It didn't fully fix the problem. The artifacts are less noticeable with some image blur. I have attached the [output image (with VGG)](https://ibb.co/p1Bfbx3), and the [target image](https://ibb.co/M8S1Tcs) for reference.\n\nWould trying any other perceptual loss function solve the problem? I've been stuck on this for a while now, any help is appreciated.",
        "It seems like you're trying to do is you're trying to remove occlusions?  I don't know if UNet is well suited for that.  You're getting the artifacts because the network is trying to hallucinate what might be there.  You could try a different loss function if you wanted, but I don't think the results on this will be as good as you hope they will be using a UNet.  You'll probably want to use a GAN, like ones used for super resolution, rather than a UNet.  I could be wrong though never fully tinkered with this.  The good news is, even though you only have 6000 images, you can easily 10x-20x that but simply adding more noise to the images and flipping them.  You can just use the cutout augmentation to add more black squares to the images.\n\nEdit - also to add to this, to check if there's an issue with the network you made itself, you can always pass an unedited image into the network, and then check it against the output.  If you train it, it should be able to basically reproduce the input image through overfitting.  If it is getting bad losses during that, there's something wrong with your network, how your inputting your data, or your loss function.\n\nEdit 2 - it may also be worth looking into PSNR loss which is used in some super resolution methods.",
        "Code? \n\nIt’s probably worth trying a variety of loss functions. ",
        "it would be easier to tell how bad or good it is by outputting showing us the cosine similarity or some intuitive metric."
    ]
},
{
    "submission_id": "1g9mdq9",
    "title": "Detecting bubbles during the fermentation process to find correlation between the progress of the fermentation and the pattern of bubbles (amount of bubbles)",
    "selftext": "some background information: I don't have experience with computer vision before other than the basic class I had in uni. But I'm an intern and I was assigned a project that I basically described in the caption. the project was started by a previous intern and his approach was trying to get the count of bubbles individually which was unrealistic because of the low budget camera and the fast movement of the bubbles.\n\nso I had a different approach which was to apply adaptive threshold and since the area with bubbles was lighter color it would be the white pixels and the liquid surface would be the black pixels. So I calculated the amount of bubbles by a percentage of the surface so the area of white pixels divided by the area of the whole frame. And It kinda worked because after doing a few tests the patterns and the results made sense compared to other parameters of the fermentation were changing simultaneously.\n\nsome of you might ask about how the recording process was done I used a transparent glass on the end of a tube and was dipped inside of the liquid.\n\nthe reason for this question is it looked so simple to me but given the low budget of the project. using classifiers to try and train a model to detect the bubbles would be unrealistic due to low image quality and the size and fast movement of the bubbles. Any suggestions on how to improve or different takes on the project.\n\nsorry for my English or if the explanation seems vague but I was assigned this project and it's my first project with computer vision with my limited knowledge in the field.",
    "created_utc": "2024-10-22T09:29:45",
    "num_comments": 2,
    "comments": [
        "Get a better camera?",
        "Since it's a computer vision sub, it's generally a good idea to share some visuals of the data. Without examples all advice you'll get is a shot in the dark"
    ]
},
{
    "submission_id": "1g9lu2y",
    "title": "Stable Diffusion 3.5 is out !",
    "selftext": "",
    "created_utc": "2024-10-22T09:07:26",
    "num_comments": 0,
    "comments": []
},
{
    "submission_id": "1g9im1l",
    "title": "Question about SSD",
    "selftext": "When trying to calculate depth in a stereo camera scenario the difference between the frames is calculated to find a point. What exactly is the input, what is the data, the only thing I can think of is the numerical value each pixel is assigned (color), if a group of pixels share a similar sum of colors then it's the point we are looking as far as I understand.",
    "created_utc": "2024-10-22T06:50:31",
    "num_comments": 3,
    "comments": [
        "Your understanding is correct.\n\nWe compare \"groups of numerical values\" of pixels on one image to other groups on the second image. The 2 positions we get are really rays starting from each camera and ending at our point, this means we can compute the 3D location of that point if we know the rays' equations (which we do: from the pixel positions, the cameras' calibrations, and their relative position and orientation). SSD compares patches of images using the squared L2 norm, but there are other, possibly more complicated ways to compare bits of images together.",
        "Numerical values that represent what, I can only think of color and no one mentions this. They just mention that 1 bit of pixels is compared to another to find a match.",
        "If your image is monochannel, then each pixel has a scalar value, typically \\[0,255\\]. If the image has 3 channels, then each pixel has a vector value, like (154,240,2), on which we can still do maths, in particular, compute the squared L2 norm of a difference. But losely speaking, we also call the value of a 3-channel pixel its color, yes."
    ]
},
{
    "submission_id": "1g9hoq8",
    "title": "Is there something that can check computer vision file formats like labelme json & yolo text for mistakes? I need to do conversions. I want to check for mistakes after that.",
    "selftext": "I found file format converters but not mistake checkers when i searched for them online.",
    "created_utc": "2024-10-22T06:06:56",
    "num_comments": 2,
    "comments": [
        "There are libraries made for this; or you can do what I do which is train a model on the entire dataset then use the model to infer each entry and manually look at ones where its output differs from the label. ",
        "There might be libraries out there that do this, but you can also write an assertion test. If you're converting `A --> B`, you can then convert `B --> A` and use something like `np.isclose` to assert that the difference is within a given threshold. Of course that assumes your conversions are perfectly reversible and correct. Aside from that, you can assert that the values are within the bounds of your image, that your `xmin < xmax` or `ymin < ymax`, and that the width / height values are > 1. You can write assertion checks that for a given annotation type, the expected number of values are present (whenver predictable), like how the YOLO format expects `c x y w h` so there should be no more than `5` values per line, and the values for `x y w h` should all be in `[0.0, 1.0]`."
    ]
},
{
    "submission_id": "1g9g2jo",
    "title": "3D-computer vision coding test? Online interview",
    "selftext": "I'm having a coding test for 3D-computer vision. I'm thinking the best way to test is to say may be coding test using opencv. Wondering how the test gonna be like? Will they have me remote to their server with opencv + other libraries installed. Anyone has done this kind of test before? Thanks.\n\n  \nUpdate: I've just finished my interview 10 mins ago. I got 1 leetcode geometry tagged and 1 CV question which she asked me to look at the code and explain what the code was for and if there was any bugs in the code, sort of dry debugging CV code.",
    "created_utc": "2024-10-22T04:42:58",
    "num_comments": 2,
    "comments": [
        "I don't think it will happen that way. Generally the coding test is the leetcode problem, and after that they start with questions. In my experience they start showing you some images relevant to their use cases and asks about how you solve the problem. In some cases they asked me some formulas, like projection matrix, to compute relative camera poses displacements, or much more theorical question related to reconstructions, but definitely not coding. If they really want to see you solving a 3d problem or something library related, they assign you a home project and ask you to use a certain library. Let me know if i am wrong and they actually do, i am curious :)",
        "Thanks for your reply. This is the first time I have a coding interview for 3D vision. I have no clue at all!"
    ]
},
{
    "submission_id": "1g9ek1a",
    "title": "Discussion on the best ways to extract data",
    "selftext": "Hi, I am working on a project that is related to MRI images of tumors. At first, I analyze these images and make segmentation for them, but how do I convert the information in the image about the nature of the tumor into data that can be used to write a medical report about the patient. What is the classification of the data? Structured or simi-structured or not How to use those data in to write a report. Thanks",
    "created_utc": "2024-10-22T03:06:00",
    "num_comments": 9,
    "comments": [
        "Unsure what you’re trying to ask.\n\nWhat is the report supposed to tell the reader? ",
        "You can get the size of the mask and their location, but your best bet is to probably talk to a subject matter expert on how they evaluate tumors.",
        "Support an information about the tumor like the spread and a level for it and as an input in my application the name of patient and age , blood type , and some genera things that are wrote in report. \nThe report target : is to make the doctor understand the case and what best way to work on it. \n\nThanks",
        "Ok thanks",
        "What type of expert that are you recommended",
        "“The AI models says X and the medical treatment encyclopedia says that Y and Z are potential treatments”\n\nX is the output of your model. ",
        "Ha… \nSo now my detecting model that work on images give me the results of classification and segmentation … which output some keys and values of information about images which are about tumor . Then take those key and values + medical encyclopedia like you say = report or documentation for the doctor. \n\nSo is that the process, sorry to make you tired, actually that my first one to compination 2 models \nCV with NLP.",
        "Haha no worries. I do want to say that what you’re trying to do is probably almost impossible unless you’re some really large research organization in which case you’re probably not on Reddit asking for help! AI is just not ready to diagnose medical conditions. \n\n But yeah, the basic process would be to use your model to determine what kind of tumors or other ailments the patient has, and then consult with some other resource to figure out the remedy.   ",
        "Ok thanks whan I ask for consultant and help ? 🙂"
    ]
},
{
    "submission_id": "1g9d428",
    "title": "Detect smart board in a classroom image",
    "selftext": "So I want to detect smart board in a classroom image, confused about which model to use yolo is real time and maybe less accuracy coz of it , searching  for alternatives. Need accurate models , not faster ones ",
    "created_utc": "2024-10-22T01:16:53",
    "num_comments": 7,
    "comments": [
        "what do you mean real-time. you can detect objects in one single frame.",
        "https://paperswithcode.com/sota/object-detection-on-coco",
        "not looking  for faster model but more accurate ones",
        "i dont get your point. yolo isn’t “real-time”. people just use it real-time , meaning they detect every frame from a live feed. you dont have to use it in real-time. you can just detect one frame , in your case photo of the classroom and try to detect your object of interest. you would just have to train it - not hard either , plenty of resource online.",
        "You aren't getting me, I meant yolo is One time processing only while other models process twice, so I am looking for an alternative which model would be best in terms of accuracy",
        "this is the first time ive heared of a model processing twice. sounds like i lack some general knowledge about this. if you dont mind explaining , what could be the use of “processing twice instead of once”? and what models use this functionality? thanks",
        "Single pass and two pass please Google it."
    ]
},
{
    "submission_id": "1g9cxnm",
    "title": "Training a single YOLO11 model to handle both object detection and classification ",
    "selftext": "I think I've been trolled by Copilot and ChatGPT, so I want to make sure I'm on the right track, and to clarify my doubts once and for all.\n\nI would like to train a single YOLO11 model/weight to handle both object detection and classification.\n\nI've read that in order to train a model to handle classification, one will have to use the following folder structure:\n\n    project/\n    ├── data/\n    │   ├── train/\n    │   │   ├── images/\n    │   │   │   ├── class1/\n    │   │   │   │   ├── image1.jpg\n    │   │   │   │   ├── image2.jpg\n    │   │   │   ├── class2/\n    │   │   │   │   ├── image3.jpg\n    │   │   │   │   ├── image4.jpg\n    │   ├── val/\n    │   │   ├── images/\n    │   │   │   ├── class1/\n    │   │   │   │   ├── image5.jpg\n    │   │   │   │   ├── image6.jpg\n    │   │   │   ├── class2/\n    │   │   │   │   ├── image7.jpg\n    │   │   │   │   ├── image8.jpg\n\nBut for my case, I would like to train the very same model/weight to handle object detection too. And for object detection, I would have to follow the following folder structure as I've tested and understood correctly:\n\n    project/\n    ├── data/\n    │   ├── train/\n    │   │   ├── images/\n    │   │   │   ├── image1.jpg\n    │   │   │   ├── image2.jpg\n    │   │   ├── labels/\n    │   │   │   ├── image1.txt\n    │   │   │   ├── image2.txt\n    │   ├── val/\n    │   │   ├── images/\n    │   │   │   ├── image3.jpg\n    │   │   │   ├── image4.jpg\n    │   │   ├── labels/\n    │   │   │   ├── image3.txt\n    │   │   │   ├── image4.txt\n\nSo, to have it support and handle both Object detection AND classification, I would have to structure my folder like the following???\n\n    project/\n    ├── data/\n    │   ├── train/\n    │   │   ├── images/\n    │   │   │   ├── image1.jpg\n    │   │   │   ├── image2.jpg\n    │   │   │   ├── class1/\n    │   │   │   │   ├── image3.jpg\n    │   │   │   │   ├── image4.jpg\n    │   │   │   ├── class2/\n    │   │   │   │   ├── image5.jpg\n    │   │   │   │   ├── image6.jpg\n    │   ├── val/\n    │   │   ├── images/\n    │   │   │   ├── image11.jpg\n    │   │   │   ├── image12.jpg\n    │   │   │   ├── class1/\n    │   │   │   │   ├── image7.jpg\n    │   │   │   │   ├── image8.jpg\n    │   │   │   ├── class2/\n    │   │   │   │   ├── image9.jpg\n    │   │   │   │   ├── image10.jpg\n    │   │   ├── labels/\n    │   │   │   ├── image11.txt\n    │   │   │   ├── image12.txt",
    "created_utc": "2024-10-22T01:02:55",
    "num_comments": 4,
    "comments": [
        "You organize it as you would for detection.\nDetection is detection of classes of objects already. Each of your label files (.txt) should contain not only the bounding box coordinates, but also the label (as an integer).\n\nFor example, in an image with 2 cats and a dog, if we say that the labor for cat is 0 and for dog is one, the label file might contain the following (in format : \"[class_id] [x_center] [y_center] [width] [height]\", normalized by the size of the image) :\n\n```\n0 0.25 0.25 0.1 0.1\n1 0.5 0.6 0.2 0.2\n0 0.7 0.2 0.15 0.20\n```\n\nHere, this label file says that there is a cat in the top left corner, of the image, a dog towards the middle, and another cat in the top right.",
        "Hey there, thanks for the quick reply and help!  \nErm, additional questions if I may.\n\n1. What if inside the image, they're several animals and objects, like a ball, 2 cats and a dog? I would like to first separate the image into groups/classes of 1. object 2. animals (through Classification?)\n2. Once I've separated them, I would then filter and grab those 2. animals bounding boxes for in-depth analysis. (through Object detection?)\n\nIn the mentioned case above, I would have 4 classes...for object detection...? Like the following?\n\n1: objects\n\n2: animals\n\n3: cats\n\n4: dogs\n\nAnd class 2: animals will be overlapped by the bounding boxes done to 3: cats and 4: dogs?",
        "When you say you want to separate the image, you mean you want to take you image (with several objects), and obtain a set of images, each being a crop of the original around each object, right?\n\nSo you run the detection, as explained previously, and obtain the list of detected objects (for each of them, you will have the class_id and the coordinate of the bounding box). You can then crop your image around each bounding box. Each resulting image will already be classified, because the detection step already gave you the class_id for each bounding box. You don't need to run a separate classification step.\n\nEDIT: I read your post too fast, sorry. I didn't notice the issue with cats/dogs being also part of the animal class.\n\nWouldn't it make more sense to not have a \"animal\" class, just cat, dog, and object, and then of course after the fact you can simply group your cats and dogs (no use of ML for that).",
        "No. Classes are distinct. No reason to have the model detect \"animal\" if you know a cat is an animal."
    ]
},
{
    "submission_id": "1g9cudo",
    "title": "Bounding box around most prominent noisy blob",
    "selftext": "Basically I want to filter out the relatively less  noisy small dots and keep only the prominent white blob, and get a mask.",
    "created_utc": "2024-10-22T00:56:07",
    "num_comments": 8,
    "comments": [
        "Run a Fourier transform on the images, then do a low pass filter filtering out all the noise, at that point, if you have intensities beyond a certain point, have those parts bounded. It seems like the dots of interest are very distinct. So a simple solution like this could work.\n\nConvolution could also do the same thing effectively but you’d have to figure out the kernel(s) yourself. Again, considering these are relatively low dimensional features, I wouldn’t imagine it would be that hard with some trial and error.",
        "First do this: [https://scikit-image.org/docs/stable/api/skimage.filters.html#skimage.filters.gaussian](https://scikit-image.org/docs/stable/api/skimage.filters.html#skimage.filters.gaussian)  \nThen use this: [https://scikit-image.org/docs/stable/auto\\_examples/segmentation/plot\\_regionprops.html](https://scikit-image.org/docs/stable/auto_examples/segmentation/plot_regionprops.html)\n\nYou might also be interested in: [https://scikit-image.org/docs/stable/auto\\_examples/segmentation/plot\\_peak\\_local\\_max.html](https://scikit-image.org/docs/stable/auto_examples/segmentation/plot_peak_local_max.html)",
        "I've came up with a simple way, just use median blur and suppress any signal that is less than the set threshold (need to adjust per image). Something like\n\n    import cv2\n    import numpy as np\n    import matplotlib.pyplot as plt\n    \n    image = cv2.resize(cv2.cvtColor(cv2.imread(\"sample.png\"), cv2.COLOR_BGR2GRAY), (400, 400))\n    mod_image = image.copy() \n    \n    threshold_multiplier = 1.5  # Signal need to be at least 1.5 times stronger than its median patch\n    mod_image = np.clip(mod_image - threshold_multiplier*cv2.medianBlur(image, 101).astype(np.float32), 0, 255).astype(np.uint8)\n    \n    threshold, res_img = cv2.threshold(mod_image, 0, 255, cv2.THRESH_BINARY + cv2.THRESH_OTSU)\n    dilated_mask = cv2.dilate(res_img, cv2.getStructuringElement(cv2.MORPH_ELLIPSE,(9, 9)), iterations=1)\n    \n    plt.figure(figsize=(10, 5))\n    plt.imshow(np.hstack([image, dilated_mask]))\n    plt.show()\n\n[result](https://i.imgur.com/hfLitaq.png)\n\nP.S. the first image I use `threshold_multiplier` of `1.5`, and `2.1` for the second one. You can change the filter size to better suits your image, also you don't need to resize the image",
        "this is a filter to enhance dots in an image, it's from a commercial library but the kernel is plain written and you can build your own according to the size you need [https://www.mvtec.com/doc/halcon/13/en/dots\\_image.html](https://www.mvtec.com/doc/halcon/13/en/dots_image.html)",
        "Smoothen, threshold, close, open, countours. Ta daa!",
        "Fourier transform is overkill. If the noise pattern is consistent, median or Gaussian blur and threshold.",
        "+1",
        "No, when you apply this for 1000 images, threshold tuning becomes very tedious"
    ]
},
{
    "submission_id": "1g9birv",
    "title": "Intel VTune profiler based optimization",
    "selftext": "Hi all, \n\nDoes anyone use Intel VTune based profiler to optimize CV algorithms, mainly optimizing data access patterns, vectorization, concurrency, etc. Anyone working in this domain? Can you please recommend any resources on this? \n\nThank you. ",
    "created_utc": "2024-10-21T23:15:21",
    "num_comments": 0,
    "comments": []
},
{
    "submission_id": "1g99pr5",
    "title": "estimating the centre point of the carton box in 3D",
    "selftext": "I'm currently using a masking method to estimate the center point of carton boxes. However, in orthographic or 3D views, this masking method tends to estimate the edge as the center of the carton boxes. How can I overcome this issue?",
    "created_utc": "2024-10-21T21:18:30",
    "num_comments": 1,
    "comments": [
        "Need much more detail to give you any useful feedback. \n\nWhat coordinate system are you operating in? What do you mean by “3D view”? By masking method do you simply mean that you’re using an image segmentation model to derive a pixel mask of each carton (then what do you do?)?"
    ]
},
{
    "submission_id": "1g98qs3",
    "title": "facechain open source TopoFR face embedding model !",
    "selftext": "Our work \\[TopoFR\\](https://github.com/modelscope/facechain/tree/main/face\\_module/TopoFR) got accepted to NeurIPS 2024, welcome to try it out !",
    "created_utc": "2024-10-21T20:23:52",
    "num_comments": 0,
    "comments": []
},
{
    "submission_id": "1g97mko",
    "title": "Should I switch from a stable web development job to a lower-paying role in computer vision?",
    "selftext": "Hi everyone,\n\nI’m currently working in web development at a corporate company with a stable salary and manageable workload. However, I’ve been given an opportunity to join a startup where I would lead the implementation of computer vision solutions. While the startup role is exciting, especially since I’ve been studying AI and computer vision for about 2 year, the position pays less than my current job.\n\nI’m passionate about AI and want to grow in this field, but I’m concerned about taking a pay cut. Do you think transitioning into computer vision now, with lower pay but more challenging and specialized work, could lead to better career opportunities and higher earning potential in the future? Does the computer vision field have strong growth prospects?\n\nThanks in advance for your insights!",
    "created_utc": "2024-10-21T19:24:43",
    "num_comments": 22,
    "comments": [
        "Honestly? No.\nIf they are considering you, someone with near zero practical cv experience according to your statements, as the cv team lead, imagine how they're doing in other sections.\nGiven these, they most definitely are headed towards bankruptcy; and it will cost you a job a number of pay cuts.",
        "If you are early in your career focus on gaining experience and money will come. Computer vision will be in high demand and fewer people will have the skills to do it compared with web dev",
        "One thing to consider, if you’re tasked with **leading** projects, consider that your experience at the new place would be better if you have a mentor or senior engineer also, that won’t necessarily hand hold you, but nudge you in the right direction every so often. \n\nGoes a long way to acquire pieces of wisdom and guidance from a senior than to be an aimless headless chicken trying to figure this monster of a field. I’ve been there.\n\nGood luck.",
        "You should not. Computer vision jobs are much more frustrating than web development. You'll be encountering  a lot of failures on daily basis. If you want a challenge then why not. But for job satisfaction I would recommend you stick to your current career.",
        "If youre interested, do it for 2 years then go back to web imo\n\nCV isn't much of a career unless you are an expert in streaming solutions / low level algorithms / optical equipment / implementing model architecture from papers /artifical training data\n\nOtherwise, you spend most of the time training off-the-shelf models which is a pita and pretty low skill\n\nDon't let me tamper your enthusiasm though, its a lot of fun but keep an eye on your skillset",
        "In my opinion\nNo, startups aren't good at all for gaining experience. Specially if they need this position you will be the only one there doing this role and by that you won't gain experience from any senior there. You will left alone trying to do some tasks and projects with noone to lead you and what to do.\n\nIf it is fine try to work in both position for a while and see if the startup actually giving you value and experience then go for it, if not then you still at your main job",
        "I don't think it would be smart to do a hard switch like that. But you may want to consider exposing yourself to CV gradatively. For instance, you can change your contract to 4 days a week and use the 5th day on CV projects as a fee lancer or contributor to open source projects. You can use that to build experience and create a network tha might lead you to another stable and well paying job.",
        "Hey, I can share my perspective because I was in your position a few years ago. I currently work at a FAANG in CV/AI problems and I am also a \"transplant\" of sorts coming as a lead from the user-facing kind of software development. \n\nThe short answer to your question is: Depends. \n\nTo help you answer it I'll instead ask you some questions that when answered can inform your decision making since now it applies to your context and not a blanket statements or other people's.\n\nFirst of all. The startup:\n\nNot all startups are made the same. You will find startups consisting of a couple of dudes with an idea to startups with Series A/B/C funding. Figure out where in this spectrum the startup you're going to falls. \n\nAre they funded? If not, I guess you have your answer here.\n\nIf they are...then how much runway do they have? 6 months? 1 year? ten years? This will define your work-life balance, risk and degree of responsibility to you. \n\nIf their runway is 6 months then it means you have to deliver whatever the startup is set up to build BEFORE that time. It is is a MAJOR red flag if the startup is reluctant to answer that question succinctly, directly and on the spot. You should expect a number \"We have X months of runway\"\n\nLets say you're satisfied with the runway...\n\nCan the startup articulate their offer? Are they crystal clear on what they're trying to build? If they can't articulate it clearly (or worse yet they refuse) this is also a major red flag. \n\nLets say they can articulate what they're building...\n\nWhat is the nature of the role?\n\nThere's different kinds of tech leads in this world. Are you going to be expected to lead the implementation of an E2E solution or the computer vision components themselves?. \n\nMost CV based startups at the beginning focus on the CV-related work which is the core of their offering and as such acquire talent with high expertise on this domain. \n\nYet the skills required to deploy those solutions into a product offering are completely different. If the expectation of this role is for you to develop computer vision algorithms or solve computer vision problems then I suggest you don't join. \n\nReal-life computer vision problems are very hard because the conditions are not sanitized like in tutorials or in school. With 2 years under your belt you may find incredibly steep learning curves. \n\nHowever if the requirement is for you to implement/deploy computer vision models and build an E2E system on web then by all means join. It is a golden opportunity. \n\nThe skills required to deploy a model on web/mobile/embedded are vastly different than the skills required to build CV algorithms. Furthermore it is easy to trivialize the deployment and end up with unmaintainable, unscalable or under performant systems.\n\nIt is easy to trivialize implementation of AI/CV models in multiple platforms. For example, In back end you have to account for compute sources and compute efficiency in order to meet scalability demands (since those will impact the startup's budget). In mobile you have to account for compute efficiency and be deep in the weeds re-implementing the model in coreML or Metal or Vulkan . In embedded you have to know which compute sources you have available and figure out ways to bring the model into that compute. \n\nAnd that's for running the model...piping the input from the user to the model in a scalable, efficient maintainable way is another challenge in itself. \n\nI say it is a golden opportunity because to this day it is a tall order to expect a computer vision specialist to develop high quality, production-level E2E solutions. Likewise it is a tall order to expect a full stack/mobile/embedded specialist to also be specialized in computer vision. \n\nAs such the biggest problem companies working with CV problems face today is productionization. An analogy in web dev terms is a world were there are back end devs, front end devs but no full stack devs. \n\nThe CV specialists will be too encumbered building a robust model in the first place. Making it run smoothly, cheaply and efficiently is, and should be, somebody else's problem. That can be YOUR problem for web domains. \n\nHow do I know this?....because that's my job! I take AI/CV models and integrate them in ways native to the platform I specialize in. There's very few people who do this because there's very few non-cv devs willing to go through the learning curve. I don't actively develop CV algorithms or train models. I lead their implementation and deployment and translate/scope for non-cv stakeholders.",
        "It depends on your interest .. ig",
        "They have very technical people with experience but no soft skills to lead and manage the team, they have problems with the organization. That’s why they are interested in my profile, I have leadership experience and some CV knowledge. But you have a point.",
        "I’ve worked my way up from junior to lead in web dev, which is why they’ve offered me the lead role in this new position. I’m really passionate about AI and have been studying and experimenting with it on my own. Now, I’m at a point where I have to decide whether to take the leap into something new, even though it comes with a lower salary. Thanks for your advice!",
        "That’s a great point, and something I’ve been thinking about. I’ve got experience leading teams in web dev, but computer vision is a different challenge. I’ll definitely check what kind of support I’d have at the new place. Thanks!",
        "First, I thought “go for it”, but CommandShot makes a valid point. I am a senior cv engineer, and even though I have talented juniors, it takes time to build up both skills and intuition to make good solutions. Make sure you are in for working your ass off and feeling clueless, preferably you would have a mentor outside your new company.",
        "Trust me cv is very different than web dev. In web dev you have to deal a lot with the architecture, system design, etc. Which requires the type of management you mentioned. In cv on the other hand, it's mostly technicallity and knowledge because problems are broken down and solved in smaller pieces. There is no need for a manager like that. But there is definitely need for a manager who knows a lot and you can ask him/her if you are lost.",
        "Uhhh what cv in industry job does not require architecture and systems design?",
        "Thank you for your honesty. I am passionate about computer vision and it is a field I really want to get into. But I may have to be very careful with this opportunity and take it slow.",
        "It does, but it is not at the service level so the organization is mostly about how optimized the architecture is and mostly reside in implementation level.Still so different that the organization in web dev.\nFor instance, we mostly try to organize data flow, avoid unnecessary computations, utilize the hardware as much as possible, aim for concurrency between preprocessing and the inference and post processing  etc. \n\nThese are different from something like micro service architecture.\n\nAnd remember, they want to hire our friend as cv team lead. Not web dev team lead.",
        "running efficient webpages is also about organizing dataflow, avoiding unnecessary computations, and maximizing concurrency. What experience do you have in high volume backend or cloud CV? If you are running a cloud CV pipeline, the only thing that really changes is that part of it is an inference endpoint. Not that there isn't art to maximizing the GPU's compute. But that can all be learned, also OP has been studying for two years.",
        "If you put it like that everything can be about those things.\n\nIts like the difference between swimming and flying, both are special skills in animals, but have totally different requirements. In web page 99 percent of the hard work is already done by the framework you are working with.\n\nBut in cv deployment , you are just a few layers above bare metal. I'm not talking about model.fit(). I'm talking about actual cv projects.",
        "There are a lot of things that go into cv depending on what the actual role is. Nowadays companies post database jobs, data analyst as ai jobs, so you never know what you are getting into with just the job title  lol. To me honestly, if all you are doing is calling api endpoints, it is at the end of the day just a software engineering role in a ml team. You can do most of it with bare minimum knowledge of cv. If you are using existing multi-modal llms etc, well then yeh all that you say works. But if he is supposed to be dealing with the entire shebang of CV involving from conception to training to inference, then that is a different beast altogether",
        "Thank you, finally someone who knows something.\n\nAlso, Let alone the deployment",
        "I said inference endpoint, not API endpoint. Also, someone still has to make the endpoints! And figure out how to serve and optimize them. Honestly, none of these things are that difficult, especially compared to multimodal/NLP. It doesn't matter how close to the metal you are getting, if you are writing custom CUDA kernels or some proprietary FPGA assembly lang. You don't have to understand formal logic or linguistics. There is some math sure. OP, don't let people who clearly don't understand what you do gatekeep what is a very approachable and enjoyable field. Jesus when did this sub become /cscareerquestions. CV is tbh, probably more fun for most people than all but the most gnarly distributed systems/ high traffic backend problems. Now is a perfect time to migrate bc everyone else is still trying to ride the NLP bandwagon. If a company is willing to give you a shot go for it. The only thing the next company will see is a CV eng with prior experience and projects, and probably more responsibility/ breadth of projects than someone from a more established company."
    ]
},
{
    "submission_id": "1g95m6h",
    "title": "Vissapp conference",
    "selftext": "Heyy! I want to know if you have some experience about vissapp? Is it as presitigous as IEEE conferences or like WACV or BMVC? What do you think? Is it good conference to attend to connect to some people etc? I have a paper in my drawer and it is not bad actually, but I just hope to submit it asap, and the fitting one is Vissapp :) ",
    "created_utc": "2024-10-21T17:42:22",
    "num_comments": 0,
    "comments": []
},
{
    "submission_id": "1g95413",
    "title": "How to determine if the image filter/mask is first order derivative or 2 nd order derivative ?",
    "selftext": "I have a 3 by 3 Mask : How do I determine if it’s first order or 2nd order \n\n-1 2 -1 \n\n-1 2 -1\n\n-1 2 -1",
    "created_utc": "2024-10-21T17:20:24",
    "num_comments": 3,
    "comments": [
        "First order typical will be mirrored about the middle horizontal or vertical numbers. Second order usually has a peak value at the middle so it looks like a Gaussian or laplacian (more visible at higher order matrices)",
        "this kernel subtracts the neighboring values to the left and right from the center value.   \nhere you can see a nice illustration of it: [https://theailearner.com/2019/05/24/first-order-derivative-kernels-for-edge-detection/](https://theailearner.com/2019/05/24/first-order-derivative-kernels-for-edge-detection/)",
        "Thanks for the clarification"
    ]
},
{
    "submission_id": "1g9373t",
    "title": "Obtaining 3D coordinates from multiple 2D images",
    "selftext": "Hi,\n\nI’m working on a project where I need to determine 3D world coordinates from 2D points captured in images from multiple cameras. The camera positions in the world frame will be known, though they may be arbitrary. I’ll also have known keypoints in each camera view, so I’ll have the 2D image coordinates for these points.\n\nI’m looking for any learning resources, academic papers, or tutorials that dive into this topic (im not exactly sure what this is called). If there are Python libraries (OpenCV or others) that can help with this, I’d love to hear about them too. But I also want to get a better understanding of the underlying concepts behind those tools. Any advice or pointers would be much appreciated! Thanks!",
    "created_utc": "2024-10-21T15:49:08",
    "num_comments": 5,
    "comments": [
        "This is basically photogrammetry\n\nThe bible of the field is \"multiple view geometry\" by Hartley and Zissermann",
        "I have a project for multicamera calibration/landmark triangulation that may provide a place for you to poke around and get a sense of things. If you really do have the all the camera parameters (intrinsic and extrinsic) and the 2d landmarks across all the cameras, then the triangulation can happen with a fairly brief function call: https://github.com/mprib/caliscope/blob/f0c69821c3a744d16b5bc44bbba0f9763a9a279c/pyxy3d/triangulate/triangulation.py#L63\n\nThe heart of that function linked above comes from a project called Anipose, BTW. \n\nIn the tests there is a script that might let you step through the parameters in debug mode a bit more easily (though there is still some multithreading going on):\n\nhttps://github.com/mprib/caliscope/blob/main/tests/test_triangulator.py\n\nThe actual call to the triangulation takes place here:\n\nhttps://github.com/mprib/caliscope/blob/9748eab9bf1d064e888bc9d737d561add53b5620/caliscope/triangulate/sync_packet_triangulator.py#L91\n\n---\n\nMore broadly on the topic of camera calibration, I had a discussion on the repo with someone where I linked to resources that I had found helpful: https://github.com/mprib/caliscope/discussions/483#discussioncomment-7291899\n\nThis tutorial in particular is where I got started, though it presents only the 2 camera scenario: https://temugeb.github.io/python/computer_vision/2021/09/14/bodypose3d.html",
        "Check OpenMVG. It is a structure from motion library.",
        "Thank you, I did come across that while researching. I’ll look more into it",
        "Thanks so much for this!"
    ]
},
{
    "submission_id": "1g8vf7d",
    "title": "How can I detect each M&M individually?",
    "selftext": "https://preview.redd.it/zpbps6jf75wd1.png?width=794&format=png&auto=webp&s=599fb9954aa67e346c46a9f36445d2a8bc94b735\n\n[Original Image](https://preview.redd.it/s8108wcm75wd1.png?width=640&format=png&auto=webp&s=5ccd3d52f80b43f7d767469f8c4148ad87db20e0)\n\nI tried to mask the M&M's using:  \n- bilateral filtering on the saturation channel  \n- canny edge detection  \n- morphological closing to the edges\n\nI would really appreciate if you could help me solve this.\n\nFor context, I am doing a detection of each M&M and classifying them in terms of color and if they have a nut. We have individual images of each M&M's by color and nut. Our framework would be to detect each individual M&M, calculating the area to segment if they have a nut or not, and afterwards compare the upper and lower bound of the HSV channels to segment by color. Is this approach correct or is it too inefficient?\n\nThis is my first Computer Vision project btw, any tips would be immensely appreciated.",
    "created_utc": "2024-10-21T10:28:25",
    "num_comments": 12,
    "comments": [
        "Maybe distance transform with watershed might segment them individually. https://docs.opencv.org/4.x/d2/dbd/tutorial_distance_transform.html",
        "You could use superpixels to oversegment the image. This should give you distinct segments that don't overlap across multiple M&M and their borders should align with the M&M's contours. Then you can merge neighboring regions if they have the same color. (Two M&M of the same color touching could create an error though.)\n\nhttps://scikit-image.org/docs/stable/auto_examples/segmentation/plot_segmentations.html",
        "A detector model like yolo is quite suited for this  task.",
        "You could check SAM2, I know its useful for segmentation, but I dont know if it suits your problem",
        "If this is a trial project probably the aim is not relying on deep learning techniques, it's for learning the basics (filtering, segmentation, region features, etc). The approach is probably what someone already suggested, blur+laplacian to enhance edges and distance transform with watershed for segmentation. Area is not the only feature you can use for the classification, there are a lot of features like first orders hue moments or circularity (nuts are more ellipses than circles). For color classification choose the correct color space (try different, you'll be surprised how much they change image perception! even thou hsv might be the best), and run a basic algo like k-means on color, then refine with morphological like you did.",
        "There's also this link https://docs.opencv.org/4.x/d3/db4/tutorial_py_watershed.html",
        "Watershed was my guess.",
        "really appreciate it!",
        "thank you!!",
        "thank you man! do you think our approach to classify the m&m's is okay?",
        "This is how I would go about it, you could also do a variant of SAM.",
        "Yeah, I think that should work. I probably would have added circle detection in first to see what comes back because regular M&Ms are more circular. Then I would have done the area as my second check. Area alone might be sufficient, I haven't tried so im not entirely sure. https://docs.opencv.org/3.4/d4/d70/tutorial_hough_circle.html"
    ]
},
{
    "submission_id": "1g8stze",
    "title": "Can't export YOLOv10n to TensorRT (via ultralytics )",
    "selftext": "I have a problem when trying to convert a *yolov10n* file from ***.pt*** to ***.engine*** using the *ultralytics export* API. In particular, I get this error:\n\n**ERROR: onnx2trt\\_utils.cpp:342 In function convertAxis: Assertion failed: (axis >= 0 && axis <= nbDims) && \"Axis must be in the range \\[0, nbDims\\].\"**\n\nabout a ***TopK*** node that has ***axis=-1***.\n\nI tried on GitHub issues but I was not able to fix it. You should be able to reproduce it because the error is thrown also when using the pre-trained yolov10n (i.e. *yolo export model=yolov10n.pt format=engine*). I am on a Jetson Orin Nano with TensorRT 8.6.2.3.\n\nEDIT: Solved using [YOLOv10](https://github.com/THU-MIG/yolov10) repo to export to ONNX. Then \\*trtexec\\* finishes without issues.",
    "created_utc": "2024-10-21T08:44:55",
    "num_comments": 12,
    "comments": [
        "I suggest to always convert to TensorRT using trtrxec, not ultralytics export. It uses a wonky tensorrt python library that usually does not match your system's.\n\nJust export to onnx using ultralytics and then use TensorRT trtexec app to convert to TensorRT on your Jetson",
        "Try passing `max_det=100` to the export command.",
        "Use the official YoloV10 repo and export with the image size specified. Then, trtexec should be able to build the engine no problem on your jetson.",
        "Already tried, not working for the same reason. The only way to generate a *.engine* file is through the ***tensorrtx*** library, but then I encounter another issue when trying to inference: \"segmentation fault (core dumped)\", which is strange because just few weeks ago it was working fine and I was able to get detections...",
        "Also, are you using the official `ultralytics`,  or the one from the YOLOv10 repo (8.1.34)? I suggest you use the official one.",
        "For official you mean [THU-MIG/yolov10: YOLOv10: Real-Time End-to-End Object Detection \\[NeurIPS 2024\\]](https://github.com/THU-MIG/yolov10)?",
        "Then basically yolov10 is using some OPS that the Jetson has no support for.\n\nWhich opset version are you exporting onnx into?",
        "Yep that is the one! I would make a virtual environment than install their custom ultralytics version first. Then use “yolo export …”",
        "Well, it can't really be like this because as I said few weeks ago I was to able to use YOLOv10n on my Jetson (through [tensorrtx](https://github.com/wang-xinyu/tensorrtx)). And now I get that segmentazione fault, which led me to try with Ultralytics (which works for v8 and v11, but not for v10 :-()\n\nRegarding ONNX, I exported it without any particular argument so it should have the default ops (maybe I'm wrong, complete non-expert on it)",
        "yes, this is what I actually did and it worked! I updated the post with the solution, it may be useful for others. Thank you!",
        "I've never managed to correctly use ultralytics export to TensorRT directly because it includes a lot of metadata that changes the output shape and makes you have to use their shitty slow postprocessing. I always go ultralytics -> onnx -> onnxsim -> trtexec.\n\nHave you tried this repo? https://github.com/Linaom1214/TensorRT-For-YOLO-Series\n\nI just tried to convert basic yolov10n onnx to tensorrt on my Orin NX using trtexec no problem (don't have a Nano, sorry)",
        "Thanks for trying, tomorrow I'll try to add more details to make it reproducible.\n\nNo, didn't know about that repo because my supervisor suggested the one I mentioned before. I'll take a look!"
    ]
},
{
    "submission_id": "1g8mbg3",
    "title": "Best options for edge devices",
    "selftext": "I am looking into deploying an object detection model into a small edge device such as a pi zero, locally. What are the best options for doing so if my priority is speed for live video inferencing? I was  looking into roboflow yolov8 models and quantizing it to 8 bits. I was also looking to use the Sony AI raspberry pi cam. Would it make more sense to use another tool like tinyML?",
    "created_utc": "2024-10-21T03:21:37",
    "num_comments": 6,
    "comments": [
        "Rk3588 is a good bet or any rockchip unit with NpU. You can run a quantized v8m model at about 30ms / frame.",
        "The Luxonis OAK series are great for edge inference, but they require a host device with USB interface.",
        "Check out the latest Pi AI camera, it runs  models locally. 8 MB RAM, but with quantisation it should run a decent small model.\n\nEdit: I see you already mentioned AI camera. Regarding any other options running on CPU - they are worth considering only if you use Pi Zero 2W   **and** you afford to wait a few seconds compute time per image, depending on model size and optimizations.  AI camera is fast (dozens fps) and keeps CPU free for other stuff",
        "This guide on ultralytics looks very interesting: [https://docs.ultralytics.com/guides/raspberry-pi/#install-ultralytics-package](https://docs.ultralytics.com/guides/raspberry-pi/#install-ultralytics-package) Essentially exporting to NCNN format which gives really quite good performance on raspberry pi 5. You would be looking at 100ms per 640x640 image best case on this hardware and YoloV11n. This is super impressive and I think the future is bright for rpi5 as an alternative to the incumbent nvidia jetsons.\n\nBy the way; if you are looking for a cheaper alternative to roboflow for building private datasets, check out [https://oslo.vision](https://oslo.vision)",
        "Thanks for all the input. I guess, second to speed, I am looking for the most lightweight option for a drone.",
        "read the ultralytics license first! Is this commercial or at some level of classified?"
    ]
},
{
    "submission_id": "1g8losr",
    "title": "6Dof camera pose estimation",
    "selftext": "Hi, i am working on a six dof tracking application. I have an uncalibrated camera that moves around a scene, I take the video and using a structure from motion i manage to build a pointcloud, this is a sort of calibration process. Once built it, i am able to match live images with cloud points and (roughly 300 matches) that are fed to a solvePnP problem in ceres solvers. Such solver tries to optimize simultaneously the focal length, a single distortion coefficient, rotation and translation vector. The final result looks good but the distortion estimation is not perfect and its jittering a bit especially when i have fewer matches. Is there a way to exploit matches in 2D between subsequent frames to get a better distortion estimation? The final aim is a vritual reality application, i need to keep an object fixed in a scene in 3d, so the final result should be pixel accurate.\n\nEDIT 1: zoom is varying along the live video, so both zoom and distortion are changing and need to be estimated.\n\nEDIT 2: the pointcloud i have can be considered a ground truth, so a bundle adjustment with 3d points refinement would (likely) have worse result",
    "created_utc": "2024-10-21T02:36:45",
    "num_comments": 6,
    "comments": [
        "Unless your camera has a zoom and/or autofocus which does move throughout the video, the calibration shouldn't change! This means you shouldn't reset the previous calibration when running it again. Maybe just refine the calibration if possible, for instance with heuristics: your Ceres pass should improve the current batch, but also previous ones, and passes with few points are more likely to be less reliable. It also means that if you're using the same camera everytime, it's just simpler to calibrate it offline once and for all, if you can.\n\nYour approach is roughly what a full SfM pipeline does: find some first guess for everything, and then do a \"bundle adjustment\": refine everything with gradient descent. You are only refining the camera calibration though, not the points' positions. This means the point cloud is possibly itself warped and affects the calibration negatively.\n\nFinally, a single distortion coefficient might just not be able to describe your lens correctly. Have you considered doing more while keeping everything the same? Have you tried and calibrate the camera offline to get a very good result so that you can evaluate the quality of your online approach?",
        "Thank you for the answer, i'll update the post with some clarification after answering you. The zoom changes along the live video, that's why i need to change zoom and distortion coefficients to estimate them on the fly, of course i provide previous frame's values since i expect the change to be small. \n\nThe input in the calibration part is just a video with the camera moving, it can be with both varying zoom or fixed one, so there is no single distortion/zoom value. \n\nI am not updating the point cloud during live tracking because i don't have enough execution time to perform a full bundle adjustment, moreover my cloud is already built with some advanced processing and anything done on the fly would have a huge noise compared to that. We can consider the initial pointcloud as ground truth. \n\n  \nI have tried different distortion model up to 3 coefficients, but i saw that if I have few matches the estimation becomes jittery and unreliable.\n\nI have already a good result with my approach, but only if I am able to retrieve more than 300 matches between pointcloud and live frame and this is not generally true (my cloud sparse). Since I got thousands in 2d domain, i was wondering if there was a way to exploit such informations to get a better intrinsic estimation.",
        "Your approach is fine. You can use 2D matches directly if the camera undergoes a pure rotation. And in SLAM/SfM, using many 2D matches on top of the map is usually possible by triangulating those new points over a few frames anyway, so they're really 3D.\n\nThis won't be trivial  to integrate, but the intrinsics should really be a function of the zoom, in most cases. For instance, you could have focal and distortion coefficient be parametric functions of the zoom. Ceres would optimize the additional parameters, as well as the per-frame zoom, again using batches from different frames together. This would especially work well with an offline calibration: you'd only optimize for the zoom at test time, not the additional parameters.\n\nedit: I forgot another not-very-helpful remark: you can try and optimize the epipolar error for 2D points, but it is tricky and can give bad results with very small baselines (when the camera doesn't move a lot). So again, this is better over many frames, which you can't easily do because of the zoom.",
        "*This won't be trivial to integrate, but the intrinsics should really be a function of the zoom ->* yes that is something i was suspecting but I don't know how to successfully build such function, because it's not guaranteed that i have full zoom coverage during the calibration video. \n\n*You can use 2D matches directly if the camera undergoes a pure rotation and no zoom->*  why only if its rotating? can i estimate an essential matrix+distortion+zoom out of pure movement? maybe replace it with an homography if its just rotating?\n\n *you can try and optimize the epipolar error for 2D points->* actually already tried. It gives bad results even if i skip a lot of video frames to increase the baseline. Thou it is promising that we came up with similar thoughts. Thank you for your help! not an easy task",
        "intrinsics/zoom: if you have access to the camera, you can calibrate it for a few zoom positions, and see what type of function would work (a piecewise LUT would work, we can still do gradient descent on it). if you don't have access to the camera, you at least have access to your current results. Not having full range of zoom during the calibration video is almost (but not really) like saying the camera changed between calibration and test :) \"do the best you can\".\n\n2D matches: during pure rotations, the depth of points do not matter, and you can compute the reprojection error and minimize it! Essential matrices do work all the time, but they only provide epipolar constraints, which are weaker. Finally, a rotation IS an homography! it is a homography wrt the plane-at-infinity! To model flow with an homography, you need a planar scene, and a scene under pure rotation is effectively \"planar\".",
        "Update fyi: 1)i got the idea of the zoom interpolation, would be interesting but i don't have such information for the moment. I guess on the fly estimation in still the only option.\n2) yes i know about homography/rotation equality. The issue is that i don't have a planar scene and my camera is not just rotating, the movement is totally not uniform, it can move very fast or stay perfectly still for seconds. I am checking orb-slam to see how they deal with that, i remember (to check) they decide to use homography or essential matrix on the fly after having determined how much the pose differs (maybe using ransac+reprojection error?)"
    ]
},
{
    "submission_id": "1g8j2wf",
    "title": "Faster ByteTrack",
    "selftext": "I’m working on a Jetson device and running a version of the ByteTrack algorithm that is essentially the same as the “standard” implementation https://github.com/ifzhang/ByteTrack\n\nAt scale, this becomes computationally expensive especially since the Jetson CPU is not powerful. Is there a way to run a version of ByteTrack on the GPU? I imagine much of the calculations could be parallelized.",
    "created_utc": "2024-10-20T23:13:29",
    "num_comments": 3,
    "comments": [
        "Have you tried NVIDIA's tracker that comes with DeepStream?\n\nhttps://developer.nvidia.com/blog/state-of-the-art-real-time-multi-object-trackers-with-nvidia-deepstream-sdk-6-2/",
        "Try norfair - light and python based - works quite well!",
        "\"At scale\" what does that mean?  How many objects are you tracking?\n\nI find ByteTrack only accounts for [2-3% of the entire time](https://github.com/swdee/go-rknnlite/tree/master/example/stream) it takes for processing a frame for inference, post processing, and rendering bounding boxes on the image."
    ]
},
{
    "submission_id": "1g8djqf",
    "title": "OCR for Books?",
    "selftext": "I’m looking for recommendations for OCR Software that automatically determine’s a PDF’s layout across pages and can output a text document that separates the document by section.\n\nI’m scanning books and would like the software to, at the very least, automatically determine the start and end of each of each chapter (regardless of layout, images, or charts) and output the result to a text document (preferably a rich text document).\n\nI’d rather not have to reinvent the wheel to make something that does this if there’s already something on the market that does this cheaply or for free.\n\nI think PaperPort or software that uses ABBYY OCR tools might be able to handle this.",
    "created_utc": "2024-10-20T17:41:24",
    "num_comments": 0,
    "comments": []
},
{
    "submission_id": "1g8dhfh",
    "title": "Connecting many USB cameras for still image capture",
    "selftext": "Can someone help me figure out how to connect 10 USB cameras to my laptop? I'm only trying to capture still frames from each camera so bandwidth really shouldn't be an issue, but it turns out that the USB controller allocates the max possible amount of memory for each camera running at 30fps even though I'm effectively running them at 0fps. I've got a lot of ideas for how to get around this but am not really sure how viable they are.\n\n1. Limit the bandwidth of each camera using something like V4L. Seems like my cheaper camera boards don't allow this. Actually it allows me to set the frame rate to 0fps but I still can't connect more than 2 at a time.\n2. Write my own USB camera driver or firmware, or find source for one online and modify it.\n3. Buy a PCIe expansion enclosure for additional USB controllers.\n4. Buy PCIe-to-SATA boards for additional USB controllers and find a way to multiplex SATA to my laptop. might have to buy a desktop computer.\n5. Buy expensive scientific cameras that allow bandwidth to be limited through API.\n6. Buy expensive fireware/ethernet cameras.\n7. USB to wifi adapter for each camera and connect via wifi\n\nAny advice would be much appreciated. In case anyone wants to know, I'm trying to make lenticular portrates with a linear camera array. I can do it currently but I basically have to connect each camera one at a time and it takes too long.",
    "created_utc": "2024-10-20T17:38:03",
    "num_comments": 9,
    "comments": [
        "Look into optitrack to see how they manage cameras with a load balancer",
        "If you use machine vision/USB3 Vision cameras, they should have a device link throughput limit that will allow you to allocate bandwidth across a hub and using the APIs you can only grab images when you need it.",
        "You could buy a few Raspberry Pi or similar SBCs. Let's say you can get a min of 2 cameras on each + 2 on your laptop. So the most you need is 4 + laptop. Then use something like MQTT/REST API to trigger them all to capture frames.\n\nIt should be cheaper than machine vision grade cameras + lenses. Plus, it'll be scalable to more cameras in the future. Just add more Pi/SBCs.",
        "It looks like they're using ethernet cameras. I've heard that would be easier but the ethernet cameras I see online are quite expensive whereas I can get a usb camera board for like 20$.",
        "Unfortunately these aren't machine vision cameras. If they were they would probably come with a nice API made by the manufacturer. However they are cheap USB camera boards and as far as I can tell the firmware they come with is limited in it's available settings.",
        "How would I send commands to the Pi to trigger the cameras? And then how would I send the images back from the Pi to my computer? I'll look into that. It does seem like a straightforward solution but I'm not sure it's substantially different from using USB controller PCI boards, which would be acting like the SBCs in that case. The downside of the PCI board option is finding/creating the extra slots. Maybe that isn't an issue with the Pi option because, presumably, they would connect to my computer via USB hub or ethernet switch at the end of the day.",
        "[https://www.youtube.com/watch?v=S7Yle8clJ30](https://www.youtube.com/watch?v=S7Yle8clJ30)\n\nI think I can do what this guy does, setting up the Raspberry Pis as servers and my laptop as client. The Python script on the Pi will wait for a message from my laptop, and then capture an image and send it back. It does seem pretty easy.",
        "pretty much, but the way he does it is way more complicated than it needs to be. Look at MQTT or writing a small Flask app with python.",
        "try this\n\n[https://chatgpt.com/share/671780e6-9ab4-8005-b95c-ad0e5941f562](https://chatgpt.com/share/671780e6-9ab4-8005-b95c-ad0e5941f562)\n\nI've not tested it but looks about right"
    ]
},
{
    "submission_id": "1g8d4x6",
    "title": "Do you use monkey patching to modify library code?",
    "selftext": "I wanted to add an extra head to mask-rcnn from torchvision, for which I needed to modify some function in the existion MaskRCNN class. Would you use monkey-patching in this situation? Would you use subclassing?",
    "created_utc": "2024-10-20T17:19:56",
    "num_comments": 2,
    "comments": [
        "Subclassing would be better I think. Changing the library itself can cause conflicts with reproducibility.",
        "Only if I feel uncomfortable about modifying the existing code base. Extending the functionality of the existing code (or the library) is a much better practice."
    ]
},
{
    "submission_id": "1g8adzt",
    "title": "Looking for CPU advice & model recommendations: Planning to get a 4080 Super for multi-camera object detection\n",
    "selftext": "Hey all, I’m planning to get a 4080 Super to run object detection across multiple warehouse cameras (triggered by sensors for efficiency). I’m considering using models like **YOLOv8** or **EfficientDet** for real-time detection, and perhaps **ResNet** or **MobileNet** for more complex classification tasks. While the system handles inference, I’ll also be doing moderately heavy tasks like coding, Excel, etc. No gaming involved. What CPU would you recommend for smooth performance across all tasks and ensuring the models run efficiently on my setup? Thanks in advance!",
    "created_utc": "2024-10-20T15:04:35",
    "num_comments": 24,
    "comments": [
        "Depends on resolution and fps",
        "Are they security cameras?",
        "Wouldn't accurately tracking the stock levels be more efficient?\n\nFor employee location tracking, then UWB beacons appear to be working in some companies. Could also be used for asset tracking, but you could be making cakes for all we know\n\nI'd be interested in people's CPU suggestions all the same, afaik it doesn't matter that much?",
        "IMO 4060 Ti 16Gb is the best bang for the buck when looking for a consumer grade GPU which will train reasonably and infer (especially YOLO family models) very well.  $600 is reasonable. Obviously get the 4080 super or more if you can afford it.\n\n  \nIf you are looking for a tool for building and annotating your dataset, have a look at [https://oslo.vision](https://oslo.vision)",
        "What do you mean by \"more complex\" classification tasks?  Resent and mobilenet arent some fancy more complex models.  All of those models are capable of doing basically any detection or classification tasks you could possibly be trying to do.",
        "720p at 10-15FPS",
        "yes",
        "I don't think this stack is a good idea. They have built-in chips for detection already. Are you trying to detect something that is not in the detection list?\n\nSecurity cameras and industrial cameras require different methods than classical GPU processing.",
        "I want to track which employee and what is he taking from the warehouse inventory (Specific Products), how are they spending there time, if they are using there phones too much, safety equipment etc. . \n\nBut my concern is if the RTX 4080 Super and AMD 7900X (with other proper hardware) are enough to handle these tasks.",
        "Considering that Amazon tried to track who was taking what off of a shelf and ended up resorting to having people just watch the camera streams and marking down what they took and when, I don't think you're going to have much luck",
        "Does it have to be real time?",
        "yeah. it’s a brutally hard problem. been involved in a version of this, walked away with a very healthy respect for how little the real world cares about my models.",
        "I appreciate the insight, but my process is much smaller and more linear compared to Amazon, with fewer variables. I can iteratively correct and retrain to make the detection manageable. I’m only tracking specific objects of different sizes in a controlled environment, and I don’t need to monitor complex processes—just simple, step-by-step tracking.",
        "I thought about there is no need to run it real time.",
        "… PERRY the platypus?",
        "My process is much smaller and more linear than Amazon’s, with fewer variables. Since this is a family business and not something I’m selling, I can take the time to try and iteratively correct and retrain the model over a long enough time frame. I’m only tracking specific objects in a controlled environment, step-by-step. I would love to hear more about your experience in it.",
        "That's what Amazon was doing. Just tracking people taking items off a shelf. They even had sensors to detect when something got taken off, how much got taken off, and where on the shelf it got taken off of to supplement the cameras",
        "Tracking isn't simple.  Even the state of art real time trackers fail a lot.  Also trackers don't rely on the models but rather just having a consistent feature vector.  You're kinda fighting a losing battle.  Don't think what you're trying to do can be done in a reliable manner without knowing the geometry between cameras.",
        "Then I think using a Gpu cluster service will be more suitable in the long run.",
        "the hardest part IMO is getting enough varied labeled data that captured the true usage patterns and all of the huge variations of seemingly simple things - *“is someone taking a thing, and what thing are they taking?”* is a very hard problem, especially when the number of things they take at a time exceeds 1",
        "I seriously doubt that's going to help as much as you think. There's too much variability in how people take things off of shelves. You would need a massive dataset. It would be a better use of your time and money to put a qr code scanner on each item and have the employees show it to the camera before leaving with it.",
        "Yeaa i got you, so do you think its feasible and giving it a shot or not?",
        "Appreciate it man! Im a data engineer so im new to data science",
        "worth a shot if you keep it very constrained and simple to start - don’t do anything fancy until you have something basic working rock solid"
    ]
},
{
    "submission_id": "1g89hiq",
    "title": "Architectural analysis on android using tflite object detection",
    "selftext": "Here is a little insight of my latest project!",
    "created_utc": "2024-10-20T14:24:16",
    "num_comments": 0,
    "comments": []
},
{
    "submission_id": "1g89bqi",
    "title": "Now that i have an engineering job, how do i keep updated on latest interesting papers ? ",
    "selftext": "Hey guys, in the past i used to work in a lab, doing researsh on computer vision & ML. Talking with professors and PhDs, i would have a good idea of new interresting articles. Now that i work in a big company, i don't have this network anymore and i don't have time to spend hours searshing new interresting articles. Are there any good ressources that aggregate cool articles related to ML & CV ?",
    "created_utc": "2024-10-20T14:16:57",
    "num_comments": 5,
    "comments": [
        "This platform tries to help keep yourself up to date with the field: [https://search.zeta-alpha.com](https://search.zeta-alpha.com) It is the best platform I know doing that, even though its usability is limited and the free tier is quite bare.\n\nAdditionally, [https://paperswithcode.com/](https://paperswithcode.com/) is a pretty good resource for checking usable work (code!) and latest SOTAs.",
        "The entire internet is your network! More specifically, places like this sub.\n\nWhat’s your industry? Is computer vision popular within it?",
        "You know the answer already, don’t you? If no, let me say that: you will not able to follow all the inventions in a hype manner, but you can develop your skills as a real-life researcher focusing on consistent results in business-related tasks. You will develop your intuition in finding shortest and reliable approaches, you will invent how to make something valuable from trash, how to make models work faster, etc. This is how the real life works. \n\nIf you want keep up:\n- negotiate research periods with your business like 2 weeks every quarter;\n- plan your finance to have a sabbatical after 2-3 years and spend 1-2 months to explore the landscape.\n\nJust to show you a sample… Even after 4 years, YOLOv4 is a to-go option for a lot of detection tasks. ResNet celebrates almost 10 years and is a great option for many tasks. Good data beat model architecture in many situations.",
        "Try scholar inbox!",
        "Have no life! 🌈"
    ]
},
{
    "submission_id": "1g85zzp",
    "title": "LLM with OCR capabilities",
    "selftext": "Hello guys , i wanted to build an LLM with OCR capabilities (Multi-model language model with OCR tasks) , but couldn't figure out how to do , so i tought that maybe i could get some guidance .",
    "created_utc": "2024-10-20T11:52:33",
    "num_comments": 39,
    "comments": [
        "kosmos-2.5 a light weighted model dedicated for OCR",
        "So if you give chatgpt an image and ask it for the text in the image, it will give it to you. So maybe you can do something similar",
        "Use qwen2-vl",
        "Llama-3.2-11B-Vision-Instruct",
        "qwen vl 7b works well with Arabic...try that and let me know the results.",
        "Try with got ocr 2.0",
        "You could try TrOCR and send its output to a LLM.",
        "Florence-2 has pretty good vision for the case you want, it can also be fine tuned on custom data. But this is not that easy and some experience in coding is needed.\n\nYou can try it here: https://florence-2.com",
        "Here is an example [llava ocr](https://github.com/NielsRogge/Transformers-Tutorials/blob/master/LLaVa/Fine_tune_LLaVa_on_a_custom_dataset_(with_PyTorch_Lightning).ipynb) for llava. However it can be a pain to generate your own data.",
        "You can train an object detection model to detect characters. Then, use the position of each caracter to reconstruct the text. Scan each detected character from left to right, reading the document line by line from top to bottom",
        "Im currently building with gpt for payslips ocr detection. In my case I’m using gpt 4o model, the user uploads a list of pdfs or photos that after being processed get sent to a s3 bucket, then I have a lambda triggering a call to my ocr service app. In here we are simply making a prompt sending the url of the payslip pointing to the s3 bucket and the prompt along with it. I’ve set it so that it returns a schema in strict json output and I’m always getting the same schema for the responses, it’s working better than textract or any other ocr I’ve tried. Even with all hugging face open source models I could find. Really powerful stuff.",
        "Term u need is LLava",
        "can it be trained for RTL language? arabic persian ?",
        "yes , tried to build some like this but got stuck at training on another language.",
        "is it avaible for training ? because i am trying to train it on RTL languages.",
        "for ocr , or do it need training ?",
        "is their guide to train it for ocr in arabic ?",
        "now i need guide to train it on arabic ( right to left language)",
        "trocr works online , not entier document",
        "i'll go and check it out.",
        "did see it but never thought of using it .",
        "tried this approach but didnt get good results , because in case of handwritten , it wont detect line well ,mix with second line.",
        "but since 4o models are closed source , i wanted to build one from scratch for academic research . \nso i've build 2 ocr models ,( one with transformers and other using LMM large multimodal language ) , and result are average of 50%right .",
        "could you explain ?",
        "should be fine. but the pretrained model may not be based on arabic persian",
        "I think you can fine tune it using llama factory",
        "Yes, I run it for OCR. Use system prompt to give persona and context as sophisticated OCR.",
        "Well it shouldn’t be easy. I don’t know if you know how llm works. But they translate text to tokens and tokens to embed vectors. The embedded vectors and the tokens are in relationships(think about a lookup table) this works for most of the languages. But for Chinese and i think also arabic(correct me if I’m wrong) the letters are completely different( for Chinese ideograms). So u should enrich the vocabulary of the llm and adapt the non linear predictions\n\n\nPS \nI misunderstood your request but the overall flow should be valid. Got ocr is good for ocr but doesn’t cover the generation of texts",
        "Not sure if it suits your case, but you could try this approach :\n\nCreate a map (or table) that associates each character with its y-position. For each character, take its y value and divide it by the y value of all other characters. Two characters will be considered aligned on the same line if the absolute value of the remainder of the division (y_char1 % y_char2) is less than a predefined threshold.",
        "When you use llm, aren't you using transformers as well? I could not understand the difference?",
        "other solutions ?",
        "ok , could i dm you in case i needed help ?",
        "a guide on train it as an ocr model ?",
        "i got lost ,",
        "tried it but no result in case where user doesn't write in straight line",
        "vision language model i mean by them.",
        "I am newbie too bro. Check for tutorial ocr with qwen2-vl-2b. Those are the only thing you need.",
        "Here is a better option:\n\nUse a segmentation model to divide an image or text into separate lines.\n\nThen, apply an object detection model to identify the characters present in each line.\n\nTo improve the character detection results:\n\nTake a reference character, for example, the character \"a\" and analyze the \"x\" position of all other characters.\n\nNow select those located to the right of the character \"a\".\n\nOf them, exclude the characters that are positioned above the character \"a\".\n\nCalculate the distance between the character \"a\" and each of the remaining characters. You can add the closest character in terms of distance to the resulting string.\n\nRepeat the process using the new character as a reference, continuing until all the characters in the line have been processed.",
        "ok"
    ]
},
{
    "submission_id": "1g81d9k",
    "title": "CloudPeek: a lightweight, c++ single-header, cross-platform point cloud viewer",
    "selftext": "https://preview.redd.it/mkwbsg22fxvd1.png?width=1946&format=png&auto=webp&s=5bddf24571cf4ffe1df08fea6d8312e8e663164a\n\nIntroducing my latest project **CloudPeek;** a lightweight, **c++ single-header**, cross-platform point cloud viewer, designed for simplicity and efficiency without relying on heavy external libraries like PCL or Open3D. It provides an intuitive way to visualize and interact with 3D point cloud data across multiple platforms. Whether you're working with LiDAR scans, photogrammetry, or other 3D datasets, **CloudPeek** delivers a minimalistic yet powerful tool for seamless exploration and analysis—all with just a single header file.\n\nFind more about the project on GitHub official repo: [CloudPeek](https://github.com/Geekgineer/CloudPeek)\n\nMy contact: [Linkedin](https://www.linkedin.com/in/abdalrahman-m-amer)\n\n  \n**#PointCloud #3DVisualization #C++ #OpenGL #CrossPlatform #Lightweight #LiDAR #DataVisualization #Photogrammetry #SingleHeader #Graphics #OpenSource #PCD #CameraControls**",
    "created_utc": "2024-10-20T08:32:40",
    "num_comments": 32,
    "comments": [
        "Does it support large pointclouds i.e. pointclouds with more than 2M points?",
        "nice work! If it also had python bindings I would be so happy",
        "Can it read dji pointclouds?",
        "Cool! Do you plan on supporting comparison between different point clouds of the same object? I do some biofuel stack monitoring, and the material compacts over time due to a bunch of different factors. Outside of volume analysis using photogrammetry-based DEMs, I use cloud compare. Would this tools provide additional value?",
        "This could be really cool to use in a WASM module for visualizations on the web. Nice job!",
        "C++ is a terrible user experience and causes the vast majority of security vulnerabilities in the world. Could you rewrite this in rust?",
        "I designed it with support for multithreading and sequential loading for pcd content by chunking and inserting into viewer, the user have control over everything even the point size. So in theory it should handle huge point cloud files after tuning the config. ge it a try and let me know!",
        "Thanks! Let me experiment and see!",
        "/r/rustjerk",
        "Unfortunately am not a RUST developer. Yet the project intended for local machines with minimal env setup like robotics sim machines docker controllers..etc",
        "Could you point out vulnerability in his code?",
        "i think he's trolling lmao",
        "You're in for a treat then. Rust is so much nicer to program with. It has a first party build system and package manager, so there's no more need to use cmake or resort to writing header only programs. \n\n\nC++ is a dying language, and everyone in the industry is rewriting their C++ code base in rust.",
        "Propaganda is strong in this one",
        "Thanks for pointing that out! will look into RUST",
        "Everyone? Seriously?",
        "Well the smart teams are. Everything about rust makes it so much more productive to use over C++",
        "Then why can't you point out the vulnerability in OP's code? And if productivity is your concern then why not use Python?. The development time takes 1/10 of that of Rust.",
        "If finding vulnerabilities in C++ were easy, we wouldn't have so many zero days. \n\n\nAnd Rust gives you the performance of C with that memory safety of a garbage collected language. And because it has a first party build system and package manager, there's no more time wasted on cmake and installing packages. It really is just a pleasant experience. ",
        "Man, get a life. From what it looks like you have just started programming like 3 months ago. You know nothing about C++ yet make big claims.",
        "I've been programming for way longer than that lol. Why are you so attached to C++ though?",
        "Why are you so attached to Rust?",
        "Cause Cmake gave me PTSD",
        "Interesting. Because you couldn't learn something as basic as CMake and now you are crusading againsta language for your own failure?",
        "No, it's because I learned too much cmake that I now crusade against C++ Everything from it's governance to it's build system, to the way you important/ install libraries, to the stupid way you print shit, everything about C++ is rotten and unfixable. ",
        "I think you should move to python. You have a lot of issues with standards and styles. Don't cry, there's still people who like C++. But I personally think Carbon is multiple times better than Rust. But then there's Mojo as well which is a better option for people using Rust.",
        "Rust has a first party linter to enforce standards and styles across the entire community. ",
        "That's exactly why it's shit. Why does someone else define the style for me?",
        "Because once you get used to it, you will be ready any rust program with ease. All rust code will feel like you personally wrote it. You can also turn it off if you really want to, but do you want everyone else turning it off as well?",
        "But why I need to get used to it?",
        "So you can read other people's code with ease. Don't you hate it when you find some C++ code you want to use and it's ugly and hard to read?"
    ]
},
{
    "submission_id": "1g81alv",
    "title": "How to know when a model is “good enough”",
    "selftext": "I understand how to check against certain metrics in other forms of machine learning like accuracy or how a model predicts something in linear regression. However, for a video analytics/CV project, how would you know when something is good enough? What is a high enough % for mAP50, precision, recall before you stop training a model and develop other areas?\n\nAlso, if the object you are trying to detect does not have substantial research done on it, how can I go about doing a “benchmark”?",
    "created_utc": "2024-10-20T08:29:19",
    "num_comments": 22,
    "comments": [
        "Nobody can tell you that.\n\nFor me, good enough is when I can convince my management to put it into production. When it no longe makes any embarrassing mistakes. ",
        "What about metrics? Put it in your application, get data and measure how is it performing. If it's well enough, keep it",
        "When it starts doing indented task.",
        "Good enough is when you made a model, ran it against unseen data and you get no errors .\nSometimes that's 2 or 3 models later or even 7 models .\nBut it's not normally the 1st model .\nRight now I'm training a model to learn the age of a person just from their hairline.\nI have 250,000 images, and I'm planning on having 2 million for the final version of the model.\nModels used for production take a long time, it's not some random python tutorial.\nThose are so misleading and make people think training models is easy .\nYou'll get there, just take a measured scientific approach for each iteration.\nAlways getting better.",
        "”Good enough” depends completely on the use case etc. In the industry you have to define some requirements for your system, or let stakeholders set them. \nIf it’s a school project, is there really a ”good enough” threshold? Isn’t the end product rather some report or similar stating the performance of your model/system together with some discussion of why/how it could be better etc? \n\nIf you have problems finding a benchmark for your specific problem try find some similar that could act as a proxy for your task, or benchmark it against doing the same task manually or with some industry tool. You probably have to get access to or gather some test data anyway.",
        "You generally could save the model as you go and then when you get like 3 epochs in a row with no improvement on validation then you would take that best earlier model. \n\nAlternatively, the upper limit on model performance is 100%, so you could probably safely stop at 100% accuracy on validation data.",
        "In a research setting “good enough” is usually better than whatever the previous “best” was.\n\nIn a business setting, you need to come up with some metrics that business stakeholders can understand (probably not mAP) and agree upon a minimum viable performance on those metrics.\n\nFor personal projects it really depends on the project and what you want to achieve.\n\nTo get a benchmark performance, you can train a very common model like FasterRCNN on your dataset.",
        "Good enough = meets business expectations versus business prepared benchmark data",
        "When the next team in the chain accepts it",
        "It's more of a \"common sense\" thing when interpreting your metrics.\nmAP is ok-ish when comparing many models on the same dataset with a single metric, but it doesn't tell you too much about usability, as it's hard to interpret in general and depending on IOU thresholds, \"ignoring\" false positives etc. There are many issues that arise from that on custom datasets.\nWhen working on detection, it is a nice thing to report, especially in your school project. But to gauge the performance of your model for your use case it might not be enough.\nI'm working in a company that actually ships detection models on many different custom datasets and more often than not, we have a \"custom\" metric for each project.\nUsually you want to know about precision and recall at a given confidence threshold (and maybe a fixed IOU threshold for the NMS). But what counts as a TP (true positive), FP (false positive, detection without an object), FN (false negative, missed detections) is the custom part and e.g. the coco metrics reflect that with their ap50/ap75/ap95 metrics using different IOU thresholds.\nBut you don't have to use IOU as your matching criteria. Often it's rather important, that the model detects \"something\" even if it has zero IOU with the object, i.e. for small objects. So you could use GIoU or just the squared distance between the centers to match your results.\nHaving evaluated your metrics in this way is the first step. Now you have to interpret the result depending on your use case. Note down what the consequences are for the model to miss an object and vice versa when the model detects objects that are not present. So you know what your minimal requirements for precision and recall should look like. Sometimes it's only important that correct number of objects are detected and you can use an image level metric. \nOnce you're satisfied with the selected metrics it's time for the first live evaluation. It might be rather disappointing to see your model, that performs so well on the test set, miserably fail in a live scenario, but it's a good reminder, that you probably do not have enough data to train AND evaluate your model. \nNow you enter the \"data collection -> annotation -> training -> evaluation -> testing\" cycle until your test set metrics reflect the live performance. \n\nAll of the steps above have a lot of nuances and sometimes there are shortcuts, but proper dataset creation and evaluation are the most important areas in your experiment design. \nIf you can control lighting conditions, remove color without losing information etc. -> do it. Make the task as easy as possible before throwing the next SotA model on it. \n\nThere is a lot more to consider when deploying models, but that's another topic.",
        "I see, that follow up answer makes perfect sense as well.",
        "Yea I read the metrics after training, just curious if anyone had a good way of determining “good enough”",
        "I might have gotten misled by “some python tutorial” so completely agree on that. I’m currently doing this as part of my final year project at a university but I do not have any CV background so this is my first project on CV with a limited timeline, probably cant produce a fully fledged model but I can give them a proof of concept.",
        "Interesting model. Curious how well\nIt works.",
        "I see, thank you for putting it this way. Will look into finding other tools as well.",
        "Gotcha on that.",
        "Thank you for the suggestion, one of the previous replies was also about benchmarking it against other industry standards. Will look into finding a tool or a few like that.",
        "Gotcha on that.",
        "I was not talking about the training metrics. Put the model in the real world app (or environmen) , and watch it working. Usually just counting good/bad detections is a good starting point to see if it is good enough",
        "My final year project was reading doing OCR from products .\nNot for a checkout but to see could it do OCR on all types of product boxes.\nThis was in 2008, so the technology was not as developed as it is now",
        "Im going to have 3 ways to detect the age in a face, and let them vote.\nSo that's it's not learning just the face.",
        "Oh ok , I was thinking the only input was the hairline.  Like just a single crop showing someone’s forehead lol. "
    ]
},
{
    "submission_id": "1g7yy8x",
    "title": "Book title",
    "selftext": "Hello everyone,\n\nI saw a book somewhere on this subreddit that concerned how to write a computer vision paper, or at least it was titled something along the lines of that. I can't find it using search, so I would grateful if someone could tell me what book it is. Or perhaps recommend a book that gives me a starting point. Thanks in advance.",
    "created_utc": "2024-10-20T06:40:46",
    "num_comments": 0,
    "comments": []
},
{
    "submission_id": "1g7we4k",
    "title": "Instagram pages for latest CV papers & news?",
    "selftext": "Are you aware of some IG pages with educational videos on latest computer vision papers and news? ",
    "created_utc": "2024-10-20T04:16:02",
    "num_comments": 8,
    "comments": [
        "Really bro? Out of so many many online platforms you've chosen the one with the most gullible people on the face of the earth?\nThat's not a place to look for knowledge.",
        "Bro just stop right there. Quit this train of thought. 30 second videos are not how you want to be getting informed on the latest research. Sign up to a newsletter so you can do quick reading, and then dive into the paper later, or setup some Google scholar alerts for particular topics.",
        "I follow the researchers or some accounts that post the latest news in cv on X. If the work belongs to the  researcher then they generally provide the paper and the github page if exists. When you start to follow these kind of accounts, the X algorithm starts to show you researchers from the similar areas.\n\nLinkedIn is also good for this but I find X more effective.\n\nLike it was said in previous comments, Instagram isn't the place for this. It is just a fake world made by people in real life.",
        "Fuck instagram, this is what you need: scholar-inbox.com",
        "I’ve been thinking of crossposting to my IG kevinwoodrobotics, but right now all of my cv paper reviews and news are on YouTube if you want to check it out \n\nhttps://youtube.com/@kevinwoodrobotics?si=PSzcYDJGQhl0QW0x",
        "What’s a good newsletter? How would I find them for other topics? I only ask because I’m weary of googling anything anymore with all of the sponsored results",
        "If you think google and other search engine rankings are biased, I don't know what to tell you about how social media newsfeeds are put together...",
        "What are you trying to say? I’m not asking about social media sources"
    ]
},
{
    "submission_id": "1g7w8v8",
    "title": "Working Project",
    "selftext": "So I'm currently working on a project rhat detects defects in a machine for a construction company.\nThey want to know the measurement of some tools by capturing a photo of it.\nI told them it only can happen if the camera used is advanced to get the ditance or comparing the tool with another tool knowing its measurements but they said both solutions aren't good.\nSo is there any way or should i decline it?\nI never been working on a measurements project before",
    "created_utc": "2024-10-20T04:06:13",
    "num_comments": 4,
    "comments": [
        "There's some details here that are really important. If the camera is static and you know the distance to the object then you can do the measurements reasonably well, especially if the tools are relatively flat. But if they just want to take a picture with their phone or something then they're going to struggle without a lot of work. It's unlikely even the lidar on the newest iPhones is going to be accurate enough to give you a reasonable estimate of the size",
        "tell them the field of AI doesn’t care if they don’t like your solutions. it ain’t magic.",
        "Can they consider a stereo camera setup?",
        "It might be late but i'll try to give an answer anyway. If you are working with a single STATIC camera, without any knowledge about the object or any reference you are almost doomed. However, if your object lies on a flat surface and you are able to keep the distance between the object and your camera constant (you said it's a machine, don't know which kind but sometimes in production you know the size of the object and you have an encoder so you can move your camera accordingly) you might be able to \"rectify\" the image. The \"rough\" idea is to calibrate the camera not to retrieve intrinsics, butnto obtain some rectification maps for a given distance, in such a way you have a mm/pixels ratio to convert the pixel's measurements in millimeters. Of course this is more reliable with large focal lengths and large camera-object distance to reduce perspective artifacts, or even better with a telecentric lens. Not digging more in details because i need more info if this is viable option. Otherwise you can use stereo, laser scanners, tof cameras, structured light, sfm, and whatever.. tons of techniques"
    ]
},
{
    "submission_id": "1g7ocvf",
    "title": "For roboflow users, is 800-1000 image dataset for object detection doable on a free plan?",
    "selftext": "Is it possible to do a plastic bottle, tin cans, and paper wastes detector using only the free plan of Roboflow. (We will use various brands to be able to detect specific types of waste like coke, sprite, etc)\n\nWe haven't started anything yet as of now, and we're just curious if we can pull it off. We're only required to have a minimum of 800-1000 dataset. We're going to be using Rasberry Pi and YOLOv5 for this. Thank you!",
    "created_utc": "2024-10-19T18:58:39",
    "num_comments": 1,
    "comments": [
        "You’re able to have the number of images listed in each tier here [Roboflow Pricing and Plans](https://roboflow.com/pricing) This means you can upload and download as many times as you’d like if your total images stay within the total number approved in the tier you are using."
    ]
},
{
    "submission_id": "1g7ng02",
    "title": "Exploring 3D Inpainting Techniques for Multi-View Image Consistency",
    "selftext": "I'm exploring the possibility of a 3D generative inpainting task. While 2D inpainting works well for single images, it falls short when trying to generate consistent results across multiple views of the same scene.\n\nThe goal is to take multiple input images and generate a consistent representation of an object from different angles or perspectives, keeping the background context in mind. Essentially, it's about generating the same object across various viewpoints based on the camera's position.\n\nIs this problem solvable with current techniques? My understanding of ML theory isn't enough to figure out how this could be done effectively.\n\nIt seems somewhat similar to using LoRA, but in a 3D context where the object needs to be coherent across perspectives. While prompt engineering could help by providing detailed descriptions, the random nature of generative models makes it challenging to ensure consistency, even when using the same seed for different viewpoints.\n\nAre there any existing methods or approaches that could achieve this, or any ideas on how to proceed?",
    "created_utc": "2024-10-19T18:06:29",
    "num_comments": 3,
    "comments": [
        "This is way beyond my ability currently but it sounds doable. Image generation isn’t truly random otherwise it wouldn’t work in the first place.\n\nFollowing so I can come back in a year once I better understand this stuff lol ",
        "ReconFusion ( [https://arxiv.org/abs/2312.02981](https://arxiv.org/abs/2312.02981) ) uses a SD prior when training a zipNeRF, it's not outpainting per se, but you do get consistent appearance for multi-view scenes. Anyway, look at NeRF type of approaches, because having being consistent across views kinda implies a 3D representation.",
        "Lora can make some really good repeatable characters however problem I think is being able to key frame them in time to identical camera angles"
    ]
},
{
    "submission_id": "1g7l8h7",
    "title": "computer vison self chekout project",
    "selftext": "i am working towards buliding a self chekout system would lke to hear some suggestons which microcontroller or sbc to use , many refered raspberry pi but i'm in doubt whether other low cost processor can run this model or should i run the model on the cloud and just leave the image processing part to the proceesor ",
    "created_utc": "2024-10-19T16:11:45",
    "num_comments": 4,
    "comments": [
        "Wouldn't it fit best a more robust GPU capable system, like NVIDIA Jetson Nano?\n\nTry finding some comparison as how many FPS it runs facial detection models on both Jetson Nano vs RPI. I recall seeing something like this, based on the lib compiled enabling the GPU vs not using on Jetson Nano.\n\nSo if you need something bit more real-time, your call would be a more robust platform. But if budget is your priority, there are many low-budget Allwinner H3 processor based systems (OrangePI) or Rockchip (Luckfox Pico Plus) on AliExpress worth checking, at least worth for prototyping/testing as they cost as cheap as 15 dollars.",
        "don't underestimate how complex a problem this is, people have been trying to solve this for years with huge budgets and still haven't succeeded.",
        "Check out the rk3588 based sbcs.",
        "Im just trying to touch that tip of iceberg here..i am curious about this technology in solving this niche problem and my motive is to limit the domain area of this appliction to a limited set of packaged supermarket items in that way i hope i could build build something that resonates with this idea and since im fairly inexperienced with the hardware implementation here im seeking help"
    ]
},
{
    "submission_id": "1g7enha",
    "title": "Struggling with Footvolley Player and Ball Detection - Need Advice on Tracking and Body Part Recognition",
    "selftext": "Hey everyone,\n\nI'm working on a model to detect players on a footvolley field, identify when and with which part of the body a player hits the ball, and track who made the hit. So far, I've been using YOLO for player detection, the Tracktor system for tracking, and pose estimation to identify body parts.\n\nUnfortunately, things aren't going as well as I'd hoped. I'm facing significant challenges with:\n\n* **Re-identification**: When players move or change angles, the tracking system loses track of their IDs.\n* **Ball tracking**: I’m having trouble accurately tracking the ball, especially when multiple players are involved.\n* **Body part detection**: Detecting which part of the body (head, foot, etc.) hits the ball consistently has been really tricky.\n\nHas anyone here worked on something similar or can offer advice on how to improve these aspects? Any suggestions or alternative approaches would be really appreciated!\n\nThanks in advance!",
    "created_utc": "2024-10-19T10:57:53",
    "num_comments": 0,
    "comments": []
},
{
    "submission_id": "1g7ea1c",
    "title": "Storing ML video annotations in mp4 / fmp4 / cmaf fragments",
    "selftext": "Are there any libraries or examples showing how to store bounding boxes in an mp4 / cmaf fragments? i am hoping to simplify our ML ops by storing this data together in the same mp4 file, and I believe it should be possible, but i cannot find any examples of it being done.\n\nright now we have to write out our detections and classifications to a separate file and its a real pain to work with. \n\nif i could get it into our video segments then i would be able to move around video and annotations together via hls or dash and i would be 100% sure the video files and annotation files havent gotten mixed up somehow, and the video itself would still be playable by standard players (without the annotations visible but still very useful). and in our app we could modify the player to parse out and draw the annotations without needing special synchronization logic.\n\ndo examples of how to do this exist?",
    "created_utc": "2024-10-19T10:40:49",
    "num_comments": 10,
    "comments": [
        "You could put into metadata using ffmpeg",
        "If you can't find any examples or tools to do this, you may want to reconsider your plan.\n\n\nOr at the very least, also build tools to insert and extract the annotations so they are easily usable by other tools.\n\n\nOne problem with inserting annotations is that you are now modifying the source data. In most situations source data is best left immutable. Annotations can always be recomputed/edited.",
        "Are you sure you want to do this?\n\nI went down that route once (with still images) and it quickly turned my application into a brittle nightmare. The tipping point was when I had to write a custom backup solution because a a simple change to an annotation would result in the entire multi-megabyte file being re-copied over the network. Had this been a cloud-based app that would have instantly lead to really expensive charges. \n\nThere’s a principal in software engineering called “separation of interests” and this goes against that. You want each component of the system to be as independent of the others as possible to improve flexibility and performance. Keep in mind that your application is part of a larger system (the OS, the filesystem, other software, maybe the cloud) and those systems were probably engineered with that principal in mind as well. ",
        "i considered this but wasnt sure how to do 1 metadata per media fragment in the file. i am hoping to impl LLHLS using the #ext-x-byterange tag, so ideally the metadata would be part of each segment rather than the overall file\n\nbut this is a pretty good fallback idea if i cant figure that out since i typically have been doing multiple media fragments per file anyways.",
        "> If you can't find any examples or tools to do this, you may want to reconsider your plan.\n\ni could leave things the way they are, but i feel something like this would be very helpful, potentially reducing the compexity of some of our display and archival systems.\n\n> Or at the very least, also build tools to insert and extract the annotations so they are easily usable by other tools.\n\nhls.js has features for extracting cmsg klv metadata, so to me it feels like it should be possible in a somewhat standardized way, but i havent figured out how to write cmsg klv metadata yet.\n\nhttps://github.com/video-dev/hls.js/blob/master/docs/API.md#enableemsgklvmetadata\n\nthere may be other ways to do this kind of thing as well i am not aware of as well, as i inly recently started looking into it.",
        "right now i use fmp4 cmaf muxed h264 encoded video over low latency hls to transport a preview stream to clients to view the video plus data. theres about 200ms from live with this approach.\n\nif the annotations are in the fragments, i dont need to refetch or make any extra network calls. i can piggyback on the llhls protocol and get lots of nice features like prefetching, caching, and byterange requests. and annotations can be enabled and disabled in the client and interacted with rather than being statically drawn on top. if the video is downloaded and shared, the annotations would be gone if viewed in a normal player, but the video would still work, and if it is shared the annotations could be recovered directly from the video without needing to ask for the annotations file as well. the video could even be labeled directly and used for training since i didnt have to modify the original video to display the annotations.\n\nannotations are still going to be stored in both the primary and long term audit database as rows. this is the \"source of truth\". but theres no guarantee they will be available in the database within 200ms for fetching from clients. whereas if they are in the fragment, I do not have to fetch anything extra.\n\nthey dont need to be recomputed in my usecases as a majority of the value is obtained from doing this as close to real time as possible. if we switch models it doesnt need to be applied against historical data outside of training runs.\n\ndo you have any advice in your approach for things you would do if you were to do something similar? anything at all about it that worked well?",
        "one thing i wanted to add is i can provide one or more audio track in band and out of band, i can provide captions and subtitle tracks in band and out of band, but right now i only know how to provide annotation data out of band via the m3u8 file with #EXT-X-DATERANGE tags or with an entirely separate json file.\n\nall of these types of data are timing sensitive. i do get not always wanting them all together, and i wouldnt want the media file be the soruce of truth, but it feels weird that i literally dont know how to provide annotation data inband without coopting other metadata fields like i do with audio and subtitles.\n\nit would be one thing if i knew how to do it and decided it wasnt a good fit for any of my usecases after trialing it, but i feel like i should know how to do it.",
        "You can abuse captions and store boxes as text. But it's way easier to just zip the video and annotations together with no compression if you want a single file solution that's easy to process.",
        "Sorry I can’t help you! Sounds like you’re well past the point where performance optimizations can justify breaking some “rules”.",
        "thank you, i think the caption idea would likely work, i hadnt looked into it until now but it seems promising\n\nedit: will explore emsg boxes first because that seems like the intended way to do this as of 2024"
    ]
},
{
    "submission_id": "1g7e9c0",
    "title": "Detecting speed of vehicles using realtime cctv footage",
    "selftext": "I am doing a project that requires me to build a system that detects whether vehicles are going over the speed limit and captures the number plates using cctv footage in real time.i have built a system that can find speed of vehicles in downloaded video files but I don't know how to make it work using real time footage and also how to make it capture the number plate. Can anyone help",
    "created_utc": "2024-10-19T10:39:57",
    "num_comments": 1,
    "comments": [
        "given the perspective of the camera(s), calibrate road pixel locations into relative distances (meters/miles). These distances WILL NOT BE LINEAR so don’t just say X pixels is the same as Y meters. Then get car segments (not bounding boxes that vary too much) and ideally map something approximately universal  like wheel locations. From there, between frames, calculate distance. Use time between frames and these distances to get instantaneous speed. Then, as these will be error-prone and noisy, do some type of average (eg geometric mean) over a window of time to get a more dependable speed. The more cameras and more samples the better. Having real speeds of cars will also help refine a method like this"
    ]
},
{
    "submission_id": "1g7e5gd",
    "title": "Sign language detection project",
    "selftext": "Can someone help me find a dataset suitable for real time sign language detection with s webcam project , and if someone have experience in such project can he help with some materials?",
    "created_utc": "2024-10-19T10:34:53",
    "num_comments": 2,
    "comments": [
        "Youtube with the signer standing in front of a static background - filter for only the moving parts and standardise frame size, you can extract the closed captions too. Just a thought.",
        "This has been done to death and you can find a ton of dataset just by a simple Google search."
    ]
},
{
    "submission_id": "1g7b3jk",
    "title": "Vehicle Detection and Classification in Night-Time Images with Blur and Light Interference",
    "selftext": "Hi everyone! I'm relatively new to computer vision and currently working on a project to detect and classify vehicles (Car, Bus, Motorcycle, Truck, etc.) in images taken at night. These images are fetched every 3 minutes, but I’ve been facing a few challenges:\n\n1. **Blur**: The images often suffer from motion blur, making it difficult for models to detect vehicles clearly.\n2. **Light Interference**: Streetlights, traffic lights, and vehicle headlights are creating a lot of noise in the images. I'm concerned that these light sources might confuse the model and reduce accuracy, especially when trying to differentiate between vehicle lights and other sources.\n\nI’m planning to use **YOLOv11** for the vehicle detection and classification task but want to make sure I optimize the preprocessing step. Specifically, I’m looking for advice on:\n\n* How to **deblur** the images effectively.\n* Techniques to **reduce the interference from external light sources**, while still keeping the vehicle headlights intact for detection.\n* Any tips or tricks that could help improve the performance of YOLOv11, especially for night-time images.\n\nAny suggestions on preprocessing pipelines, filters, or general guidance would be hugely appreciated. Thanks in advance!  \nSome sample images:  \n\n\nhttps://preview.redd.it/zcxerlpxbqvd1.jpg?width=1920&format=pjpg&auto=webp&s=5fb29b3c4891c5042ecd000a53ad97fd6ece6fe0\n\nhttps://preview.redd.it/4mi6mccqbqvd1.jpg?width=1920&format=pjpg&auto=webp&s=468969523835df507687a50c899d17e33c4927e9\n\n",
    "created_utc": "2024-10-19T08:15:42",
    "num_comments": 9,
    "comments": [
        "Use Gaussian or median filters to smooth out high-intensity areas without affecting vehicle headlights significantly. ,  \nOpenCV's Motion Deblurring",
        "Don’t bother. Training your network directly on this low quality imagery will work better than trying to manually improve the image quality beforehand. The core reason is that you would have to hand-tune the corrective filtered and somehow know when to apply them automatically. Get that wrong and you’ll really screw things up. Imagine if you deblur an already sharp photo for example. \n\nIf you must, then here’s a starting point for your research. https://stackoverflow.com/questions/58803611/how-to-motion-deblur-an-image-using-opencv-and-python",
        "Thanks i would try them out.",
        "Oh and if you have any control over the cameras that is a big help. ",
        "Thanks for the reply. I was thinking to have seperate models for daytime and nighttime images. Thus I thought why not also preprocess nighttime images. ",
        "Also would it be better if I just use a single model ?",
        "Sure that makes sense and is a good idea. I still think you’re best off just letting the model learn how to handle the “raw” images. And I don’t think it will necessarily work much better than one larger model. ",
        "So I should be using two models with raw images. Right?",
        "I would just use one model.  It will learn which filters to use when. \n\nOne reason for this is because otherwise you reduce the amount of imagery available for each model to learn from. Most of the information in the imagery is similar whether it’s day or night. "
    ]
},
{
    "submission_id": "1g78mix",
    "title": "@help",
    "selftext": "u/help\"My bicycle was stolen last year, and it was my primary mode of transportation. I have CCTV footage of the incident, but the quality is poor, and the person's face isn't clearly visible. Is there any way someone can help enhance the video to identify the person?",
    "created_utc": "2024-10-19T06:12:17",
    "num_comments": 3,
    "comments": [
        "lol we need to just auto ban these stupid as shit unblurring requests",
        "no!\n\nit's not that people don't want to help, but really it's just a bad idea coming from TV shows. There's a more complicated discussion to be had here of course, but the short and clear answer is \"no\".",
        "lol entirely wrong sub"
    ]
},
{
    "submission_id": "1g741tv",
    "title": "Seeking Guidance on Text to Photo Image Synthesis for My Undergraduate Thesis",
    "selftext": "Hi everyone,\n\nI'm an undergraduate Computer Science student currently working on my thesis focused on text to photo image synthesis (from sketch). I have a basic understanding of machine learning and deep learning concepts such as CNNs, RNNs, and LSTMs, but I'm looking for guidance on how to dive deeper into this specific area.\n\nCould anyone suggest the essential topics I need to study, relevant algorithms, or frameworks to explore for this project? Additionally, what are some recent papers or contributions I should look into for inspiration and how can I further contribute to this field?\n\nThanks in advance for any advice or resources!",
    "created_utc": "2024-10-19T00:58:26",
    "num_comments": 0,
    "comments": []
},
{
    "submission_id": "1g720lz",
    "title": "Recommendations for good realtime facial emotion recognition (FER) models",
    "selftext": "Hi everyone. I am relatively new to the field, and I need to use an accurate facial expression recognition (FER) model that can be used in realtime, ‘in the wild’ scenarios. I was previously using the DeepFace emotion detection out of the box, but classification accuracy is too low in real world scenarios (eg. faces slightly looking away from camera). I was wondering if you have any recommendations on how to proceed?",
    "created_utc": "2024-10-18T22:27:38",
    "num_comments": 3,
    "comments": [
        "Googling TikTok FER will bring up some interesting results. There's an ffmpeg based model that can do FER and emotion classification on downloaded TikTok videos and I assume other videos too. \n\nThe model I've used gives a video game like real time tracking of the six or so \"Ekman emotions\" as they appear in the subject's face.",
        "checkout some hugging face model (not knowing exact name but its provide some cool model)",
        "Just a word of caution if you have any connections to the EU: emotion detection is one of the most regulated topics in the AI act. So you night get some legal problems and have difficulties still finding good algorithms."
    ]
},
{
    "submission_id": "1g70j8h",
    "title": "Model training very high accuracy but then when ran on the same data, always gives the first class no matter what.",
    "selftext": "I trained an image classification model and I'm at my wits end with it. There are two folders, \"house\" and \"not\". it simply is to detect if a house is in the image. The training goes through with very high accuracy, but then when i go to use the model it is always assigning NOT no matter what, even when ran on the exact same data that i trained it on. Any ideas? \n\nCode I trained with\n\n[https://pastebin.com/KjjRr0yr](https://pastebin.com/KjjRr0yr)\n\ncode Im calling with \n\n[https://pastebin.com/jB4ZJdgT](https://pastebin.com/jB4ZJdgT)\n\n  \nany help is so greatly appreciated.",
    "created_utc": "2024-10-18T20:52:37",
    "num_comments": 5,
    "comments": [
        "`for i in predicted_classes:\n        if i < len(class_names):\n            predicted_class_names.append(class_names[i])`\n\nTry replacing with\n\n`for i in range(len(predicted_classes)): \n    if i < len(class_names): \n        predicted_class_names.append(class_names[i])`",
        "When you use GPT code without understanding:\n\n`img_array = preprocess_input(img_array)`\n\n> The preprocessing logic has been included in the efficientnet model implementation. Users are no longer required to call this method to normalize the input data. This method does nothing and only kept as a placeholder to align the API surface between old and new version of model.\n\nhttps://www.tensorflow.org/api_docs/python/tf/keras/applications/efficientnet/preprocess_input\n\nYour preprocessing between the training and prediction code is different. The preprocessing in the training code is unnecessary.\n\n> Note: each Keras Application expects a specific kind of input preprocessing. For EfficientNetV2, by default input preprocessing is included as a part of the model (as a Rescaling layer), and thus keras.applications.efficientnet_v2.preprocess_input is actually a pass-through function. In this use case, EfficientNetV2 models expect their inputs to be float tensors of pixels with values in the [0, 255] range.\n\nhttps://www.tensorflow.org/api_docs/python/tf/keras/applications/EfficientNetV2B0\n\nSo your training code is double preprocessing the image, while your prediction code is using a dummy preprocessing method that doesn't do anything.",
        "Check if the images are loaded in properly in the 'code Im calling with' section by using something like matplotlib.pyplot.show() and make sure that is not just a blank image. \n\n  \nYou could use the tf.keras.utils.image\\_dataset\\_from\\_directory() function to load the images too.",
        "Thanks for the insight. I never claimed to be an expert and will fully admit I’m out of my wheelhouse. I’m just trying to design something to help people. I will look into this and try to implement!"
    ]
},
{
    "submission_id": "1g6yt4e",
    "title": "Looking for Professors in Computer Vision Who Supervise Students from Other Universities – Any Recommendations?",
    "selftext": "Hi, I am looking for Professors in Computer Vision who supervise students from other universities\n\nIn short, I don't have a supervisor that I can discuss with. Also, although I have work as a SWE since 2020, I don't have mathematical background because my bachelor degree is Business Administration. So, for now, I am only confident to be able to publish to a SCI Zone 3 journals\n\nLong story short, I am going back to academia to research Computer Vision, oversea. Unfortunately, I joined to a research group that is very high achieving (each of the research group's published papers are SCI Zone 1) but because I don't speak their language, the supervisor left me on my own (I am the only international student and whenever I contacted him through app, he said to ask the senior. Yet, I saw with my own eyes that my supervisor is doing his best to teach the local students a Computer Vision concept. That is why I felt being left behind). \n\nAnother example, we have meetings (almost daily, including on Sunday afternoon) and I attended each one of them but I did not speak for the entire duration because they do discussion in their own language. The only thing that I can do is open a Google Translate or try to listen for key words and also read the papers (which is written in English) shared on the screen.",
    "created_utc": "2024-10-18T19:12:09",
    "num_comments": 4,
    "comments": [
        "sounds like you need to transfer",
        "What do you work on or would like to work on? There's supposed to be a match in interests/skills between advisors and advisees.",
        "This is a no no.\n\n1. I have tested the water, I once need to attend the research group meeting and asked for leave permission from the course's teacher. The mentality here is that research is more important than coursework, so she let me. But that's not the main point, the main point is that when I attended the meeting, she contacted my supervisor to check. In other words, my supervisor is well connected with other teachers\n\nLong story short, after several discussions with seniors from different research groups, I concluded that the wall can talk. In other words, if I switch supervisor, I have to risk losing my supervisor face and face the consequence during thesis defense. Also, there is no guarantee that there is a supervisor who is willing to advise me. I have not found a supervisor that A) can speak English well B) and have interest in Computer Vision",
        ">What do you work on or would like to work on?\n\n1. I am working on medical imaging. Hopefully, once I do research on a full time basis, I can found a common problems and then publish papers solving it. So, hopefully, by doing so, I can get better chance to publish in SCI Zone 3 journals or higher\n2. Originally, I would like to work on defect detection, robust against glare. However, I am required to publish 3 papers in SCI Zone 3 journals, also my supervisor decided our research direction\n\nSo, this is my time table overview:\n\n1. the 1st semester: 11 courses (26 credits / 416 hours) + research on medical imaging and peer review\n2. the 2nd semester: 2 courses (5 credits / 80 hours) + research on medical imaging and peer review\n3. the 3rd semester until 6th semester (it's a 3 years Master degree): research on common problems and peer review\n\n>There's supposed to be a match in interests/skills between advisors and advisees.\n\nThe match in interest was because of I would like to work on Computer Vision and his research group is working on Computer Vision. That's why I contacted my supervisor, and then I got the acceptance letter"
    ]
},
{
    "submission_id": "1g6ug0r",
    "title": "Object detected, dynamic RoI updated as object moves?",
    "selftext": "still new to this\n\ni was wondering if a couple of things. \n\nhow common is RoI for cameras? Is it only for the machine vision (industrial?) cameras, or can all cameras offer it (for Raspberry Pi)?\n\nIs it possible to start the camera system with no RoI, detect an object (which will move about), place a RoI around the object, and update the RoI as the object moves from image-to-image? Is that done at the sensor-level such that the camera ONLY sends image data from within the RoI? So essentially if I had a high-resolution camera and wide FoV (so a large image size), I can vastly reduce the amount of image processing by only sending the RoI data to be processed?\n\nBecause I will have stereo 180deg FoV cameras, to be detecting and then tracking a single object which will move. The cameras and scene are stationary and the object is the only thing moving. I'm wanting to find ways to reduce my image processing requirements as I don't need the rest of the image, only the information of the object (where the object is in the scene).",
    "created_utc": "2024-10-18T15:27:03",
    "num_comments": 0,
    "comments": []
},
{
    "submission_id": "1g6so0t",
    "title": "I can't choose between a few different cameras for image processing.",
    "selftext": "Hello, i would like to explain what i will use the cameras i mention for first. I will attach the camera to a UAV and get the image from the camera to the Jetson on the UAV and do some image processing in python with opencv. The cameras i had in mind are:\n\nAllied Vision Alvium 1800 U-240c\n\nBasler ace2 a2A1920-160ucBAS\n\nBasler dart R daA1920-160uc\n\nI couldn't quite grasp the difference between the Basler ace2 and dart R series, also i have some C and CS mount lenses at hand so a CS mount camera would be better. I have seen that the dart R series don't have a framebuffer but i don't know how much of a difference that would make when capturing live feed from a camera.\n\nAny tips or help would be wonderful.",
    "created_utc": "2024-10-18T14:02:38",
    "num_comments": 1,
    "comments": [
        "The ace2 has a full metal housing and the dart is a board level camera. That's and the missing frame buffer are the biggest difference (if you choose a camera with the same sensor).\n\nThe missing frame buffer in a dart can lead to some missing frame. If your jetson have some hick ups and don't pick up the frame in time, they get deleted as there is no space to buffer them. On a camera with a frame buffer (ace2, dart m) the frame can be buffered in the frame buffer and delivered later to the jetson.\n\nI don't think that you get in trouble if you miss some single frame, so you should take a dart R camera. It's far lighter which is probably a big concern for a UAV."
    ]
},
{
    "submission_id": "1g6qxiv",
    "title": "where does the emsg atom go in a cmaf fmp4 fragment",
    "selftext": "when i put it top-level the file becomes unplayable.\n\ni tried putting it in my moof atom but that didn't seem to work either see below:\n\n    .. omitting heres ..\n    [moof] size=8+2352\n      [mfhd] size=12+4\n        sequence number = 17\n      [traf] size=8+2328\n        [tfhd] size=12+4, flags=20000\n          track ID = 1\n        [tfdt] size=12+8, version=1\n          base media decode time = 432000\n        [trun] size=12+2168, flags=701\n          sample count = 180\n          data offset = 2368\n        [emsg] size=8+104\n    [mdat] size=8+6269818\n\ni can't find any documentation on where this is actually supported to go so it will be able to be parsed by players like hls.js.\n\nthe schema type is ID3 but I am unsure how to format an ID3 message in the \\`message\\_data\\` part of this message either.\n\ncould anyone advise?  \n",
    "created_utc": "2024-10-18T12:46:01",
    "num_comments": 1,
    "comments": [
        "I found this document which answers my question, the emsg goes before the moof.\n\nI don't really understand why my emsg is resulting in my video not being playable but at least I know where it should go now.\n\n[https://www.uvcentral.com/files/CFFMediaFormat-2\\_1.pdf](https://www.uvcentral.com/files/CFFMediaFormat-2_1.pdf)\n\nI am seeing \"Overread mdat by 108\" in VLC, and my \\[emsg\\] size=8+100, so clearly something is still wrong but its closer."
    ]
},
{
    "submission_id": "1g6mt0j",
    "title": "Instensity Reduction on Obtured Dental CT",
    "selftext": "Hello all!\n\nI’m researching the segmentation and canal-type classification for the second mesial-buccal canal. My dataset consists of NIfTI files containing the teeth I want to classify. Some of these canals are obturated, causing the white to \"outshine\" the rest of the image.\n\nhttps://preview.redd.it/kdfe257injvd1.png?width=517&format=png&auto=webp&s=9ffd1427a993a87e50158f7e876b2df30793d455\n\nI tried applying contrast-limited adaptive histogram equalization (CLAHE) and a median filter, but the results showed no significant changes.\n\nAny help on this would be appreciated, thanks! ",
    "created_utc": "2024-10-18T09:48:58",
    "num_comments": 3,
    "comments": [
        "What is the question?",
        "What is the bitwidth of your dataset pixels?",
        "If you have control over imaging, try to reduce exposure. If you think reducing exposure could lead to information loss, you can explore exposure fusion, HDR.\nAlternatively, if you believe this in this imaging setup, clahe should work pretty well, maybe try to adjust gamma first. Otherwise, just cutoff the high pixel intensity and then normalize the image and check.",
        "Sorry, I didn't realize the post was vague. My bad!\n\nWhat I want know is how I could reduce the intensity of the whiter parts of the image, in this case, the obturated canals."
    ]
},
{
    "submission_id": "1g6jh9l",
    "title": "Announcing Rerun 0.19 - Dataframe and video support",
    "selftext": "",
    "created_utc": "2024-10-18T07:32:48",
    "num_comments": 2,
    "comments": [
        "from the blogpost this platform looks really impressive. we have been dealing with data storage, querying and synchronization problems like these for multiple camera, imu und lidar sensors and it's not an easy problem to solve! great to see this is open source.",
        "Awesome to hear it! Let us know if you have any trouble trying it out or getting started"
    ]
},
{
    "submission_id": "1g6in3f",
    "title": "How to avoid CPU-GPU transfer",
    "selftext": "When working with ROS2, my team and I have a hard time trying to improve the efficiency of our perception pipeline. The core issue is that we want to avoid unnecessary copy operations of the image data during preprocessing before the NN takes over detecting objects.\n\nIs there a tried and trusted way to design an image processing pipeline such that the data is directly transferred from the camera to GPU memory and that all subsequent operations avoid unnecessary copies especially to/from CPU memory?",
    "created_utc": "2024-10-18T06:55:28",
    "num_comments": 19,
    "comments": [
        "Are you using a Jetson with unified memory (integrated GPU), or a desktop with a discrete GPU? If the former, write your camera driver to put the image in mapped (zero-copy) memory and then hand the corresponding device pointer to your CUDA pipeline.\n\nYou could alternatively use DeepStream but that’ll be harder to integrate with ROS",
        "We do all gpu related stuff with DeepStream (actually [Savant](https://github.com/insight-platform/Savant)) and transfer via the topic bus only encoded (jpeg, h264, hevc) data. NVJPEG in hardware makes it “free”.",
        "technically deepstream is supposed to do this but I think it does actually do copies in some cases, even with unified memory. I think you actually do have to write it from hand to avoid additional copies.",
        "You have different solutions for uploading data to the GPU with minimal CPU usage. Nvidia calls it GPUdirect. There are several ways:\n- a video capture card supporting rdma, Nvidia has a list of partners. \n- Nvidia has also an Ethernet card called connectx that supports rdma for gigE cameras. \n- or do it yourself: https://docs.nvidia.com/cuda/gpudirect-rdma/",
        "I had to do that, since those kind of transfers are very time consuming in a video pipeline. The only true way to be sure achieving 0 copies is migrating all the processing-inference pipeline to Gpu using Cuda.",
        "I suppose you already knew about PBOs. Here just in case : \n\n# Benefits of Using PBO:\n\n* **Asynchronous Data Transfer**: PBOs enable non-blocking data transfers between CPU and GPU, preventing performance bottlenecks.\n* **Efficient Resource Management**: By decoupling data transfers from immediate rendering tasks, you can better manage GPU resources, especially in real-time applications where performance is critical.\n* **Double Buffering**: PBOs can be used in a double-buffering technique where one PBO is used to upload data while another is used to download data, ensuring continuous operation without interruptions.\n\n# PBO Target Types:\n\n* `GL_PIXEL_PACK_BUFFER`: Used for pixel read operations (e.g., `glReadPixels`).\n* `GL_PIXEL_UNPACK_BUFFER`: Used for pixel write operations (e.g., `glTexSubImage2D`).\n\n# Example Use Case in 3D Rendering:\n\nPBOs are frequently used in applications where large amounts of texture data need to be streamed to the GPU (e.g., video frames, image-based textures) or when reading back framebuffer data for post-processing effects or screen captures. By utilizing PBOs, you can avoid stalls and keep the CPU and GPU working in parallel.",
        "We’re using a Jetson and plan to integrate a Stereolabs Zed X with its GMSL capture card.",
        "This is probably a naive question, but how would one “get started” with this? Would really love to learn how to write a camera driver and reduce unnecessary copying for CUDA pipelines. Have written CUDA kernels and modified the device tree before, but only the basics of each.",
        "Savant sounds interesting. Do you think the USB/CSI cam source adapter will be compatible with a Zed X + GMSL capture card?",
        "Check DMs.\n\nA lot of these comments are shots in the wrong direction.\n\nEDIT: Just to help the community, Nvidia Isaac ROS is the correct tech stack to use for Jetson GPU based image processing in this case.\n\n https://nvidia-isaac-ros.github.io/repositories_and_packages/isaac_ros_image_pipeline/index.html",
        "For Jetsons or NVIDIA hardware, you can look into DeepStream. It's designed to have as little overhead as possible and to minimize unnecessary GPU-CPU movements.",
        "Any V4L2 stream should work.",
        "Yes, we actually have tried to use Nitros but with our previous PCIe capture card the camera driver did not support it so we had to write our own wrapper. I want to avoid that as much as possible going forward with the Stereolabs GMSL capture card, especially since the MIPI-CSI should enable lower latency DMA.\n\nMy current issue is that I don’t see Stereolabs supporting Nitros out of the box. Looking at other comments either a gstreamer / deepstream pipeline or a custom CUDA application seems to do the trick.",
        "Isaac ROS is built to support the ZED cameras. It was in 2.0, got taken out 3.0 due to Stereolabs not updating for Jetpack 6.\n\nHowever, they've built direct support for the Stereolabs cameras, specifically the ZED line in the latest release.\n\nWe never had any problems with writing custom drivers. As far as I know, Isaac ROS and the Stereolabs ROS2 driver are immediately compatible.",
        "I know that the Zed cameras integrate well with Isaac ROS. My issue is that afaik zero-copy requires Isaac Nitros and I can’t find anything related to that in the Stereolabs documentation so I’m trying to find alternative ways to achieve zero-copy.",
        "Well, at this point I think I've identified what current open source options are there and you're familiar with them.\n\nCan't suggest much more beyond it, so best of luck! If you discover a clever solution that gets you to zero-copy, please follow up if you find a solution!",
        "I think for now the plan is to use the Zed ROS2 wrapper with Isaac ROS until we find the time to experiment with deepstream to see if that will improve performance.",
        "Yep, the ROS2 wrapper with Isaac ROS got us pretty far doing similar work.\n\nBest of luck!"
    ]
},
{
    "submission_id": "1g6gw6l",
    "title": "Ethics in artificial intelligence and computer vision",
    "selftext": "I hope this [article](https://www.opencv.ai/blog/ethics-in-ai) is interesting to you. I'd like you to please read an article investigating AI bias and its hidden consequences, especially in computer vision. It offers some great insights into the ethical side of our work and how to avoid pitfalls we might not even realize we’re stepping into. Maybe it could be a good start to discuss it in this thread. ",
    "created_utc": "2024-10-18T05:30:26",
    "num_comments": 4,
    "comments": [
        "Thanks for sharing. This is a really important subject as AI tools become more prolific. One angle I feel the article misses is the potential ethical benefits of AI by  removal of human biases. For example, the day of the week can affect how you are sentenced by a human judge and not to mention the latent/active racial biases that may be present in those judges. Also, with the right model, the decision process can be more transparent than a human decision. Specifically you can determine which input components dominate the decision making. Nevertheless, the biases outlined in the article should be top of mind for all of us here who are creating products that can acutely affect people’s lives.",
        "Interesting article (though I'm biased since I've been an OpenCV user since the very beginning). I'm glad the article mentions the \"black box\" issue as it comes up a lot in my work. As a CV consultant, I've been surprised that my clients are all over the spectrum when it comes to explainability:\n\n1) Critical importance:  Obviously, anything involving health or safety, (e.g. medical diagnostics or autonomous vehicles). Having an expert human in the loop is a requirement, and will be for the foreseeable future.  \n  \n2) Moderate importance:  Applications where it's helpful to understand why the model behaves the way it does, but the sheer volume of content makes detailed explanations less practical (e.g. content moderation, OCR). Explainability could be useful for developers, but probably doesn't much impact the end user.  \n  \n3) Little importance: Anything where the aesthetic result is the end product (video games, midjourney-style art generation). It's \"good\" if it looks good.",
        "I think that's why thinking about the ethical issues of AI is important now — when we have the opportunity to make a change at the philosophical level, at the level of approach (and careful regulation, for example).",
        "Thank you, very good division. Apparently, the famous European AI Act has followed a similar split. We will see how effective this solution is."
    ]
},
{
    "submission_id": "1g6g4to",
    "title": "Multiple Single object detectors or Single Multi object detector?",
    "selftext": "Me and some university group mates have just begun working on a project that revolves around the tracking of surgical tools in laparoscopic surgery videos. However, when researching the state-of-the-art trackers used, we started wondering what type of tracker would apply to our case: Single or multi object trackers? \n\nMany definitions of multi object trackers seem to be something along the line of \"Multiple object tracking (MOT), aims to estimate trajectories of multiple target objects in a video sequence\", which does fit our case as we want to track multiple tools at the same time. However, most use cases of MOT seems to be tracking pedestrians, fish or other objects that are very similar looking. \n\nWe're curious if it would be more beneficial to use multiple single object trackers, as each tool we want to track is 'unique' in the sense that there will never be more than one scalpel, grasper, forceps, etc in the frame (And these will all look very distinct from each other).\n\nTLDR: Is MOT the best solution for tracking multiple objects of 'different' classes, or would instantiating multiple single object trackers be better for this? ",
    "created_utc": "2024-10-18T04:48:54",
    "num_comments": 3,
    "comments": [
        "MOT is a problem, not a solution. Several methods just do object detection per frame and then just link the various detections to form trajectories in time. It makes a lot of sense for pedestrians, because as you noted, they're similar, numerous, occlude each other, etc... So your question can be reduced to \"single multiple-objects detector\" or 'multiple single-objects detector\".\n\nThe answer is always usecase dependent! so you'd ideally need to compare the two approaches. But, start with the multiple class detector, because this one is already functional for your problem! Whereas the other approach involves N times more training and N times more computations at test time. Backbones can be heavily re-used, it's economical to do multiple classes.",
        "For a situation like identifying instruments, I would go with multiple single object detector so you can identify exactly what type your are seeing",
        "Okay thanks for the clarification!  \nWe are considering doing as you've suggested with comparing the two approaches. This will probably also give us the best understanding of the advantages of each approach"
    ]
},
{
    "submission_id": "1g6fz1k",
    "title": "Seeking guidance on Professional Development Workflow a Python Deep Learning GUI",
    "selftext": "Hi everyone, I am a working student in Germany and I've been assigned a solo project by my company, but I haven't received much guidance from my supervisor or a clear professional workflow to follow. I'm currently a second-year student in an AI Bachelor program.\n\nProject Overview: The project involves developing a Python GUI that enables users to perform an end-to-end deep learning workflow. The functionality includes: Annotating, augmenting, and preprocessing images; Creating deep learning models using custom configurations. The goal is to make this process code-free for the users. From the beginning, I was tasked with building both the backend (handling images and training DL models) and the frontend (user interface).\n\nProject Nature: I believe my project lies at the intersection of software engineering (70%) and deep learning (30%). My supervisor, a data scientist focused on deep learning research, doesn't provide much guidance on coding workflows. I also asked my colleagues, but they are developing C++ machine vision applications or researching machine algorithms. So they aren't familiar with this project. There's no pressing deadline, but I feel somewhat lost and need a professional roadmap.\n\nMy Approach and Challenges: I've been working on this for a few months and faced several challenges:\n+ Research Phase: I started by researching how to apply augmentations, use deep learning frameworks for different use cases, and build user interfaces.\n+ Technology Choices: I chose PyQt for the frontend and PyTorch for the backend.\n+ Initial Development: I initially tried to develop the frontend and backend simultaneously. This approach led to unstructured code management, and I ended up just fixing errors.\n\nInspiration and New Direction: Recently, I discovered that the Halcon deep learning tools have a similar application, but they use C++ and it's not open-source. Observing their data structure and interface gave me some insights. I realized that I should focus on building a robust backend first and then design the frontend based on that.\n\nCurrent Status and Concerns: I am currently in the phase of trial and error, often unsure if I'm on the right path. I constantly think about the overall architecture and workflow. I just realized that if I am given a task in a company, so it's straightforward. But if am given a solo project, it's kind of hard to define everything. \n\nI am seeking advice from professionals and senior engineers with experience in this field. Could you recommend a suitable workflow for developing this GUI, considering both software engineering and deep learning aspects?\n\nAnyways, I still want to do my best to complete this project.\n\nThank you all for your help!",
    "created_utc": "2024-10-18T04:39:40",
    "num_comments": 0,
    "comments": []
},
{
    "submission_id": "1g6fxju",
    "title": "Car Plate Detect: Yolo8 + Deep_Sort + PaddleOCR = It doesn't work satisfactorily",
    "selftext": "Hi everyone,\n\nI'm trying to develop a personal project that consists of receiving real-time video using RTSP from a camera installed in a very busy location. I need to perform OCR on each vehicle that passes by this camera.\n\nI'm using a Yolo8 model that I trained to recognize the license plate on vehicles and it's working relatively well, recognizing it in most of the frames.\n\nProblem: For the same vehicle, depending on the frame analyzed, the OCR (paddleOCR) sometimes makes an inaccurate reading, generating different license plates. The same vehicle appears in several frames until it disappears for good.\n\nI tried to use deep\\_sort to track these vehicles with the intention of recording all the recognized license plates for each track\\_id and, at the end, checking which license plate is most likely using the score and number of times it appeared, something like that. The problem is that deep\\_sort has not been working as expected, sometimes assigning the same track\\_id to the car that is right behind the vehicle in front and sometimes even changing the track\\_id of the vehicle in subsequent frames. In other words, the same vehicle can have 3 track\\_ids, depending on how fast it is going in the video and in how many frames it appears.\n\nI have been looping and reading frame by frame, sending it to YOLO only when motion is detected to improve performance and then I track it with deep\\_sort.\n\nDoes anyone have any suggestions for an approach that I can try?\n\nps: I have tried a huge variety of different parameters in deep\\_sort.",
    "created_utc": "2024-10-18T04:37:18",
    "num_comments": 3,
    "comments": [
        "Why not use ~~bytesort~~ ByteTrack that comes with `ultralytics` since you're already using YOLOv8?",
        "You can try the SOTA model trained on a dataset that best matches your dataset. \n[https://paperswithcode.com/task/license-plate-recognition](https://paperswithcode.com/task/license-plate-recognition)",
        "Can you share a video of your current progress/issues?"
    ]
},
{
    "submission_id": "1g6fu0v",
    "title": "GPU for Real-time object detection.",
    "selftext": "I'm new to CV and I want to do a simple project where a 6 2mp CCTV cameras detects people and seat occupancy in the library. What GPU would you recommend for this kind of setup?\n\nLike this: [Library seat detection: tabletop implementation (video1) (youtube.com)](https://www.youtube.com/watch?v=CuB9HgXosaA)",
    "created_utc": "2024-10-18T04:31:30",
    "num_comments": 2,
    "comments": [
        "First, sorry I can't answer your question directly, my use case doesn't analyze live video like that, so I'm not sure what the necessary hardware looks like. That said, here's a few things to think about.\n\nI'm not sure exactly what your goals are or what you want the end product to look like, but I'm going to suggest that you don't need live overlays at 30 frames per second. It makes for a cool demo, but polling information that often isn't really helpful in your situation. You could poll the seat occupancy every 1 second and still effectively have real-time info, but you're analyzing 174 less frames:\n    \n    Total frames = 30fps*6cams = 180 - 6 frames actually analyzed = 174\n    Or, 96.7% less frames\n    ...if my math is right.\n\nEven 1 second is kind of overkill, but you probably don't even need a GPU for this. To give you an idea, I'm running a 22 camera setup that, when triggered, analyzes 34 images at 1/3rd of a second intervals. It's using a Quadro p400 v2, a sub-$100 workstation GPU that draws all power from the PCI slot. It's not often that every camera is triggered at the same time, but if they are, the system has no problem with it (as long as it's not continuous). You can further reduce the load if the cameras have a substream that runs at like 480p. YOLO models are typically trained on 640 max dimension images, unless you're trying to detect seats really far away, it probably doesn't do any good to analyze full size images."
    ]
},
{
    "submission_id": "1g6ds0a",
    "title": "Is it possible to detect if a product is taken or not just based on vision similar to the video below, without any use of other sensors like weight etc? I know we can use Yolo models for detection but how to classify if the person has purchased the item or placed it back just based on vision.",
    "selftext": "",
    "created_utc": "2024-10-18T02:09:42",
    "num_comments": 22,
    "comments": [
        "this is an extremely hard problem. there’s just no easy or straightforward way to do this, and definitely not with just plain ole object detection.",
        "I know you said not using other sensors but rfid",
        "Weight detection?\n\nThe owner has to go through a calibration routine while filling the vending machine:\n\nStep 1: put one of each product in the vending machine.\n\nStep 2: machine logs weight of each product\n\nStep 3: put all products inside\n\nStep 4: perhaps confirm the right amount for each product?",
        "YOLO alone, no no.\nThere are decent works in VLM that combine vision with context understanding very well. But even those are not reliable enough to work alone.",
        "Sure, but do you want it to be consistent and reliable? If yes then probably no. ",
        "Is a really hard problem depending on how broad you want the solution to be. Consider object detection. Often a single product can change packaging temporarily for a promotion or permanently with packaging upgrades. You would have to handle all the models and retraining required to handle those cases and that is just one product. As for checking what has been taken, you could try a monocular depth model but again you’ll probably have to parametrize the output based on the dimensions of the product and that won’t necessarily generalize to other products or even the distance of the camera to the products. Just to understand how hard the problem is, Amazon spent billions on the project and so far has failed to create a generalized solution for even the limited product selection they cover, and considering their complete control of the store layout and stocking procedures. Might just be easier to use a cheap weight sensor and use time series to understand stocking and removal behavior.",
        "Yea thats the obvious way lol.",
        "That is what I thought also rfids but I want a solution with vision only.",
        "how do you think this was achieved in the video",
        "Was amazon completely vision based. You think the video is faking the demo a d might fail in a complex real world scenario. There are a lot of companies with successful products like the video. Then a sensor fusion is as close it gets?",
        "https://www.livello.com/technologies\n\nAccording to their website it seems to be a combination of RFID, scales and computer vision",
        "probably multiple sensors and a lot of practice doing the right thing to get the predictions you want for a demo video decent enough to have a chance at swindling investors.\n\nso basically it’s almost certainly smoke and mirrors that will fall apart under any sort of complicated real world scenario.\n\nthis is a perfect example of “no human intelligence until we figure out cat intelligence”.\n\nAI can barely tell a detected object is the same over multiple frames. cats seem to do just fine with that problem. so yeah zero chance you’re getting this to work without far more advanced CV, which we just don’t have today.",
        "Amazon wasn't doing this with automated computer vision, but with human labelers instead. [They also gave up on it ](https://www.reddit.com/r/Cyberpunk/comments/1bud4o1/amazon_ditches_just_walk_out_checkouts_which/?utm_source=share&utm_medium=web3x&utm_name=web3xcss&utm_term=1&utm_content=share_button)",
        "I think for this kind of single fridge, hyper controlled environment, with relatively few products it could be done with just vision but scaling something like that up is tricky for all the reasons I discussed. Also notice they card in to the system so that also handles security issues. Probably charges anything that changes in the fridge to your account. I don’t see how this could be cheaper than a vending machine or what extra value it brings. As soon as you don’t control the product offerings/display/restocking, complexity explodes.\n\nEdit: forgot to answer the questions. Not sure what stack or pipeline was used by Amazon  but I know vision was a part of it. I think they might be on to something with their dash carts though.",
        "So you think the video is using multiple sensors and is not just based on camera correct?",
        "Weren't they using that data to train the model to tune performance. I might be wrong but the reason for them giving up was too much operating cost  for servers and gpus not necessarily computer vision implementation.",
        "I can’t speak to it, I don’t know, I’m just saying today’s CV with DL can get you a decent looking demo with cameras…. but a system so laughably broken it will never work in practice\n\nfor now, of course, barring advancements - with could conceivably include assembling an absolutely gigantic dataset of interactions like this to train against - that would be an advancement by itself",
        "From the article:\n\n>Out relied on more than [1,000 people in India watching and labeling videos](https://www.theinformation.com/articles/how-amazons-big-bet-on-just-walk-out-stumbled?rc=5xvgzc) to ensure accurate checkouts. The cashiers were simply moved off-site, and they watched you as you shopped.\n\nNot sure if that explicitly means they were doing the \"checkouts\" or if they were labeling data for the models.",
        "Are you a ultralytics maintainers or a big contributor?",
        "I'm a member of the community team for Ultralytics and have done some code contributions in the repo.",
        "Can I dm?",
        "Sure, but if it's an Ultralytics code/technical related question, please post in r/Ultralytics as it would be great to have it public for others as well."
    ]
},
{
    "submission_id": "1g69yfn",
    "title": "Live555 Documentation ",
    "selftext": "I am working on implementing a video grabbing pipeline using live555. But I am unable to find documentation on its functions and APIs. Has anyone worked on it?",
    "created_utc": "2024-10-17T21:27:58",
    "num_comments": 2,
    "comments": [
        "What are you trying to do with it? There might be easier routes with ffmpeg.",
        "You've been to the web site, right? Lots of documentation there, and the developer seriously documents the code, so try reading it. Live555 is an essential companion to ffmpeg when doing live streams."
    ]
},
{
    "submission_id": "1g65tcz",
    "title": "Traffic Light Detection Using RetinaNet and PyTorch",
    "selftext": "Traffic Light Detection Using RetinaNet and PyTorch\n\n[https://debuggercafe.com/traffic-light-detection-using-retinanet/](https://debuggercafe.com/traffic-light-detection-using-retinanet/)\n\nTraffic light detection is a complex problem to solve, even with deep learning. The objects, traffic lights, in this case, are small. Further, there are many factors that affect the detection process of a deep learning model. A proper training process, of course, is going to help to detect the model in even complex environments. In this article, we will try our best to train a **traffic light detection model using RetinaNet** and PyTorch.\n\nhttps://preview.redd.it/ztbjm20fuevd1.png?width=1000&format=png&auto=webp&s=982d693760d3e4ff2ba2b30b8edea74505ff4d85\n\n",
    "created_utc": "2024-10-17T17:36:22",
    "num_comments": 3,
    "comments": [
        "This is cool",
        "Thank you."
    ]
},
{
    "submission_id": "1g65p0v",
    "title": "How to correctly detect athe plane of reflective floor using a stereo camera? ",
    "selftext": "I'm trying to detect the floor plane of a reflective floor, using a stereo camera. There are a lot of light reflections that cause the 3D point cloud of the floor, generated by the camera, to be distorted. How would you go about with estimating where the ground plane is?",
    "created_utc": "2024-10-17T17:29:55",
    "num_comments": 4,
    "comments": [
        "Depth anything v2 is known to be good for reflective surfaces but it’s a mono depth method \n\nDepth Anything V2 Monocular Depth Estimation (Explanation and Real Time Demo)\nhttps://youtu.be/fs0jZx9o9rc",
        "Inter-reflection cause issues. You can solve using physics, and add polarization filters to the cameras, think sunglasses, to remove the glare. Or you can use a machine learning approach like the recent monocular depth estimation techniques that can cope with such scenes.",
        "Have you seen Apple's [Depth Pro](https://github.com/apple/ml-depth-pro) that dropped recently? It does single-shot monocular *metric* depth estimation, generating a 2.25MP depth map in 0.3 seconds. There's a [demo on huggingface](https://huggingface.co/spaces/akhaliq/depth-pro).",
        "Yes I did some in depth testing \n\nDepth Pro Apple: How Accurate is the Depth & Focal Length Estimation?\nhttps://youtu.be/no5yJje_mos"
    ]
},
{
    "submission_id": "1g65dvx",
    "title": "Help: Given FOV, sensor width and height, focal length and camera position, how can I draw a bounding box around area that the camera covers?",
    "selftext": "I am new to computer vision/OpenCV, so if I'm asking a stupid question, let me know.\n\nI have a top-down image of the area in question and know the position and specs of the camera. I want to take specs of the camera and extract/draw a bounding box for only that area of the top-down image. How can I go about doing that?",
    "created_utc": "2024-10-17T17:13:30",
    "num_comments": 3,
    "comments": [
        "This explains it pretty well in picture format around page 10.  \n\nImagine a pair of similar trisngles with the points coming together at the focal point inside the lens. The smaller triangle has its base being the sensor, and the larger triangle has its base as the surface being photographed. \n\nhttps://www.cse.psu.edu/~rtc12/CSE486/lecture12.pdf",
        "hmmm, if I get a (H,W) image, then obviously the bounding box of my view frustrum, in the image, is (H,W).\n\nNow, in 3D: each corner (0,0), (W-1,0), (0,H-1), (W-1,H-1) can be un projected with: M = K\\^{-1}\\*\\[u;v;1\\] where K is the projection matrix: K = \\[\\[ foc, 0, u0\\], \\[0, foc, v0\\], \\[0, 0, 1\\] \\]. (u0,v0) is the principal point, somewhere around (W/2,H/2), foc is the pixel focal, foc = (W/2) / Tan(fov/2) where fov is the horizontal field of view.\n\nThe 4 3D points you get are a rectangle that is right on the frustrum in the camera reference frame. They are not on the ground though. If the camera is eaxctly top-down at a height of Z, then just multiply their 3D coordinates by Z, and you'll get the extents of the camera frustrum on the ground. If the camera is not exactly top-down, then it's slightly more complicated, but not that much: you need to work out a depth per image corner. You can compute that depth from the camera position and the ground plane's orientation."
    ]
},
{
    "submission_id": "1g626ql",
    "title": "Cameras with high resolution and compatible with opencv?",
    "selftext": "Hi! I'm working on a project that involves some video from far away, and was wondering if anyone knows of high res cameras that can plug in to opencv? Ideally 2K/4K",
    "created_utc": "2024-10-17T14:39:27",
    "num_comments": 7,
    "comments": [
        "You don’t want high res, you need a good lens ",
        "Arducam even has 64MP options.",
        "Anything like one of these would work from a long ways off\nhttps://linovision.com/collections/zoom-camera-module",
        "> from far away\n\nDo you really want to use entire frame for processing? Maybe what you need is zooming lens system, if you have high resolution but most of them are not used it just slow the process down.\n\nAlso invest on good lighting if you can.",
        "make sure you understand not just the resolution but the pixel quality and noise levels they will introduce. Fewer high quality pixels are often better than a lot of noisy ones",
        "Zoom or non fixed focus cameras can be problematic if you want to calibrate them accurately, unless you can just set them once, lock and forget.\n\nWhat you need is a normal machine vision camera that takes common interchangeable lenses like CS or M12 and a longer focal length lens of the correct FOV (determined by focal length & sensor size)",
        "Absolutely right! People do not really understand how cameras work :)"
    ]
},
{
    "submission_id": "1g61hoq",
    "title": "Medical Image Classification",
    "selftext": "I have what I think is a relatively straightforward image classification task that I am looking to hire for. The goal is to develop a web based tool that a user could upload an image to and it would show the user the top 3 most similar images in an existing labeled database. The images are taken with a USB camera. The database is a proprietary database that I have generated and labelled into 5 categories and includes roughly 3000 labeled images. Other than labeling the overall image there is no other associated information used in the classification. I am a physician, and do not know anything about coding, though I have built a similar tool with a different company about 5 years ago. \n\nI am close to signing a contract with a company that will develop this tool for me. My questions are: \n\nAre there specific classification strategies I should be ensuring that the company uses to develop the tool?\n\nHow long would such a tool take to develop?\n\nThe initial quote is $12-15,000 USD to develop for the tool. The cost to integrate it into my website is a separate fee. \n\nI'd love some input. Thanks!",
    "created_utc": "2024-10-17T14:08:22",
    "num_comments": 5,
    "comments": [
        "After some quick searching, I found this blog post for you: https://huggingface.co/blog/not-lain/image-retriever \n\nThe blog develops a simple image search engine for images of Pokémon. It comes with the code as a notebook in Colab, as well as a demo web app on HuggingFace. \n\nI think this should be simple to adapt for your medical image dataset. You might be able to find some graduate student at a local university or some freelancer online to do this cheaper and quicker.",
        "You don't need classification. What you need is encoding. A simple version would be to take an autoencoder which is shaped like UNet,(which was originally developed by physicians!), up to and including the bottleneck. \n\nYou then train the model to recreate its inputs, (this is tricky for the model because the bottleneck requires it to hold a lot of info in a much smaller space.), you then take off the entire deconvolution portion and only take the bottleneck as output. \n\nAt this point, you pass your dataset into the model and save the encodings into your database. Now when you get a new image, you run it through the model, and you can judge similarity from the distance of the encoded values. This was basically how reverse Google image search worked.\n\nNowadays you could use vision transformers for embeddings, but it's basically the same.",
        "Cost sounds pretty high. This is something a lot of college students could do for pizza money. \n\nIf I was more ambitious I’d bid $3000 to develop the computer vision portion and deliver something that a web designer could easily integrate. That would cover multiple iterations to get you better results than just out of the box. ",
        "Messaged you!",
        "This is the way. Basically it converts each of the 3000 images into a simple numeric “description” where that description is “tuned” to correlate with your labels. Then the same algorithm is applied to the query image, and a metric like cosine similarity is used to find the most similar descriptions from the 3000 database.\n\nThis can all happen in a fraction of a second…3000 is a very small dataset for such retrieval. The most challenging part will probably be tuning the algorithm on such a small dataset. "
    ]
},
{
    "submission_id": "1g5yim5",
    "title": "Monocular depth estimation of video?",
    "selftext": "Hey,\n\nI just saw that Apple announced their new Depth Pro model, which got me excited because i've wanted to be able to get accurate depth estimation from my regular camera, but all the other similar models requires CUDA, which doesn't work on mac as far as i can tell. I'm just wondering if there is an easy way to use this or another mac compatible algorithm on video? It doesn't need to work in real time or anything, it's for a video art project.",
    "created_utc": "2024-10-17T12:00:04",
    "num_comments": 3,
    "comments": [
        "Aren’t you using mps on a Mac with a metal processor?  Instead of device=‘cuda’ use device=‘mps’.",
        "You can literally run it on cpu did you try downloading and running the code?",
        "Have you tried collab? That way you’re not limited to your hardware"
    ]
},
{
    "submission_id": "1g5wqtj",
    "title": "Simple javascript code that will protect American civilians from United States Military drone strikes ",
    "selftext": "",
    "created_utc": "2024-10-17T10:44:05",
    "num_comments": 2,
    "comments": [
        ">A major programming breakthrough in object recognition called Secondary Detection. Now civilians can evade deadly drone strikes with their android phones. If you don't know what secondary detection is, just read the article\n\n>[...]Basic detection (white bounding box) is basic detection of an object using your device's webcam. [...] The secondary detection results are color calibrated, meaning that using different filters used to change the appearance of the frame/video will affect the secondary detection output.\n\n\nHow is this a \"major programming breakthrough\", exactly?",
        "Well that was interesting."
    ]
},
{
    "submission_id": "1g5vr2d",
    "title": "Tracking, person detection and pain points",
    "selftext": "Hello everyone, \n\nI have a side project about video blurring. \nI have several videos of myself in public, the goal is to create something that would blur every person within the video, except the selected person. \nCurrently coding in python, on windows OS, the end goal will be to have a .exe application, no installation required. \n\nHere is the workflow so far when running my code : \nUser select a video, click on the person of interest, each frame of the video is extracted and saved in a temp folder, the folder path and the point coordinates (x,y) are given to SAM2.\nSAM2 segment the selected person and for each frame, store the mask coordinates, the bbox (x1, y1, x2, y2) coordinates.\n\nOnce SAM2 finished, I loop over the whole frames dictionnary and for each frame, call YOLOv8 to detect all persons within the frame. \nThen, I compute IOU between the stored tracked bbox (given by SAM2) and every YOLO detected person bbox, I keep the highest IOU of all computed ones for that specific frame and if the IOU > 0.8 I do nothing, otherwise I blur the bbox using simple gaussian blur. \n\n\nThis works pretty well but I have some pain points that I’m willing to solve and help would be appreciated. \n\n1 - SAM2 is not designed for tracking but works very well, what other models with very good performances and perhaps less computational expensive could I use ? \n->I already tested OpenCV trackers and they do not perform well. \n\n2 - Is there a better model for person detection than YOLOv8, it sometimes fails to detect a person or have trouble giving the whole bbox when there is object in first plan. \nImagine a lamp in front of the person, YOLO will only give me the half body of the person and stop almost where the lamp begins. It cause issue because SAM2 return the whole body BBOX, so the resulting IOU is too low to pass my arbitrary treshold of 0.8\n\n3 - How could I accelerate the frame extraction process ? For long videos (more than 30minutes), that process can be very very long. \n\n4 - The goal would be to have a first version that will performs very well, quickly and can be runned on a 4070. And a second smaller version, that could performs OK, only on CPU, no matters if process take 12hours for 1hour video. \n\n5 - Keeping the same architecture, how could I accelerate the whole process ? \nFor now, I already use async_loading_frames method for SAM2, instead of loading in memory all frames. \n\n\nThanks a lot ! \n",
    "created_utc": "2024-10-17T10:01:58",
    "num_comments": 4,
    "comments": [
        "Cotracker3 from meta ai is a pretty good option for tracking. It just came out and great for occlusion\n\nCoTracker3 Meta AI: ROBUST Tracking Under Occlusion and Scaling\nhttps://youtu.be/9PKmkbyrRFw",
        "I’ll give it a try, thanks!",
        "How is it going?",
        "I gave it a try, cotracker is clearly faster than SAM2 but I found it not that good for tracking people when there are overlaps, it simply lost track."
    ]
},
{
    "submission_id": "1g5t4m5",
    "title": "Why can't Yolo segment anything in this picture? I thought the circles would be easy to detect but it has no detections.",
    "selftext": "https://preview.redd.it/7hkyy49z0cvd1.jpg?width=2048&format=pjpg&auto=webp&s=35ed8eb9863ade3dbc8814d873279f801e81f68a\n\n",
    "created_utc": "2024-10-17T08:09:30",
    "num_comments": 17,
    "comments": [
        "Did you fine tune the model to do this very task? One of the things I learned in ML is if the task does not involve real world objects, fine-tune it with the task-specific dataset.",
        "Not sure what you’re using. Most YOLO models are by default only trained on the COCO dataset which teaches them to detect stuff like dogs and toothbrushes and cars. \n\nAlso most YOLO models only provide bounding boxes; they don’t segment. \n\nWhich YOLO are you using and did you train it to detect circles drawn on paper?",
        "Try hough circles. You can tune it to find different circle sizes \n\nhttps://youtu.be/VfDR-8OSExk?si=_Uqx7_JRLvrQRYj8",
        "Firstly, it is detecting anything in general?\n\nIs there a class for the circles in the training data?\n\nYou can also look into Hough circles?",
        "Also the model called Segment Anything can probably this. ",
        "Others already helped you, but for something like this you might want to look at Meta's SAM/SAM2 models for segmentation. They're very impressive. ",
        "I used Yolov8m by the way. I feel like this should be really easy but I don't know why it isn't working.",
        "You will surely need to teach your model on a custom dataset, and Yolo will work perfectly.\n\nRecently I did this for counting hairs from microimages",
        "It wouldn't do that unless you have trained it to. The pretrained models are trained to detect circles.",
        "I already use hough circles, but they're not good enough since there are sometimes overlapping bubbles and irregularly shaped stuff. I'm thinking of going with morphological transform.",
        "Thanks SAM works great! Actually it's so funny I found a paper on SAM fine-tuned specifically for these bubble pictures that was published just this year.",
        "[deleted]",
        "Why do you feel that way? If you know how the pretrained Yolov8m was trained, then you should have a pretty good idea of whether it will be able to detect these circles or not.",
        "Ok makkes sense",
        "Yeah, this post reminded me of exactly that.  Glad to help.",
        "Floodfill!",
        "Lol yeah Idk if there's some pre-existing algorithm that can connect lines into some bounding box to find the centroid of each automatically. That would bring it from CV to CS I feel."
    ]
},
{
    "submission_id": "1g5t3il",
    "title": "Vision transformer model ",
    "selftext": "Hi,\nI was using mvitlarge_v2 for a multi-class image classification and my current accuracy is 94% \n(On val), Resnet and CNN's were giving somewhere around 85% What can be done to improve this accuracy \nAny unique pre processing step or tweaking the architecture  you have in mind which I should try ? help would be appreciated ",
    "created_utc": "2024-10-17T08:08:08",
    "num_comments": 10,
    "comments": [
        "It's gonna be hard to help if all we know is you're comparing a VIT and a CNN... No info about classes, image sizes, distributions...\n\nHow can you expect us to give meaningful advice if we don't have at least the liminal specs for a CV project? It's like clients who expect you to reach 99% \"good predictions\" on something they haven't even explained yet lol",
        "You want to improve the VIT performance or the CNN’s?",
        "what does your augmentation story look like? maybe focus on that a bit, you can also try oversampling for your underrepresented classes",
        "Ik it sounds dumb of me to ask without providing any information..maybe you can look at the other comment I have given some info",
        "Vit \nTho if anything else which gives better results than 94% I'll try it",
        "    transform_train = transforms.Compose([\n        transforms.Resize((224, 224)),\n        transforms.RandomHorizontalFlip(),\n        transforms.RandomVerticalFlip(),   \n        transforms.RandomRotation(15),\n        transforms.ColorJitter(brightness=0.2, contrast=0.2, saturation=0.2),  \n        transforms.ToTensor(),\n        transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]) \n    ])\n    transform_val = transforms.Compose([\n        transforms.Resize((224, 224)),\n        transforms.ColorJitter(brightness=0.2, contrast=0.2, saturation=0.2),  \n        transforms.ToTensor(),\n        transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]) \n    ])\n    \n    \n    currently this",
        "It’s all dependent on the task and available data. Can you describe what you’re working on? \n\nSomething I almost always do is run data through multiple models in an ensemble and vote on the results. I also look for inherent augmentations in the data, like if I’m classifying cars in videos, I’ll track each car and then classify it across all available frames, then take the average. ",
        "Well I really can't tell a lot about the dataset because of some restrictions maybe I can provide you some specifics in the dms but it's a multiclass classification of RGB medical images \ndata I have are in the form of sequential video frames 40k+training and 20k val 5k test  (test set is not in sequence )",
        "Ok so sequential is a good clue! Try consistency loss to self-supervise the training if your existing labels might not be perfect. \n\nAre the classes balanced and does your 94% metric account for that?",
        "no the classes are not balanced I have applied random weighted sampling for that"
    ]
},
{
    "submission_id": "1g5ona9",
    "title": "Approximate Object Size from Image without a Reference Object",
    "selftext": "Hey, a game developer here with a few years of experience. I'm a big noob when it comes to computer vision stuff.\n\nI'm building a pipeline for a huge number of 3D Models. I need to create a script which would scale these 3D Models to an approximately `realistic` size. I've created a script in blender that generates previews of all the 3D Models regardless of their scale by adjusting their scale according to their bounding box such that it fits inside the camera. But that's not necessarily what I need for making their scale 'realistic'\n\nMy initial thought is to make a small manual annotation tool with a reference object like a human for scale and then annotate a couple thousand 3D models. Then I can probably train an ML model on that dataset of images of 3D models and their dimensions (after manual scaling) which would then approximate the dimensions of new 3D models on inference and then I can just find the scale factor by `scale_factor = approximated_dimensions_from_ml_model / actual_3d_model_dimensions`\n\nDo share your thoughts. Any theoretical help would be much appreciated. Have a nice day :)",
    "created_utc": "2024-10-17T04:27:02",
    "num_comments": 7,
    "comments": [
        "Say the reference humans you use are from 1.6 to 2.0m tall, it doesn't sound far fetched that your annotation set helps someone figure out dwarves' height distribution if they are on the dataset and you have a model to recognize dwarves.\n\nWhat if you don't? that means what if you're trying to figure out the scale of an object type that wasn't in the dataset? there's really no way to generalize. That suggests that the dataset, in order to be useful should include a way to recognize all types of objects. Second, humans aren't all the same size, so such a dataset could only help identifying the relative size distributions!\n\nIt feels to me that listing the size distributions in real units for many classes is less work than building a dataset and a model that will guess sizes. It sounds undoable to list all objects types, but it is even more undoable to have a model that generalizes to real physical sizes of unknown object types without references.",
        "Do you know of any dataset that has object dimensions? If it does exist, I can probably caption the previews of the 3D models using blip or smth similar and then make a relation along the lines of 3d model -> preview -> object name -> object size",
        "If we take image there , how can we identify the size . As you say taking human as refference from 1.6-2m similarly we take few more objects but in an image a person might be standing few metres behind a bike then how can we scale it with one example depth can not be found",
        "I don't. But you can try and find \"*metric* depth\" datasets, that is depth estimation datasets for which the absolute (not relative) depth is provided in physical units. Those datasets contain typical objects like furniture, humans, cars... so if they also have object detection annotation, you'll be able to get the physical scale of objects from object detection data + metric depth. If they don't, you could run some object detector as a proxy.\n\nFor instance, Mapillary has a metric depth dataset: [https://www.mapillary.com/dataset/depth](https://www.mapillary.com/dataset/depth)",
        "in what I described above, I'm really only detecting it's a bike (or a human), which works at \"any\" distance, and then sampling a physical size for bikes (or humans). Because this is a 3D model, then we can scale it relative to its own 3D extents, we don't rely on its on-screen size.\n\nAlso because this is a 3D model, then its depth is also already known by the 3D engine I'm using to display it.",
        "I thought 2d picture and images 👍"
    ]
},
{
    "submission_id": "1g5nadv",
    "title": "Adding new classes to pretrained YOLOv7 with new data without affecting existing classes' accuracy",
    "selftext": "",
    "created_utc": "2024-10-17T02:57:22",
    "num_comments": 1,
    "comments": [
        "Not really sure this is possible. \n\nYour best bet is probably adding a second head and leaving the rest of the model alone. "
    ]
},
{
    "submission_id": "1g5j8fq",
    "title": "Ideas for project",
    "selftext": "Guys i need ideas for my final year project. The niche is AI. Anything related to Generative AI or Computer Vision or Machine learning etc. Can have implementation of Rag etc. im open to ideas. Please suggest something.",
    "created_utc": "2024-10-16T22:00:12",
    "num_comments": 25,
    "comments": [
        "automatic PSA grading of Pokemon cards.",
        "Live Streaming volumetric area into live 3d models",
        "Using computer vision to identify and sort recyclable items out of bulk trash. Eg: E-waste, plastics, glass. Bonus points for using the same and quick read / error correction on barcodes for sorting recycling further into sub categories. Eg: different sub of plastic based on the bottle. Almost everything has a barcode, easier than manual sort and gives a reference for comparison in a computer vision model.",
        "Take image of a cloth and image of a person. Drape that peice of clothing on the person. This has huge usecase in malls and clothing stores.",
        "Chess figures real time detection and tracking application",
        "Might be hard, but try physics based cloth simulation using deep learning",
        "Given a video clip of someone walking, make a system that will change what they are wearing or make them carry something they were not in the original video. This would require human body tracking, full body pose recovery, integration with one of the open source virtual clothing try on repos, and then all the nonsense of ironing out the integration details, creating a useful UI, and (of course) making the thing run fast enough that you don't need some monster GPU and hours to see a result.",
        "Find a pieces of clear plastic in ground meat.",
        "Gaussian Splatting or NeRF related ig (image/video to 3D scene).",
        "image to animation   \nllive image to animation ,v2v   \nthen build a short form youtube system .   \nI'm currently working on this idea .",
        "1) Using computer vision for the blind\n2) Packaging suggestor by scanning the product for wrapping/packaging industry.\n3) Helping solve rubik's cube by tracking user movement.",
        "Photo editing to remove a person with a prompt. ",
        "Medical image analysis, you can work on detection, segmentation or classification tasks. There are tons of public datasets available!",
        "Inviting you to r/Rag",
        "Real time is a trend right now with avatar creation",
        "3d medical scans to 3d image and then pass that to a model for classification etc",
        "House prices prediction 😎",
        "As if my supervisor will say yes after hearing bout Pokemon cards.",
        "For that search virtual try-on, ootdiffusion and fashn. Google also did that with best results. GL",
        "Is there an open source dataset for it?",
        "what models are you exploring in this space??",
        "Haha classic",
        "Automatic grading is used in maaaaany fields, and Pokémon is a billions/year business.. ",
        "diffusion"
    ]
},
{
    "submission_id": "1g5hdhv",
    "title": "GPU recommendation",
    "selftext": "Hi everyone,\n\nI’m working on an autonomous driving project using the CARLA simulator and need advice on choosing a GPU. My budget is around 600-800€. I’m considering a used RTX 3090 or a new RTX 4070 Ti, but I’m unsure if I should prioritize VRAM over raw power.\n\nAlso, my university might provide server access, but I still need a GPU for local work. Should I invest more in a powerful GPU or rely on the servers for heavier tasks?\n\nAny advice or recommendations would be greatly appreciated! Thanks!",
    "created_utc": "2024-10-16T20:08:02",
    "num_comments": 0,
    "comments": []
},
{
    "submission_id": "1g5exyc",
    "title": "Small object detection with a sparse class",
    "selftext": "I'm going to describe through analogy a problem I am trying to solve in object identification (tracking in video actually, but bounding boxes from individual frames remains the method).\n\nI am classifying windows by their state.  The windows can be labeled as open-windows, closed-windows, or unknown-windows (a catchall for if the window is not sufficiently visible, in poor lighting, too far, or otherwise difficult to classify).  In the raw image files, the objects are relatively small, and some can only be properly (manually) annotated by really jacking with the image contrast and brightness (to not over-use the unknown-windows class).  Closed-windows objects vastly outnumber the open-windows with unknown-windows somewhere between.\n\nI've been starting with a YOLOv11 architecture, but I'm not really sure that is appropriate versus say building out a ResNet type from scratch.\n\nTechniques that could be useful:\n\n\"Tiling\" the images into smaller images and using an inference slicer like SAHI to help identify small objects.\n\nImage augmentation through color jitter and other direct modifications of the RGB values.\n\nI know that in simple image classification, inverting the class initializations (the class that is in 90% of the images is initialized with a 10% probability and visa versa) can help deal with the sparse-class problem, but I'm not sure how I would implement that in YOLO object tracking.  Really, I just need a sounding board to help shake lose some cobwebs as I am the only person in my workgroup that can work on this particular problem.  No objects/classes already built into YOLO matter to me.\n\nEDIT:  My images start with decent resolution, ranging from 1080 to 4K.",
    "created_utc": "2024-10-16T17:58:41",
    "num_comments": 5,
    "comments": [
        "A rule of thumb that I use for data annotation.\n\nIf I can easily look at an image and detect the object with no ambiguity, then it’s good for AI\n\nIf the object is ambiguous/hard to see or requires modification of the image. I throw it out. \n\nI’ve found that ambiguous objects can severely damage the model performance. \n\nI’ve also learned that AI does not solve all the detection problems; sometimes I use a simpler CV method to get the result",
        "When you say \"simpler\", how do you mean?",
        "I would think about chunking it out. Your first problem is finding small windows. Your next problem is deciding if they are open or not. \n\nPresumptively your scene won’t be changing much as in your camera won’t be moving. So once you know where a window is it’s not moving. Now you classify if it’s open or closed. \n\nIs one train of thought.",
        "For me simpler would be non AI based methods, a random example would be using a binary filter to highlight features and then determine the maximum or minimum edge locations to approximate position.",
        "Darn. That's where my analogy fails. The images (not of windows, I was just trying to draw a parallel problem description) are taken from a drone in flight."
    ]
},
{
    "submission_id": "1g58zzy",
    "title": "CoTracker3 Meta AI",
    "selftext": "Have you guys seen the new dense tracker? Pretty good under occlusion. What do you guys think of it? ",
    "created_utc": "2024-10-16T13:19:28",
    "num_comments": 0,
    "comments": []
},
{
    "submission_id": "1g58xv2",
    "title": "Generate Numerical Data ",
    "selftext": "Creating numerical data, it's not as straightforward as generating text or images because the numbers must make statistical sense. The current available current methods may not be sufficient to generate statistically relevant numerical data.\n\nWant to create a AI prototype that can generate synthetic Numerical data?",
    "created_utc": "2024-10-16T13:16:59",
    "num_comments": 0,
    "comments": []
},
{
    "submission_id": "1g58bkg",
    "title": "YOLOv7-tiny train and val obj_loss plots diverge pretty early",
    "selftext": "",
    "created_utc": "2024-10-16T12:50:21",
    "num_comments": 0,
    "comments": []
},
{
    "submission_id": "1g57n48",
    "title": "Dataset to detect a person fatigue",
    "selftext": "I am trying to work on a project that uses computer vision to determine whether a person is fatigue by analysing their posture and movement. I am new to computer vision, is there any recommendation of where I can get the dataset to kick start the project? Thank you in advance.",
    "created_utc": "2024-10-16T12:20:40",
    "num_comments": 2,
    "comments": [
        "what type of fatigue ? and what will be the environment around the person ?",
        "For example, basketball players getting tired after a period of playtime and started moving at slower pace, getting more hunched and other symptoms, which can help coaches detect when to make substitutions and who should be subbed out first."
    ]
},
{
    "submission_id": "1g56tux",
    "title": "Pupil and eye detection using laptop or pc's camera",
    "selftext": "I want to make a gaze measurement system that tracks the pupil and eye of the user and computes what is the attention span, when he/she needs to take a break, etc. But I am not sure will I be able to do it using a laptop's 1080p inbuilt webcam within a distance of 2 feet from the screen. Please share some resources or some previous works related to this. Any other alternatives are also welcomed.\n\n",
    "created_utc": "2024-10-16T11:46:05",
    "num_comments": 1,
    "comments": [
        "Google’s MediaPipe offers robust facial landmark detection with a pre-built eye-tracking model\n\nGazeTracking Python library specializes in gaze tracking with a webcam. It can detect the user’s gaze direction and focus"
    ]
},
{
    "submission_id": "1g51rff",
    "title": "Is Python appropriate for parking machine software?",
    "selftext": "I'm sorry if this is not the appropriate sub for this.\n\nI am a junior developer assigned to a project(my first ever) where I have to create a parking machine software.  \n  \n[https://youtu.be/u40r1tlHFsg?feature=shared](https://youtu.be/u40r1tlHFsg?feature=shared)\n\nSomething like this, one which is present in most malls, where it prints a receipt and prints out your ticket or charge. It would need to detect number plates, have data about the car, timings etc available to the owner.\n\nThere is existing software for this, it's in java and they want me to improve this software. They have asked me to use java again due to it being much faster than python and better in real time tasks. I do know Java is faster and used more in hardware-software communication but could the work be done in Python?\n\nFor starters I have to create a license plate detection system, which is quite easier in python but they said that python will be slower. If anyone does have any experience on this and/or could guide me to at least learning the theoretical aspect so I can at least understand the language to use for my overall project, I'd be really grateful.\n\n",
    "created_utc": "2024-10-16T08:14:29",
    "num_comments": 13,
    "comments": [
        "Anything running license plate detection in python is going to be running the bulk of its processing in C. It'll be just as fast in python as anything else. \n\nBut I would imagine that they want to run it in an embedded device that may have difficulty installing/running Python without significant overhead\n\nFWIW Opencv has a Java implementation that would probably be a better choice",
        "I wouldn’t advise you to implement a new feature in a language that isn’t actually used in the system they have at the moment. This would just lead them to think that they wasted their time on you by allowing you to work on this project. It’s fine if you want to just learn but if you want a real impact, I’d use Java like the rest of the system is.\n\nIf the device is running Java as a glorified web cam to send data to the server to read and analyze the data, yea sure you could probably run python on a server to do all that, but again, you’d be developing a whole new work flow for the company that I’d guess that they wouldn’t like. (Added cost & complexity) Maybe I’m getting old but try and adapt to the environment you put yourself in rather than reinventing the wheel.",
        "Firstly you need the requirements for the project. What exactly need to be done, how fast should the system operate, how bif is the throughput. According to to the requirements you choose a appropriate language.\n\nSome the requirements can be: it works with the present system or it expand the present system without influencing the current features. Or even write it in java. \n\nWith such requirements it get more difficult to you any language you want.\n\nPython being slower then java would not matter if the application is slow anyway or if the throughput is really low. If you have minutes for your algorithm, why should you hurry?",
        "Python is totally fast enough, no worry.\nThe python library to do the image professing and machine learning parts are just wrappers for C code. So it will pretty much run as fast as C code, as far as the image processing parts are concerned, which are likely to be the only parts that take time (probably 99% or the execution time will be that) at all in such a system.",
        "I can’t think of a good reason not to use Python unless this entire system needs to be self contained in a small embedded device, which doesn’t make a lot of sense given the system will be taking payments. What happens if someone steals the embedded device, will they be obtaining sensitive data and avoiding paying their fee? Instead I would use simple software on the ended dead end, maybe not using Python, and then have a separate system that runs the CV tasks remotely. This portion can run Pyrhon. \n\nSome of the world’s largest software systems run on Python. It’s plenty fast. https://www.reddit.com/r/Python/comments/rzr0z2/is_python_suitable_for_enterprise_applications/",
        "Will the deployment be using NVIDIA hardware?",
        "Thank you for your response. Do you think it'd be good enough for machine reading, creating an application, taking the cctv camera stream and then getting the data from it?",
        "Yes, but actually I can if it's the better choice, the option is with me, it's a single client and i'm more like a freelancer here.",
        "i3 3rd gen, 4gb ram",
        "Sure",
        "Look into RTSP streaming and opencv",
        "plenty of repos you can use as a starting point for license plate segmentation / reading. Important part is a good enough camera / camera angle / lighting conditions. As far as hardware and software tools & requirements, that depends on the latency / cost requirements. Is it okay to send the image to cloud for processing? etc.."
    ]
},
{
    "submission_id": "1g4zubc",
    "title": "2D to 3D (depth estimation)",
    "selftext": "Hello everyone!\nI am trying to understand what algorithm can be used to estimate the depth of a point I'm tracking with a monocular camera. \n\nBasically I got an ego vehicle that moves mostly along the longitudinal axis and tracks an object that can be represented as a single point. \n\nThe ego moves towards that point with the purpose to align to it. So it can create a curve or straight trajectory. \n\nIf I have the odometry of my ego vehicle and the 2D point in the image plane. How can I estimate the depth or 3D point with the sequence of images and the odometry?\n\nI was thinking image triangulation might not work because the image pairs generated are desplace mostly along the longitudinal axis and not latitudinal.",
    "created_utc": "2024-10-16T06:49:46",
    "num_comments": 12,
    "comments": [
        "you are correct, triangulation doesn't work in the exact direction of *the focus of expansion*, that's the pixel toward which the camera is moving. And around it, triangulation will also be bad in practice, that's where even the smallest 2D error can yield a very large depth error.\n\nIt's quite easy to see: there is no triangle to form using rays that point to subsequent images of your object, since they are aligned in this case! This also shows that camera orientation doesn't play a part in this, it's really all about the direction of translation.",
        "If the camera is calibrated and you know the camera motion between images, then the trajectory of a single tracked 2d point can be triangulated into a single 3d point.\n\nIt is true that if the point you track is exactly on the focus of expansion (or epipole, or simply \"in the direction of motion\") then it can't be reconstructed. \n\nIf you represent the camera projection from 3d to 2d as a matrix, then the inverse of this matrix, when applied to a 2d point, will provide a projection ray for that point in the 3d world. Tracking a point will provide multiple rays, and their intersection is the triangulated point your are looking for.\n\nForget the mono depth approach if you need any kind of accuracy. Mono depth can't provide metric depth, so it's not for autonomous navigation.",
        "If you have a sequence of images then there are models like Depth Anything https://depth-anything.github.io/ that will at least give you a fair amount of estimation of the background. I'm not sure if it'd precisely locate your 'single point' target though. Anything you could to to enlarge and make-visually-distinct would help it a lot.",
        "Do you know the size of the object? Is the size big enough (spans a few pixels at least) as seen from the camera?",
        "If you have the camera parameters you can find the z value of your point using similar triangles based on the focal length. The x and y will come from your image",
        "don't know particular algorithm. but you may search stereo vision, it's used to compute depth of objects in 2d images.",
        "Thats what I thought, thanks for replying. You can have the special case where you are initially align to the object making the 2D projection be at the center of you X axis in the image plane and while you move towards it the displacement would be only on the Y direction. From a top-down view those rays overlap so no triangle is formed. \n\nI asked ChatGpt and the recomendation was to use deprh from motion, structure from motion or a DL approach. What do you think?",
        "Thanks for replying.\nI am trying to understand where exactly the FoE lies within my image.\nThis image shows me its at the center and generates a cone around it.\nhttps://images.app.goo.gl/ZfqYubzWsUq2kCTPA\n\nSo if I am heading straight to an object the rays point at the same direction as the FoE. Meaning that I could only triangulate objects that are perpendicular to me.\n\n For example if I am heading down a road I could track and triangulate the building at each side of the road but not what is infront of me?",
        "Thanks for replying, but there is this special case where if you are initially align on the longitudinal axis and moves towards the object, the displacement is moslty along the Y axis of your image. From a top-down view there is no triangle formed as the rays overlap.",
        "But we ARE talking about a limitation of the geometric aspect behind structure from motion, be it deep learning or not. So chatGPT's advice is a bit meaningless, just like the other comments who basically recommand triangulation :)\n\nNow there are single image depth models, and those do not have that limitation. You can try them, but if your target is point-like, I have my doubts.\n\nAnd finally, a word of caution: most depth estimators do NOT give an absolute distance, they give a relative depth only. There are *metric* depth estimators, but again, they work by implicitly assuming a lot of things about the usual scenes, so their performance will depend a lot on your actual setup.",
        "Exactly. Anything exactly at the FOE is not providing any parallax (motion induced by depth).\nHowever, you can reconstruct anything around the FOE, and how you define \"around\" depends on you speed, how far are the objects, and the accuracy you want.",
        "That’s really only true if the objects are very small and far away. This is because only the edges of the object are not along that axis, only the center is. "
    ]
},
{
    "submission_id": "1g4z85l",
    "title": "Advice Needed for Implementing High-Performance Digit Recognition Algorithms on Small Datasets from Scratch",
    "selftext": "Hello everyone,\n\nI'm currently working on a university project where I need to build a machine learning system from scratch to recognize handwritten digits. The dataset I'm using is derived from the UCI Optical Recognition of Handwritten Digits Data Set but is relatively small—about 2,800 samples with 64 features each, split into two sets.\n\n**Constraints:**\n\n* I must implement the algorithm(s) myself without using existing machine learning libraries for core functionalities.\n* The BASE goal is to surpass the baseline performance of a K-Nearest Neighbors classifier using Euclidean distance, as reported on the UCI website; my goal is to find the best algorithm out there that can deal with this kind of dataset, as I plan on using the results of this coursework for another University's application. \n* I cannot collect or use additional data beyond what is provided.\n\n**What I'm Looking For:**\n\n* **Algorithm Suggestions:** Which algorithms perform well on small datasets and can be implemented from scratch? I'm considering SVMs, neural networks, ensemble methods, or advanced KNN techniques.\n* **Overfitting Prevention:** Best practices for preventing overfitting when working with small datasets.\n* **Feature Engineering:** Techniques for feature selection or dimensionality reduction that could enhance performance.\n* **Distance Metrics:** Recommendations for alternative distance metrics or weighting schemes to improve KNN performance.\n* **Resources:** Any tutorials, papers, or examples that could guide me in implementing these algorithms effectively.\n\nI'm aiming for high performance and would appreciate any insights or advice!\n\nThank you!",
    "created_utc": "2024-10-16T06:20:46",
    "num_comments": 2,
    "comments": [
        "I would try the easiest version like LeNet as a baseline and try variants of AlexNet",
        "Thanks for the suggestion! Are there variants of the AlexNet that can work with such small dataset?"
    ]
},
{
    "submission_id": "1g4upri",
    "title": "Image vectorization",
    "selftext": "I'm a developer working on my own image vectorization algorithm, to fully vectorize any raster image to a vector-based image. For this reason, I'm now trying to ascertain what the currently available solutions can do. As far as I've been able to tell, there are no algorithms or solutions for vectorization that do the following:\n\n1. Produce a result in the form of vertices and edges\n2. Describe all information in the image\n3. Can handle complex gradients\n\nI have a few questions about this:\n\n* Do you know any such solutions to exist?\n* If not, do you have any insights as to why that is?\n* Would you want such a tool, to solve a problem you face in your work? How so?\n\nIn particular, I'm interested in applications of this for AI (classification and generation). Any and all thoughts on this matter are greatly appreciated! I look forward to hearing from you.\n\n",
    "created_utc": "2024-10-16T01:43:14",
    "num_comments": 6,
    "comments": [
        "tools like this do exist, unless i misunderstood what you're proposing: [https://www.adobe.com/express/feature/image/convert/svg](https://www.adobe.com/express/feature/image/convert/svg)  \n[https://github.com/visioncortex/vtracer](https://github.com/visioncortex/vtracer)  \n[https://paperswithcode.com/paper/raster-to-vector-revisiting-floorplan](https://paperswithcode.com/paper/raster-to-vector-revisiting-floorplan)  \n[https://arxiv.org/pdf/2306.06441](https://arxiv.org/pdf/2306.06441)\n\ncertainly, there is no method that \"describes all information in the image.\" just like with raster graphics, where information is discretized spatially into pixels, and color depth is represented by n bits, vector graphics also have their practical limitations because it's simply not feasible to have an infinite amount of vectors. a reasonable goal could be to get the closest result with a limitation to n vectors.",
        "Thank you for sharing those materials, I'll go through them and see how they compare! In the meantime, regarding the \"describe all information\" part, perhaps this is a good way of looking at it: Such a solution would be able to recreate the original image perfectly.",
        "And perhaps to further clarify that idea: The worst case scenario of a vectorization would be that you need as many vectors to describe the image as the original raster version did. But the algorithm should ostensibly look for all sorts of opportunities to describe larger parts of the original image with fewer vertices.",
        ">the algorithm should ostensibly look for all sorts of opportunities\n\nOn the first glance what you're describing is NP-hard, so not quite easy. Lossless raster-to-vector is also probably impossible in  general case (unless I'm missing something)",
        "Interesting claims! Why do you believe its an NP hard problem?"
    ]
},
{
    "submission_id": "1g4s091",
    "title": "[P] Train a model on persian language ",
    "selftext": "hello guys , i want an assignment about train ocr model on persian language,  but could find a way to train it , 5he developer's documentation is hard to understand . could you help me?",
    "created_utc": "2024-10-15T22:20:26",
    "num_comments": 12,
    "comments": [
        "Which model are you trying to train? Have you tried this https://huggingface.co/hezarai/trocr-base-fa-v2",
        "Hi bro, finally an Iranian (same as me). \nUse crnn. Works fine on simple images",
        "tried but failed , i am trying to train GOT-OCR2.0",
        "and what about on unseen images also images of documents ( multple lines mix bteween handwritten and text written) ?",
        "Understood! what I see there is to provide a bunch of pdf and their corresponding text. I think the community would be able to help you more if you point to your problem more precisely.",
        "Usually these types of scenarios are solved in two steps, detection, then recognition.",
        "just need training the model .",
        "at first i tried to use yolo for detection but didnt work , it start mix lines after i tried A* algorithm approach did produce some resutls but when line start getting curves i stops .\nso tried ocr model , till now the best option is GOT-OCR2.0  but now arrived at the point where how to train it .",
        "on other language",
        "I suggest you search Google scholar for related papers. This much complexity requires  an ocr expert. My field of study is object detection.",
        "ok , thank you , if possible i shall send you what i've tried",
        "Sure no problem. Just dm me. I'll help as much as I can."
    ]
},
{
    "submission_id": "1g4nq50",
    "title": "Multi label classification",
    "selftext": "I am training a classifier to classify 3 different categories.\n\nThe first classification contains 7 variables, 0-7 of which may be present, so the model will output 1 or 0.\n\nThe 2nd classification is a number 1-3\n\nThe 3rd classification is a number 1-3\n\n\nWould it make sense to train 1 model for this task? Or to split it up into 2 or 3 models?",
    "created_utc": "2024-10-15T18:16:56",
    "num_comments": 4,
    "comments": [
        "That description doesn't help me visualize it at all. Is it even computer vision?",
        "Can u give us more details on what does the model actually do?",
        "What are you talking about? Are there 3 classifiers on top of your classifier?",
        "Yeah really unclear what you’re asking but it seems like you can just have more classification categories say if you have 3 groups and 3 subgroups instead just make 9 categories.\n\nAlternatively 2 classifiers, OR SVM from the features of the first classifier to a secondary classification (1 or multiple svm trained).\n\nSvm generally quicker, but it’d still be quick with two classifiers.\n\nI might be misunderstanding what you’re asking for tho."
    ]
},
{
    "submission_id": "1g4nnjq",
    "title": "[R] Your neural network doesn't know what it doesn't know\n",
    "selftext": "Hello everyone,\n\nI've created a GitHub repository collecting high-quality resources on Out-of-Distribution (OOD) Machine Learning. The collection ranges from intro articles and talks to recent research papers from top-tier conferences. For those new to the topic, I've included a primer section.\n\nThe OOD related fields have been gaining significant attention in both academia and industry. If you go to the top-tier conferences, or if you are on X/Twitter, you should notice this is kind of a hot topic right now. Hopefully you find this resource valuable, and a star to support me would be awesome :) You are also welcome to contribute as this is an open source project and will be up-to-date.\n\n[https://github.com/huytransformer/Awesome-Out-Of-Distribution-Detection](https://github.com/huytransformer/Awesome-Out-Of-Distribution-Detection)\n\nhttps://preview.redd.it/p8h7dc0zq0vd1.png?width=1754&format=png&auto=webp&s=7704564053d38f067c6a42d4ce2d642bd93edf13\n\n\n\n\n\nThank you so much for your time and attention.",
    "created_utc": "2024-10-15T18:13:08",
    "num_comments": 39,
    "comments": [
        "Cool, part of my PhD research was in this field (operating ML based vision systems in manufacturing, which needs to adress OOD and data drift). I am always astounded how almost every (non ML) engineer points out this problem in production, but only a few in academia are actually aware or even suggest practical approaches.",
        "The basic thing is that whatever you do you cannot extrapolate . You can prove with some reasonable assumptions that some interpolation scheme converges whereas it is not the case for extrapolation. The idea is that you cannot deal with totally unseen information. \nIf we were able to extrapolate,  2 samples would be enough to solve all the world problems...\n\nSo the only point is to detect that you are currently extrapolating. If your query is close enough from the border of your dataset distribution, then the prodivted output maybe ok..but otherwise, no way...",
        "Does any of this actually work?",
        "How does this help me right now?  I’ve investigated pretty much any plausible source for not generalizing, tested every possible solution, resorted to idiotic things like “let’s throw a Transformer in there” and ultimately realized “there’s not enough signal amongst the noise”. What does this add?",
        "Maybe because they don’t know what they don’t know",
        "Reminds me of this Lex interview with Jeremy Howard\n\n[https://www.youtube.com/watch?v=Bi7f1JSSlh8](https://www.youtube.com/watch?v=Bi7f1JSSlh8)",
        "That’s a good intuition. Per Alyosha Efros, everything is just nearest neighbor mumbo jumbo :D",
        "Yeah, that is basically it. But detecting when extrapolation is happening is very crucial for a lot of fields, especially where the model \"guessing\" is not an option. Eg. automotive production.\n\nI am always skeptical towards anything that claims to go beyond that.",
        "What about cases where there are compositional properties of a sample? Like say you train on data points that have property A and B and some that have A and C but never one that has B and C.  Would it not be possible for an algorithm to extrapolate for the last case? Or is that considered interpolation?  I mean a linear model can do it.....(for problems where that is applicable)",
        "But in high dimensional space (nearly) everything is technically extrapolation",
        "If you cripple the problem enough :D.",
        "Hi there,\n\nMaybe I don't fully understand the exact problem you're facing; I assume it's related to OOD generalization? Also, I'm not sure what you mean by \"investigating every possible solution\" :D. This is still an active area of research. Whether you are a researcher or a practitioner, it's perhaps a good idea (if you haven't already) to read a couple of survey papers in the repo. Then, you can adapt suitable methods to your applications, improve upon existing ideas, or even come up with novel ones.\n\n  \nCheers",
        "Yeah, but based on your data you can deduct whether the current sample is similar to samples seen during training, basically doing outlier detection. For example, you can do statistical testing, which is done for most manufacturing processes, especially in automotive (google SPC). Since images are a bit more complex, you need to derive low-dimensional features for them, then do statistical testing.\n\nThis can be done by looking at distribution of weights inside the lower levels of your network (an approach I had bad experience in practice and would highly advise against!) or use some form of AE (which I prefer, especially for industrial data which can be encoded quite well by an AE, since it has low structural inter-sample variance).",
        ":D",
        "To my opinion, what you propose is not pure extrapolation as you add additional information to the data (i.e. the system that generate the data is such that observation has some properties that we may exploit). Let me illustrate this on another example. \n\nFor instance, if you say, I know that the data are ALWAYS generated by a linear system. Then using very few samples you can train a linear model and be confident in the extrapolation. You know it will work because you picked the linear model based on the additionnal info. This works because you added the information that the process is ALWAYS linear. So basically you did not worked on an interpolation problem (purely data based, where the model structure is unkown) but on model identification (the model structure is a priori know and only some parameters needs to be fixed).\n\nNow change it to 'in SOME region covered by the training dataset, the process is roughly linear'. Then you cannot extrapolate anymore because you do not have any valid information outside the region covered by the training dataset. \n\nNote that even for classical interpollant you cannot even demonstrate the convergence properties without additionnal information. In general, you need some smoothness assumptions. Then based on these assumptions, you can prove that adding more data leads to a prediction error. A perfect counter example is to try to interpolate a pure random signal (without any further specification). It is impossible because the signal has no exploitable properties, basically none of the smoothness assumptions hold.\n\nSo basically, under some mild assumptions interpolation is possible, extrapolation is in general impossible without additionnal information on the process that generated the data.",
        "Can you elaborate? I do not get it. \n\nYou learn from the data, whatever their dimension is. This is where the information is. I do not think that data dimension has anything to do with interp or extrap capability. You could deal with a 2d problem in 2d (x-y regression) and you are going to experience the same issues. \n\nTo the best of my knowledge, we are able to store existing information but not create information ex-nihilo. As a result, extrapolation is impossible in general. If we are lucky, the model may extrapolate well in some neighborhood of the training dataset border, i.e. we extrapolate by using some of the information somehow contained in the training dataset and captured by the model. \n\nIf the query gets too far from the training dataset border, then no valid information is available to deal with such a case. Literally, this an ill posed problem and there is absolutely no way to provide a reasonable answer without additional information.",
        "It depends on the dimensionality of the internal model you learn, not the dimensionality of the incoming data. This is basically the whole reason for regularisation.",
        "I spend to much time explaining to colleagues we simply cant detect when the models see something «new», and I had honestly come to peace with that it will stay this way for the foreseeable future. If something actually works, i would love to know. My best guess now is to run all the data through a large vlm and cross my fingers it returns a signal that could be used.",
        "Yes, I’m being provocative to say I’ve tried everything. I prayed for grokking, to no avail. Tried every architecture I could implement, but I’m just an engineer looking for actual stuff I can use. So yes I missed a bunch of theoretical stuff. But I think my point is that a LOT of real world problems suffer from low signal to noise which hasn’t been solved by anything academic or fancy. Hence my question. Solve the signal to noise ratio problem and I’m ready to listen.",
        "But this assumes you have in hand the source.  \nSince the models don’t save the source but are brushed by it a tiny bit on every iteration, you can maybe asses the prominent pathways/tendencies and compare to that, but it’s very fluid.\n\nBut it’s an idea - training a model, saving source and making it RAG and GraphRAG accessible for anchoring.",
        "https://arxiv.org/abs/2110.09485",
        "Assuming you are talking about detecting OOD, then I concur that the problem won't be 'solved' for a foreseeable future. This is an ill-posed problem with no universally-agreed upon definition of what really is OOD (although there have been attempts to classify the problem; refer to the nice survey papers in the repo).\n\nOn the other hand, consider the image classification problem, you could also say if the test input is close enough to your training data in some space, then you classify it as in-distribution. That's what many of these papers do, where the said space refers to some feature space induced by the data semantics, usually via the maximum likelihood loss during training (especially for the post hoc methods).",
        "[https://proceedings.mlr.press/v222/mascha24a/mascha24a.pdf](https://proceedings.mlr.press/v222/mascha24a/mascha24a.pdf) Practical application, works quite well in reality (running in production for 2 years).",
        "Gotcha. It's still far from a solved problem though. In fact, it's arguably one of the major problems facing ML systems today. Hopefully, the resources are still useful to some degree to you, as this is the best we can do at this moment.",
        "FYI I soak up papers like a sponge but know I miss a lot. I’m looking for something I’ve not seen or (my bad) overlooked",
        "What does RAG have to do with this? Last i checked, this is the computer vision sub? And yeah, on most real-world CV applications **you have access to the sources** since you train on them. My approach, for example, is to train an AE on the same data with the models upper layer as encoder as an additional OOD detector.",
        "I agree with the paper, that in high dimensional space, it is very likely that we do not have enough training data to ensure that a query belongs to a convex hull of some samples. The question that arise is how many of the original dimensions are actually meaningful/useful? Probably much less that this. The autoencoder behavior provide us with the intuition that dimensionality reduction is possible to some extent, indicating that there should exist some projection such that most of the \"information\" is preserved. \n\n As a result, from this paper, I think the conclusion is that neural network do not formally interpolate (according to the paper's authors definition) but provide a \"rough\" estimate of the information. I would put a bet on the fact that they interpolate in a lower dimension space (however, I'm not sure we could define a projection operator from the original space to the lower dimension one easily).\n\nIndeed, this paper did not convince me that neural network are able to extrapolate, in general. Extrapolation is somehow possible in some particular case, such as border of training dataset (possibly belonging to a low dimensional space). The paper suggest that they may extrapolate in a region with is related to the original data (and they make it clear that we cannot say \"inside\" the training data convex hull) . But beside specific cases, the extrapolation is related to the creation of true/unbiased estimate from \"nothing\" which is not possible.",
        "Thanks! Will read",
        "Understand. Also understand my frustration most papers use toy datasets even if they are “accepted” as metrics. Real world is so much harder.",
        "RAG is about finding content fast, not about text or even LLMs specifically.\nIf you want to compare a model with similar images, you can generate embeddings from the image you analyze and match it semantically with the sources to find similarities.\n\nSo you can compare your output with similar training material.\n\nUnless you’re fine with simpler ways",
        "I also agree that this does not mean that any extrapolation is possible. I just say that the classic extrapolation vs interpolation notation is not ideal and what people really mean is a fuzzy \"how far are we away from the original training data\". But networks are definitely able to give meaningful results slightly outside the data distribution. But it gets worse the further you get and how fast it gets worse is dependent on the problem at hand as well as inductive biases engineered into the model (say some physics based model where we tune a few parameters in a very constraint way will likely perform better for longer than a MLP trained on the same problem).",
        "As someone working in real-world, I can confirm. What is your exact field of appliance, maybe I can help you?",
        "But \"comparing\" is quite problematic in the world of images, since pictures may be similar, but their high dimensional representation may be not. For example, I had to work a lot with metallic surfaces. They produce a lot of different patterns in the image based on small varieties that in the end do not matter to the application.\n\nSo you have to compare them at a level that actually encodes the features influencing the model's decision. There are methods who use a RAG like search function (to lazy to find the actual paper rn), but in my practical experience they don't work well enough.",
        "I do face recognition and a ton of vectors.\n128 vectors per face and it's impossible to know which vector corresponds to what feature.\nHigh dimension vectors are a curse.\nGreatest when they work.\nBut when you have a set of false positives, it's nearly impossible to see why.",
        "Waoh",
        "How do you vectorize? Segmenting or different filters or?",
        "You run your i.age through say DLIB or SFace and it out puts 128d vector.\nThat's 128 numbers, in the range of -2 to +2.\nAnd it's meant to represent the faces features, but no one has an idea what features.",
        "Oh, I thought 128 Vectors, not 1 of 128d",
        "I ran 1 million faces through SFace ( SFace was only trained on 100k faces ).\nAnd after analyzing enough vectors I found 4 vectors that would align to the gender , and skin color .\nReally strange. The accuracy was 70% but it was weird what SFace had learned .\nBut SFace had not learned eye color, lip color or hair."
    ]
},
{
    "submission_id": "1g4mm5i",
    "title": "YOLO11 (YOLOv11) much better or not? ",
    "selftext": "Anyone compared yolov11 with the older ones? Seems like there’s a new yolo every few months and it’s always just marginally better. ",
    "created_utc": "2024-10-15T17:20:51",
    "num_comments": 17,
    "comments": [
        "There’s been some benchmarks tests I’ve seen. It performs better in *some* circumstances, so you’d have to benchmark it against others for your use case.\n\nIt would honestly be great to see the next yolo innovate through open source with an MIT license",
        "I think the focus for this version was actually improving the efficiency of the model. It achieves similar results, according to their tests, but will do so quicker/using less flops",
        "I benchmarked fine-tuning it across 100 different datasets & shared my results here: [https://www.youtube.com/watch?v=8UB-lk4hG2I](https://www.youtube.com/watch?v=8UB-lk4hG2I)",
        "It's only about 2% better, but requires less compute to do it.",
        "Edit: see the comment above this!\n\nDid they benchmark it using roboflow100 or whatever that big many-domain dataset is called? It’s a dataset comprised of many other unrelated object detection datasets, basically the whole purpose is to test if models perform well generally or not.  But yeah, at this point the entire yolo series is just tweaking small details and usually it doesn’t make a big difference. I still use yolov4 in production and it works just as well as v8 for my data. ",
        "I saw this on LinkedIn: https://www.linkedin.com/posts/yolovx_yolov5-vs-yolo11-yolovx-ugcPost-7252197674953719809-P0Fc?utm_source=share&utm_medium=member_android\n\nI couldn't have chance to test yolov11 model by myself. Like mentioned on the post, I also think that yolov5 is still competitive with more lightweight architecture. However, as a developer, I have no motivation to use any ultralytics products because of their latest actions on licensing their work.",
        "The big thing with yolloV11 wasn't how much better it could perform it was how much more efficient it could perform while doing the same tasks as older models",
        ">It would honestly be great to see the next yolo innovate through open source with an MIT license\n\nIf you're talking about ultralytics, it's 0% chance they'll  do that. Their licenses, annoying marketing, and stupid github bot are the main reasons I'll try my hardest to never use them anywhere and urge others (non-learners) to also avoid them",
        "oh it's going to happen soon ;)",
        "When does the licensing start? Is it for all their stuff?",
        "Are you referring to this?\n\nhttps://github.com/WongKinYiu/YOLO",
        "If you go to any of their yolo github page, you will see the license of the model. For yolov5, it is AGPL-3.0 which means if you use this model in a network environment and sell service like api, you should make your code public.",
        "also !",
        "Jesus that is horrible,",
        "You’re free to develop your own yolo and release it under a more permissive license. ",
        "I wish I could !",
        "I always seriously consider commercial licenses. It usually saves a TON of money compared to doing something yourself as long as you assign a reasonable value to your own time. \n\n The Ultralytics stuff is pretty reliable and flexible, certainly a lot easier to get up and running than almost anything else.  How else can you pip install an image classifier, object detector, and instance segmentation model all with a single command and consistent interface? That’s also tintegrayed with various online services. \n\n Obviously most of us devs like to build things ourselves and don’t mind working weekends to do so…"
    ]
},
{
    "submission_id": "1g4lcem",
    "title": "Passing non-visual info into CV model?",
    "selftext": "How would one incorporate non-visual information into a CV detection model?\n\nTo illustrate how valuable this would be, imagine a plant species detection model that could take into account the location in which the photo was taken. Such a model could, for example, avoid predicting a cactus in a photo taken at the North Pole. If a cactus were to appear in the photo it would be rejected (maybe it's a fake cactus? An adversarial cacti, if you will)\n\nAnother example is identifying a steaming tea kettle from the appearance of steam suplomented by a series of temperature readings. Steam is only possible if the temperate is or was recently at least 100 degrees, otherwise what looks like steam is something else. \n\nI can do these kinds of things in post processing but am interested in incorporating it directly within the model so it can be learned. ",
    "created_utc": "2024-10-15T16:18:33",
    "num_comments": 20,
    "comments": [
        "I have thought about this a lot. I want to integrate geometry information into a CV segmentation model. One suggestion was to add additional channels to hold the vector encoding whatever you want to add. Seems wasteful but I have not tried it. \n\nIt would be great if anyone with actual experience in this question would respond with specific ideas.",
        "I always wanted to try using MetaFormer for doing satellite-related work. Pairs images and metadata into a shared embedding and seemed pretty cool:\n\nPyTorch implementation: https://github.com/dqshuai/MetaFormer\n\nFrom “MetaFormer: A Unified Meta Framework for Fine-Grained Recognition” \n\nPaper: https://arxiv.org/abs/2203.02751",
        "Sounds like a transformer type architecture may be able to integrate something like that",
        "Proper CNNs don't really work like that very well because they don't really work in latent space, (for the most part). They're agnostic filter finders. \n\nThere are some tricks you can do by adding additional channels, but really in a lot of cases it would be an additional input into the FCNN at the end. \n\nFor steam, you would just add that as additional layers in the image input. CNNs don't care if it's an actual RGB image. Thermal inputs are fine as long as it's spatial. You can even involve binary info on pixel levels and stuff if you have that info at inference time. \n\nFor location, that's most likely better served as input to the FCNN as one hot info, or passing in an encoded text from a text encoder or something, (but I imagine if you're doing that you might as well be using a transformer based architecture all around)",
        "Maybe can search things like turn images to tokens",
        "I’ve been reading a lot about visual language action models; these might be interesting to research for this topic \n\nMy main interest is in its ability to train on multiple types of data and perform in multiple modes",
        "You want to positionally encode the values and then combine them with the image features via feature like linear modulation (FiLM).\n\nLook up the literature on diffusion models. There they want to pass in the amount of noise in the image and use positional encoding to do that .",
        "you just described multimodal machine learning lmao",
        "I’ll second that. I actually have cases like you describe where “geometry” could be provided as extra data. \n\nOne example is detecting things on a product display where you would give the model a vector “map” of the shelve structure. This can be obtained extremely accurately and then the model can use it to only infer products that are aligned with the shelves, rather than possibly floating in space. ",
        "Thanks, this sounds almost exactly like what I’m looking for! I would never have guessed to search for “fine grained recognition”…\n\nFrom the paper’s abstract:\n“ Is it possible to use a unified and simple framework to utilize various meta-information to assist in fine-grained identification? To answer this problem, we explore a unified and strong meta-framework(MetaFormer) for fine-grained visual classification. In practice, MetaFormer provides a simple yet effective approach to address the joint learning of vision and various meta-information. ”",
        "That’s kind of what I was thinking. But how? \n\nAny ideas on possible search terms? I’m not finding anything / it’s impossible to wade through irrelevant results. Literally every model I can find either takes in a RGB images only, or if it includes additional inputs they’re also image-like. ",
        "Thanks. I had been struggling with encoding the data into more channels but sometimes it just can’t be done. Plus there’s the problem of keeping those channels separate from a model that “wants” to colvolve all channels. \n\nMaybe I’ll just try concatenating the extra data to the FCN’s input and see if that’s good enough. Sound like a nice simple approach, at least!",
        "Interesting, thanks!\n\nWould this work for information that can’t really be tied to specific positions in the image? For example maybe I want to provide the model with the time of day when the photo was taken. ",
        "I know, but everything I would find related only to OUTPUTTING multiple modes. Like generating a segmentation mask and a depth map. ",
        "Multimodal Large Language Model\n\nVision Language Model\n\nClip- https://arxiv.org/abs/2103.00020\n\nLLaVa\n\nA lot more I either can't remember, or that aren't truly important for basics. Medical images use what you're describing a lot though, because textual information about medical diagnosis seems to aid models in learning to segment and diagnose over images alone.",
        "A lot of the modern cv techniques uses that. You can take a look at depth anything v2 for example to get an idea. Dino also uses transformers",
        "Yup. Again, look at the diffusion models. They pass in the amount of noise added to the image globally. ",
        "You're searching in the wrong area; the most accessible multimodal machine learning model-ChatGPT-literally does what you're asking for. It can take in both image and text inputs and process them in parallel. Look into using transformers.",
        "Thanks! Also that’s a good tip to look for medical imaging research. I can definitely see why extra data would be essential there (patient history, lab results, etc)",
        "Oh I see. Thanks!"
    ]
},
{
    "submission_id": "1g4htno",
    "title": "Is this a reasonable way to choose the best model?",
    "selftext": "Hi! I'm working with YOLOs for a project about autonomous navigation and I have two datasets, similar but coming from different sources (A: 11.5k images from multiple contributors, B: 6.5k images from single one. 5 classes.). As of today, I started training a YOLOv8n with different data in order to get best result, then I plan to keep data fixed and evaluate other versions (v10 at least). \nI have the following three models:\n\nBasic choice:\n1. Train: A' (80%), Val: A' (10%), Test: A' (10%)\n\nMore training data but same val/test set as before:\n2. Train: A'+B* (85%), Val: A' (7.5%), Test: A' (7.5%) [B* is subset of B with distribution similar to A]\n\nMore balanced split (but different test set):\n3. Train: A\"+B*' (80%), Val: A\"+B*\" (10%), Test: A\" (10%)\n\n1 already gives good results, but adding more data should be positive. However, since 3 is tested on different images than 1,2 (but still same scenario...), I can't really compare results.\nThe idea I had is to use 2.8k images from [B-B*] and then compare, but the thing is those images have really unbalanced instances of the 5 classes (reason why I filtered them out) and I'm not sure the test gives usable result. I'm quite confident it is okay since I just have to have a winner, but my tutor disappeared and I don't want to be stuck.\n\nAppreciate any help, thanks!",
    "created_utc": "2024-10-15T13:40:06",
    "num_comments": 3,
    "comments": [
        "I had a kind of similar situation, in which I had a high quality source A and a low quality source B, but B being closer to what the model would actually encounter in real world use.\n\nI tried several different things, from using only A or only B to mixing them up and sampling train,test,val from the uniform mix. The conclusion from my experiments, and from what I see in papers, from other's experiments too, is that more data is not always better; in particular when the added data is of low quality. \n\nWhat I ended up doing to test the dataset is crafting a test set that is as representative of the real world use cases as possible. To do that I played with the seed for train_test_split and monitored both some statistics such as class balance and the quality of the images until I was happy with the result. \n\nThen I used my cherry picked test split to test different combinations of A and B. (I use the term *test split* but it is actually a *validation split* since I use it to adjust hyper parameters)\n\nBtw I was also using Yolov8, but had significantly less images",
        "Use robust models for real-life deployment. When You re not sure which model is the best(not enough robustness), use ensembly learning. In this way You can combine multiple models and get a better accuracy (compared to individually).\nThis is the approach for the industry and for advanced research.",
        "Seconding this. I've found a lot of success using ensembles of models. If  the models disagree on a datapoint  that is a good datapoint to manually review and feed back into the training process.\n\nAnother thing you can do is to use the model(s) to re-label your datasets. This is a way to combine them datasets into one. It of course will make some mistakes and you can manually fix those."
    ]
},
{
    "submission_id": "1g4ghoz",
    "title": "\"Automated\" camera calibration?",
    "selftext": "I am only getting into Computer Vision but I am doing a master program closely related to it. I am wondering about the concept of camera calibration, as I understand it, it is to translate camera images to a coordinate system, giving some idea of absolute distances, e.g. between objects.\n\nAlso, it seems like this is a hard topic, as camera calibration seems to require some extra setup to make it work. I saw some example with a checkerboard where I assume you learn the distances between the checkerboard \"checks\" and then you can use this to understand the actual distances of the images.\n\nI am wondering if there is any research or progress in the field of calibration for CV. For example, if we can assume at least that the camera will be static, can we deduce distances between objects with some heuristics?",
    "created_utc": "2024-10-15T12:43:48",
    "num_comments": 4,
    "comments": [
        ">I am only getting into Computer Vision but I am doing a master program closely related to it. I am wondering about the concept of camera calibration, as I understand it, it is to translate camera images to a coordinate system, giving some idea of absolute distances, e.g. between objects.\n\nCameras measure in only 2d i.e. only down to a projection, no matter how they are calibrated. To get any 3d information, you need additional information e.g. an additional camera viewing from a different position. Else you can't tell the difference between small and far away (https://www.youtube.com/watch?v=MMiKyfd6hA0)\n\n>Also, it seems like this is a hard topic, as camera calibration seems to require some extra setup to make it work. I saw some example with a checkerboard where I assume you learn the distances between the checkerboard \"checks\" and then you can use this to understand the actual distances of the images.\n\nThis is kind of the easy part and a reasonably well solved problem. It's a common and not too difficult tasks to calibrate cameras with techniques like this to subpixel accuracy.\n\n>I am wondering if there is any research or progress in the field of calibration for CV. For example, if we can assume at least that the camera will be static, can we deduce distances between objects with some heuristics?\n\nThis is called monocular depth estimation. It needs some additional heuristics and is pretty difficult and I wouldn't consider it accurate at all (in a metrological/surveying sense). It's kind of a separate topic from traditional photogrammetry though.\n\n[https://www.robots.ox.ac.uk/\\~vgg/hzbook/](https://www.robots.ox.ac.uk/~vgg/hzbook/) this is the goto book on the topic, there are others",
        "The main goal of camera calibration is to find the intrinsics, extrinsics and distortion of your cameras. If you want to find out absolute distance you’ll need depth map to create 3d point cloud, do triangulation or some type of slam",
        ">monocular depth estimation\n\nThis gives me a good lead to look into. Besides that, let me clarify what I meant with my second paragraph: I understand it is fairly trivial to do and to make it accurate, but my point was that it seems a bit complex to have to calibrate in this way. In other words, calibration isn't trivial in contexts where we're not able to setup the calibration with the likes of checkerboards.\n\nExample: I have video images of a football pitch, and I want to deduce the length between the players. *Maybe* I can apply some logic that we know the 16-meter box is indeed 16-meter, and then use that as the \"calibration atom\", but I'd imagine it's not that easy.\n\nHarder example: I have video images of a person running on a field of grass (but no football pitch). There's nothing that indicates any size or length. Are there any heuristics that can determine, even if wildly inaccurate, anything about the absolute distances of these video images?",
        "OK I think there are kind of two \"subproblems\" in there: the classical \"camera calibration\" that you commonly use a checkerboard for is for accurately determining the optical properties of the camera e.g. its focal length, which is needed for later 3d reconstruction, and is more or less a prerequisite for any of this kind of computer vision task (or at least something that has to be solved as part of the problem). The second aspect is more like pose-estimation i.e. working out where the camera is given other information.\n\nFor your simpler example: I'm not aware off the top of my head of any fully out of the box solution for this, but certainly it's mathematically possible. You need a certain amount of extra information, but you have that from e.g. field markings. Essentially you identify certain points in the camera, identify their coordinates in the real-world, then you write down all the equations for camera projection, and solve a big system of equations. A related problem to determine the camera position from some knowledge about the scene is e.g. \"PnP\" [https://en.wikipedia.org/wiki/Perspective-n-Point](https://en.wikipedia.org/wiki/Perspective-n-Point) \n\nIn the second case it's much harder. If you really have no references, it's just plain impossible. However, in the case of a football player, you do at least have something: you know they have a height of e.g. 1.8m and certain body proportions and so on. You know how physics works. You know they e.g. have to touch the floor while running. In this specific case, this is human pose estimation, there are various libraries for this. [https://paperswithcode.com/task/pose-estimation](https://paperswithcode.com/task/pose-estimation)"
    ]
},
{
    "submission_id": "1g4ghcv",
    "title": "labelme: Is there a way to maintain viewport the same in between different images?",
    "selftext": "Hello, I'm currently working in a project where I'm annotating microcospe slides using labelme. \n\nThe issue I'm having is the following: I deal with different Focuses (each a different image), meaning that the same element will be in the same place throughout all of the images. However, when I go from an image to the other the viewport resets and I get lost on where I was, these images are huge... is there a way to maintain it from image to image?",
    "created_utc": "2024-10-15T12:43:23",
    "num_comments": 2,
    "comments": [
        "I haven’t looked through the code in depth, but I imagine it wouldn’t be too difficult to call whatever function normally sets the viewport, passing it your own arguments. That is unless it’s not exposed by Qt. \n\nMaybe start here: https://github.com/wkentaro/labelme/blob/main/labelme/app.py",
        "that seems like an interesting idea, imma take a look into it! thanks"
    ]
},
{
    "submission_id": "1g4apho",
    "title": "Eye contact correction with LivePortrait",
    "selftext": "",
    "created_utc": "2024-10-15T08:40:39",
    "num_comments": 15,
    "comments": [
        "Homie needs to blink",
        "This is very uncanny valley",
        "It might be because he's reading without looking but the one in the right looks a bit weird",
        "Just wear some glasses with googly eyes stuck on the front\n\n9/10 Tinder dates can't tell I'm asleep...",
        "Try a more realistic use case.\n\nIn this example the eyes in the input video are unrealistically static with no blinking. As a result the process is very easy... Actually a vfx artist could have done this much faster without any ML.",
        "Over a year ago, there were some impressive demos of this, but many required complex software or were too slow. We took inspiration from NVIDIA's Broadcast and LivePortrait's facial expression control to build this pipeline.\n\nIt makes use of some of the similar inverse transforms you might expect to isolate the eye region of the face and then makes use of LivePortrait’s regional control on top.\n\nRead about LivePortrait region control [here](https://github.com/KwaiVGI/LivePortrait/blob/main/assets/docs/changelog/2024-08-19.md).\n\nTry the API playground [here](https://www.sievedata.com/functions/sieve/eye-contact-correction).\n\nThought this would be a fun project to share with the community!",
        "Comfyui implementation please. Best would be adding option to existing liveportrait implementation.",
        "It's subtle but very convincing.",
        "it's interesting...",
        "which one is the corrected one?",
        "you can explore a bunch of the options. there's an \"enable\\_look\\_away\" option and a few other things you can tweak.",
        "I much disagree. It's pretty damn convincing",
        "How is it better than NVIDIA Broadcast? Seems similar at least at first glance.",
        "it is mentioned in the video.\n\nthe right side, where the eyes are directed towards the camera.",
        "kind of cool that you couldn't tell :)\n\nbut yes the corrected one is on the right"
    ]
},
{
    "submission_id": "1g48vjl",
    "title": "How do you personally deal with ambiguous annotation ?",
    "selftext": "I found this paper of *Schwirten & co* : [Ambiguous Annotations: When is a Pedestrian not a Pedestrian?](https://arxiv.org/html/2405.08794v1#S3.F1.sf1)  very interesting and I wanted to have a discussion about how you also deal with ambiguous annotations.\n\nAs a picture is worth a thousand words:\n\n[Image from \\\\\"Ambiguous Annotations: When is a Pedestrian not a Pedestrian?\\\\\"](https://preview.redd.it/xvotygs8exud1.png?width=831&format=png&auto=webp&s=abd7f17844f0eba6cdee2883cad79dec96168797)\n\nI also personally have a use case where: \n\n* if pedestrian is there with no/low ambiguity, I should detect it.   \n* If it's there with medium to high ambiguity, I don't care if I detect it or not (FP and FN are both acceptable)\n* If pedestrian is Not there with no ambiguity, I should Not detect it\n\nIn my use case, I calculated the inter-observer variability and saw my annotators have up to 40% difference in the way they annotate ambiguous images. But they also don't have the same definition of \"ambiguous images\".\n\nI've seen some people use soft labelling. Some people use multi-stage annotation campaign with an agreement system, or skip every ambiguous images.\n\nIf you have experience working with this kind of usecase I would be interested if you could share it ! \n\n",
    "created_utc": "2024-10-15T07:21:37",
    "num_comments": 15,
    "comments": [
        "Most of the time I provide the annotators a set of guidelines where I describe how they should address those use-cases. I tend to pick the choice that minimises the inter-observer variability (the less they have to think when annotating, the best). Obviously writing the perfect guidelines is not possible, so my check-list usually goes like this:\n- Use a tool that allows you to do some kind of review for each annotated asset (multi-stage annotation, possibility to raise issues, consensus, possibility to skip the asset, etc).\n- Provide clear guidelines to the annotators on they should handle occlusion, ambiguity, etc. Illustrate the guidelines with as many examples as needed.\n- Sometimes, once the dataset is annotated, I also use additional tools such as Encord or Voxel51 to compute embedding for each annotated image and/or bounding box/polygon. It’s usually faster to detect if an annotator didn’t follow my guidelines by looking at the outliers (instead of each annotation one by one).\n\nIn the end, I second what has already been said: « make a decision and live with it », as long as it’s consistent. The worst thing to do is to change your guidelines during the annotation phase.\n\nGood luck !\n\n-",
        "Make a decision and live with it",
        "My models benefit a lot from omitting ambiguous data. This is interesting to see. I’ve been wondering how many others have found the same thing\nEdit spelling",
        "I've done some annotation and tracking of pedestrians. Imo a good way is to look at videos and backward or forward track the pedestrian in time to be certain. What also causes ambiguity is occlusion, like shown in one of the images, and the definition of how to annotate partially occluded objects. Annotating partial objects works better for object detection, approximate occlusion of the full sized object is better in the case of tracking.",
        "What I have done in the past for object detection is to train the model on clear data and then run it on ambiguous data. If it detects it, I just let it be. If it doesn't, I don't make it.\n\nThe idea is to use the model's output to judge whether the features it learnt from clear data works with ambiguous images, so that the labeling is not adversarial.",
        "At our company, we tackle ambiguous annotations by implementing strict guidelines and ensuring our annotators receive proper training to handle edge cases. We also use a consensus-based approach, where multiple annotators work on the same data, and we use the most agreed-upon label. For more complex cases, we integrate AI-assisted tools to speed up the process while maintaining high accuracy. This combination helps us maintain quality while efficiently managing large-scale annotation projects. How do other teams handle ambiguity in their workflows?",
        "I am currently working with CV for the medical domain, and subjective evaluations are all over the place. 30% inter-truther variability is just another Wednesday. In our case, annotators are also reluctant to follow strict annotation guidelines. Those are trained health professionals, they will dispute your definitions. \n\n  \nWe haven't found a silver bullet solution, but I have seen some strategies that help mitigate that.  \n1. Instead of giving them a strict definition, do the opposite, and allow them to follow their own definitions. Additionally, provide a harmonization mechanism. For instance, when detecting objects (eg cancerous nodules) instead of asking them to only identify the cancerous ones, tell them to detect anything they find relevant and provide them with some categories you can use for filtering at a later stage. In the nodules case, those categories could be things like composition, malignancy, speculation, etc.\n\n2. 2+1 annotation flow. I've learned that the FDA has been recommending this format. Two annotators will detect/segment/classify your data and a third more experienced one will choose which annotation is the most correct. This will bias the annotations towards the adjudicator but helps reduce inconsistencies.\n\n  \n3. Instead of measuring performance on a metric such as IoU or AP, measure if your model is statistically non-inferior to the annotators. In that way, if the task in inherently subjective, you can show your model is trust-worthy because its predictions are just as good as of a human.  OFC, that assumes a human would be good enough for the task.",
        "There is a paper in the crowd counting literature where they treat the annotations as bayesian point estimators. This might be something that could be applied to your usecase as well:\n\nhttps://arxiv.org/abs/1908.03684",
        "Really depends on the changes. I sometimes append new info to the guidelines in the middle of the project if one of annotators gets a new edge case which isn't covered by the guidelines. This way you're future proofing it, piece by piece",
        "Okay so you went for an approach of \"I know it is a pedestrian, because I watched the video\" even if in a particular image it might be difficult for the model to tell. Did it show better result or did it confuse the model ?",
        "so what kind of metrics do you use if it detects ambiguous data that was not annotated ? skip FP ?",
        "Did you manage to propagate the new guideline  to the already annotated assets without starting the annotation process over?",
        "I have never seen a project where the guidelines were not amended at some point during the annotation process. Data always has some surprises.",
        "In my case, the first training would not have ambiguous data.\n\nThere would be ambiguous data in the second training, but it would be labeled in a way to reinforce the previous model's predictions. I don't use any separate metrics to account for FPs related to ambiguous data. I think reinforcing the previous model's predictions on such instances would minimize such cases.",
        "These changes are relatively minor and do not affect already existing annotations. It's more like a discovery of something completely new which isn't covered in the guidelines, so you add a new a bullet point with info how such cases should be handled from now on."
    ]
},
{
    "submission_id": "1g47muu",
    "title": "GigE Error with OpenCV and Python 3.11 ",
    "selftext": "Hello!\n\nI'm using a DALSA linea C4096-7. I'm trying to connect to the camera via opencv but without success. I'm using the brand's SDK, sapera LT v8.31.\n\nThe error I get is as follows.\n\n`[ WARN:0@170.390] global cap_ffmpeg_impl.hpp:453 _opencv_ffmpeg_interrupt_callback Stream timeout triggered after 30076.866000 ms`\n\nHas anyone ever encountered this kind of problem?",
    "created_utc": "2024-10-15T06:24:33",
    "num_comments": 9,
    "comments": [
        "You have to download ffmpeg executable and add to the opencv path. The \"pip install\" version of opencv doesn't come with ffmpeg executable.",
        "do you have some wiki whit that ?",
        "is this basically what you do?\n\n[https://www.youtube.com/watch?v=JR36oH35Fgg](https://www.youtube.com/watch?v=JR36oH35Fgg)",
        "Pretty much yes. Or you can provide the path to opencv, if you want to keep ffmpeg local.",
        "ok but i install opencv in pip and use a venv",
        "I mean provide the ffmpeg path to opencv. \n\n```\nimport cv2\nimport os\n\n# Set the FFmpeg binary path\nffmpeg_path = \"C:/path/to/your/ffmpeg.exe\"  # Update this path to your FFmpeg executable\n\n# Set the environment variable\nos.environ[\"OPENCV_FFMPEG_CAPTURE_OPTIONS\"] = ffmpeg_path\n\n# Test video read/write functionality\ncap = cv2.VideoCapture(\"video_file.mp4\", cv2.CAP_FFMPEG)\n\nif cap.isOpened():\n    print(\"FFmpeg successfully set and video opened.\")\nelse:\n    print(\"Failed to open video. Check FFmpeg path.\")\n```",
        "I tried it and I got the same error... instead of the \"video\\_file.mp4\" I used a variable to go to the IP of the camera",
        "Can you access the stream using the other way? Is there an authentication or something?",
        "There is no authentication. I don't know if the SDK has something to do with it or if I need to open access to the camera?"
    ]
},
{
    "submission_id": "1g44qou",
    "title": "Train Loss > Val Loss",
    "selftext": "hello, i have trained yolo v8. the loss results are as follows the loss of val is one step lower than the loss of train. when i look on the internet it says that this is so because normalazotn is used, i wonder if this could badly affect the result of my model or what should i do to avoid this.\n\nval loss with dot dot val loss with straight line train loss\n\n[cls loss](https://preview.redd.it/cmq7wb97gwud1.png?width=1839&format=png&auto=webp&s=b243f5512c4d8a61592a11d4910464cba0714dd3)\n\n[dfl loss](https://preview.redd.it/82mu6d8agwud1.png?width=1842&format=png&auto=webp&s=16b129141a8f844483d8ad7a01b97dce755fc1e6)\n\n[box loss](https://preview.redd.it/1aep2gragwud1.png?width=1847&format=png&auto=webp&s=96598fcabc9dde81904f70eb1232e1b1bb174754)\n\n  \n",
    "created_utc": "2024-10-15T03:45:21",
    "num_comments": 1,
    "comments": [
        "Till train loss > val loss, it indicates that you can train your model for further epochs, without losing much generalisability."
    ]
},
{
    "submission_id": "1g44qdb",
    "title": "Suggestions on Fast Referring Expression Segmentation Models?",
    "selftext": "I have been doing a landscape scan on available models thus far, and I can't seem to find a prominent model that is known to be fast and effective at the same time.\n\nI have been mainly focusing on papers from the following resources:\n\n* [https://paperswithcode.com/task/referring-expression-segmentation](https://paperswithcode.com/task/referring-expression-segmentation)\n* [https://github.com/Qinying-Liu/Awesome-Open-Vocabulary-Semantic-Segmentation?tab=readme-ov-file#referring-image-segmentation](https://github.com/Qinying-Liu/Awesome-Open-Vocabulary-Semantic-Segmentation?tab=readme-ov-file#referring-image-segmentation)\n* [https://github.com/MarkMoHR/Awesome-Referring-Image-Segmentation?tab=readme-ov-file#3-traditional-referring-image-segmentation](https://github.com/MarkMoHR/Awesome-Referring-Image-Segmentation?tab=readme-ov-file#3-traditional-referring-image-segmentation)\n\n[EVF-SAM](https://github.com/hustvl/evf-sam) which is currently one of the best performing models on benchmarks, but it mentions \"EVF-SAM is designed for efficient computation, enabling rapid inference in few seconds per image on a T4 GPU.\" That is still relatively slow for me, ideally I would like to find the FastSAM/MobileSAM equivalent (they are not capable of Referring Expression Comprehension, unfortunately).\n\nI am looking for efficient ways to detect \"items in hand\" which can vary from person to person. It should have a clear outline/mask of the item being held - imagine big paper boxes, a cup of coffee, trash bag, mop, shopping bags.\n\nIf you have any other suggestion on how I may achieve this specific task for my project in an efficient way, would love to hear your thoughts as well.",
    "created_utc": "2024-10-15T03:44:45",
    "num_comments": 0,
    "comments": []
},
{
    "submission_id": "1g43f5t",
    "title": "Using Google, OpenAI api for object detection",
    "selftext": "For a potential product I’m making to release to the market, which needs to do object detection, I’m using openai for object detection to create a product faster and release to the market, and while i build a comp vision model from scratch and build my own dataset, until it has a good accuracy. Is it okay to use openai api for this for a product going in the market ?",
    "created_utc": "2024-10-15T02:08:02",
    "num_comments": 1,
    "comments": []
},
{
    "submission_id": "1g42xz0",
    "title": "Extracting data from a graph",
    "selftext": "I would like to extract for each player, the times at which they played (gray area).   \nI have not managed to think of a way of doing it.   \nAny ideas are really appreciated.\n\nThank you!\n\nhttps://preview.redd.it/2o3x21kprvud1.png?width=1143&format=png&auto=webp&s=fcc0af11c4af3c6978237acbbe9130be01c76a6d\n\n",
    "created_utc": "2024-10-15T01:28:55",
    "num_comments": 6,
    "comments": [
        "Does grey mean on? Just count the pixels",
        "I’m working on something similar. Well, sort of similar, anyway. What architectures are you exploring? \n\nI think ViTDet is a good place to start. I would say try a SigLIP-so400m (SoViT) but I’m not so sure it’s going to be the best option for really detailed graphs or charts.\n\nI’ve tested the GOT_OCR_2.0 model and most of the time the structure of the chart/graph is captured, but extracting and placing the data doesn’t happen. It’s just too fine grained.\n\nI’m considering using a VitDet-Vit-B encoder but dropping the patch size from standard 16 to 8x8 or 12x12. I know MiniCPM-V 2.6 is using 14x14, but they’re using a SigLIP architecture - I think. Still, the model doesn’t capture or extract the level of detail required for a graph or chart like that. \n\nLet me know what you find! I’ll share my results soonish. Good luck.",
        "Have you tried just running the images through the GOT_OCR_2.0 model? Or the MiniCPM-V 2.6 model? That might get you started. They’re both pretty good.\n\nI’m working on building a similar model using either the ViT-B/L backbones and ViTDet structure. I’m reading an adaptive granularity paper now to see if it will be useful in the task. I also think one reason GOT_OCR_2.0 is so good is because of the lack of actual token compression. They do some really simple downsizing, but they don’t compress the tokens in the same way MiniCPM-V does. \n\nI will keep you updated. \n\nAlso, you could probably fine-tune either of those models I’ve mentioned, use an image preprocessor to make the inputs high resolution (PIL, etc.) and get decent results.",
        "I had no idea really on where to start! Anything is an improvent on what I have!",
        "Thanks, will soon and also follow how you are going!",
        "Hey. I’m working on it now. I’ve used the GOT_OVR as a starting point. My version is currently a little more computationally intensive, but I might be able to fix it. I’ve optimized the attention mechanisms to be a little more efficient.\n\nI’ve also made sure to make the entire model device agnostic. So, I’m building locally on my MacBook using MPS for testing and it’s going well. Once in a while it will fail for OOM; I’ll switch to CPU to finish the test.\n\nI’m upgrading quite a bit. I think it will work. \n\nIf you have datasets for that specific task - I’d love access. I’d obviously share the dataset with appropriate credit, or keep it private if you like. \n\nI’ll keep you posted."
    ]
},
{
    "submission_id": "1g42smo",
    "title": "CCMA: Model-free and Precise Path Smoothing [2D/3D]",
    "selftext": "",
    "created_utc": "2024-10-15T01:16:08",
    "num_comments": 1,
    "comments": [
        "If you find this helpful—the code for the CCMA is freely available: [https://github.com/UniBwTAS/ccma](https://github.com/UniBwTAS/ccma)"
    ]
},
{
    "submission_id": "1g41ori",
    "title": "Camera suggestions ",
    "selftext": "Suggest me a camera or ip camera or cctv project from which I can directly access the footage to the project without any requirements.Should be cheap too if possible ",
    "created_utc": "2024-10-14T23:47:47",
    "num_comments": 1,
    "comments": [
        "This series would be suitable for you: [https://www.e-consystems.com/gige-cameras.asp](https://www.e-consystems.com/gige-cameras.asp)"
    ]
},
{
    "submission_id": "1g41cok",
    "title": "Fire detection model training approach",
    "selftext": "Hi all, I am planning to train a yolov11 model to detect fires and have decided to train it using the FASDD smoke and fire dataset, particularly the FASDD_CV and FASDD_UAV. I was just wondering which approach is better:\n\n1. Train the yolov11 model on FASDD_CV first then perform transfer learning to get the model to detect fires from a UAV angle as well using the FASDD_UAV dataset.\n\n2. Combine the 2 datasets and just train the yolo model on the combined dataset.",
    "created_utc": "2024-10-14T23:22:50",
    "num_comments": 6,
    "comments": [
        "What angle will you be inferencing at? They look pretty different and have different applications. Why do you think you would need to train on both?",
        "Very cool. What tool will you be using to combine the data sets?",
        "I was just thinking about creating a model that is more robust, i.e able to look at a fire in different settings be it enclosed spaces or aerial views.",
        "Will be using python to manipulate the files so there is a good mix of CV/UAV in the train/val/test split",
        "IMO, you shouldn't be using the same model for aerial and normal views. You should have separate models for them. I doubt you will encounter both images in the same deployment such that the same model needs to be robust on both.",
        "Good point, I’ll train it on the CV dataset first"
    ]
},
{
    "submission_id": "1g3z1ey",
    "title": "Revealing obscured objects using principles of vision (no DL)",
    "selftext": "When merging multiple images of the same (planar) scene taken from different viewpoints, it is well known that disruptive visual artifacts occur if, for example, the planarity of the objects does not hold true.\n\nSurprisingly, exploiting this artifact can create see-through effects that enhance the visibility of in-focus objects, even when they are significantly obscured by out-of-focus elements. This technique is particularly valuable in search-and-rescue operations and ground fire detection, where RGB or thermal signals may be obscured by trees or foliage. For instance, placing the target plane near the ground (in-focus) reduces the impact of trees and foliage (out-of-focus) on the integrated image, enhancing detection rates despite visual obstructions.\n\nI'd like to share a brief summary along with a toy search-and-rescue scenario that illustrates this effect and is also enjoyable to experiment :). The code is kept simple and should be easy to comprehend.\n\n[Revealing a heavily obscured object by exploiting out-of-focus properties in image stiching](https://preview.redd.it/exrdud5jduud1.png?width=757&format=png&auto=webp&s=b3bb28e6d2039295b435708bde13975870714c94)\n\nRelevant Links\n\n* **Main project:** [https://github.com/cheind/image-stitch/](https://github.com/cheind/image-stitch/) \n* **Stitching fundamentals**: [https://github.com/cheind/image-stitch/blob/a3331c443e5c483feb33658b518ecf2ba4aec091/PlanarImageStitching.md](https://github.com/cheind/image-stitch/blob/a3331c443e5c483feb33658b518ecf2ba4aec091/PlanarImageStitching.md)\n* **Rubber-duck rescue:** [https://github.com/cheind/image-stitch/blob/a3331c443e5c483feb33658b518ecf2ba4aec091/OutOfFocusAnalysis.md](https://github.com/cheind/image-stitch/blob/a3331c443e5c483feb33658b518ecf2ba4aec091/OutOfFocusAnalysis.md)",
    "created_utc": "2024-10-14T20:52:15",
    "num_comments": 0,
    "comments": []
},
{
    "submission_id": "1g3w272",
    "title": "Easiest way to implement dense slam?",
    "selftext": "So I have been working on a project to create 3d models real time and I have a pretty good visual odometery system built.  Using pose data and RGB-D data what would be the easiest way to generate a point cloud?\n\nThanks",
    "created_utc": "2024-10-14T18:14:15",
    "num_comments": 1,
    "comments": [
        "Try this [tutorial](https://learnopencv.com/monocular-slam-in-python/) to start.  Or, if you prefer C++, that's the way to go.   I'm sure there are also C++ OpenCV tutorials for libraries that way."
    ]
},
{
    "submission_id": "1g3nw8j",
    "title": "How to prevent partial object detection?",
    "selftext": "I'm currently training object detection models using YOLOv8 from Ultralytics. One of my specific use cases requires that we **do not detect partially visible objects**. If even a small part of the object is missing or blocked, I want the model to **ignore it** and not make a detection.\n\n  \nTo give a simple example, let’s say I’m trying to detect stars. If a small part of one star’s arm is not visible in an image, I wouldn't want the model to detect it. However, the model currently gives very high confidence (90%+) for these partially blocked objects.\n\nhttps://preview.redd.it/pt6ng28sorud1.png?width=410&format=png&auto=webp&s=c1942fa88392bdd3092de9ad3d5967d230acf9fb\n\nI considered adding these partially blocked objects as negative samples in my training/test sets, but they are **infrequent** in my dataset, and collecting more examples is challenging.\n\nI’ve experimented with **automatic augmentation**, where I randomly crop parts of labeled objects to simulate partially visible objects. I added these augmented images as negative samples (with no label) so that the model would learn not to detect them. This has helped **somewhat**, but I still get too many false positives when real partially blocked objects appear.\n\nSince the objects vary in size, shape, and orientation, using box size as a filter doesn’t help. I’m also planning to turn off certain augmentations (like mosaic) in the YOLOv8 config to see if that makes a difference, but I’m stumped on what else to try.\n\nDoes anyone have advice on how to improve this further? \n\n  \n",
    "created_utc": "2024-10-14T12:07:29",
    "num_comments": 15,
    "comments": [
        "I think you’re on the right track with preventing Ultralytic’s augmentation from generating partial samples, and also supplementing with your own partial samples that you intentionally don’t label. \n\nYou could also try labelling your partials as a separate class named “partial” and see if that helps. \n\nDo keep in mind that most OD models are designed intentionally to detect partials so you’re fighting against that. May have to come up with a separate method. You could try a simple classification model that works on the cropped area, possibly. ",
        "Post Detection Route - After detection, find a method that will feedback the completeness of the object. For example the star, you could do a key point detection to see if the number of key points is present or if your camera is at a fixed focal length and your objects are the same size, then discard objects with a smaller box area. You'll need to get creative and I cannot advise without more information. \n\nTraining Route - You'll need to add more negative samples, think of the negative samples as its own class. You should balance the number of negative samples with the number of positive ones to force the model to converge on a proper discrimination setting. Things like removing mosaic will be helpful as mosaic will partially block objects (which work against you, you  already know this though). Other than that, there is no easy way around this. Object detection is data centric, the model will learn what you give it. If you only have 1 negative sample and 100x positive samples, it won't be able to discriminate properly.",
        "After detection you can do segmentation and compare with the full shape then discard the partial ones",
        "Hi there, I guess I would try those approaches:\n- As suggested by others, try to annotate those partial objects with a dedicated « partial » label.\n- Otherwise, annotate all the objects (including the partials) under the same label. Train the model. Then, during the inference, add a post-processing step that classifies each detected bounding box. I would say it depends on the object you’re trying to detect, but I guess this classification step can be either implemented using a naive approach or maybe train a classification model, trained only on those extracted objects (generating a dataset here from your main dataset should be easy). Obviously this additional post-processing step depends on many other constraints related to your application and business.\nGood luck!",
        "I would concur the keypoint detection suggestion. The keypoints have a visibility flag which is used to indicate whether they are visible or not. So you can use that to determine whether it's partially visible (one or more keypoints are missing) or fully visible. It's also useful because you don't need to worry about disabling augmentations for this. `ultralytics` would automatically mark keypoints that are out of view with visibility 0. This would help reduce false negatives.",
        "I'm dealing with a problem very much in this vein. To give an analogy, I'm looking at windows. I want to class them as open_window or closed_window. If I see enough of a window, I can cleanly bounding-box them during annotation.  However, if it is only partially visible or otherwise obscure, I have an \"unknown_window\" class label.\n\nMy problem is further complicated by the relative sparseness of the \"open_window\" class, by the objects being small in the images, and some less-than-ideal lighting conditions.  All have tricks to address them, but there is going to be some serious parameter tuning",
        "Thank you for the response!\n\nI will first go ahead with labeling the negative samples as their own class, since I can easily automatically annotate the cropped ones I am doing, and also fairly easily label the real examples in my dataset since there are not many of these examples. \n\nRegarding the separate classification model, won't it also possibly suffer from a similar issue as the object detection model, and classify these partials as being 'complete' objects?",
        "Hi, thanks for the reply! Key point detection is an interesting suggestion; I will evaluate how feasible it is to annotate the whole dataset with key points. \n\nSo is labeling the negative samples as their own class better than leaving them unlabeled? I was thinking that if we had to have a separate class for these negative samples, the model might struggle to actually learn it as a feature since this can be so diverse.",
        "You probably don’t need to create a class of incomplete stars, just increase the number of negative samples as if it were a class. Your on the right track with adding negative samples , I’m suggesting that you need to add more of them",
        "This is the best answer.. especially if you have uniform shapes and orientation",
        "Hi, thanks for the reply. Unfortunately, I do not have objects that are consistent in size or shape; they can fluctuate a lot in each image.",
        "Thabk you!",
        "Hi! I might have another project soon that sounds almost exactly this! Maybe we can update later on what approach/method worked best.\nBest of luck.",
        "The suggestion to train a separate classification model is mostly so that you’re not fighting against a model architecture that was intentionally designed to handle partial objects. \n\nBasically the classification model only has one job to do. ",
        "You can scale them to uniform length first and then compare to make it scale invariant"
    ]
},
{
    "submission_id": "1g3n2iz",
    "title": "FSD End to End AI Method",
    "selftext": "Do you guys think this will be the end all solution for unsupervised fsd? ",
    "created_utc": "2024-10-14T11:33:48",
    "num_comments": 3,
    "comments": [
        "No",
        "Yes",
        "Maybe"
    ]
},
{
    "submission_id": "1g3jv71",
    "title": "How to compute Average Perpendicular Distance (ADP) for a multi-class semantic segmentation?",
    "selftext": "I found from different papers this metric to evaluate the performance of a segmentation model (one of the papers is \"Evaluation framework for algorithms segmenting short axis cardiac MRI\"), but none of them explain how to compute it. They only describe what it is, I need some help to implement it! I expect a result in meters, which rapresent the mean distance between the real countours and the predicted ones. If someone have some experience with that, it will be nice.",
    "created_utc": "2024-10-14T09:24:08",
    "num_comments": 2,
    "comments": [
        "I would find the centroid, radially sweep it at some desired increment and find the first two intersections then take the difference and average that up",
        "Thanks for your answer. I have a true mask and a predicted one, I can find the centroid but I don't catch the part of \"radially sweep it and take the intersection\". My masks (true and predicted) are superposed so I have the intersection from the beginning, can you explain again your idea? Thanks in advance."
    ]
},
{
    "submission_id": "1g3ifmw",
    "title": "Pre-trained Model for Shadow Removal",
    "selftext": "Shadows in my images are causing issues for my object detection model. It seems to be getting confused by the shadows and mistaking them for objects.\n\nI'd like to pre-process the images using a pre-trained model before feeding them to my model, similar to how the [rembg ](https://github.com/danielgatis/rembg)library removes backgrounds . It doesn't have to be wrapped in a library, having the model file would also suffice.\n\nUnfortunately, I wasn't able to find a readily available solution, after looking through paperswithcode website. While I found [this project](https://github.com/IsHYuhi/ST-CGAN_Stacked_Conditional_Generative_Adversarial_Networks) ,  the download link for the model appears to be broken and the repository itself is a few years old.\n\nWould anyone be aware of similar models for shadow removal? Any suggestions would be greatly appreciated!",
    "created_utc": "2024-10-14T08:24:49",
    "num_comments": 1,
    "comments": [
        "The correct approach would be to add those images as background/null images in the training dataset. Involving another model is not a good idea."
    ]
},
{
    "submission_id": "1g3h29l",
    "title": "Python packages that decide if image likely contains text before running OCR on it?",
    "selftext": "Hey, OCR can sometimes be recourse intensive, is there a cheaper python package that can first decide if the image likely contains text before running OCR on he whole image?",
    "created_utc": "2024-10-14T07:27:03",
    "num_comments": 7,
    "comments": [
        "Virtually all OCR packages already do that; detection followed by recognition on the promising regions. You don’t have to reinvent the wheel.",
        "simplest solution for me was to run Paddle OCR's text line detection first, to see if it detects any text lines, before running any character recognition.",
        "Maybe just run tesseract on it? Or train a simple CNN.",
        "Maybe use ResNet / object detection to determine if the image contains a document like object first?",
        "MSER or EAST depending on efficiency/accuracy requirements. \n\nOpencv contrib has Extremal Region Filter algorithm too",
        "use EAST in OpenCV:  \nhttps://pyimagesearch.com/2018/08/20/opencv-text-detection-east-text-detector/",
        "try the CPU optimised version   \n[https://docs.openvino.ai/2024/notebooks/paddle-ocr-webcam-with-output.html](https://docs.openvino.ai/2024/notebooks/paddle-ocr-webcam-with-output.html)"
    ]
},
{
    "submission_id": "1g3h11v",
    "title": "I need computer vision and machine learning project ideas for a master's program thesis.",
    "selftext": "I'm learning the fundamentals of computer vision and machine learning. I've been trying to understand the working of some models and frameworks as a starting point (foundationpose/yolo/mobilenet/efficientnet/pytorch/tensorflow). I use my asus tuf a17 gaming laptop to run everything. I want to work on a project that can make me ready for an actual career role. So far, it's been difficult for me to figure out what's best. I've been thinking a bit about multimodal LLMs. If possible, then i want to do something practice-oriented & not theory-oriented.",
    "created_utc": "2024-10-14T07:25:39",
    "num_comments": 6,
    "comments": [
        "Maybe look at case studies published by established AI solution brands, that way you can tell which fields are actually being invested into in the real world. This will also help you in your career further down the line, tbh. For example, Gigabyte which makes AI servers has computer vision and machine learning stories in automotive, climate research, and healthcare. Give them a read, see if they inspire you:  \n\n\nAutomotive: https://www.gigabyte.com/Article/gigabyte-s-arm-server-boosts-development-of-smart-traffic-solution-by-200?lan=en\n\n\nClimate research: https://www.gigabyte.com/Article/decoding-the-storm-with-gigabyte-s-computing-cluster?lan=en\n\n\nHealthcare: https://www.gigabyte.com/Article/researching-cellular-aging-mechanisms-at-rey-juan-carlos-university?lan=en",
        "1. Diffusion Model to generate tiny stickers. \n(Data could be an issue, getting caption + stickers, Pokemon dataset exists though)\n\n\n2. ColPali to retrieve information out of visual documents. \n\n\n3. OCR in general is very good topic,  confluence of both Computer Vision and NLP. Will make career ready. ",
        "Got it! Thanks!",
        "Sir. I m a PhD student I m working on cereal crop disease. I wanna a recent CP, DL or ML techniques as FSL and Attention mechanism for image classification and detection \nCould you help me or give me ressource site",
        "These look interesting. I'm gonna check them out. Thanks!"
    ]
},
{
    "submission_id": "1g3gsr0",
    "title": "How do you verify if a model has learn a specific feature representation?",
    "selftext": "For context, I am reading CVPR papers. From these papers, I have a question \"how do the authors verify that the module is responsible for learning the feature representation?\". For example, I am reading \"2024-CVPR-*Training Like a Medical Resident: Context-Prior Learning Toward Universal Medical Image Segmentation*\". In this paper, the authors proposed \"context-prior learning\". I was wondering \"how do the authors verify that the model learn the context of an image and is there a way to visualize it?\"\n\nPS: I am in need of guidance, to be frank, I am looking for a supervisor",
    "created_utc": "2024-10-14T07:15:28",
    "num_comments": 2,
    "comments": [
        "What do you mean by ' a specific feature '",
        "I guess people plot the receptive fields"
    ]
},
{
    "submission_id": "1g3e743",
    "title": "Is there any face analysis API for this?",
    "selftext": "I'm working on a project requiring face analysis scores like wrinkles, Texture, Dark Circles, Oiliness & Eyebad. Do you know of any paid or free solution for this?",
    "created_utc": "2024-10-14T05:11:10",
    "num_comments": 3,
    "comments": [
        "Search something like ' face landmark/keypoint detection' . You may get some idea to solve your problem."
    ]
},
{
    "submission_id": "1g3bf5m",
    "title": "Need Help in Modification Architecture",
    "selftext": "I am currently struggling to implement an architecture that utilizes MobileNetV3 with a U-Net decoder. While both components are available in the [Segmentation Models Pytorch](https://github.com/qubvel-org/segmentation_models.pytorch) (SMP) library, I'm uncertain about how to properly add the CBAM (Convolutional Block Attention Module) after the last block of the encoder, as shown in the architecture diagram below:\n\nhttps://preview.redd.it/taggjmkuroud1.png?width=623&format=png&auto=webp&s=30a63ef79f0eca69352df316dd9085d2db4725fc\n\n  \nFor the CBAM implementation, I found this resource: [CBAM Implementation](https://github.com/Peachypie98/CBAM/blob/main/cbam.py), but I'm not sure how to integrate it into my model.\n\nAny guidance or suggestions would be greatly appreciated! \n\nP.S. I am a beginner and still learning.\n\n\n\n",
    "created_utc": "2024-10-14T01:57:38",
    "num_comments": 4,
    "comments": [
        "i am not a pro in this stuff but, the shape can be an issue later on if the didnt match together   \nlike the input and out shape of the up-conv",
        "the output of the last downsamling path is input of the CBAM and the output is the intput to the first upsampling path?, i am not an expert but what are you facing?",
        "still struggling",
        "u/WorthPreparation8313 yes I know this but struggling with the implementation. Shape / size should match and where should I make modifications in SMP library encoder part / decoder part?"
    ]
},
{
    "submission_id": "1g36mzx",
    "title": "Editing 3D scenes like ChatGPT",
    "selftext": "[https://github.com/Fangkang515/CE3D](https://github.com/Fangkang515/CE3D)\n\nWe have released the code for our **ECCV paper: Chat-Edit-3D**.\n\nWe utilize ChatGPT to drive nearly 30 AI models to enable 3D scene editing.\n\nIf you find it useful, please give our project a star!\n\nhttps://reddit.com/link/1g36mzx/video/klk62a3a0nud1/player\n\n",
    "created_utc": "2024-10-13T20:00:48",
    "num_comments": 0,
    "comments": []
},
{
    "submission_id": "1g369b3",
    "title": "Yolov11 Differentiate different products of different sizes",
    "selftext": "Hello, I'm currently developing a model that I'm training about products that have almost the same design but are different sizes. Is there any recommendation about how should I train my model or how can I differentiate them? They will be on shelves in the super market. I have some examples I will upload.\n\n[220g](https://preview.redd.it/tg3401hgwmud1.png?width=3024&format=png&auto=webp&s=263f35fbeb11a9c0b91244735ae4da67a85f458a)\n\n[410grams](https://preview.redd.it/9a81ljqhwmud1.png?width=3024&format=png&auto=webp&s=593a5842fbf38fb9601a8beaa2c04ba3027b2d00)\n\nThanks\n\n[shelf](https://preview.redd.it/2mmkcj1lwmud1.png?width=3024&format=png&auto=webp&s=1c1fddd83e8cc25de2d89ca672fd3cf214648c76)\n\n  \n",
    "created_utc": "2024-10-13T19:38:59",
    "num_comments": 16,
    "comments": [
        "It will work if you train with different sizes",
        "Ive done something similar, the model was able to differentiate between two different sizes of a cup that looked almost identical. You should be fine using two different classes in the same model.",
        "Easy unless the can and label look identical and the images that will be gathered are captured from varying distances. If the distance does vary you may need a model with global context, but the compute cost will 📈",
        "Banana for scale",
        "Solutions :- \n\n1. Different class same model works but has chances of failure. \n\n2. Count the number of cans that can fit in a shelf. ( using object detector) similar to car parking problem. \n\nLarger cans will be less in number than smaller ones. \n\n3. Fix your position of object and camera,  by this you will be able to get the size of the cans. \n\n4. You can also use sfm to get the size of object ( maths will be involved)",
        "You can train a yolo model to detect all cans and train a separate classifier like facenet to differentiate them",
        "Most common approach would be to train a detection model for each can, and assign each a unique label: corn small, corn medium, corn large, etc. \n\nIf that doesn’t work, here’s an interesting approach:\n\nFirst, apply a detection model to identify and extract bounding boxes from the image. Next, pass both the detected bounding boxes and the image into a segmentation model. Each segmented region is then processed by a classifier to determine the type of can.\n\nTo infer the can size, the maximum bounds of the segmentation masks are used. For instance, smaller cans of corn will exhibit different maximum bounding box sizes compared to larger ones.\n\nThis assumes you aren’t taking photos at extreme angles (where perspective and warp comes into play). \n\nMay or may not work, and it’s certainly not pretty, but worth a shot!",
        "These items have the exact same features. It will be very difficult to differentiate between the two. \nWhat you can do is measure the height of small and big containers and then check the height in pixels to check which item is small and which one is big.",
        "yoloNAS",
        "but should I do two different models? One to differentiate different objects and one for sizes? or just the same one?",
        "the can and label are almost identical. just the gram label is different, but the thing is since im taking it from shelves, the only difference should be the height.",
        "how does that work xd?",
        "Same model is ok",
        "Yolo is scale invariant",
        "Almost identical is not identical - model will do fine.",
        "Just kidding. If there is some hint for the model to pick up the size it will learn it. In this case, the diameter to height ratio is different. Just try it :)"
    ]
},
{
    "submission_id": "1g36313",
    "title": "Why some labeling tools like roboflow only provided polygon instead of more finer mask?",
    "selftext": "I find polygon not fiting my objects very well. Model I am using is SegFormer\n\n\n\neg\n\nhttps://preview.redd.it/3l4rkgowumud1.png?width=667&format=png&auto=webp&s=668f2c62c05753af015c12522b4035e966014d68",
    "created_utc": "2024-10-13T19:29:20",
    "num_comments": 2,
    "comments": [
        "In CVAT, you can hold Shift and use the cursor to draw a finer mask. Not sure about Roboflow.",
        "Can't really see what's going on in that screenshot but in Roboflow you can use the slider to make the polygon more or less detailed."
    ]
},
{
    "submission_id": "1g32jq7",
    "title": "Best Way to Make PDF Searchable (Contains English and French handwriting and characters)?",
    "selftext": "I am a newbie who is trying to make a large pdf searchable.  It is a very large medical file containing English and French typed text and handwriting.  I heard that Google Cloud Vision is the best for multilingual files and best for handwriting, but I don't understand how to use it in the console.  Isn't there a website for non-techie people to submit a pdf and get back a searchable pdf?  I just want accurate results- I don't mind paying.",
    "created_utc": "2024-10-13T16:18:27",
    "num_comments": 4,
    "comments": [
        "Hi OCRvision can help you to convert multi-language PDFs to searchable PDFs. It won't support handwritten text",
        "Could you convert pdf to markdown and search it as code?",
        "which tool is best for messy handwritten doctor notes?",
        "What does markdown mean"
    ]
},
{
    "submission_id": "1g327ul",
    "title": "Anxiety of Math",
    "selftext": "Hello, I’m currently in my first semester of a Master’s in Computer Engineering, and I’m 22 years old. After working in the industry for a year as an ai ml engineer, I’ve come back to academia and start to master degree. But im struggling to understand math in computer vision course.\n\nMy instructor has given us a list of key papers for the midterm, and one of them is *\"Distinctive Image Features from Scale-Invariant Keypoints\"* by Lowe. I’m having a realllyyyy hard time understanding the mathematical concepts, especially when it comes to things like the Laplacian formula, finite difference approximations, etc. I forgot things in calculus, differential equations shortly in math and now my anxiety is growing as I struggle to keep up.\n\nI’m wondering if anyone else has gone through a similar experience. Do others struggle with these concepts, or am Im just a fool and stayed behind? Is it too late to get back on track with this kind of math? I’d really appreciate any advice or reassurance from those who’ve been through this.\n\n",
    "created_utc": "2024-10-13T16:01:38",
    "num_comments": 10,
    "comments": [
        "It’s hard and confusing, all those papers are hard and overly confusing. Just immerse yourself in it, keep studying it and you’ll start to see its fundamental building blocks. Over time it will get easier",
        "Check this tedx talk by the legendary Khan(founder of Khan Academy)  https://youtu.be/-MTRxRO5SRA?si=pqpztdEFYuzMss-t . This method really helped me, as I really sucked at math, and this was an eye opening moment for me, and I started understanding and appreciate maths more after this talk. Essentially the idea is that whenever you feel the gap in concepts, you have to dedicate your time learning that concept so you acquire the knowledge specifically for that topic. \n\nThe main thing I also find important is identifying the concepts. Once you have a concept X, you may need to know Y and Z. So identifying this is important and it’ll boost your understanding for X.\n\nSo the next time you encounter it, when you read it, it will be very familiar, and you’ll be able to understand it.   Following this approach, I felt that you shouldn’t be ashamed to revisit very old concepts if you don’t understand it. If you doubt your skill in it, revisit it, build your foundation in it. So In your case, id suggest you to revisit calculus from start. There are so many good materials online that can help you understand this visually and it’ll help so much in the long run :) . I hope this helps, sharing my experience here, gonna sleep now :)",
        "In my experience, the math in papers are much simpler conceptually than the notation would have you believe. I would use 4o or Sonnet to help you understand the concepts behind the math, then the notation.",
        "Read about SIFT in the book \"Digital Image Processing\" by gonzalez and woods, it is much clearer than the paper. The math can be hard but once you understand how gaussians and their derivatives work you're all set",
        "Would you mind sharing all the key papers ya'll are reading? Love to do what are foundational papers to computer vision.",
        "Yeah, probably. If it wasnt hard that would be pointless",
        "Thanks, im currently using 4o. Which do you prefer for this level of science and math",
        "I cant paste all of them. there is a lot of them can you send me a private message?",
        "Pretty much on the same level I would say",
        "Check your dm. Thanks."
    ]
},
{
    "submission_id": "1g2yjpe",
    "title": "Operating industrial computer vision camera.",
    "selftext": "Hello,\n\nso far, I made my projects using consumer grade cameras, who nicely encode and compress the video in-body. I am now trying to set up a more professional system and I am looking at camera such as this one:\n\n[https://en.daheng-imaging.com/show-109-3089-1.html](https://en.daheng-imaging.com/show-109-3089-1.html)\n\nIts USB3 data interface, high resolution and rather high framerate (43fps), so I expect the stream to be quite large.\n\nThere is not much info on the data sheet. I understand that I will have to use a Genicam /USB3 Vision compatible control software. Then I could capture the stream with OpenCV and encode/compress the raw signal before writing to disk (I do not need real time inference BTW).\n\nI am trying to figure out about the hardware needed to handle it. If the video stream is raw, then it must be really large. If I were to use 4 of these cameras simultaneously, where would be the obvious bottleneck ? USB3 interface on the computer ? Loading on RAM (I have DDR5) ? I suppose that signal processing and compression can follow (since consumer grade cameras and smartphones does it with limited hardware).\n\n  \nSorry if my request feel a bit confused, this is my level now. Perhaps someone with experience can point me the pitfalls =)",
    "created_utc": "2024-10-13T13:07:33",
    "num_comments": 11,
    "comments": [
        "If you use Lucid Vision (big recommend) cameras, they have integrated jupyter into their own sw, so you can easily play around and export into your own projects.\n\nAnd yes, industrial cameras usually transmit raw data. With usb you should be mindful of cable length, as the drop-off is significant.",
        "With 4 of these cameras you will be generating close to 200Mb/s of data per second if I did the math right, which is not little. I do not now with USB3.0 in particular, but in general networking can be a major bottleneck, specially with larger setups and longer cables. Do you need cameras synchronization? This can make things significantly harder (NTP, PTP), and require fine tuning network/hardware settings to minimize frame losses can get complicated (e.g. use for networking CPUs with direct access to RAM and network interfaces, tweak buffer ring size...).\n\nSince you don't need real time processing, maybe directly writing to disk the raw data isn't a bad idea and could alleviate a little bit the processing required for storage, which could be another bottleneck. You could just read them as BAYER raw format at inference time. You will probably have to test what is faster in your specific hardware and use case, store raw or encode and store.\n\nIn general, these kind of setups are so hardware-dependent that is hard to predict what will be problematic, and everything boils down to testing and measuring. There can be nuances such as hardware layout or NUMA distribution that can conditionate the overall behaviour of the system and produce unexpected results.",
        "with proper cameras like this you can control loads of stuff like region of interest, so maybe if you don't need the full frame sending back just set the ROI to be smaller.",
        "Always recommend basler cameras. They have a good API and good support",
        "Ok, I will also ask quotes from them. Is the software license based, or is it a one time buy ?",
        "Hi.\n\n\nNo I do not need sync, the idea is to parallelize independent digitization channels.\nI was actually fearing much more data. A digitization run can take up to 10 minutes. So I would have to store 120 Go of data after four runs.\n\nThe setup is tabletop, I can keep the computer close.\n\n\nThanks for the insights !",
        "Full frame will be needed =)\nBut thanks !",
        "I believe their SW is free",
        "Indeed I did the math wrong :D\n\n4200x2160x43x4 ~ 1.56*10e9 bytes/second ~ 1.5Gb/s of raw data with the 8bits pixel format. Actually the camera also supports 12bits pixel format, so I guess using that format there would be ~50% more data.\n\nSo for ten minutes this will be around 900Gb of data.\n\nIf you don't need synchronisation it will make everything much easier.",
        "Whoa, thanks for the maths. It's closer to what I was expecting. I guess I start with one camera and figure this out :)",
        "Also, they tend to include their own recording SW usually in C++ so it's super fast and optimised (depending on the manufacture). Modern CPUs can do really impressive real-time lossless compression so you can have a play with logging raw vs compressed and trade off settings appropriate to your HW."
    ]
},
{
    "submission_id": "1g2xtwa",
    "title": "YOLOv8: model misses 5-10% of similar objects in clustered formations",
    "selftext": "We have trained a YOLOv8 nano model on 5000+ images with each 1 to 50+ birds from a top down view (drone footage). Since the birds are seemingly 'easy' objects to detect (they are all white with black on top), we really doubt the issue here is the (size of the) training data. Moreover, we have the checked the dataset multiple times for noise and errors, which of course will remain here and there but we think it is negligble. We have had a similar problem using RCNN models.\n\nThe birds breed in clustered formations. When we apply the model on new drone footage (inference), there is often 5-10% of the birds that is missed. And we fail to really understand why. The missed birds are logically (almost exactly) similar to birds that are detected and are seemingly randomly missed. Below a few close ups of predictions to illustrate this, with yellow outline to birds that are missed.\n\nhttps://preview.redd.it/9czosd8ipkud1.png?width=416&format=png&auto=webp&s=3babb1a294a50d97b38541107ffa56d5bb694cb4\n\n[This bird has spread its wing, so perhaps that is a reason](https://preview.redd.it/j2xahd8ipkud1.png?width=368&format=png&auto=webp&s=11cf1322a629a661fc00dcc14002cd8d5b1bedf8)\n\nhttps://preview.redd.it/ldw5te8ipkud1.png?width=275&format=png&auto=webp&s=9d14103c5c3ccfdffc492ec3e5e8dfa0e87f1f1d\n\nFor further reference, below is an image of the training:\n\nhttps://preview.redd.it/t1iy22vxqkud1.png?width=2400&format=png&auto=webp&s=7df2706f5e4883d61a960882d3cb742ac4a8985c\n\nSince drone mosaics are large, we tile them to use during training and prediction, with each tile a size of about (mostly) 5 to (some times) 10 meters. During training, we resize the images to slightly lower than the average pixel size. During prediction, we use the exact size of the slices. To increase the training data, we have added some overlap here and there between the tiles used for training. During prediction, we standard use overlap (varying 10 to 50%)  but we still encounter the same problem. We have tested varying tile sizes and confidence thresholds as low as 10%. The drone imagery used for training has been taken at varying locations (bushy, sandy, etc.) and the tested imagery above is actually from a location that is included. Despite all this, we fail to see the exact root cause.\n\nWe are therefore interested in whether people here have some experience in this issue or have suggestions. Why are some birds missed, even though they are nearly 100% identical to birds that are detected? I can clarify further things if needed.\n\nThanks in advance.",
    "created_utc": "2024-10-13T12:35:38",
    "num_comments": 4,
    "comments": [
        "Okay this one is interesting so here is my take:\n- Are you sure the model has been trained with enough epochs? It seems like, from the training graphs, 20 epochs might not be long enough. I guess that’s also why you have such low confidence scores during the prediction (e.g. some of the birds have a score < 0.5, which is weird with some many annotated instances).\n- Maybe you can try a larger version of YOLO (S, M, etc), unless you have hardware limitations.\n\nLet me know if you manage to find what went wrong!",
        "Increase the epochs to 1200 and patience to -1. Get the model to overfit first",
        "Not really sure if this is the culprit but I recently found in my own project that adding a border/padding around the input image during predictions significantly improved performance.\n\nCheck out the comments by omar-ogm in this thread: https://github.com/ultralytics/ultralytics/issues/2783#issuecomment-1706449763",
        "Yes, 20 epochs is not enough for YOLO. For context, the COCO pretrained YOLOv8 models with 118k images were trained for 500 epochs. YOLO takes longer to converge compared to FRCNN. So you need to use much higher epochs.\n\nAnd using YOLOv8m is recommended as YOLOv8n has significantly lower performance compared to the YOLOv8m. YOLOv8m provides the best tradeoff between accuracy and speed for YOLOv8."
    ]
},
{
    "submission_id": "1g2wz5v",
    "title": "Finding Sport Matches Videos for Computer Vision Project",
    "selftext": "I am trying to do an image-processing project for my university that requires tons of team-based sports (football, basketball, volleyball, etc.) videos as my dataset. Is there any recommendation of where I can find these video. Preferably those without a lot of changing camera angles. Any help will be truly appreciated. Thank you.",
    "created_utc": "2024-10-13T11:59:09",
    "num_comments": 3,
    "comments": [
        "Maybe someone who streams Fifa on Twitch?",
        "For soccer, there is [SoccerNet](https://www.soccer-net.org/home) which offers different databases for different tasks (it can only be used for research projects.)",
        "Thanks for the reply but unfortunately I am hoping to look for real-life gameplay."
    ]
},
{
    "submission_id": "1g2pmae",
    "title": "OCR for expense tracking ",
    "selftext": "Help me please. I'm a student and I'm completely new to OCR. I don't know if this is the right place to ask but hear me out. So we have this project for a major subject that we need to build an expense tracking app with ocr for receipts. The app will scan receipt and it will categorize the items in it. But our professor asked us how will we categorize each item in the receipt once it's scanned. Like of you bought food, toiletries, and other items at the same time, it will categorize each item and not just put all of it under one category like grocery. How can we do it? I'm completely new to this so I don't know the technicalities of it. ",
    "created_utc": "2024-10-13T06:26:23",
    "num_comments": 3,
    "comments": [
        "Star with a single picture and make sure you can read it and find the digits using ocr. Then store the data in some json format. Write another script to parse and compute that data",
        "As the other suggested, use existing cloud OCR service - send image, receive text. \n\nStarting from here, you can:  \n1. formalize the text into structured data, probably just \"date\", \"name\", and \"amount\" that you need. You may simply use regex or text parser for this if you don't want to involve AI\n\n2. put them into a list, you can use this API to categorize the items: [https://app.fina.money/doc/vAmbM52OaDgRal](https://app.fina.money/doc/vAmbM52OaDgRal) (see [post](https://medium.com/nerd-for-tech/how-to-build-a-machine-learning-service-for-classifying-financial-transactions-68bb722ad817) about how that works)\n\n3. Optional: run analysis like total by category and display the result with data visualization.",
        "So, if you don't need to make your own algorithm, you can use Azure Document Intelligence to scan receipts (you can scan 500 receipts monthly). It is extremely good. Then, you can use local llama3.2 3b, to ask it to categorize expenses, and return it in json format, than you can take that json, and process it."
    ]
},
{
    "submission_id": "1g2oymb",
    "title": "Is it worth investing in a \"Deep learning oriented\" GPU instead of an RTX 3070Ti?",
    "selftext": "I currently own an RTX 3070Tİ along with a Ryzen 3800XT CPU. However, I would like to step up my game in terms of model training. By the way, 3070Ti serves me perfectly.\n\nI was just looking towards the A series and Quadro series GPUs, but I don't know which metric should I chase after to get a significant benefit in terms of model training speeds.\n\nI started to game less and less in my free times, so sacrificing a bit of gaming performance is not an issue for me. \n\nAnd also the price point is important. I would not want to invest in a deep learning enterprise GPU twice the money of an RTX3070Ti if it does not halve my model training speed.\n\nBy the way, I don't know if such GPUs require ECC RAM. If that is the case, I would not even consider them and try to buy a newer generation of RTX 40 series. \n\nFor background, my datasets are not that large (often smaller than 1gb) and I mainly train transformer decoders. \n\nAny GPU recommendations are greatly appreciated! ",
    "created_utc": "2024-10-13T05:51:46",
    "num_comments": 14,
    "comments": [
        "3070Ti is fine for deep learning.\n\n\nThe only reason for upgrading is either more VRAM or more cuda cores.\n\n\nDesktop GPUs top out at 24Gb VRAM (3090 or 4090), and then it's a big jump in price to get 48GB.",
        "I’m really curious about the kind of ML related work that folks tends to do at home with their own machines and GPUs: \n\nIsn’t it cheaper to use a cloud GPU provider? \n\nShall I assume that whoever does this,  is already working in a related ML field as their 9-5 jobs? \n\nIs it just a hobby?",
        "I think you're better off buying multiple consumer gpus. Used 3090s are pretty cheap, and if you can put 4 of them in a rig then it's going to be better than an equivalently priced setup using datacenter GPUs (in fact the entire setup would be cheaper than a single GPU)",
        "3090 can do NVLINK though, so you can use 2 * 3090 + NVLINK",
        "It is indeed fine. \n\n*\"The only reason for upgrading is either more VRAM or more cuda cores.\"*\n\nThis is what I want actually.",
        "Not sure this is accurate. Gigabyte has a desktop AI training product they call the AI TOP (www.gigabyte.com/WebPage/1079?lan=en) that can handle local AI training up to 70b parameters, and one of the GPUs they recommend for this setup with Radeom Pro W7900 with 48gb vram: www.gigabyte.com/Graphics-Card/W7900-AI-TOP-48G?lan=en) Granted they also have 16gb and 32gb options. If OP wants a \"consumer-tier\" GPU that's pushing into enterprise territory, you might want to look at this. ",
        "Some ML things require privacy and data must be kept in system, yes, cloud is cheap but this is one of the many reasons why people build systems for ML.",
        "It is just a hobby for my case. Of course cloud options might be a lot cheaper. And my work is nowhere near where I should consider security, so cloud services are MUCH cheaper for me.",
        "The datacenter GPUs are in my opinion \"absurdly\" priced for the regular hobbyst deep learning enjoyers. Regular high end consumer GPUs can also do the trick. \n\nI get your point about having multiple, since then it is possible to train two models at once using two GPUs.",
        "Compared to gaming, does NVLINK provide a significant benefit for DL applications compared to just utilizing the motherboard itself for parallelization?",
        "Yes but AMD drivers are shoddy. Best to avoid until AMD sort it out.",
        "Training two models independently isn't the main point of having multiple GPUs, though. Multiple GPUs allow you (very roughly speaking) to split the dataset and model into the GPUs, which speeds up the training a lot",
        "> motherboard itself for parallelization\n\nI don't have experience with NVLINK implementing other than using some GPUs connected with NVLINK.\n\nI think it helps with throughput and latency. This article is a good read : https://www.fibermall.com/blog/nvidia-nvlink.htm#NVLink_vs_PCIe_A_Comparative_Analysis",
        "Of course, but that only starts making a significant difference when your dataset and model size is large enough to dominate the transfer latencies between the two GPUs right? I have access to a workstation at work with 3 RTX Quadro 6000s. When I use mirrored strategy on tensorflow to utilize all of them, the training time does not become t/3 because my model only has 200k ish parameters and my dataset is only around 500mb."
    ]
},
{
    "submission_id": "1g2ov5z",
    "title": "Metric Moncular Depth Estimation - Thoughts? ",
    "selftext": "We've seen a surge in large-scale models capable of estimating depth from single images. Which ones do you think are the most impressive?\n\nHere are a few top contenders:\n\n* **DepthPro** (Apple)\n* **DepthAnythingv2** (TikTok)\n* **UniDepth** (ETH Zurich)\n* **Metric3D** (Mu Hu et. al.)\n\n**What do you think is the most promising method for metric monocular depth estimation?**\n\nI'm particularly interested in the potential applications of this technology. Could it be used for:\n\n* **Autonomous vehicles**\n* **Virtual reality**\n* **Robotics**\n* **Architectural visualization**\n\n**Where do you think future research in this field is heading?** Will we see even more accurate and efficient models? Are there other areas of depth estimation that need more attention?\n\nLet's discuss!",
    "created_utc": "2024-10-13T05:46:35",
    "num_comments": 1,
    "comments": [
        "I’ve played with depth pro and depthanythingv2 and they seem to be pretty impressive, but for accurate reconstruction there definitely is some more work to be done. Definitely not a trivial task but I think we are getting close considering we are only using monodepth. Every technique seems to have its own benefits for example depth pro is great for boundaries whereas depthanything is good for transparent surfaces. Depthcrafter is good for temporal. If only they can combine them all"
    ]
},
{
    "submission_id": "1g2mrmg",
    "title": "How to handle discrepancy for training dataset with lossy compressed images and camera stream during inference?",
    "selftext": "I have a training dataset which was created by annotating images from a camera saved as jpg files i.e. the dataset has been lossy compressed. However during inference the images come directly from the camera stream and have therefore not been lossy compressed. When I run my model I can see that it performs better on the test set which is loaded from jpg files than on the direct camera stream (the camera images are clearly sharper).\n\nWhat is a good way to handle this discrepancy? I would like to avoid having to retrain or finetune the model to better handle images directly from the camera stream. One solution I see is to simply reproduce the jpg artifacts during inference by writing the image to a jpg file and then reload it into memory (maybe there's a library that does this without the overhead of actually writing to file?). But it feels a bit overkill to have to write/read from the file system and might slow down inference a lot.\n\nAny good ideas how to go about this?",
    "created_utc": "2024-10-13T03:33:05",
    "num_comments": 9,
    "comments": [
        "I’m honestly surprised this doesn’t come up more often.\n\nYour idea to compress the new images to jpeg should help. Both cv2 and pillow can write jpeg “files” to memory (buffer) instead of to permanent storage so that will help efficiency . You can also control the parameters to get the amount of compression correct. https://stackoverflow.com/questions/52865771/write-opencv-image-in-memory-to-bytesio-or-tempfile\n\nAnother thing to try in conjunction ( if you have spare compute) is to feed each frame/image through the model multiple times with slight perturbations. For example you could try different levels of joeg compression, sharpening, brightness, contrast, etc.  Then you average or vote on the results. If two of the three versions say a photo is a dog and the third says cat, then you pick dog.  (This voting can be done for object detection too). \n\nThe above can also be done by training different models and passing the image through each. That’s called model ensembling. Probably less helpful for you but though I’d mention. \n\nYet another thing to try is to retrain the model on images which you’ve passed through some kind of “jpeg artifact fixer”. Maybe you could even train that fixer using your own newly recorded imagery. This is probably the most theoretically sound approach because it’s essentially shifting your already labeled compressed imagery back into the same domain as what is streaming off the camera pre-compression. The challenge is whether it can do so effectively. \n\nAnyways, good luck and report back please! This is the kind of real-world problem that I wish we saw more of in the sub…",
        "If the model is significantly sensitive to something like that, then it's not a good sign and probably a sign of overfitting. Did you apply augmentations during training?",
        "Using a memory buffer instead of writing to files worked great and the images have the exact same values (compared to writing/reading from files) with the default parameters. Thank you!\n\nThat is probably the solution I will go for in this project because the model is already working very well for my use-case. However in theory I should be able to benefit from more clear images directly from the camera so if I needed better performance then I would have tried to avoid compressing the images during inference (possibly with one of the other suggestions you mentioned).\n\nI'm still interested in discussing this issue if anyone have any interesting comments. Maybe I should've avoided the problem from the start by not lossy compressing the images. But at the same time it does have the benefit of less storage for the dataset.",
        "It's not significantly sensitive but I've ran on it on the same images directly from the camera and after going through jpeg compression and it consistently performs slightly better on the latter.  \n  \nYes a lot of common augmentations like translation, rotation, scale, HSV (hue, saturation, value) and noise. I guess the reason these augmentations aren't solving the problem is that none of them upscales the images back to original quality which obviously is hard if not impossible since the images are lossy compressed.",
        "That’s great! \n\nI normally don’t worry at all about the size of datasets until they start costing more than some reasonable amount to store per year. At work this basically means  we don’t use lossy compression on any still images. PNG is fine. We’d rather spend an extra $500 on hard drives than spend that much of our time researching and testing compression tradeoffs. \n\nFor video streaming off a camera where we don’t know yet which frames will be used for model training, I usually go to H265 or AV1 with very high quality parameters, which ends up being pretty close to lossless and takes up a fraction of the space versus saving each frame as a separate JPEG file. If we start to run out of space I’ll try to get someone to review the video and pick out timestamps worth saving and I use FFMPEG to isolate those portions of the video into new video files without re-compressing…so the remainder can be deleted. ",
        "Seems like a good strategy! But I feel like it shouldn't be extremely uncommon to have to compress your dataset in industry. I wonder what the most common way is to handle the difference between training data and production data in that case. Probably some of the methods that you mentioned before are used.",
        "TBH I think it usually just doesn’t matter to industry. A model that detects dogs with 96% accuracy isn’t really any more useful than one that does so at 95% accuracy, at least for most companies. And I’m guessing that’s all the difference that most compression makes. \n\nProbably it’s the problems that rely on fine details where he difference is more stark and it sounds like maybe you’re in that category, as am I. Much of my work involved detecting defects which tend to be quite subtle. Imagine for example trying to categorize if paint was applied evenly on a surface. JPEG artifacts really mess with that. \n\n ",
        "You're probably correct and maybe those companies that do care spend more on storage to avoid this problem. But for sure there has to be some industry best-in-practice solution if someone encounters this problem. At least it would be interesting to know what that solution would be if it exists."
    ]
},
{
    "submission_id": "1g2m688",
    "title": "YOLO metrics comparison",
    "selftext": "Lets assume I took a SOTA YOLO model and finetuned it to my specific dataset, which is really domain specific and does not contain any images from the original dataset the model was pretrained for. \n\nMy mAP@50-95 is 0.51, while the mAP@50-95 of this YOLO version is 0.52 on the COCO dataset (model benchmark). Can I actually compare those metrics in a relative way? Can I say that my model is not really able to improve further than that?\n\nJust FYI, my dataset has fever classes but the classes itself are MUCH more complicated than COCOs. So my point is it’s somewhat of a tradeoff between the model having less classes than COCOs, but more difficult object morphology. Could this be a valid logic?\n\nAny advice on how to tackle this kind of tasks? Architecture/methods/attention layer recommendations?\n\nThanks in advance :)",
    "created_utc": "2024-10-13T02:49:58",
    "num_comments": 6,
    "comments": [
        "Yolo's different architectures are trained on Coco dataset (80 object classes)... And if you have a multi class custom object detection dataset, there is nothing to compare with Coco benchmarks.... Your dataset is unique and you have to \"go through the tons of slush\" to extract that 1 ounce of gold....And Yolo's training uses \"massive compute power\" and you may not have that, while training any of the SOTA Yolo architectures on your custom dataset, and it further reiterates that you can not compare with the Coco benchmarks.....If you randomly, see the images in Coco dataset, the size of the \"objects\" relative to the image size can be of \"medium\" to \"large\" size... Very few \"small\" size objects or \"Xtra small\" objects.... If your objects are medium to small to Xtra small, the game of complexity starts....Added to that, if your images are diverse (tends to infinity), when you test/evaluate your trained model on unseen images, you will have TP's which flatter you, and very small percentage of FP's and FN's which will make you look at beyond SOTA....",
        "Great question and I’m sure there is no right answer! \n\nI just view performance measured in standard benchmark datasets as a useful guide. \n\nDo keep in mind that most of what makes these models tick comes from how they handle low level visual features coupled with how they correlate those low level features together. Those qualities are common across most visual domains even ones that we as humans consider quite distinct. \n\nSomething that’s missing from most COCO comparisons is an evaluation of which specific images a model performs best on compared to other models. For example two can both have 0.52 map but one is screwing up every image where very fine details is essential, and the other handles those examples perfectly but screws up anything where subtle colors are important. However that is a fictional example that’s rarely as bad as it sounds.\n\nAnyways, I guess the point I’m making is that it’s probably fine to use COCO scores to pick a model for your own data. But do try a few different models just to be sure. ",
        "> My mAP@50-95 is 0.51, while the mAP@50-95 of this YOLO version is 0.52 on the COCO dataset (model benchmark). Can I actually compare those metrics in a relative way? Can I say that my model is not really able to improve further than that?\n\nNot really. They're different datasets. You can reach 0.9+ mAP@0.50-0.95 scores depending on your dataset.",
        "Thanks for the response, it’s really obvious that you’ve got lots of experience doing this :)\nI could truly use an advice: how do i get to a more complex and advanced modeling from using Ultalytics Python module? Should I use default YOLOv8 (for example) as a base and then tweak attention mechs and model depth, conv blocks etc. training and measuring the performance on my dataset? I feel really stuck in this task…\nThanks in advance!",
        "Thanks for the response :)\nAny advice (very generic obviously) about what’s worth paying attention to reach really high mAP50-95? Should I tweak the original YOLO structure implementing attention mechs and conv blocks or its too much?",
        "The highest gain comes from better training data. The next highest would be from using a larger variant or image size, particularly if the object is small or has finer details. You can modify architecture, but I wouldn't expect that to increase the score a lot."
    ]
},
{
    "submission_id": "1g2lh5g",
    "title": "Help",
    "selftext": "",
    "created_utc": "2024-10-13T01:56:01",
    "num_comments": 0,
    "comments": []
},
{
    "submission_id": "1g2kb5x",
    "title": "Augmented Reality App using OpenCV and Python",
    "selftext": "",
    "created_utc": "2024-10-13T00:27:48",
    "num_comments": 6,
    "comments": [
        "some filtering would help with the flicker",
        "Source code???",
        "Wow, I missed using Aruco. They make wonders",
        "Can you share the code please?",
        "Oh that’s a good idea!",
        "I have it on my website kevinwoodrobotics"
    ]
},
{
    "submission_id": "1g2k32n",
    "title": "Accident detection using yolov8",
    "selftext": "Hi,\nI am building a prototype using yolo v8 for accident detection \nMy model is trained and is marking boxes for the test video when accident class\nBut what I want to do is that run this on jetson nano..and when an accident is detected it sends a signal to a led light and blinks it \nDoes anyone have idea how this can be achieved as I am in an urgent need of help",
    "created_utc": "2024-10-13T00:10:42",
    "num_comments": 7,
    "comments": [
        "The Jetson has GPIO pins that you can use for this purpose.",
        "This kinds of commands should be in the driver library",
        "Also you could do a really cheap setup, using an Arduino as a GPIO and communicate via USB command.\n\nSo you detect with Yolo and then  send a USB output to the Arduino to blink a light. You could probably do this for <$10",
        "Yea but how will I work around \n1) how to run yolo model on jetson nano\n2) when accident frame is detected it glows led",
        "Which kind",
        "`ultralytics` has a [guide](https://docs.ultralytics.com/guides/nvidia-jetson/) deploying to Jetson.\n\nFor GPIO, you can search for Python examples of controlling GPIO in Jetson. There are a lot of them.\n\nHere's one from NVIDIA's examples controlling LED:\nhttps://github.com/NVIDIA/jetson-gpio/blob/master/samples/button_led.py",
        "Thank you will look into this"
    ]
},
{
    "submission_id": "1g2dva3",
    "title": "Help: Gathering image dataset with fixed quality",
    "selftext": "im using yolov8, given that a camera willl always take 1280x720 resolution, will gathering dataset with same resolution and same camera will result in better performance or not really?\n\nAssuming the model will always be used on that camera",
    "created_utc": "2024-10-12T17:33:44",
    "num_comments": 1,
    "comments": [
        "Yes. But you also have to make sure it's not simply the same object captured from slightly different angles. That's a very common mistake. Doing so may cause the model to underfit because it's not getting exposed to diversity and hence not being forced to learn richer features. For example, if all your objects are red in training data, you might find that after training it detects any red object as a positive detection.\n\nYou want images that are informative and diverse. Not just the same object captured from slightly different angles and lighting 50+ times.\n\nEven worse when you randomly split this dataset into train and val, the val would simply contain those slightly different variations of the images the model has seen during training and will incorrectly show a very high score."
    ]
},
{
    "submission_id": "1g23mn3",
    "title": "Seeking a small model for extracting important frame. ",
    "selftext": "Assuming that we have a human object interaction video, there will only one action is executed and one object is interacted during the video. There are 3 states of the video is start state, middle state and end state. I'm focusing on the middle state when the object is interacted. I will take a frame throughout that state. of course, that frame have to be clear, less blur as possible. Does anyone have any suggestions to me?",
    "created_utc": "2024-10-12T09:15:50",
    "num_comments": 6,
    "comments": [
        "You haven't mentioned what the output of the model would be. It should be able to find the frame that's the clearest?",
        "Yes, the clearest frame and that frame come from middle state ( the state contains the executed action)",
        "Do you have training data?",
        "Yes",
        "Have you tried any traditional methods?\n\nSomething like this:\nhttps://pyimagesearch.com/2015/09/07/blur-detection-with-opencv/",
        "No, i haven’t. Sorry for the lack of information, I actually need a frame that we can be easy classify the object after detecting the object. (less blur and occluded as possible ).btw,  I’ll try your method "
    ]
},
{
    "submission_id": "1g21hk0",
    "title": "Aruco Markers",
    "selftext": "I’ve been experimenting with Aruco markers a lot lately and wanted to know if it’s still used by most people or if there are new alternatives now. Thoughts?  ",
    "created_utc": "2024-10-12T07:36:38",
    "num_comments": 6,
    "comments": [
        "Aruco are difficult to detect if they are far away or have significant tilt especially with Opencv. It's easier to work with regular chessboard features with custom features to differentiate the orientation. But yes many online tutorials will tell you to use Aruco boards. Many engineers will swear by Aruco or charuco because it's easier to develop code but experienced ones will tell you that they are not reliable in dynamic conditions.",
        "I've seen more practical implementations using AprilTags recently:\n\n[https://robotics.stackexchange.com/questions/19901/apriltag-vs-aruco-markers](https://robotics.stackexchange.com/questions/19901/apriltag-vs-aruco-markers)",
        "We tried using them in production and they are harder to read than QR codes and they also do not give accurate position information like the author claims.\nAnd there C libraries for Android are not documented correctly.\nIt was a bad decision to use those for us",
        "Yep DO NOT USE THEM. They are bad for real world production. And I do not consider robotics Demos using Aruco codes a real world example.\nWe deployed Aruco codes across 25,000 devices and boy did we regret it.",
        "What did you end up doing instead?",
        "We started with 2 markers, so there would be redundancy and for configuration.\nI left a year after we deployed them, but we had talked about adding an extra 2."
    ]
},
{
    "submission_id": "1g20uvs",
    "title": "Implement a face detection algorithm from scratch using Haar-like features and\nIntegral Image computation.\n• Capture an image of yourself using a webcam or upload a face image.\n• Compute the Integral Image to efficiently calculate pixel sums over rectangular\nregions. Use integral image ",
    "selftext": "We cannot use cascade classifier ",
    "created_utc": "2024-10-12T07:05:53",
    "num_comments": 3,
    "comments": [
        "1. What even is your question?\n2. We're not here to do your homework \n3. You were actually given the whole solution so why not follow the steps??",
        "I am really new to cv and just wanted to know how to approach this qn ie where to learn this from , online help etc",
        "google.com \nopenai.chatgpt.com"
    ]
},
{
    "submission_id": "1g20gnn",
    "title": "Help: Project Image Rectification",
    "selftext": "in my company, i was \"Somehow\" asked to lead a project for image rectification, the higher ups demand to not use third party api / services and thanks to a certain \"consultant\" i was also forced to use hough transform as an initial stage.  \n  \nAs I started working on the project, I got stuck on estimating the affine transformation. Usually, the affine transformation requires both source and destination points for its algorithm to work, but manually defining the destination points for production is impractical.\n\nWhat I’ve done so far:\n\n* calculates peaks and determine corners for the source points, using this corners i then map destination points. However, this causes the image to be rotated in an unexpected way\n\nIs there a way to automate the destination mapping process to make this work effectively?",
    "created_utc": "2024-10-12T06:45:38",
    "num_comments": 3,
    "comments": [
        "Rectification for stereo? Or some other purpose?\nWhat you need to do depends on what you are trying to achieve...",
        "I did a slightly similar project, if it helps, here's the [POC](https://github.com/SaBe22/perspective_control) I started. It was inspired by this paper [Auto-rectification of user photos](https://static.googleusercontent.com/media/research.google.com/fr//pubs/archive/42532.pdf). As I recall, my implementation wasn't complete (I didn't have enough time to finish it): I could only handle vertical lines and not horizontal ones (or vice versa).\n\nHope this helps.",
        "Can try to train a feature detector"
    ]
},
{
    "submission_id": "1g1xps8",
    "title": "annotations from xml to txt",
    "selftext": "Hey, so i have dataset where labels are annotated and exported as xml files. What would u use to export that correctly to txt files? I have seen roboflow, but 250euros for month for 10k images is not worth it :).\n\n  \nSo for example import whole dataset with xml, convert to txt, download dataset. Anything similar to roboflow thats free?",
    "created_utc": "2024-10-12T04:04:24",
    "num_comments": 3,
    "comments": [
        "Write a quick python script ?",
        "You can try labelformat https://github.com/lightly-ai/labelformat",
        "This. Use your favourite LLM if your python is scetchy"
    ]
},
{
    "submission_id": "1g1vxwo",
    "title": "How to post-process lens compression given RGB and depth image?",
    "selftext": "Lens compression is a phenomenon that happen when you capture two image of a subject using different focal length, and maintaining the overall scale of the subject by changing the object distance. It is better explained [here](https://go.photoshelter.com/photographers/blog/a-mathematician-weighs-in-on-lens-compression/)\n\nIn a sense, the longer the focal length, the more compressed the background is. Given that we can now use monocular depth estimation model to estimate the full depth map, is it possible to simulate the same effect after the taking the photo?\n\nTo limit the complexity, I decided to assume I have a wide angle photo, and want to compressed the background like in telephoto. My current thought is to separate the RGB image into planes based on its depth, and re-composite them back with a scaling factor (dependent on depth value), and use `cv2.inpaint` to infill the missing information. But I don't know if this method is correct, or not. Moreover I don't know if the scale factor in what function formula (I suspect it is linear). Any ideas?",
    "created_utc": "2024-10-12T01:50:11",
    "num_comments": 2,
    "comments": [
        "Your approach is ok.\n\nConvert the image to a point cloud using the camera calibration and depth values (ex: a point (u,v) will be at depth(u,v)\\*K\\^-1\\*\\[u;v;1\\] where K is the projection matrix, ignoring radial distortion). From then on, you can re-project each 3D point assuming a different camera calibration and/or position. Use the original and updated positions to define a warp grid you can apply with cv.remap.\n\nYou can just scale the newly projected positions so that they cover the same (H,W) area as the original images, or you can also re-position the camera along its principal axis so that your subject covers the same area. That last approach involves a change of point of view and thus possibly some outpainting, but it's not vital or the first thing to do here.\n\nThere are papers that tackle the same problem, here is random one: [https://arxiv.org/pdf/2302.12253](https://arxiv.org/pdf/2302.12253) that does something more advanced regarding the warping/inpainting/outpainting. You'll find simpler approaches in the references.",
        "Thank you, I will read up the research, and tested a few ideas"
    ]
},
{
    "submission_id": "1g1uuu4",
    "title": "Segmentation approach aware of occlusion ",
    "selftext": "Are there segmentation approaches that are aware of occlusion? I want to segment road from a photo, but with cars parked, pedestrians and other objects segmentation networks tend to produce mask with holes in place of occlusion object.\n\nI was thinking I might use inpainting networks first to remove unwanted objects. But wanted to know first if there are other solutions?",
    "created_utc": "2024-10-12T00:24:48",
    "num_comments": 1,
    "comments": [
        "What do you mean aware of occlusions?\n\nYou said they produce masks with holes. That’s cause there are holes. Those pixels are something else. And the training data most likely has them labeled as something else"
    ]
},
{
    "submission_id": "1g1ubcy",
    "title": "How to get CV Internships?",
    "selftext": "I am a first year Masters student looking for CV/Graphics internships in the US. Any advice?\n",
    "created_utc": "2024-10-11T23:43:44",
    "num_comments": 8,
    "comments": [
        "Apply like hell",
        "I never got any internship since I only applied for a couple of them (I'm not from US). But I can suggest you freelancing. You learn much more in my opinion and make some money as well. Plus, you can explore various fields and see what attracts you the most. It  helps  you find the subfield of your interest.",
        "Do they accept freelancer with little experience lol?",
        "Where and how do you recommend one starts with freelancing in this field?",
        "What?",
        "He said do freelancing for money, not for getting a job.",
        "It probably differs region to region for example I'm in Iran so I don't have access to international websites due to the sanctions so I used local websites. For you it's probably way easier. There are so many freelancing websites out there.",
        "Oh okay"
    ]
},
{
    "submission_id": "1g1tn4s",
    "title": "Depth Pro from Apple",
    "selftext": "Anyone played with the mono depth estimation from Apple yet? ",
    "created_utc": "2024-10-11T22:54:18",
    "num_comments": 4,
    "comments": [
        "Yes it’s good",
        "I did some test and the error was pretty high for the distance",
        "How did you test yours?",
        "Mono depth almost always requires some form of scale calibration to your specific domain, camera parameters, etc."
    ]
},
{
    "submission_id": "1g1tdnq",
    "title": "Can someone recommend some papers or tutorials for 6DoF Pose Estimation?",
    "selftext": "I have a background of deep learning and I am very familiar with image classification, semantic segmentation and object detection.  However, I don't have a mult-view geometry background, so I am not a SLAM or VIO expert.  At this point, I hope to transfer to a different team in my org, who are focusing on 6DoF object pose estimation (i.e. hands, pens or game controllers).  So I hope to first get a basic understanding of the big picture before I talk to them and I hope to receive some guidance from you experts. Thanks in advance.\n\n(1)  What mainstream techniques are currently used in industry to perform 6DoF object pose estimation?\n\n(2)  Were traditional computer vision algorithms still used in this field nowadays? Considering my background of computer vision but not multi-view geometry, how difficult do you think for me to transfer to this area?\n\n(3)  I searched online to find a lot of top conference papers since 2017, but they look quite diverse to me. Can someone recommend some papers or tutorials with systematic learninng path?  Take object detection as an example, most beginners will start from Fast-RCNN to Faster, to YOLO, SSD and centernet. ",
    "created_utc": "2024-10-11T22:35:59",
    "num_comments": 20,
    "comments": [
        "Check out FoundationPose from Nvidia. Then go to connected papers and look at all the other publications relating to FoundationPose. It's a good method to read a lot of different methods solving the same problem.",
        "I am also on similar path of exploring multiple view geometry. Prof. Daniel Cremers from TUM has courses on it for starters.",
        "There are several approaches for 6d position estimation, like cad model matching where we need to train the model with a deep learning model with cad objects, and another category we need to collect the 6d position information for our dataset and train the models with 3d anchor box outputs(yolo 3d). \n\nI think you're focusing on detection from the rgb image alone.so you can check this: https://github.com/liuyuan-pal/Awesome-generalizable-6D-object-pose?tab=readme-ov-file\n\n you can check this for yolo: https://github.com/ruhyadi/YOLO3D",
        "Your experience in CV, segmentation and object detection will all be important to succeeding in pose estimation. You can build on your experience. \n\nIn simple terms, you'll need to first determine a basis pose (location and rotation) for a recognized object.\n\nLikely the diversity of what you're finding on this pose topic is due to there being far more information pertaining to objects with predetermined poses (e.g., CAD model objects). And that's fine - because you need to have a basis pose for an object in order to estimate any new pose. \n\nAs someone already said, there are different ways to describe poses. There are pitch, yaw, role; EULER angles; quaternions; rotation matrices, etc. They are all generally the same but each one has its own edge cases. \n\nI'd suggest finding out which of those pose description methods are being used by the team you'd like to move into. And then look specifically for the functions pertaining to their work. \n\nThat should help narrow things down for you. \nBest of luck and learning!",
        "Yes. Study CAD object models to understand how they have local axes and locations (a pose). \nYou could use a CAD model for training, but I would not. I would use images of a real object. \nAnd I wouldn't use a symmetrical bottle unless it has colors distinguishing its sides. Otherwise you're likely to have difficulty later determining any rotation around its center axis. Maybe use a six colored box with different length, width and height. \n\nFor those last two steps:\n- For your test object images, decide where the object's origin will be (e.g., its centroid, a consistent edge), something you can rely upon or determine.\nAdd the pose data to the classification data for a few of your images, and retrain your image recognition. \nYou might want to start with 30, 60 and 90 degrees off of each axis. This will keep you from being overwhelmed and will let you see if you're on the right track. This will enable determination of rotations.\n- The last step is needed to compare pose locations. Every object has a relationship with things around it. And if it's an object in motion over time its new location relates to its old one. This should be much easier than the previous step.",
        "Move this to a reply",
        "I have some content on that if you’d like to take a look",
        "Thank you so much.  FoundationPose seems to be a really hot paper published this year with SOTA results. How will you comment on this paper?  I mean, is it some brand new method that is quite different from earlier existing methods?  Will it be well accepted in industry considering latency, energy efficiency and other practical concerns? Sorry for asking so many questions,  I just hope to receive some high level guidance from experts like you.",
        "Thanks for your advice. I am doing self study these days on these topics:  Rotation Translation, Transformation matrix (These look straightforward to me),  and then eular angles, quarternion (got lost already) and will look at lie group and lie algebra (I guess it would be too much for me now).  Meanwhile, I am looking into PnP and RANSAC.  Could you please give me some guidance which of these topics are most relevant to 6DoF pose estimation nowadays?  Do I need to have a deepdive on all of them?  (Sorry I am new to this field)",
        "I can't thank you enough. Really appreciate your guidance. \n\nSome more questions if it is OK:  I am reading PVNet paper,  which highlights \"keypoint based method\", is this some mainstream approach for pose estimation or is there any other popular approach than this?  \n\nI understand the keypoint detection part, but get confused by the PnP part.  I know PnP receives coordinates of  several 2D points and coordinates of the corresponding 3D points, and returns the transformation between 2D and 3D to get camera pose.  But where does the 3D points coordinates come from?  Are they the world coordinates or something else?  Sorry for my naive questions.",
        "Thanks. I will definitely check it out.",
        "Can you please share the content with me?",
        "Where is the content posted",
        "It might be easier for those here to give more advice with some additional specifics on the problem(s) needing to be solved.\n\nFrom what you've said so far it seems to me you're looking in the right direction with PnP and RANSAC in OpenCV. \n\nPose estimation won't be possible until you can:\n- reliably identify through classification an object\n- reliably identify it from any angle\n- assign an origin and axis to the object\n- relate the object to an anchor point (which could  change depending on the use case)\n- rinse and repeat (this is where the heaviest lifting happens)",
        "Sorry but I can't advise you on the \"keypoint based method\".\n\nBut on your other questions about the 3D points...\n\nThis come from you. By knowing the 3D dimensional geometry of your subject (or at least a part of it that you can rely upon). Obviously you need inimate knowledge of your subject. So in my proposed box scenario for testing, you'll want to know its dimensions. \n\nMost people and tools use meters for unit of measure. But otherwise don't worry about \"real world coordinates\" - yours likely isn't a use case where the added complexities of GPS or state plane coordinates would bring anything to your party.",
        "But yeah I’d would start off understanding how they solve it using traditional approaches like with pnp and markers. Then dive into ai ways like foundation pose",
        "click on their profile name, there's links to videos they've made",
        "Thank you for the guidance.  I will assume the first 2 can be solved by training a 2D object detection for an instance (i.e. tens of thousands of images of a same CAD model such as a bottle).  As for \"assign an origin and axis to the object\"  and \"relate the object to an anchor point\", I don't have a clue what it means and how to achieve it yet LOL  I guess I need to focus on these two aspects.",
        "Thanks for your answer.  Right, my use case is simply detect the pose of a remote gaming controller. Let's say I have a CAD model of this controller, and I can get eight 3D points of bbox surrending this controller. No matter where it is in real world,  the lower left corner of the 3D BBOX is simply (0,,0, 0) defined by myself, right?  To be simple, my 3D points will be (0,0,0), (0,1,0), (0,0,1), (1,0,0), (1, 1, 0), (0,1,1), (1,0,1), (1, 1, 1), and my 2D point in the image will be (2.45, 0.18), (1.22, 0.15), (3.46, 0.01), (1.23, 1.55), (1.35, 2.13), (0.15, 1.01), (2.25, 2.20), (2.56, 4, 67).  The transform matrix produced by PnP based on these 16 points will be my final 6DoF object pose. Is my understanding correct?",
        "I think you're making it harder than you need to. There should be no need to use all of the bbox's points. You might want to use them for training but I dont think so. All you should need to know are 3 axially aligned ponts nearest to the camera, and you'll have your axes."
    ]
},
{
    "submission_id": "1g1t52n",
    "title": "How to get the mask for a single class using Segment Anything Model",
    "selftext": "Hi, \n\nI have been trying to have fun playing with the Segment Anything Model. One thing I noticed is since it segments literally anything in the image, how can I make it segment only one thing from the image (or one type of objects). Suppose I am trying to remove any thin objects such as ropes/wires from the images. When I generate the masks, and then try to get the rope masks, based on the physical dimensions (such as ropes are thin and long), it does not seem to work. So I was wondering is there a way I can segment only the rope like objects from SAM generated masks.",
    "created_utc": "2024-10-11T22:19:00",
    "num_comments": 7,
    "comments": [
        "You can use [Grounding-SAM](https://github.com/IDEA-Research/Grounded-Segment-Anything), here you provide the object class name as text, post that Grounding-Dino model captures all those objects of the class via bounding boxes and get segment map for each by prompting those bboxes in SAM.",
        "I’ve done some tutorials on it if you want to check it out",
        "In the same line as Grounding Dino, there is another model called Owl vit that is used in this repo for the task you want to do, I have used for segment only clothes and it worked so good. (Check the v2 of the Owl vit model in hugging face too, it has better results).\n\nThis is the repo I have used and modified for my purpose: https://github.com/ngthanhtin/owlvit_segment_anything",
        "I am checking it out! Thanks!",
        "I'd love to! Can I get any link or what should I search?",
        "Hi, I did check one of your video out, at the end you did show how to get a single mask using the coordinates but what if I do not know what the coordinates are for that mask?",
        "You could use some thresholding techniques or color spaces likes hsv to localize your roi"
    ]
},
{
    "submission_id": "1g1t363",
    "title": "Similar object detection process technology stack",
    "selftext": "I'm looking for suggestions for the process or technology stack I should use to help with this specific use case. I've made some computer vision stuff in the past for filtering but this is a bit out of my wheelhouse. \n\nhttps://preview.redd.it/v0qzkbwod9ud1.png?width=388&format=png&auto=webp&s=4b8db3967a332ed87db29f07e80224614b1384ac\n\nhttps://preview.redd.it/meeu2e4qd9ud1.png?width=382&format=png&auto=webp&s=edad62ecd62eb7cc8e39d83ee0cc3761b1ea6681\n\nThe basic idea being the house from zillow is matched to it's google street view counterpart. they are similar objects but will be shown at different angles, sometimes different colors, etc. I started annotating my dataset on Roboflow but I realized I don't really need object detection as much as I need object *comparison* and similarity here as things tend to be pretty different between a listing photo and the streetview equivalent. The internet seems to be pushing me to the yolo stuff but I'm not even sure this is the right tool for the job. \n\nI have a constant stream of data for this (nearly unlimited) so the potential to do some deep learning is certainly there. I'm just not sure what that would entail, or if a more lightweight solution like openCV or something could handle this. Any high level advice for how to get started would be appreciated. \n\n",
    "created_utc": "2024-10-11T22:15:25",
    "num_comments": 3,
    "comments": [
        "You could try feature matching.\n\nhttps://www.reddit.com/r/computervision/comments/1evwi9i/all_worthy_feature_matching_algorithms_in_a/",
        "Thanks for this I'll look into it! Would something like this be applicable to a situation where it needs to both find the features and then match them? or would I need to train it on the specific features? Problem being that I won't know what feature a house has. It would, in theory, need to assess that itself and match off of it. With a large dataset, I could likely train it on common features but there will be a lot of variance because of the variety of home design options.",
        "Guess I could have answered the question in my comment if I had actually looked at the link first! Thanks!"
    ]
},
{
    "submission_id": "1g1lsxc",
    "title": "Fine-Tune GPT-4o Vision Models for Image Classification",
    "selftext": "GPT-4o models have proven powerful at handling multimodal tasks (text + images). \n\nHowever, for **highly domain-specific data**, such as detecting surface defects in manufacturing or monitoring quality control in retail, **general-purpose models** might not deliver optimal performance. \n\nFine-tuning GPT-4o models to your specific visual dataset allows you to **achieve higher accuracy** for tasks like defect detection, visual inspections, and beyond.\n\nThe linked article provides a step by step guide and plug and play code for you to fine tune GPT-4o with your data for image classification.\n\nWhat use case do you have for fine tuning GPT-4o?",
    "created_utc": "2024-10-11T15:24:50",
    "num_comments": 2,
    "comments": [
        "Is there any comparison with CNN based classification models?"
    ]
},
{
    "submission_id": "1g1jr3o",
    "title": "Camera for CV",
    "selftext": "Hi Im looking for a camera that has following features \n\n1. HDR, I need it to see things in dark and in light \n2. CS mount \n3. Can be easily plugged to a laptop for development \n4. Global shutter\n5. Low weight \n\n\nI have looked at Oak cameras but can't find much detailed info on those, price is not that important but cheaper the better. I already have a2A2448-75ucBAS but its too heavy and its bad at seeing thigs both in the dark and light.",
    "created_utc": "2024-10-11T13:49:37",
    "num_comments": 7,
    "comments": [
        "plug your criteria into the filter\n\n[https://www.edmundoptics.co.uk/c/cameras/1012/](https://www.edmundoptics.co.uk/c/cameras/1012/)",
        "flir/teledyne",
        "Oak is the way to go.",
        "OAK cameras don't check most of the boxes\n\nNo CS-Mount, no global shutter, and only SME-HDR (only for single frames)\n\nThe edmundoptics links is good, but remember that they are a hardware distributor which only have some producers in their repertoire and I can't find an HDR checkbox there.\n\nFinding HDR machine vision cameras is a bit more tricky, I think.\n\nGoogling it finds LucidVision (Triton – 5,4 MP Sony IMX490 CMOS) and a lot of niche-cameras with up to 180dB range.\n\nThere are a few variables, you didn't mention: Resolution, FPS and interface.\n\nIf the Triton doesn't fit your use, maybe check which sensor for automative from sony (they should all have HDR) checks all your boxes: https://www.sony-semicon.com/en/products/is/automotive/automotive.html and then google for cameras with that sensor.",
        "> Triton – 5,4 MP Sony IMX490 CMOS\n\nIm sorry, I am more of a software guy than hardware. This camera looks rly good. But the weight is a bit much. Do u know of a camera with 60fps minimum?(can be with binning) The interface itself doesn't matter that much as long as you can connect it to a normal PC. Resolution would be beset if full HD or better.",
        "No sorry, couldn't find anything in a quick search.",
        "Thanks anyways, the sony site is really useful"
    ]
},
{
    "submission_id": "1g1je4r",
    "title": "Synthesize Spatial VQA Data from Images with VQASynth 🎹",
    "selftext": "",
    "created_utc": "2024-10-11T13:33:10",
    "num_comments": 0,
    "comments": []
},
{
    "submission_id": "1g16hpn",
    "title": "Yolo v8-10 anchors",
    "selftext": "I ve heard about anchors and the possibility to implement it within YOLO fe. using Ultralytics. However, I’ve got absolutely no idea about how to calculate those anchors for my own dataset neither how to implement it. Thanks in advance!",
    "created_utc": "2024-10-11T03:18:40",
    "num_comments": 1,
    "comments": [
        "Anchors are automatically calculated for v8-v10. They might require manual adjustment for anchor(-box)-based heads which is true for v1-v5 and v7. But v6, v8-v10 are all using anchor(-box)-free heads."
    ]
},
{
    "submission_id": "1g165c3",
    "title": "Custom Faster R-CNN with SwinV2 FPN Backbone Implementation",
    "selftext": "I was using the [fasterrcnn\\_resnet50\\_fpn\\_v2](https://pytorch.org/vision/main/models/generated/torchvision.models.detection.fasterrcnn_resnet50_fpn_v2.html#fasterrcnn-resnet50-fpn-v2) model provided by PyTorch in my project.\n\nBut I was not happy with the performance so, when looking for alternatives, I've read about the [SwinTransformer V2](https://www.microsoft.com/en-us/research/blog/swin-transformer-supports-3-billion-parameter-vision-models-that-can-train-with-higher-resolution-images-for-greater-task-applicability) , and I've found it promising especially for applications with high-res images.\n\nThat's why I've decided to create custom Faster RCNN model with swin\\_v2\\_base backbone. Here is my implementation;\n\n`Class IntermediateLayerGetter(nn.ModuleDict):`\n\n`def __init__(self, model: nn.Module, return_layers: Dict[str, str]) -> None:`\n\n`if not set(return_layers).issubset([name for name, _ in model.named_children()]):`\n\n`raise ValueError(\"return_layers are not present in model\")`\n\n`orig_return_layers = return_layers`\n\n`return_layers = {str(k): str(v) for k, v in return_layers.items()}`\n\n`layers = OrderedDict()`\n\n`for name, module in model.named_children():`\n\n`layers[name] = module`\n\n`if name in return_layers:`\n\n`del return_layers[name]`\n\n`if not return_layers:`\n\n`break`\n\n`super().__init__(layers)`\n\n`self.return_layers = orig_return_layers`\n\n`def forward(self, x):`\n\n`out = OrderedDict()`\n\n`for name, module in self.items():`\n\n`x = module(x)`\n\n`if name in self.return_layers:`\n\n`out_name = self.return_layers[name]`\n\n`out[out_name] = torch.permute(x, (0, 3, 1, 2))`\n\n`return out`\n\n`class BackboneWithFPN(nn.Module):`\n\n`def __init__(`\n\n`self,`\n\n`backbone: nn.Module,`\n\n`return_layers: Dict[str, str],`\n\n`in_channels_list: List[int],`\n\n`out_channels: int,`\n\n`extra_blocks: Optional[ExtraFPNBlock] = None,`\n\n`norm_layer: Optional[Callable[..., nn.Module]] = None,`\n\n`) -> None:`\n\n`super().__init__()`\n\n`if extra_blocks is None:`\n\n`extra_blocks = LastLevelMaxPool()`\n\n`self.body = IntermediateLayerGetter(backbone, return_layers=return_layers)`\n\n`self.fpn = FeaturePyramidNetwork(`\n\n`in_channels_list=in_channels_list,`\n\n`out_channels=out_channels,`\n\n`extra_blocks=extra_blocks,`\n\n`norm_layer=norm_layer,`\n\n`)`\n\n`self.out_channels = out_channels`\n\n`def forward(self, x: Tensor) -> Dict[str, Tensor]:`\n\n`x = self.body(x)`\n\n`x = self.fpn(x)`\n\n`return x`\n\n`class CustomSwin(nn.Module):`\n\n`def __init__(self, backbone_model, FPN=True):`\n\n`super().__init__()`\n\n`# Create a new OrderedDict to hold the layers`\n\n`return_layers = OrderedDict()`\n\n`for i in [1, 3, 5, 7]:`\n\n`return_layers[f'{i}'] = str(i)`\n\n`# Define the in_channels for each layer`\n\n`in_channels_list = [128, 256, 512, 1024]`\n\n`# Create a new Sequential module with the features`\n\n`backbone_module = nn.Sequential(OrderedDict([`\n\n`(f'{i}', layer) for i, layer in enumerate(backbone_model.features)`\n\n`]))`\n\n`# Create the BackboneWithFPN`\n\n`self.backbone = BackboneWithFPN(`\n\n`backbone_module,`\n\n`return_layers,`\n\n`in_channels_list,`\n\n`out_channels=512,`\n\n`extra_blocks=None`\n\n`)`\n\n`self.out_channels = 512`\n\n`def forward(self, x):`\n\n`return self.backbone(x)`\n\n`def load_backbone(trainable_layers=2):`\n\n`backbone = swin_v2_b(weights=Swin_V2_B_Weights.DEFAULT)`\n\n`# Remove the classification head (norm, permute, avgpool, flatten, and head)`\n\n`backbone.norm = nn.Identity()`\n\n`backbone.permute = nn.Identity()`\n\n`backbone.avgpool = nn.Identity()`\n\n`backbone.flatten = nn.Identity()`\n\n`backbone.head = nn.Identity()`\n\n`# Freeze all parameters`\n\n`for param in backbone.parameters():`\n\n`param.requires_grad = False`\n\n`# Unfreeze the last trainable_layers`\n\n`for layer in list(backbone.features)[-trainable_layers:]:`\n\n`for param in layer.parameters():`\n\n`param.requires_grad = True`\n\n`return backbone`\n\n`backbone = load_backbone()`\n\n`backbone_with_fpn = CustomSwin(backbone)`\n\n`anchor_generator = AnchorGenerator(`\n\n`sizes=((8, 16, 32, 64), # For feature map 1`\n\n`(8, 16, 32, 64), # For feature map 2`\n\n`(16, 32, 64, 128), # For feature map 3`\n\n`(32, 64, 128, 256), # For feature map 5`\n\n`(64, 128, 256, 512)), # For the pool layer`\n\n`aspect_ratios=((0.5, 1.0, 2.0),) * 5 # Same aspect ratio for all feature maps`\n\n`)`\n\n`# Use the feature maps from the layers 1,3,5,7 & pool`\n\n`roi_pooler = MultiScaleRoIAlign(`\n\n`featmap_names=['1', '3', '5', '7', 'pool'],`\n\n`output_size=7,`\n\n`sampling_ratio=2`\n\n`)`\n\n`model = FasterRCNN(`\n\n`backbone_with_fpn,`\n\n`num_classes=NUM_CLASSES,`\n\n`rpn_anchor_generator=anchor_generator,`\n\n`box_roi_pool=roi_pooler,`\n\n`min_size=1224,`\n\n`max_size=1632,`\n\n`)`\n\nI also tried to implement FPN, but I'm unsure if my implementation is correct. The Swin Transformer outputs tensors in \\[Batch, Height, Width, Channels\\] format, while PyTorch expects \\[Batch, Channels, Height, Width\\]; to address this, I used \\`\\`\\`torch.permute(x, (0, 3, 1, 2))\\`\\`. I can get outputs from the model, but I don't want to train it if there's an error because my dataset is relatively big and will use a lot of resources for training.\n\nI am not an expert by no means, and this is the first time I created a custom model. Any insights or suggestions would be greatly appreciated.",
    "created_utc": "2024-10-11T02:54:44",
    "num_comments": 6,
    "comments": [
        "Train on a small dataset and see if you can get it to overfit",
        "Second this. If it can’t overfit easily, then a NN is difficult to debug (underfitting is too hard to distinguish from a bad model). See if it can predict well on training data",
        "So, I trained the architecture on a dataset 100 images and it worked better than the version compared to the version without my FPN implementation. Although I have a question, about the MultiScaleRoIAlign, I want to usually detect really small objects (10 x 10 pixels) in high res images. \n\nDo you think output\\_size=7  is okay for this? When I tried to change it to 14, it added 70 million more params to the model, which was surprising to me.",
        "You’ll probably need to play around with different methods, like splitting the image up into smaller quadrants. Hope this helps \n\nhttps://blog.roboflow.com/detect-small-objects/amp/",
        "It looks like you shared an AMP link. These should load faster, but AMP is controversial because of [concerns over privacy and the Open Web](https://www.reddit.com/r/AmputatorBot/comments/ehrq3z/why_did_i_build_amputatorbot).\n\nMaybe check out **the canonical page** instead: **[https://blog.roboflow.com/detect-small-objects/](https://blog.roboflow.com/detect-small-objects/)**\n\n*****\n\n ^(I'm a bot | )[^(Why & About)](https://www.reddit.com/r/AmputatorBot/comments/ehrq3z/why_did_i_build_amputatorbot)^( | )[^(Summon: u/AmputatorBot)](https://www.reddit.com/r/AmputatorBot/comments/cchly3/you_can_now_summon_amputatorbot/)",
        "Thanks a lot! I was already implementing the patch method, but now with that library in the blog post, my code looks much cleaner.",
        "Thanks a lot! I was already implementing the patch method, but now with that library in the blog post, my code looks much cleaner."
    ]
},
{
    "submission_id": "1g153hs",
    "title": "Who would like to help us build something great? 😎",
    "selftext": "If you are the person to help us, please answer a few questions here: [https://tally.so/r/mDDabX](https://tally.so/r/mDDabX)\n\nThank you thank you!",
    "created_utc": "2024-10-11T01:32:55",
    "num_comments": 0,
    "comments": []
},
{
    "submission_id": "1g135ix",
    "title": "Real time table tennis ball tracking on mobile",
    "selftext": "I'm working on a university project to develop a real-time ping pong (table tennis) ball tracking mobile app. The app should ideally be cross-platform, but I’m willing to focus on Android if developing for iOS is too complex. I would prefer to use a pre-trained model, if available, to simplify the process. I need a step by step guidance on how to do this as I have never done a computer vision project before. For example:\n\n**Where to start:** Where can I find a pre-trained model, and how do I download and use it in a mobile app?\n\nT**ools and technologies**: Which tools and frameworks should I use (e.g. TensorFlow Lite, OpenCV, Yolov11)? How do I integrate them for object detection and tracking?\n\n**Approach**: Should I use additional techniques (like color or sound detection) to improve accuracy? What strategy should I follow for real-time performance on mobile?\n\nAny advice on these points would be greatly appreciated.",
    "created_utc": "2024-10-10T23:04:07",
    "num_comments": 7,
    "comments": [
        "Have you taken videos of a table tennis match with a mobile phone and check the quality?\nIf you want 3d trajectories , you have to considered that you need to calibrate your camera and figure where it is relative to the table.\nIf you intend to capture hand held video , expect that you will need to track the camera movement too.\nTracking and reconstructing trajectories does not require much neural nets, you can do a lot with simple filters and classical tracking methods.\nOverall, you should simplify the project and start with non real time processing and fixed camera. Don't code mobile apps before you have a proof of concept that works for the simplified situation.",
        "what are the metrices you want to provide the user ? the score ? or the trajectories or what ?",
        "Yolo\n\nDon't overcomplicate it.",
        "Thanks for the suggestions. The phone/camera will be fixed, so it won't be moving. My main concern is what tool will allow me to track the ball movement, in real-time, and on a mobile phone.",
        "score ideally, but trajectories is also an option",
        "Is Yolo good for detecting and tracking small objects like a table tennis ball?",
        "SAHI technique can help with small object tracking"
    ]
},
{
    "submission_id": "1g0y48h",
    "title": "Darknet YOLO to ONNX, is it possible?",
    "selftext": "I have two questions. One is about how to convert my YOLOv4 model from Darknet to ONNX. And my other question is whether anyone knows if inference can be done with ONNX using multiple GPUs.\n\nThanks!",
    "created_utc": "2024-10-10T18:04:39",
    "num_comments": 4,
    "comments": [
        "https://stackoverflow.com/a/62815292\n\nWhy onnx? Darknet is pretty fast.",
        "thank you btw",
        "because in production we use onnx",
        "Same here. Onnx in production all the time."
    ]
},
{
    "submission_id": "1g0xlxo",
    "title": "Export PyTorch Model to ONNX – Convert a Custom Detection Model to ONNX",
    "selftext": "Export PyTorch Model to ONNX – Convert a Custom Detection Model to ONNX\n\n[https://debuggercafe.com/export-pytorch-model-to-onnx/](https://debuggercafe.com/export-pytorch-model-to-onnx/)\n\n  \nExporting deep learning models to different formats is essential to model deployment. One of the most common export formats is ONNX (Open Neural Network Exchange). Converting to ONNX optimizes the model to utilize the capabilities of the deployment platform effectively. These can include Intel CPUs, NVIDIA GPUs, and even AMD GPUs with ROCm capability. However, getting started with converting models to ONNX can be challenging, even more so when using the converted model for inference. In this article, we will simplify the process. We will **export a custom PyTorch object detection model to ONNX**. Not only that, but we will also learn how to use the exported ONNX model for inference with CUDA support.\n\nhttps://preview.redd.it/g0hz1pgdw0ud1.png?width=1000&format=png&auto=webp&s=685f76732fd9755d1a38e5839d0a609e1c024eba\n\n  \n",
    "created_utc": "2024-10-10T17:38:15",
    "num_comments": 4,
    "comments": [
        "Don’t you just call `torch.onnx.export`?",
        "That's true. However, beginners generally face issues when exporting object detection models and whether to choose static or dynamic shapes. The article further goes on to show how to write a video inference script that can run and load the ONNX model for inference in real-time videos.",
        "I don’t see any explanation of dynamic vs static access. In fact you write\n\n> It is important to export the ONNX model for a specific width and height. \n\nWhich is certainly not always true even if it is for your model and application.\n\nI’m sure you meant this for beginners but frankly I think if people want to understand onnx export from torch they’re better off reading the various [torch docs](https://pytorch.org/tutorials/advanced/super_resolution_with_onnxruntime.html) on the matter.",
        "Appreciate your feedback. Will try to do better."
    ]
},
{
    "submission_id": "1g0up6q",
    "title": "Starting new r",
    "selftext": "Hi fellow Computer Vision Experts! We're starting a new r/, focusing more on collaboration and discouraging spam & brand affiliations 😊 \n\n🤞Join us on r/computervisiongroup",
    "created_utc": "2024-10-10T15:12:34",
    "num_comments": 2,
    "comments": [
        "I don't see much spam and brand affiliations here. Have you some links for such problematic posts. Otherwise I don't see a reason to switch.",
        "Dude wants to have his own sub that's it 👀"
    ]
},
{
    "submission_id": "1g0thvb",
    "title": "How to find average velocity from self-generating particles that can stick to each other? I guess the normal way would be to hand label but I was wondering if there's an automated way to average across the whole video. Sometimes the blobs stick together and sometimes there's random noise or flashes",
    "selftext": "",
    "created_utc": "2024-10-10T14:18:03",
    "num_comments": 12,
    "comments": [
        "What's the source of the data that creates the binary image?",
        "First, you should remove some noise. You could do an opening for each frame, and then use a bitwise\\_and operator between a frame and the next one. In this way, some of the random flashes should be removed, plus you could treat the correspondences as keypoints.  \nThen you could use something like optical flow to estimate their new positions, then their velocities. Finally i would use RANSAC to get the flow velocity",
        "Needs more info.",
        "First define the “particles”",
        "Without better resolution and less noise, I don't think you can do much in this case.",
        "A spatio-temporal fourier might work for this kind of motion.\nThis would provide the average motion without requiring explicit tracking. Relates to \"optical snow\" work by Langer, Mann, and others.\nAs an example, I used it to measure the average speed of water in a waterfall.",
        "It's kind of like a very low resolution video of someone pouring sand into a strong wind during a lightning storm and you're zooming in on the sand particles right when it's being poured. Can't really do traditional piv cuz it's too low resolution. This is gaussian background subtracted video.",
        "Ok great thanks",
        "ionized boron in plasma",
        "Sorry, I meant in the image, what defines a particle? For example, is 1 pixel considered a particle or a group (blob) of pixels? \n\nWhat are the limits of the size? What happens if the blob starts as 1 but breaks into 2, is it now 2 blobs?\n\nThen after you define the blob, the process is straight forward. \n\nTrack the blob centroid over time and distance and you can calculate velocity",
        "Oh that's what I'm kind of asking for. I don't know exactly what to define in order to find the flow. It would be easy if it's just following centroids. But then I'd also want to automatically tag blobs that form.",
        "To me it looks like your video is too noisy in order to determine the blobs, I’d suggest looking at a different approach"
    ]
},
{
    "submission_id": "1g0rqcc",
    "title": "action recognition using poses + image embeddings?",
    "selftext": "Looking for some help designing a model for categorizing individual video frames. The videos are to be processed offline and are long duration.\n\nAn illustrative example is a five-minute-long video captured by an action camera on a skier's helmet. I would want to identify the scenario where the skier jumps off a ramp, flies over another skier who is standing still, and then successfully lands. Every frame between them leaving the ramp and landing needs to be assigned to that category. To do this, the model needs to remember that the skier was approaching a ramp (once they are in the air the ramp is no longer visible), that the other skier was standing still (relative pose stays the same across frames), and that they don't crash after the landing. \n\nAssume that I have thousands of videos already annotated with the class of each frame. \n\n**What I was thinking is to feed human pose coordinates and DINO embeddings (of each frame in it's entirety) into a bi-directional LSTM. The poses and DINO embeddings would be obtained ahead of time and concatenated into a single vector. DINO embeddings are \"in domain\" for the actual task (which is not related to skiing!).** \n\nIs that a sound approach? I've never done anything like this before (video analysis) but am pretty sure I can't just feed entire videos into any model that can run on regular hardware!  \n\n\n",
    "created_utc": "2024-10-10T13:01:15",
    "num_comments": 1,
    "comments": [
        "Check out *LLM as a regressor* and references therein on page 8 of [this](https://arxiv.org/pdf/2312.17432) review paper. You could query the time stamps of the events described in plain language. Along these lines you could also try Google Gemini Pro for experimentation. I know it can consume a long video and a text prompt though I don’t think they have fine tuning for that feature yet and I doubt the default models will deliver great accuracy in your domain.  In these examples you could try to incorporate the pose information into the query. It would at least give you a sense of how those data may benefit before you dive into building a whole new model.  \n\nFloating another idea, you could query the accelerometer metadata often available with these action cameras. You should be able to identify periods where the device is in flight (>> -9.8 reading on the z axis). Presumedly this big jump off a ramp will have the longest duration. Isolating those flight periods might reduce your search space or indeed they may even have characteristic traces you could classify directly using a rolling window as input and your labels derived from the video. Also there exists many time series ml models that might suit accelerometer data. However you use it, I believe the accelerometer data represents a strong signal in your domain.\n\nI’ll be curious to know where you go with this so any project links I can follow would be appreciated."
    ]
},
{
    "submission_id": "1g0rlxd",
    "title": "Final year project",
    "selftext": "Hi guys I am in my final year of computer science, and I need a very unique and complex idea for my final year project based on computer vision. suggest me something.",
    "created_utc": "2024-10-10T12:55:58",
    "num_comments": 8,
    "comments": [
        "Deblurring license plates.",
        "I would try to do something with diffusion models, they seem like fun!",
        "Complex: Detect anomalous behaviour of humans in a video. It will prevent thefts in retail environments",
        "Deepfake detector - lot to show but also lot of resources available",
        "try to reimplement a recent paper on a smaller scale. What about [DART](https://arxiv.org/pdf/2410.08159) that came out 2 days ago? The paper is quite well written and could make for a super interesting project (probably a complex challenge though)",
        "*enhancing* license plates",
        "maybe we can work around with something like diffusion, just change the noise procedure with something like blurring (idk if its valid or not, lol). so its periodically remove blur at inference."
    ]
},
{
    "submission_id": "1g0rcbn",
    "title": "Need help with an image recognition model that's distinguishing between 2D black-and-white glyphs. ",
    "selftext": "Hello! I am a beginner when it comes to CV and ML stuff in general, and for my work I needed to build a model that's using an Intel RealSense to distinguish between glyphs that will be printed onto a page. The glyphs are black-and-white line art with some shading that's all greyscale, and the model I've been using is having trouble predicting the correct glyph. To be more specific, it's only ever predicting the more detailed out of the two glyphs. All of my references are photos I took with the RealSense and then just cropped out the parts that weren't paper and glyph. I'm looking for some help either getting my current model to be able to distinguish the glyphs or find some other more efficient way of doing this. I'll paste my current code below. Unfortunately it's partially AI generated cuz I have no idea what I'm doing, and I would love any assistance I can get from you all, and I'm more than willing to clarify any details I can. Thanks!\n\n    import os\n    import numpy as np\n    import tensorflow as tf\n    import keras.api as keras\n    from keras.api.preprocessing import image_dataset_from_directory\n    from keras.api import layers, models\n    from keras.api.callbacks import EarlyStopping\n    from keras.api.saving import save_model\n    import tf2onnx\n    import random\n    \n    \n    \n    \n    import h5py\n    print(h5py.__version__)\n    \n    # Parameters\n    BATCH_SIZE = 64\n    IMG_SIZE = (512, 512)  # Resizing images to 224x224 (you can adjust this)\n    EPOCHS = 100\n    DATA_DIR = '.\\Documents\\Workspace\\Big Book\\Assets\\ProjectAssets\\Scripts\\ML Stuff\\dataset'  # Change this to the path of your dataset\n    os.environ['TF_KERAS_NATIVE_SAVE'] = '1'\n    \n    \n    \n    # Load dataset\n    print(os.getcwd())\n    print (tf.io.gfile.listdir(DATA_DIR))\n    train_dataset = image_dataset_from_directory(\n        DATA_DIR,\n        validation_split=0.2,\n        subset=\"training\",\n        seed=random.randint(0, 1024),\n        image_size=IMG_SIZE,\n        batch_size=BATCH_SIZE\n    )\n    \n    validation_dataset = image_dataset_from_directory(\n        DATA_DIR,\n        validation_split=0.2,\n        subset=\"validation\",\n        seed=random.randint(0, 1024),\n        image_size=IMG_SIZE,\n        batch_size=BATCH_SIZE\n    )\n    \n    num_classes = len(train_dataset.class_names)\n    print(f\"Number of classes: {num_classes}\")\n    \n    # Normalize the pixel values to [0, 1]\n    normalization_layer = layers.Rescaling(1./255)\n    \n    train_dataset = train_dataset.map(lambda x, y: (normalization_layer(x), y))\n    validation_dataset = validation_dataset.map(lambda x, y: (normalization_layer(x), y))\n    \n    # Cache and prefetch for performance\n    AUTOTUNE = tf.data.AUTOTUNE\n    train_dataset = train_dataset.cache().prefetch(buffer_size=AUTOTUNE)\n    validation_dataset = validation_dataset.cache().prefetch(buffer_size=AUTOTUNE)\n    \n    # Define the model\n    '''\n    model = models.Sequential([\n        layers.Input(shape=(IMG_SIZE[0], IMG_SIZE[1], 3)),\n        layers.Conv2D(32, (3, 3), activation='relu'),\n        layers.MaxPooling2D((2, 2)),\n        layers.Conv2D(64, (3, 3), activation='relu'),\n        layers.MaxPooling2D((2, 2)),\n        layers.Conv2D(128, (3, 3), activation='relu'),\n        layers.MaxPooling2D((2, 2)),\n        layers.Conv2D(128, (3, 3), activation='relu'),\n        layers.MaxPooling2D((2, 2)),\n        layers.Flatten(),\n        layers.Dense(512, activation='relu'),\n        layers.Dense(num_classes, activation='softmax')  # Change output based on number of classes\n    ]) '''\n    \n    data_augmentation = tf.keras.Sequential([\n        layers.RandomFlip(\"horizontal_and_vertical\"),   # Randomly flip images\n        layers.RandomRotation(0.2),                     # Rotate images by up to 20%\n        layers.RandomZoom(0.2),                         # Random zoom in/out\n        layers.RandomContrast(0.2)                      # Adjust contrast\n    ])\n    \n    model = tf.keras.Sequential([\n        layers.Conv2D(32, (3, 3), activation='relu', input_shape=(512, 512, 3)),\n        #data_augmentation,\n        layers.MaxPooling2D(2, 2),\n        layers.Conv2D(64, (3, 3), activation='relu'),\n        layers.MaxPooling2D(2, 2),\n        layers.Conv2D(128, (3, 3), activation='relu'),\n        layers.Dropout(0.5),\n        layers.MaxPooling2D(2, 2),\n        layers.Conv2D(256, (3, 3), activation='relu'),  # Add more layers\n        layers.MaxPooling2D(2, 2),\n        layers.Flatten(),\n        layers.Dense(512, activation='relu', kernel_regularizer='l2'),\n        layers.Dropout(0.5),\n        layers.Dense(num_classes, activation='softmax')\n    ])\n    \n    # Compile the model\n    optimizer = tf.keras.optimizers.Adam(learning_rate=0.0001)\n    model.compile(optimizer=optimizer,\n                  loss='sparse_categorical_crossentropy',\n                  metrics=['accuracy'])\n    \n    early_stopping = tf.keras.callbacks.EarlyStopping(monitor='val_loss', patience=5, restore_best_weights=True)\n    model_checkpoint = tf.keras.callbacks.ModelCheckpoint('best_model.keras', save_best_only=True)\n    initial_learning_rate = 0.001\n    drop_rate = 0.5\n    epochs_drop = 5\n    \n    def step_decay(epoch):\n        return initial_learning_rate * (drop_rate ** (epoch // epochs_drop))\n    \n    lr_schedule = tf.keras.callbacks.LearningRateScheduler(step_decay)\n    \n    history = model.fit(\n        train_dataset,\n        validation_data=validation_dataset,\n        epochs=EPOCHS,\n        callbacks=[model_checkpoint, lr_schedule]\n    )\n    \n    # Save the model\n    #model.save('image_classification_model.keras')\n    save_model(model, \"image_classification_model.keras\")\n    \n    # Evaluate the model\n    loss, accuracy = model.evaluate(validation_dataset)\n    print(f\"Validation Loss: {loss}\")\n    print(f\"Validation Accuracy: {accuracy}\")\n    \n    \n    \n    import tf2onnx\n    \n    # Load the Keras model\n    model = tf.keras.models.load_model('image_classification_model.keras')\n    \n    # Save the model in TensorFlow's SavedModel format\n    saved_model_path = \"./saved_model\"\n    model.export(saved_model_path) \n    \n    print(f\"Model saved in SavedModel format at: {saved_model_path}\")\n    \n    # Path where the SavedModel is located\n    saved_model_path = \"./saved_model\"\n    onnx_model_path = \"image_classification_model.onnx\"\n    \n    # Convert the SavedModel to ONNX\n    #python -m tf2onnx.convert --saved-model ./saved_model --output image_classification_model.onnx --opset 13\n    \n    print(f\"ONNX model saved at: {onnx_model_path}\")",
    "created_utc": "2024-10-10T12:43:56",
    "num_comments": 1,
    "comments": [
        "Create bounding boxes on the glyphs with cv2 find contours and just feed it one at a time\n\nMight not hurt to use some form of morphological operations or edge detection to beef up the image before you give it"
    ]
},
{
    "submission_id": "1g0kyb4",
    "title": "Counting Cows",
    "selftext": "For my graduate work, I need to develop a counter that counts how many cows walk underneath the camera. I have done some other ML work, but never with computer vision. How would be best to go about training this model?\n\nDo I need to go through all my training data and label the cows and also label each clip with how many cows went under the camera? Or do I just label each clip with the number of animals? \n\n\n\n# I am a complete beginner in computer vision and just need help finding the right resources to educate myself on how to do my project.",
    "created_utc": "2024-10-10T08:06:12",
    "num_comments": 17,
    "comments": [
        "I read that title and thought \"why is a post about a 90s rock band in the computervision subreddit?\"\n\nApparently I need my coffee\n\nThere are a number of preexisting models out there that can detect things. I've used \"detectron\" before, but this is outside my specific expertise so I'm sure there are much better / newer options. \n\nYOLO would be the first thing I looked at, and this seems to indicate that might not be a bad spot: https://www.mdpi.com/2076-2615/13/22/3535",
        "Just use ultralytics yolo trained on coco. One of the classes is cow. Note that if you have a camera overhead the model might not work very well given that none of the training data are from such a perspective. Having said that, you probably only need a couple of thousand images  from your vantage to significantly improve performance. CGPT can walk you through the steps.",
        "jus finished training a yolo 8 on aerial and satellite images, use ultralytics like others said and there are a couple of notebooks out there with a step by step of loading the pre-trained (search “yolov8 fine tune”), how your data should be formatted, fine-tuning parameters for training etc\n\nit’ll be simple for you though, bunch of cow images in different orientations, set up a yaml or something with ur classes, split data and labels in test train val and ur set.\n\nNo need to train the model on videos (?) for yolo object detection. \n\nHow it will work is your labels or annotations with be bounding box coords. When you train your model, you’ll pass in your image and the label with the coordinates for the bounding boxes around all the cows in your images. \n\nso let’s say maybe you got a video like 10mins long of like 100 cows moving into a pen I would just chop up that video into image frames and use it as a starting point for example.\n\nresources :\n[https://github.com/roboflow/notebooks/blob/main/notebooks/train-yolov8-object-detection-on-custom-dataset.ipynb](https://github.com/roboflow/notebooks/blob/main/notebooks/train-yolov8-object-detection-on-custom-dataset.ipynb)\n\nto get your feet wet i strongly recommend just following along with a youtube vid or tutorial and try to train a model on the same custom data they used then once your familiar switch and start prepping your own data",
        "I would at all costs try to avoid having to train your own model. A suggestion would be to use the Megadetector model that was developed in part by Microsoft for game cameras (weird I know). It detects animals, people, and vehicles. Assuming there won’t be any other animals aside from cattle (sounds like you’re describing an agricultural setting where there is likely nothing other than cows) the animals group can work for cows. It uses YOLOv5 architecture and is pretty easy to get running. Cheers.",
        "Just use yolov3 with coco, you could add a line to see how many cows cross the line. Check roboflow newsletter for examples",
        "\"They paved paradise, put up a parking lot\"",
        "Thank you! Would there be a way with this method to eventually distinguish between cows and calves? What would I have to do to accomplish that?",
        "Why avoid training at all costs? Given that the cows in OPs images are taken from above, it’s unlikely that coco trained models will perform well. Retraining is easy and will significantly improve OPs performance with only a couple thousand new images within their domain.",
        "I like the idea of not having to train my own model, but as was stated in another comment, all my video will be from above, so perhaps the performance of these prexisting models wont be as good. Also, I would like to be able to distinguish between cows and calves eventually. Is that feasible with the megadetector model?",
        "Use the size of the BBOX to determine cow or calf",
        "You would need to retrain with cow, and calf as distinct classes. However you should beware double detections that can happen when two classes have very similar features. That is, for an image there may be  one cow and your detect a cow and a calf. Make sure you do class agnostic NMS to help to mitigate this issue. Also be aware that if all of your training data come from single overhead vantage at a fixed height, then  you may find that the data don’t generalize well to higher and lower vantages.",
        "Does retraining mean not using pretrained weights but having some other init of weights?\n\nWhile if u have used an pretrained weights of a mode, its called transfer learning(fine tuning?)",
        "I have actually had pretty decent performance on overhead images (that’s for game species like deer, but still would probably hold true for cattle). For me the expensive part of implementing any sort of CV model is training and generally my ideas aren’t original and someone with more time and experience than me has already trained a pretty capable model",
        "It can mean starting from fully random weights or training from existing weights and “freezing” (not updating the weights during training) parts of the graph or even freezing everything except the fully connected classification layer. It’s almost always better to start from pre trained weights, especially in your case where you have existing weights that have been specifically trained on a class in your dataset. Fine tuning is not a well defined term but it usually means taking existing weights and making smaller changes per step than we’re used in the initial training process. Really though it’s all just training with different parameters.",
        "Fair enough! Thanks for the insight. Do you think using that model with a little tweaking, I could run this in real time to display a count of how many animals have crossed a threshold under the camera? Maybe with a Nvidia Jetson?",
        "Very easily. I always recommend on jetsons converting models to TensorRT models. Seems to really help with performance.",
        "Thanks for the help!"
    ]
},
{
    "submission_id": "1g0idii",
    "title": "Trying to use ultralytics to measure the speed of a when it passes a certain point.",
    "selftext": "As the car is moving from L to R I'd need the detection line to be more or less directly in the middle of the video, top to bottom.\n\nIn one of their examples they have the tracking done horizontally - their video is 1280x720\n\n    line_pts = [(0, 360), (1280, 360)]   \n\nThis provides a point which appears to be 0 on the x axis and 360 pixels up on the y axis (I'm hoping I've read that correctly). The second point is then 1280 on the x axis, and still 360 pixels up on the y axis.  This provides a straight line it seems.                 \n\nThe measurements of the video I have are 2688x1520. I need a line that covers basically the opposite of the above, I need it to be vertical in the video as opposed to horizontal.  I'm not 100% sure how I would tell it the points I need?\n\nIn my head is would be \\[(1344, 1520)\\] (half way across the x axis and at the top of the y?) and the second point to be \\[(1344, 0)\\] still half way across on the x axis but 0 on the y axis. OR, the first y point at 0 and the second y point at the top?\n\nWhen I go to run the code I don't get the line down the middle of the screen whilst the code runs. The output speed estimation is basically just a copy of the original footage (same res) with no tracking on it at all.\n\nI've probably done something really silly so I'd appreciate a nudge in the right direction.",
    "created_utc": "2024-10-10T06:06:50",
    "num_comments": 5,
    "comments": [
        "havent read the whole story, its better to add images but for speed:  \nyou need 2 things  \ntime and distance  \nfor time you will have fps  \nfor distance you will need PPM  \nand you will get speed",
        "```\nline_pts = [(0, 360), (1280, 360)]\n               [(x1, y1), (  x2,  y2)]\n```\n\nThe line is draw between endpoints. Since `x2 - x1 = 1280 = img_width` and `y2 = y1 = img_height / 2` the line is horizontal, since `y2 - y1 = 0`. For your image, if you want to vertical line instead, you'd have points like\n\n```\nline_pts = [(1344, 0), (1344, 1520)]\n```\n\nWith this, `x1 = x2 = img_width / 2` meaning `x2 - x1 = 0` the your line should be drawn vertically.",
        "Indeed.\n\nThanks for the response.\n\nUA has pre trained models for object detection and speed.\n\nThe script takes the dimensions and FPS and estimates the speed based on its previously trained models.\n\nFor me it’s more about telling it when I want it to take the speed. I’m messing up or misunderstanding the coordinates or the video isn’t like enough/car isn’t in view long enough.\n\nI’d done a very amateur estimate of the car speed and I just wanted to see if UAs trained models could give me a result as well.",
        "The aim I think you're after, is to understand when an object (assuming left to right motion) has its bounding box cross the vertical line and then exit the vertical line. In doing so, you should be able to calculate the time it takes for the object to pass by a fixed point (the other way would be to use the center of the object and use two fixed points/lines). Using the frame-count and FPS, you can calculate the total time. Then you need distance, which you can try to approximate, but with a single camera setup, it will be difficult to get something extremely precise, but you could probably get close by an order of magnitude.",
        "there are 2 ways  \n1. Create a dictionary with all speeds + center of the bounding box cordinated, then filterout the ones which falls inside your range  \n2. there must be a function to get the speed right ? call it once the object is in desired range, simple"
    ]
},
{
    "submission_id": "1g0dbkm",
    "title": "Need Help Checking Research Paper Implementation.",
    "selftext": "Hey everyone,\n\nI’m currently working on implementing a research paper related to Robust Road Vanishing Point Detection, but I’m unsure if my implementation is accurate. I followed the methodology and algorithms as outlined in the paper, but the results are not matching my expectations, and I’m not sure where I might be going wrong.\n\nHere are the details of my project:\n\n* **Research Paper:** A Robust Road Vanishing Point Detection Adapted to the Real-world Driving Scenes - [https://www.mdpi.com/1424-8220/21/6/2133](https://www.mdpi.com/1424-8220/21/6/2133)\n* **Problem Domain:** \\[Detecting Vanishing Point\n* **My Implementation:** [https://github.com/takshrana/Robust-Road-Vanishing-Point-Detection](https://github.com/takshrana/Robust-Road-Vanishing-Point-Detection)\n\nIf anyone has experience with this paper or similar implementations, I’d really appreciate it if you could take a look at my code/results to help identify where I might be going wrong or any improvements. I can provide more details, code snippets, or visual outputs if needed.\n\nThanks in advance for your help!",
    "created_utc": "2024-10-10T00:24:52",
    "num_comments": 0,
    "comments": []
},
{
    "submission_id": "1fzzdyz",
    "title": "Detecting objects with similar shape but different scale/dimension? ",
    "selftext": "I need some ideas on how I can write MV project that finds out two pipes for example that have same shape but are scaled by factor of 2 to 4. \n\nMy understanding so far is that iphone provides some AR library that gives pixel depth and we can get distances between pixels. Is that a good approach? \n\nThanks.  \n",
    "created_utc": "2024-10-09T12:08:29",
    "num_comments": 5,
    "comments": [
        "your idea is just providing a scale estimate using metric depth after detecting an object?\n\nthe library does not give the distance between pixels directly, but the distance *to* pixels. And make sure you get metric depth, not relative depth. iPhones after the 12 have a time-of-flight sensor that can give you metric information.\n\nYou'll face the following difficulties: first, within a bounding box, which pixels are  actually your object? if your object detector provides segmentation masks, then you're kinda covered. And also figuring out precisely the size of an object irrespective of its orientation, for a beach ball that's not a problem, but for a looooong pipe the relationship between on-screen extent and physical size isn't trivial.",
        "This sounds very similar to a video I did on template matching. Maybe you can do segmentation and apply a multiscale template matching. Depending on your pov of the objects this might work",
        "My idea is to segment and provide layers to user to select. After that look for depth perception to get some estimate of object in 3D. Which is also not super easy but somewhat feasible! Because it will require some understanding of which angle image is looking from.",
        "Can’t you please share the link of the video.",
        "If you're using an iPhone with a 3D camera, you can already convert an image into a point cloud! (without having to figure out the orientation of the camera wrt any object)",
        "But then i will have to deal with ML on point clouds! Not fun in my opinion but may be thats the only way to do it properly! Is PointNet a good library for that?",
        "No, you can still detect things in 2d on images, the 3d part kinda comes for free if you have a 3d camera like the iphone does.",
        "So use 2d to detect then use 3d to classify shapes."
    ]
},
{
    "submission_id": "1fzymix",
    "title": "Do you know of any accessible public camera to test my ml vision software with",
    "selftext": "I am building a crowd counting software that has some additional features. So i wanted to dabble with yolov5 as the suggested model. I want to test its accuracy and in general play with the model. So do you know of any public, especially in crowded places, camera that i can hook into. Any other suggestions you might have is also much appreciated.",
    "created_utc": "2024-10-09T11:36:01",
    "num_comments": 11,
    "comments": [
        "I would just use a video that has already been recorded",
        "Since pretrained models for cars are pretty good, how’s about a highway camera feed? [This is one](http://newtoreno.com/ca-i80-webcams-sodasprings-wb.htm) up near Tahoe that has consistent traffic. You would have to inspect the page and pull the network stream link but that shouldn’t be too tough. Then from there OpenCV has some nice functions to read in video streams through RTSP/RTMP. Cheers",
        "Why not use it on prerecorded videos? At least that way you can verify the count if you need to.",
        "New York Live feed: [https://www.earthcam.com/cams/newyork/timessquare/?cam=tsrobo1](https://www.earthcam.com/cams/newyork/timessquare/?cam=tsrobo1)",
        "Here are couple of publicly available cameras : \n\n[https://www.earthcam.com/](https://www.earthcam.com/)  \n[http://www.insecam.org/](http://www.insecam.org/)",
        "Thank you for the suggestion",
        "But my intended use case is for people. It might be a good idea for me to get used to the model just for practice though. How can i access it?",
        "you will overfit on the prerecorded videos!  \nimagine you build your app around a specific video recording only to come to the conclusion that, that said video was relatively easy and your app doesnt work under certain conditions.   \ndynamic video streams allow you to better test your code.   \nI myself would have started with a recorded video at first, have an initial implementation and then run it against several public cameras to see how it performs.",
        "Thank you",
        "Access what exactly?",
        "Access to the real time feed so that i can use as an input to the model if i am not missing anything"
    ]
},
{
    "submission_id": "1fzxzvb",
    "title": "Need Advice on Final Year Project (OpenCV Attendance System) – Is It Too Simple? What Technologies Should We Use?",
    "selftext": "Hi everyone,\n\nMy teammates and I are working on our final year project, and the idea is to create an attendance system using OpenCV for face recognition. The system will capture student faces with a camera and automate the attendance process. We’re planning to use Spring Boot for the backend to manage the database, user management, and attendance logs.\n\nWe’ve heard that this kind of project might be considered *too simple* since there are a lot of pre-built APIs and tools that can do most of the work for us. Is this true? **How can we make the project more challenging and show more effort?**\n\nAlso, we’re looking for suggestions on:\n\n* **What advanced technologies or frameworks** (like YOLO, TensorFlow, etc.) could we use to make the facial recognition more impressive or accurate?\n* **What database would you recommend** for handling attendance data efficiently with Spring Boot?\n* **What technologies, frameworks, or features** can we integrate to show that we’ve really put effort into the project, instead of just using ready-made APIs?\n\nAny advice would be really helpful! Thanks in advance!",
    "created_utc": "2024-10-09T11:09:28",
    "num_comments": 4,
    "comments": [
        "Unless this is the topic of the class, I would suggest considering something less objectionable than face-based surveillance technology.",
        "Hi, great idea. I need such a system myself. Accounting of employees' working hours in companies is very relevant now. Make a product faster, I'm ready to sell ))))",
        "Thanks for your feedback! We’re focusing on computer vision for our graduation projects this year, which is why we came up with this idea. We could change our project from a student attendance system to an employee attendance system for a company. This might help with privacy concerns since it would fall under company policies.\n\nThat said, we’re still wondering if the project is too simple and how we can demonstrate that it’s not. We can also talk to the company where we’ll test the system about privacy issues to make sure we’re covered. I appreciate your thoughts!",
        "In my opinion there is nothing wrong with the main concept being simple. A good way to go about it would be to implement your current idea extremely well first (maximise performance). Once you have time, you can focus on adding extra value (features), and perhaps you can market it as more than just an employee attendance system after doing so."
    ]
},
{
    "submission_id": "1fzxqgr",
    "title": "Looking for Budget-Friendly Camera Suggestions for My Capstone Project (Face Recognition)",
    "selftext": "Hi everyone,  \nI'm working on my capstone project, which is an attendance system using face recognition with OpenCV. I'm looking for a budget-friendly camera that I can mount on the classroom wall to capture the students' faces effectively. It should have decent coverage and work well for real-time video processing. Do you have any recommendations for cameras on Amazon that would work for this purpose?",
    "created_utc": "2024-10-09T10:58:54",
    "num_comments": 0,
    "comments": []
},
{
    "submission_id": "1fzwqg8",
    "title": "Is there a 2023/2024 version of a full course like Justin Johnson@UMich or Andrew Karpathy@Standord?",
    "selftext": "Are there any free YouTube playlists of Computer Vision classes? Justin’s from 2019 is the latest publically available and that uses tech from 5 years prior to 2019, and 2019 itself is 5 years prior to now. So before I commit to 30 hours of a 2019 course, I wanted to see if anyone had more modern recommendations?",
    "created_utc": "2024-10-09T10:16:47",
    "num_comments": 1,
    "comments": [
        "Even though Justin's lectures are 5 years old, I believe they're still relevant today. The foundational knowledge remains the same, and it's neat to see how the state-of-the-art models were back then compared to how they are today. Also, his lectures are wonderful! I keep coming back to them every so often."
    ]
},
{
    "submission_id": "1fzw53y",
    "title": "Feature matching/detection algorithm for perspective changes?",
    "selftext": "I am writing an application using OpenCV to stich together images into a panorama.\n\nOne step is identifying common features between the images. I played around with SIFT, ORB, AKAZE etc - and while they do work, as soon as the perspective, illumination or quality between the images differs to much,  they start to perform very poorly.\n\nAre there algorithms that are more reliable? I heard about various deep-learning based algorithms, but it seems none of them are implemented in OpenCV? (maybe I am wrong).\n\nWhat algorithms are more reliable than the mentioned ones? Thanks.\n\n  \nI also recently stumbled over a modified variant of SIFT, namely ASIFT:  \n[https://ducha-aiki.github.io/wide-baseline-stereo-blog/2020/08/06/affine-view-synthesis.html](https://ducha-aiki.github.io/wide-baseline-stereo-blog/2020/08/06/affine-view-synthesis.html)\n\n  \nHas anyone experimented with this yet? It seems like it is not in standard opencv, and considering I am working with Rust (opencv-rust wrapper), implementing it myself just to test it would be very tedious. Would love to hear your thoughts on this.\n\n  \n",
    "created_utc": "2024-10-09T09:52:30",
    "num_comments": 2,
    "comments": [
        "https://www.reddit.com/r/computervision/comments/1evwi9i/all_worthy_feature_matching_algorithms_in_a/",
        "Thanks, that is interesting. Unfortunately they dont seem to have ASIFT. :/"
    ]
},
{
    "submission_id": "1fzv7bk",
    "title": "Nvidia Jetson Nano with ROS2 and Yolov8 working with the GPU",
    "selftext": "Hello, if anyone had some nightmares for having yolov8 and make it doing inferences on the GPU while having ROS2 Iron on a Nvidia Jetson Nano (blocked at Ubuntu 20), here's a docker image serving has a base image for your projects :\n\n[https://github.com/aaalloc/jetson-nano-ros2-yolov8](https://github.com/aaalloc/jetson-nano-ros2-yolov8)",
    "created_utc": "2024-10-09T09:13:33",
    "num_comments": 2,
    "comments": [
        "Yeah jetsons can be frustrating to use",
        "I don’t know about the new one but the the Jeston nano 4GB that is EOL and so blocked at Ubuntu 20, is really a pain In the ass to work with …"
    ]
},
{
    "submission_id": "1fzuwqx",
    "title": "https://zoom.us/webinar/register/WN_JFxgAvQ0RQqCF4re06rYcA#/registration",
    "selftext": "",
    "created_utc": "2024-10-09T09:01:24",
    "num_comments": 0,
    "comments": []
},
{
    "submission_id": "1fzui3j",
    "title": "Help me installing YOLO, I'm a begginer.",
    "selftext": "Hello, after finishing my BsC in theoretical physics 3 months ago, I embarqued on a project that I have to use YOLO on. I'm very new to programming, knowing only matlab and very simple python, and especially new to computer vision. \n\nI've been following a lot of tuturials on how to install everything but there are always errors occuring mid instalation.\n\nSo far I've watched and followed more than 10 tutorials and github pages.\n\nThe one that seems the best was the AS-One library but there have been errors such as \"failing to build wheel for lap\" when doing the \"pip install lap\". This is not the only one but the first one I've encountered.\n\nIf anyone could either reccomend me any tuturials or guides to follow, or even send me a dm with instructions I would really appreciate it!",
    "created_utc": "2024-10-09T08:44:24",
    "num_comments": 5,
    "comments": [
        "Easiest is to install `ultralytics`.\n\nYou can follow the docs for installation.\n\nhttps://docs.ultralytics.com/quickstart/\n\nIf you have an NVIDIA GPU, you may have to install PyTorch with CUDA first before installing `ultralytics` which can be done by using the command on the website (after you select the right options):\nhttps://pytorch.org/get-started",
        "Ultralytics is the easiest way to start",
        "/solved",
        "THANK YOU SO MUCH.\n\nit finally worked",
        "thaaaanks It worked!!1"
    ]
},
{
    "submission_id": "1fzu9vd",
    "title": "what is the difference between these two methods of calculating dinov2 feature similarity of 2 images?",
    "selftext": "I want to calculate the similarity between two images, but after googling, I found that there are two methods to calculate similarity. Method one comes from [medium](https://medium.com/aimonks/clip-vs-dinov2-in-image-similarity-6fa5aa7ed8c6), and method two comes from [kaggle](https://www.kaggle.com/code/abdelkareem/dinov2-instance-retrieval).\n\n- Method 1\n```\nfrom transformers import AutoImageProcessor, AutoModel\nfrom PIL import Image\nimport torch.nn as nn\nimport torch\n\ndevice = torch.device('cuda' if torch.cuda.is_available() else \"cpu\")\nprocessor = AutoImageProcessor.from_pretrained('facebook/dinov2-large')\nmodel = AutoModel.from_pretrained('facebook/dinov2-large').to(device)\n\nwith torch.no_grad():\n    img1 = Image.open(img1_path)\n    img2 = Image.open(img2_path)\n    \n    inputs1 = processor(images=img1, return_tensors=\"pt\").to(device)\n    outputs1 = dinov2_model(**inputs1)\n    image_features1 = outputs1.last_hidden_state\n    image_features1 = image_features1.mean(dim=1)[0]\n\n    inputs2 = processor(images=img2, return_tensors=\"pt\").to(device)\n    outputs2 = dinov2_model(**inputs2)\n    image_features2 = outputs2.last_hidden_state\n    image_features2 = image_features2.mean(dim=1)[0]\n    \n    cos = nn.CosineSimilarity(dim=0)\n    dinov2_sim_1 = cos(image_features1, image_features2).item()\n\n```\n\nThe similarity of dinov2 calculated by these two methods varies greatly, which one should I use? \n\n- Method 2\n\n```\nimport torch.nn as nn\nimport torch\nfrom PIL import Image\nimport torchvision.transforms as transforms\n\ndinov2_vitl14 = torch.hub.load('facebookresearch/dinov2', 'dinov2_vitl14')\ndinov2_vitl14.to(device)\ndinov2_vitl14.eval()\ntransform = transforms.Compose([\n        transforms.Resize((224, 224)),\n        transforms.ToTensor(),\n        transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n    ])\n\nwith torch.no_grad():\n    img1 = transform(Image.open(img1_path))[:3].unsqueeze(0)\n    img2 = transform(Image.open(img2_path))[:3].unsqueeze(0)\n\n    image_features1 = dinov2_vitl14(img1.to(device))[0]\n    image_features2 = dinov2_vitl14(img2.to(device))[0]\n    \n    cos = nn.CosineSimilarity(dim=0)\n    dinov2_sim_2 = cos(image_features1, image_features2).item()\n```\n\n\nWhat is the difference between these two methods? Which method should I use to calculate?",
    "created_utc": "2024-10-09T08:34:47",
    "num_comments": 3,
    "comments": [
        "I'm assuming method 1 is average pooling the [PATCH] tokens to get a final embedding, whereas method 2 directly uses the [CLS] token present in the model as the final embedding.\n\nIf a [CLS] token is present in your model, just use it since the model is trained with it.",
        "I would also add that these are not the same models: the Huggin Face model is the base-size model (86M parameters) and the torch model is the large size model (300M parameters). There's no reason why the calculated features should be identical.",
        "I tested using the dinov2-large model in method one. Like method two, both use large-size models, but their results are still significantly different.\n\nhere is the code of method 1\n\n\\`\\`\\`\n\nprocessor = AutoImageProcessor.from\\_pretrained('facebook/dinov2-large')  \ndinov2\\_model = AutoModel.from\\_pretrained('facebook/dinov2-large').to(device)\n\n\\`\\`\\`\n\nand the results\n\n\\`\\`\\`\n\n  0 dinov2\\_1 0.87 dinov2\\_2 0.66 \n\n  1 dinov2\\_1 0.80 dinov2\\_2 0.65 \n\n  2 dinov2\\_1 0.85 dinov2\\_2 0.76 \n\n  3 dinov2\\_1 0.57 dinov2\\_2 0.71 \n\n  4 dinov2\\_1 0.81 dinov2\\_2 0.38 \n\n  5 dinov2\\_1 0.74 dinov2\\_2 0.65 \n\n  6 dinov2\\_1 0.65 dinov2\\_2 0.66 \n\n  7 dinov2\\_1 0.88 dinov2\\_2 0.74 \n\n  8 dinov2\\_1 0.63 dinov2\\_2 0.73 \n\n  9 dinov2\\_1 0.80 dinov2\\_2 0.66\n\n\\`\\`\\`"
    ]
},
{
    "submission_id": "1fzocd8",
    "title": "Best deraining/rain removal networks?",
    "selftext": "Hi, I am working on a project on object detection under heavy rain. What are the best networks for rain removal? I have tried a few diffusion models but the output either had no improvements or were utterly crap.\n\nI think the distribution shift between synthetic data and practical images may be too big…",
    "created_utc": "2024-10-09T03:37:19",
    "num_comments": 7,
    "comments": [
        "Had better results by including weather impacted images of the objects during training: rain, sleet, snow, every time of day, every season, every variation of weather and time of day the object could be observed, year round. Including partially obscured views of all the above. Created via  combination of real world captures and photo realistic synthetic data.",
        "I've been building a simple API to change weather of my images and am using diffusion models to do it - been seeing some pretty decent results. Happy to help if still needed",
        "Hahaha my project’s requirement is to perform image deraining with just severely degraded images provided. Feels like trying to perform a miracle at this point!",
        "Hi yes I’m still working on it. You are aiming for better downstream object detection as well?",
        "You need imagery pairs, same object same view same lighting conditions, with and without rain to train your system.",
        "Yes with only LQ images, I am limited to pre-trained models. But they understandably perform extremely badly on specific practical data. I’m clueless how to proceed from here on…",
        "Probably need synthetic data since I guess getting pairs like that will be easier and rain can be simulated in a game engine."
    ]
},
{
    "submission_id": "1fzo9jd",
    "title": "Train GOT-OCR2.0 ON ANOTHER Langage",
    "selftext": "did anyone checked GOT-OCR , if you scroll down , you might find some step on train , fine-tune etc.. , \ncould you assist me in train or fine-tune it on another language instead of LTR (like english , french) i like to train it on RTL (like urdu , hebrew and arabic).",
    "created_utc": "2024-10-09T03:31:52",
    "num_comments": 0,
    "comments": []
},
{
    "submission_id": "1fzo7hv",
    "title": "Using openCv to recognise items on a table and send coordinates to a PLC.",
    "selftext": "What is the best way to get the image? The camera I am using is communicating with GenIcam, and the Plc is communicating with Tcp/ip. What is the best way to get images and to send data to my plc?",
    "created_utc": "2024-10-09T03:27:48",
    "num_comments": 3,
    "comments": [
        "Calibrate camera to real world coordinates \n\nDetect object coordinates \n\nSend object coordinates to PLC",
        "Most cameras come with python sdk like (Ex. Basler - pyplon). You can use that to extract frames from the camera, maybe simple socket program to send info to the plc",
        "Sorry, my question wasn't worded correctly.\nI meant what libraries or programs to use with python to get image with genIcam or gigE vision.\nAnd what to use to send it to a PLC"
    ]
},
{
    "submission_id": "1fzo4i7",
    "title": "Theft Detection in Retail Stores",
    "selftext": "Hy everyone. I want to ask has anyone worked on theft detection in retail stores on CCTV cameras  \nAny help will be appriciated, anything from a video to an article  \ni have seen a video which i dont remember was on Linkedin or on Reddit which was related to theft   \nplease drop your help in comments   \nThanks",
    "created_utc": "2024-10-09T03:22:03",
    "num_comments": 6,
    "comments": [
        "it is an extremely difficult problem, up there with self-driving.",
        "I have worked on it.\nIncredibly hard problem.\nPeople think they can use CCTv to detect stealing, what actually works well is tags on high value items.\nCCTv needs to detect people near high value items instead",
        "I am little bit more optimistic. It is definitely hard, but achievable with the right approach.   \n  \nOne thing to keep in mind is that relying only on cctv may not provide the best angles or coverage. You  want to consider additional cameras closer to areas of interest, like near shelves or exits.",
        "The problem with most vision systems is they analyze a single frame.",
        "yeah that can be a achieveable target but in retails theres are chocolate bars to snacks   \nthose are like difficult to manage",
        "Those are low value products though."
    ]
},
{
    "submission_id": "1fznwbv",
    "title": "Looking for the Best Way to Automate Light Placement in Floor Plans [Input/Output Example Attached]",
    "selftext": "Hi everyone,\n\nI’m working on automating a task where I need to place lights in floor plans based on room layouts and furniture placement. The lights need to be positioned at specific distances from walls, windows, and objects like beds or sofas. I’ve attached an example of the **input floor plan** and the **desired output** with lights and labels placed.\n\n**Current Process:**\n\n* So far, I’ve tried using tools like **OpenCV** and **object detection frameworks**, but they haven’t been accurate enough for reliably detecting the room boundaries.\n* Now, I’m trying to use a **segmentation model** to break the floor plan into rooms, but I’m unsure if this is the right direction.\n\n**What I Need:**\n\n1. Automatically detect the rooms in the floor plan.\n2. Classify the rooms (e.g., Bedroom, Living Room, etc.).\n3. Automatically place lights based on the room size, walls, windows, and objects.\n4. Label the lights according to type (e.g., \"WW1\", \"DL1\").\n\n**Question**:\n\n* What’s the best way to automate this process? I’m looking for something reliable that can handle different room layouts without much manual intervention.\n* Should I stick with **image segmentation**, or is there a better method for detecting rooms and placing lights?\n\n**Input/Output Example Attached**: (Left is input, Right is output)\n\n* **Input**: The basic floor plan without lights.\n* **Output**: The same floor plan with lights placed and labeled.\n\nI do have a small dataset of these images\n\nThanks for your suggestions!\n\nhttps://preview.redd.it/f74q4cbufptd1.png?width=2594&format=png&auto=webp&s=da3cc110d06efcbabc93e87f5dcf5558f416e968\n\n",
    "created_utc": "2024-10-09T03:06:23",
    "num_comments": 2,
    "comments": [
        "You don't need deep learning for this. The room boundaries are their own color, so use that information to detect them. Then fit rectangles so they overlap the room boundaries, and call each separate rectangle a room. Look for text within the room to label it. Etc, etc.\n\nYou'll need a special case for the shower in the bathroom, but there aren't too many special cases right?",
        "Yeah, but the problem is the floor plans are of different variations. Some have colored boundaries, some do not."
    ]
},
{
    "submission_id": "1fznbq9",
    "title": "Google street view type data source ",
    "selftext": "Curious if anyone knows of a Google Street view style data source that is open for commercial use? (licenced or open) ",
    "created_utc": "2024-10-09T02:24:20",
    "num_comments": 1,
    "comments": [
        "Mapillary seems to have the same data but I haven't yet figures out how to use their API to get that (granted I haven't spent a lot of time trying, but it wasn't obvious)"
    ]
},
{
    "submission_id": "1fzn843",
    "title": "YOLOs-CPP: Seamlessly Integrate YOLO Models in Your C++ Projects!",
    "selftext": "Hi everyone! I’m excited to share my latest project, \\*\\*YOLOs-CPP\\*\\*, which provides high-performance real-time object detection using various YOLO models from Ultralytics.\n\nhttps://github.com/Geekgineer/YOLOs-CPP\n\n# Overview\n\n\\*\\*YOLOs-CPP\\*\\* offers simple yet powerful cpp single headers to integrate YOLOv5, YOLOv7, YOLOv8, YOLOv10, and YOLOv11 into your C++ applications. With seamless integration of ONNX Runtime and OpenCV, this project is designed for developers looking to leverage state-of-the-art object detection capabilities in their projects.\n\n# Key Features\n\n* Support for multiple YOLO models standard and quantized.\n* Optimized inference on CPU and GPU.\n* Real-time processing of images, videos, and live camera feeds.\n* Cross-platform compatibility (Linux, macOS, Windows).\n\nand more!\n\n# Example Usage\n\nHere’s a quick snippet to get you started:\n\n    ```cpp\n    \n    // Include necessary headers\n    #include <opencv2/opencv.hpp>\n    #include <iostream>\n    #include <string>\n    \n    #include \"YOLO11.hpp\" // Ensure YOLO11.hpp or other version is in your include path\n    \n    int main()\n    {\n        // Configuration parameters\n        const std::string labelsPath = \"../models/coco.names\";       // Path to class labels\n        const std::string modelPath  = \"../models/yolo11n.onnx\";     // Path to YOLO11 model\n        const std::string imagePath  = \"../data/dogs.jpg\";           // Path to input image\n        bool isGPU = true;                                           // Set to false for CPU processing\n    \n        // Initialize the YOLO11 detector\n        YOLO11Detector detector(modelPath, labelsPath, isGPU);\n    \n        // Load an image\n        cv::Mat image = cv::imread(imagePath);\n    \n        // Perform object detection to get bboxs\n        std::vector<Detection> detections = detector.detect(image);\n    \n        // Draw bounding boxes on the image\n        detector.drawBoundingBoxMask(image, detections);\n    \n        // Display the annotated image\n        cv::imshow(\"YOLO11 Detections\", image);\n        cv::waitKey(0); // Wait indefinitely until a key is pressed\n    \n        return 0;\n    }\n    \n    \n    ```\n\nCheck out this demo of the object detection capabilities:\nwww.youtube.com/watch?v=Ax5vaYJ-mVQ\n\n    <a href=\"https://www.youtube.com/watch?v=Ax5vaYJ-mVQ\">\n        <img src=\"https://img.youtube.com/vi/Ax5vaYJ-mVQ/maxresdefault.jpg\" alt=\"Watch the Demo Video\" width=\"800\" />\n    </a>\n\nI’d love to hear your feedback, and if you’re interested, feel free to contribute to the project on [YOLOs-CPP GitHub](https://github.com/Geekgineer/YOLOs-CPP).\n\n\\*\\*Tags:\\*\\* #YOLO #C++ #OpenCV #ONNXRuntime #ObjectDetection",
    "created_utc": "2024-10-09T02:16:21",
    "num_comments": 14,
    "comments": [
        "Great work. I have a few detail specific questions though since I'm dealing with something similar.\n\n1- are you using libtorch? How do you handle the linking? Purely dynamic, purely static or something in between?\n\n2-since most processors and most gpus don't support fp16, does quantization  have positive effects on the inference? I figured it affects very slightly since the heavy lifting is done in the processors rather than DMA MANAGER.\n\n3- do you plan to handle the dockerizing process? If so Will you use purely c++ or integrate python for managing requests? If you use python how do you share the data? A shared buffer? Message passing? Or any other method?\n\n4- have you considered to profile memory to avoid stack overflow?\n\n5- do you release allocated memories or rewrite to the same addresses?",
        "Neat. Just at a quick glance / a note: I am pretty sure SafeQueue can deadlock. The bounded version is correct.",
        "Isn’t Ultralytics conduct questionable practice? From what I heard from darknet contributors, every time researchers publish an improved YOLO, the next thing we know Ultraytics rebrand it as their own and increase the version number by 1\n\nCMIIW",
        "C++ is a security risk and a terrible overall user experience. Could you rewrite this in rust?",
        "> -since most processors and most gpus don't support fp16, does quantization have positive effects on the inference? I figured it affects very slightly since the heavy lifting is done in the processors rather than DMA MANAGER.\n\nI think you are mixing up things. Everything from compute capability 7.5 and up should fully support fp16 (and before that have some nuanced restrictions).",
        "Thank you for your kind words and for taking the time to delve into the details of **YOLOs-CPP**. I’m happy to address each of your specific questions below:\n\n1. **Use of libtorch and Linking**: YOLOs-CPP does not utilize libtorch; instead, it leverages ONNX Runtime for model inference, enhancing performance and compatibility. Dynamic linking is used, linking ONNX Runtime as a shared library. The CMake configuration is set to locate these libraries dynamically, and a build script automates the setup.\n2. **Quantization Effects on Inference**: Quantization reduces model weights and activations from FP32 to lower precision formats, leading to smaller model sizes and potentially faster inference times, especially on compatible hardware. Although quantization may slightly decrease accuracy, techniques like Quantization Aware Training can mitigate this loss. The performance benefits primarily come from reduced computational complexity and memory bandwidth requirements.\n3. **Dockerization Plans**: Currently, YOLOs-CPP does not include Docker support, but plans for future implementation consider both pure C++ and Python integration. C++ Dockerization maintains performance, while Python could facilitate API requests. Data sharing methods may include shared memory or message passing, with a sample Dockerfile provided for a pure C++ approach.\n4. **Memory Profiling Considerations**: YOLOs-CPP practices careful memory management, releasing dynamically allocated memory to prevent leaks. While memory profiling isn't currently integrated, tools like Valgrind and AddressSanitizer are suggested for future use to enhance robustness and performance.\n5. **Memory Management Strategy**: The application employs dynamic memory allocation and deallocation, ensuring all allocated memory is released after use. Standard Library containers manage their own memory, avoiding the need for manual management. The strategy avoids rewriting to the same addresses, enhancing stability and preventing memory corruption.\n\nThis approach ensures YOLOs-CPP remains efficient and reliable for real-time object detection applications.",
        "lol",
        "Bro just said something to get down voted.",
        "OK, you are kind of right about gpus, but vector extentions do not support fp16.",
        "**Specific to Your Observation:** You mentioned that most processors and GPUs don't support FP16, leading to the assumption that quantization might have minimal effects since the heavy lifting is managed by the processors rather than the DMA Manager. Here's a clarification:\n\n* **Processor and GPU Support:**\n   * Modern GPUs, especially those designed for deep learning tasks, **do support FP16** operations. For example, NVIDIA's Tensor Cores are optimized for FP16, providing substantial speed-ups.\n   * On CPUs, support for low-precision arithmetic varies, but even where direct support is limited, the reduced data size from quantization can lead to performance improvements due to better cache and memory bandwidth utilization.\n* **DMA Manager Considerations:**\n   * While data transfer rates managed by the DMA Manager are a factor, the primary performance gains from quantization come from the reduced computational complexity and memory bandwidth requirements during inference rather than data transfer alone.\n\n**Conclusion:** Quantization can have **positive effects** on inference performance by reducing memory usage and leveraging hardware acceleration for low-precision operations. The actual impact depends on the specific hardware capabilities and how well the **ONNX Runtime** optimizes for the quantized models on that hardware.",
        "Dude.  Every single answer by you is 100% AI generated. It makes you look like a complete tool.",
        "LMAOOOOO",
        "He does that every time C++ gets mentioned in this sub.",
        "Apologies don't have time to write the answers myself am super busy!!"
    ]
},
{
    "submission_id": "1fzliy8",
    "title": "Standard practice for real-time objefr detection",
    "selftext": "Hi everyone. \n\nI want to use YoloV8 on a raspberry Pi 5 to detect some objects from a camera feed. The idea is to recognize the object, then crop the image around that object, and then do some more processes with that cropped image.\n\nThe thing is, for each image, overall, from starting the detection to running the following process to completion, it takes almost 1 second. So my question is: What's the standard way to manage such a situation? Should I set up the camera to take only one image per second (so every image is passed to the detection model one at a time)? Or should I stack a few images together to detect on them at once? Or to make sure the whole thing doesn't get stuck if the process ends up taking more time than the delay before the next frame? \n\nNoob questions, probably, but I would appreciate any advice. ",
    "created_utc": "2024-10-09T00:00:20",
    "num_comments": 13,
    "comments": [
        "standard way is to use a more powerful sbc, likely with an nvidia gpu",
        "Ncnn can help with some optimisation on Pis. But the model you opted for is simply to heavy. Go for of those lite models with a MobileNet backbone.\n\n\nBatching can help with throughput but it increases latency and memory consumption. And it requires GPUs to see the benefits of batching. Anything with little to zero parallelism capability and batching only hurts.\n\n\nIn short, there is no standard way, just tradeoffs.",
        "model optimisation - take a look at apache tvm, openvino, tf-lite and about converting models from fp32 to fp16 or int8",
        "Get the ai hat for rpi5 and you will get nearly realtime detection using the Hailo accelerator. If you can't buy that then just run in a loop processing as fast as you can, in your case about 1 fps.",
        "1) Not an option here.\n\n2) Even if it was an option, I don't see how that answers any of the questions in my post.",
        "I'll look into that, thanks.",
        "My company's firewall doesn't let me install the hat unfortunately.\n\nThanks for the rest.",
        "well usually, you'd downscale the image and use a more compact model, that answers your question? No, you dont slow the image stream to try to match your processing time, you process the next frame as soon as available",
        "No, that doesn't answer my question. My question isn't about performance. My question is \"What's the way to process the images? One at a time? In a batch? Lowering the frame rate of the camera to match the model speed? Etc\"\n\nMy question would be the exact same even if I had the most powerful computer in the world, as long as I have a camera with a frame rate higher than the model's output rate.",
        "if you slow the image stream down, what happens if the yolo processes image faster/slower than expected? the program hangs around waiting for the next image to process, not a standard way of doing things. idk what you mean by a batch, yolo only takes 1 image at a time. Standard way is to keep the image stream as default and process whenever the program is ready to process the current frame, frames during the yolo processing are disregarded.\n\n Imaging you're a factory worker picking stuff off a production line, you dont slow the production line down, you'd just pick the stuff up as fast as you can despite the speed",
        "Imagine you are a factory owner. If your workers process a material as soon as it arrives, each end product costs 10 dollar. Now if they wait for 8 of them to arrive and process all 8 in a batch, the cost is cut in half. But the worst-case latency significantly increases because (1) the first one the comes in a batch will have to wait for the last one and (2) the first and the last one will come out of the batch processing nearly at the same time.\n\n\nThat is batch inference. It comes with tradeoffs. And choosing the optimal batch size is a research topic covered extensively.",
        "All models can make batch inference. Aren't we doing batch training, you can do the same for the inference as well. So, yolo models can take more than one frame to process. The question is, can your device handle such input stream at once for a given time you desired, e.g. 1 sec?",
        "That answers it. Thanks."
    ]
},
{
    "submission_id": "1fzk1ku",
    "title": "Help Needed with Waymo Open Dataset",
    "selftext": "Hello all!! I was just wondering if any one of you has ever worked on Waymo Open dataset. I have been trying to practice implementing a 2D object detection model on that.\nBut I’ve been struggling with the data processing of the .parquet files. Can anyone help me by sharing some tips or steps on how they have done Data processing and even model building (Confused about whether to convert the images to TFrecord or Not).\n\nThankyou for the help in advance",
    "created_utc": "2024-10-08T22:12:24",
    "num_comments": 0,
    "comments": []
},
{
    "submission_id": "1fzgy41",
    "title": "Extracting data points from a graph with multiple lines",
    "selftext": "Each line is a different color (red, blue, grey). I’m just trying to extract the gray line. \n\nI don’t want a manual tool b/c I’d like to grab ~500 data points from the line.\n\nAnd obviously, a csv data of the data is not accessible to me. The graph is a PNG.\n\nThank you!",
    "created_utc": "2024-10-08T19:10:10",
    "num_comments": 3,
    "comments": [
        "i have a limited experience with CV , but what i would do is find the exact pixel color value from the graph (in this case grey) and loop over the image with OpenCV , check pixel values and if its a match , add it to the second image or proccess the values (such as location of the pixel in the png itself , which would give you value data from the graph)",
        "So, you want to extract 500 points ftom the gray line while you have only a single image, right?",
        "Make a binary mask with numpy and just use mask = (array == target_value)"
    ]
},
{
    "submission_id": "1fzcy4e",
    "title": "Satellite imagery object detection ",
    "selftext": "I am using yolov5 with sahi to detect objects in large satellite imagery. The detector does well but ends up with a lot of false positives (many of which are natural terrain that appear to have some features that resemble the object).\n\nI have a lot of these detections that I would like to build a classifier for to help me weed out the false detections and strengthen the likelihood of having positive detections. \n\nThe size of the objects vary. Some are larger and some are smaller than the input size for a vision classification model. \n\nI've trained a classifier on the positive examples chips (30k total) as one class and false positives as another. The model doesn't seem to work well against my test set.\n\nAre there any recommendations for selecting a classification model? \n\nWhat is the recommended way of handling the varied input sizes? \n\nAre there anylessons learned for satellite imagery chip classification? ",
    "created_utc": "2024-10-08T15:49:36",
    "num_comments": 5,
    "comments": [
        "I think you should just be fixing the detection model or use a better model instead of using a classifier as an additional step.\n\nDid you train the model with sliced images just like SAHI? Because you want your training images to be similar to inference. If you're inferencing on heavily zoomed in sliced images, while training on images that are not sliced similarly, there's a high likelihood of false positives.\n\nI have used a classifier similarly as a second step after object detection to remove false positives, and it never worked well.",
        "Are your false positives water or vegetation? Or are your false positives things that are not seen in your training data? \n\nI’m seeing more or less a similar issue with my training data not having any empty water or fully vegetated negative chips to train on originally. I plan on adding some data on my next training running.",
        "https://arxiv.org/abs/2102.00103 check out Figure 1 on Page 4. The best way is to use Torchvisions ROI pool to handle resizing the input chips for the classifier. Probably also vmap to handle the looping in a fast performant way. When using a secondary classifier keep everything in tensor space, it's far more efficient. https://pytorch.org/vision/main/generated/torchvision.ops.roi_pool.html",
        "Mostly natural terrain like cracks in the earth and texture in mountainous areas. What types of objects are you detecting?",
        "At the moment I’m working with the NOAA ERI (post hurricane) data. \n\nFocused on debris piles and boats, but planning to expand to other environmental concerns as well. Planning on posting the labeled dataset, at least the GEOJsons, under a CC attributable license in the next day or two."
    ]
},
{
    "submission_id": "1fzbxfg",
    "title": "Can I Label with the SAM Model?",
    "selftext": "I want to be able prompt something basic, like car, house, light switch or something to that effect, and SAM segment the image and cutout my chosen prompt. I'd only need like 3 choices, I don't need to be able to prompt everything, just a couple things most common. \n\nSAM works great at segmenting everything but I need to be able to sift through and only select specific things accurately. \n\nWhat would be the best way to do this? ",
    "created_utc": "2024-10-08T15:03:09",
    "num_comments": 4,
    "comments": [
        "I can Highly suggest using GroundingDino as a box prompt generator and use that as input to SAM/SAM2. Check these repos: https://github.com/IDEA-Research/Grounded-Segment-Anything and https://github.com/IDEA-Research/Grounded-SAM-2\n\nI use them for zero-shot-object tracking in a real-time robotic system and the result is impressive so far!",
        "Cool idea. I tried to use SAM in the past and found out it is much more frustrating out of the box than I thought from their demos.",
        "nice, how's the latency and what hardware you running on",
        "I mostly found the segment anything option to be very bad. When you use bbox guiding it becomes much faster and simpeler. SAM2 has tracking possibilities by using a memory encoder, might be helpfull to refine your masks using memory."
    ]
},
{
    "submission_id": "1fzbcmn",
    "title": "Can I use ultralytics YOLO for my business?",
    "selftext": "Hi!\n\nI hired a freelancer recently, and her code contains usage of code from the Ultralytics YOLO library. I want to use that code in my business eventually. Obviously, I'll be using it to make money.\n\n  \nMy question is, am I allowed to use that code in my business, no prob, or do I need to ask her to look at alternatives? \n\n  \nSome people said it's fine to do so without needing to give license fee, etc., while others said otherwise...\n\nI was confused, so here I am on reddit to ask for some help. \n\n  \nAppreciate it!\n\n",
    "created_utc": "2024-10-08T14:37:30",
    "num_comments": 6,
    "comments": [
        "They use the AGPL license which dictates that if you use it for your business, your whole codebase needs to be opensourced. If you dont want to opensource, you need to pay for an \"enterprise\" license.",
        "I use them for prototyping bc they have a great pipeline built for training. Once I get what I need, you can quickly port over your parameters and rebuild your own model. The key to success is a solid data set, once you have this you can train on any model with decent performance",
        "Ultralytics has a page on their licensing options: https://www.ultralytics.com/license\n\nIt’d seem to me that if you want to use it commercially then you need to reach out to them and talk turkey, else you’re bound to adopt an open-source model.",
        "They are overpriced",
        "What models do you usually end up using for the products?",
        "Mobilenet, YOLO (non ultralytics), ResNet"
    ]
},
{
    "submission_id": "1fzab0z",
    "title": "How difficult is implementing CV for a robotic cell with only mechanical background?",
    "selftext": "Good morning gentlemen, as a sort of R&D guy in my machine shop, I am currently trying to innovate last century's manufacturing technology to a suitable today's standards: we have a machine, a plunge grinder, where an operator has to load a single component into it, press a button and unload the machine every 15 seconds. As I strongly believe that humans are much more capable than robots, I finally convinced the owner to build a robotic cell to relieve the operator of this tedious task: the robot has also improved the productive capacity by reducing the cycle time to 9 seconds, and the operators can now concentrate on quality control. \n\nThe only problem with the robot is that it only does what you teach it to do: a human can detect defects and debris in the parts or understand whether the component is correctly placed in the machine, skills that robots simply lack.\n\nTo overcome this problem, I would like to implement a custom CV script that is trained in production to detect these defects or incorrect positioning: I have no competence in CV or ML, but I strongly believe that manufacturing engineers need to start considering the practical use of AI, and I want to make a leap into this topic.\n\nHow difficult will it be? What could be a starting point?\n\nImagine no time problem and since it is for internal use, no necessity to work ASAP.",
    "created_utc": "2024-10-08T13:52:37",
    "num_comments": 4,
    "comments": [
        "In this case your best bet is looking for a more turnkey system from a vendor like Cognex. The tradeoff is that these more user-friendly and easy to use systems charge for the convienience, but for a single proof of concept setup it would save a lot of headaches. Their sales team will usually help guide you towards the right products.",
        "The difficulty would rely on:\n\nThe size of the labeled dataset. \n\nThe subtleties of defects\n\nThe time it is allowed to process for \n\nThe hardware capabilities \n\nThe amount of errors allowed\n\n\nI do think that if hardware isn't an issue, and you have some thousands of labeled images, you could probably do this relatively easily for a baseline model. It seems like it's just a classification problem which is the simplest of deep learning CV problems, (especially because labeling is just binary).\n\nA starting point would probably be to use something like a resnet and fine-tune on the dataset you make. It should be relatively fast and powerful. \n\nPositioning might not even be a deep learning problem tbh. It depends on how it looks, but I'm pretty sure traditional CV could figure out this *type* of problem in many cases.",
        "Considering up to 2% of faulty components in production batch of 2 millions components there is a lot of sampling images to train the model. Components may vary in length or diameter but the defects are always the same: burrs, scratches or too thin coating. Dataset is not an issue.\n\nMain job is classification, OK or NOK: since we work with microns, optical measurement is not reliable enough at the moment.\n\n  \nThank you a lot!"
    ]
},
{
    "submission_id": "1fz8b5n",
    "title": "Failed interview, frustration, and tips",
    "selftext": "Pro tip: DON'T try to cheat, mention cheating, etc. on your interviews; it will just result in you losing the opportunity.  I even knew this a priori but I just did it anyway after helping a different candidate on Pramp\n\nInterviewer asked me to explain UNet, the YOLO attention algorithm, and calculate the covariance by hand\n\nI'm so frustrated and scared from job hunting for many weeks.   I just have to practice even more and read further for practicing computer vision and more for deep neural networks as well\n\nThis industry is so harsh and binary; pass or fail, etc.  I'll just have to hop back on the horse and keep spamming applications later today and/or tomorrow.  Soft skills are hard to measure and quantify the results from, and there's just so many \\*@$king quiz type questions the interviewer could ask.  Makes me so angry\n\n\nEDIT: the comments have been extremely helpful.   I should go review my classical computer vision and learn plenty more\n\nIs Kaggle a good place to focus, or should I gain more theoretical knowledge first?  I should work on both.  Please recommendations for how you learned and studied all of it 🙏\n\n",
    "created_utc": "2024-10-08T12:27:52",
    "num_comments": 26,
    "comments": [
        "You cheated?!\n\nI appreciate you are just venting but this post is 4 different half-finished stories.",
        "We do this sometimes. But only when applicants claim something in their resume but don't seem to know what it is.\n\nI interviewed 3 object detection expert today. None of them knew what nms is or why do we use fpn or what even fpn is. None knew how does attention mechanism work, what was its innovation, all they said was it gives us the query key value. I am not the employer, just interviewer.\n\nAnother guy came in claimed to be expert in segmentation, even did his thesis in segmentation. Didn't know how unet work. \n\nAll that a side, none of them knew Jack shit about c++ and deployment, even tough they all mentioned c++ as a special skill. \n\nSo yeah, I agree with the interviewer, and if you claimed to know what was going on in unet or Yolo, and didn't actually know, that's your fault.",
        "If you need to cheat for Unet, attention and covariance then best of luck for further interviews.",
        "No",
        "I am currently preparing for CV interviews. Could you please tell me what the most important neural networks I have to be aware of and how they work to pass any interview since there are a lot, e.g. Yolo variants, ResNet, EfficientNet V1 and V2, ConvNeXt V1 and V2, Swin Transformers, GAN networks, UNet++ and UNet, SAM V1 and V2, CLIP, etc?\n\nAnother question is that if I know in high level how it works is it enough? To which extent should I know each neural network? I am expecting that I should list the high-level components of the network and how they work together. At the end, everything is packaged in libraries, it is a bit rare to implement something from scratch. That is why I think that high-level knowledge of each network might be sufficient.",
        "Hey, I don't know very detailed about the object detection but I know that for example NMS is used to get the correct bbox, FPN makes it easier to detect smaller objects, etc.\n\nI don't know the detailed concept about the computer vision at low level but I like using different open source projects, I can run, compare, use different kind of things related to AI because I love tinkering with the latest researches that come.\n\nWhat kind of role should I be looking at? Will it be considered as backend only, right?",
        "What are the other tough questions for CV?",
        "Totally agree, not mentioning camera calibration, anchor-based or anchor-free, connected components. Then come to other like image gradient, gaussian blur. I think it need to take half of year to prepare.",
        "Oh.  I should prepare better",
        "I don't know what subfield you working in, but let's say object detection.\n\nIf you claim to be an expert, you must know how encode labels, how does vision Transformer encode labels and converts images to sequences, what is the criteria of one stage CNN based architecture today (it is usually backbone + neck + head). Learn what each block do.\nYou have to know why and how does reset prevent gradient exploding ( someone said this yesterday and didn't know how it does, he had just heard about it). There will probably be a few question in this regards, knowing digital image processing is a must, but sometimes interviewers don't know themselves. The rest I think is based on your experiences and skill such as python  or c++ programming. C++ is a huge plus  almost everywhere.\n\n\nI wouldnt  care about packages like ultralytics since they have a very well documented APIs. You can learn to use them in matter of minutes.",
        "No offence, but high school students can do that now, thanks to the APIs provided by companies like ultralytics. You need experience. You need knowledge. Pick a subfield and learn every little detail about it, along side every trick used in deployment. If it's hard for you to go without a job, try freelancing for a while.",
        "These are not tough questions for CV. If I were interviewing a candidate and they couldn't explain to me what attention was and why the Swin Transformer is important for vision, it would be solid grounds for me to reject them.\n\nU-Net is a super simple CNN with skip-connections that candidates should be aware of. Not knowing how to calculate covariance, I might overlook that depending on answers to other questions although it is very simple.",
        "Depends on the job description, as there are no generic CV jobs.",
        "I already finished my PhD in AI testing and I want to move to industry. I have been working with a Dutch company voluntarily for a year on different projects on object detection, segmentation, and OCR. Thus, I am not an expert in specific subfield.",
        "Oh.  I should prepare better",
        "No offence but I've seen so many Doctors who don't know anything (I'm a master student as of now and will do my PhD) , and the industry doesn't care about your degree, only your experiences. You probably going to have a hard time finding a job.",
        "What’s your CV/ML experience?",
        "Not accurate, there are many positions in industry which require PhD degree especially in the R&D departments. PhDs are like masters when they start in the industry. How you judged that I will probably have a hard time, however, you know nothing about my technical background!",
        "I started a company that utilized CV, took cs231n, and some graduate coursework in the subject.  But no PhD or previous job outside of what I was learning myself at my startup.",
        "It helps, but it may not be enough.  Prepare for a month, and you'll be fine",
        "Well, first of all, no need to get defensive, you said yourself that you have not focused on any specific subfield. Second of all, I agree with you on R&D requiring PhD or some strong research background, but I was talking about development and deployment, if you review my comments above I did not mentioned R&D. And also, there not many companies out there which can afford R&D.",
        "Thank you for constructive feedback!",
        "In the CV industry, you should have good and strong knowledge in each subfield, not enough to be an expert in just object detection as you mention at the first comment since nobody will hire you to work only on object detection. You should be able to work on different subfields as a computer vision engineer.",
        "Again, I agree with you, but you have to be an expert in at least one. Just knowing some stuff in each one doesn't help you build up a project. Sorry to say this but it looks like to me that you are trying to convince yourself that you are highly skilled knowledgeable. Don't get me wrong I don't mind, I wish  you the best actually but be realistic, industry only cares about revenue. And what you ar describing does not generate it.",
        "I don't mention at all that just knowing some stuff in each subfield will help you build a project as you claim. Again, you should have strong knowledge in each main subfield of CV such as object detection, and segmentation. Lastly, your problem is that you make the discussion personally. \n\nWhen you would like to answer a question of someone, go directly and answer it technically, don't assume or talk personally how this person will struggle or how he is thinking or convincing. \n\nLook at each of your replies, you find yourself talk personally even with the last one. Why I didn't mention that you try to falsely convince yourself that you are an expert in CV by only an expert in only one subfield and started to analyze your personality like your are low-level expert!\n\nThe discussion became personal rather than technical!",
        "Anyways, wishing you the best. I hope you land a good job."
    ]
},
{
    "submission_id": "1fz7dy0",
    "title": "Can I finetune LayoutLM with any label?",
    "selftext": "I’ve been seeing a few examples of how people finetune the LayoutLM model, but they all use the same labels (question, answer, other, header, etc.). Can I use different labels for my specific task?",
    "created_utc": "2024-10-08T11:49:29",
    "num_comments": 0,
    "comments": []
},
{
    "submission_id": "1fz3515",
    "title": "RPi Cam Module 3 Wide Image Quality Differences",
    "selftext": "I am working an a project to do some CV work with python and OpenCV. I am using a RPi Camera Module 3 Wide and I am getting wildly different images when capturing from them command line vs via a python script.\n\nIf I execute the command:\n\n`rpicam-still --width 2304 --height 1296 -o images/cli_test2.jpg`\n\nI get the following result:\n\n[Command Line Output](https://preview.redd.it/msotufen0ktd1.png?width=1560&format=png&auto=webp&s=25a983a93a6760f089a83baef3c0310ffce6f2a1)\n\nI wrote a very simple python script to display the camera output and optionally save the image to a file. I will post the script below but I get the following result:\n\nI am clearly not setting something correct in the script. Any suggestions on how to get the image quality from the script to match the command line is much appreciated:\n\n[Program Output](https://preview.redd.it/zkhva3wp0ktd1.png?width=1560&format=png&auto=webp&s=15ad7c96dd24238f3926a442e84a721ba6a803b2)\n\n    #! /usr/bin/python\n    import cv2\n    import numpy as np\n    import time\n    from picamera2 import Picamera2, Preview\n    from libcamera import controls\n    \n    print(\"Step 0: Setup Camera\")\n    \n    cam_w = 2304\n    cam_h = 1296\n    \n    picam2 = Picamera2()\n    picam2.preview_configuration.main.size = (cam_w, cam_h)\n    picam2.preview_configuration.main.format = \"RGB888\"\n    picam2.start()\n    print(\"Wait for Camera to Stabilize\")\n    time.sleep(2)\n    \n    while True:\n        frame = picam2.capture_array()\n        # Copy frame to proc_img    \n        proc_img = frame.copy()\n    \n        # Do ops on proc_img here \n    \n        # Display the live image\n        cv2.imshow(\"PiCam2\", proc_img)\n    \n        # press 'p' to snap a still pic\n        # press 'q' to quit\n        c = cv2.waitKey(1)\n    \n        if c == ord('p'):\n            #snap picture\n            file_name = \"output\" + time.strftime(\"_%Y%m%d_%H%M%S\") + \".png\"\n            cv2.imwrite(\"images/\"+file_name, frame)\n            pic = cv2.imread(\"images/\"+file_name) \n            cv2.namedWindow(file_name)\n            cv2.moveWindow(file_name, 0,0)       \n            cv2.imshow(file_name, pic)\n            print(\"Image Taken\")\n        elif c == ord('q') or c == 27: #QUIT\n            print(\"Quitting...\")\n            break\n    \n    # When everything done, release the capture\n    #cap.release()\n    cv2.destroyAllWindows()",
    "created_utc": "2024-10-08T08:52:44",
    "num_comments": 2,
    "comments": [
        "I could not say off the cuff what would be causing this. Have you opened up the CLI script and looked into the differences of the settings? Have you compared the image data between the two images (i.e is one image 8 bit and the other using float, different color settings)?\n\nYou probably know this, but looks like you'll need to go though line by line to find out how the image data is being handled.",
        "I normally have to try these things out, but my guess is because you're in preview mode in the second example. Try create_still_configuration()"
    ]
},
{
    "submission_id": "1fz16yf",
    "title": "PhD suggestion ",
    "selftext": "Hi!! Do you think a PhD in Computer Vision is worth going for, at this point of time. I can see NLP is almost saturated with so many new technologies and neural networks coming up. Do you think Computer Vision too will be saturated soon and a PhD won’t be much of a thing? If not, which areas within Computer Vision do you recommend for, to go for a PhD.",
    "created_utc": "2024-10-08T07:30:51",
    "num_comments": 3,
    "comments": [
        "I would think that if you are at a level where you are considering pursuing your PhD, someone else's opinion on your own professional choices are irrelevant at this point. I am pretty sure the only person who has to decide if pursuing a PhD is worth it is you. \n\nIf you are a dog food company that needs an engineer to help develop a CV package tracking system, do you need a PhD, probably No. \n\nIf you are the Department of Energy that is developing cutting edge nuclear fusion rod inspection CV, do you need a PhD, probably Yes.",
        "Talk to your potential advisors or other professors you are interested to work with. If they are good, their advice will be miles better than what reddit can offer.",
        "If you want to go in to defense in the US, then I suggest getting the PhD."
    ]
},
{
    "submission_id": "1fyzwde",
    "title": "Best monocular depth foundation model",
    "selftext": "As now we already have several foundation models for that purpose such as :- \n- DepthPro (just released) \n- DepthAnyThing \n- Metric3D \n- UniDepth \n- Zoedepth \n\n\nAnyone has seen the quality of these methods in real-life outdoor scenarios? What is the best? Run time? I would love to hear your feedback! \n",
    "created_utc": "2024-10-08T06:33:13",
    "num_comments": 1,
    "comments": [
        "You can try most at HuggingFace for free. Not sure about inference speed, for some reason papers almost never mention that.\n\nhttps://huggingface.co/spaces/akhaliq/depth-pro\n\nhttps://huggingface.co/spaces/depth-anything/Depth-Anything-V2\n\nhttps://huggingface.co/spaces/JUGGHM/Metric3D\n\nCouldn’t find one for Unidepth, but I do know one exists\n\nhttps://huggingface.co/spaces/shariqfarooq/ZoeDepth"
    ]
},
{
    "submission_id": "1fyzmz4",
    "title": "Redefining Visual Quality: The Impact of Loss Functions on INR-Based Image Compression",
    "selftext": "",
    "created_utc": "2024-10-08T06:20:42",
    "num_comments": 0,
    "comments": []
},
{
    "submission_id": "1fyxe12",
    "title": "improvements suggestions",
    "selftext": "Hello everyone!  \nI've been asked to improve a pre-existing MobileNetV3Large that performs 96% of accuracy in a small training dataset (45k images and 16k classes).  \nThe loss that was used back then was the TripletLoss.\n\nWhat may you recommend me to do in order to explore new possible improvements?",
    "created_utc": "2024-10-08T04:23:00",
    "num_comments": 0,
    "comments": []
},
{
    "submission_id": "1fyxda2",
    "title": " Is Computer Vision still a growing field in AI or should I explore other areas?",
    "selftext": "Hi everyone,\n\nI'm currently working on a university project that involves classifying dermatological images using computer vision (CV) techniques. While I'm eager to learn more about CV for this project, I’m wondering if it’s still a highly emerging and relevant field in AI. With recent advances in areas like generative models, NLP, and other machine learning branches, do you think it's worth continuing to invest time in CV? Or would it be better to focus on other fields that might have a stronger future or be more in-demand?\n\nI would really appreciate your thoughts and advice on where the best investment of time and learning might be, especially from those with experience in the field.\n\nThanks in advance!",
    "created_utc": "2024-10-08T04:21:46",
    "num_comments": 59,
    "comments": [
        "Computer vision is still a growing field.\n\nWhile it might not be blooming as fast as NLP or generative models, it still has its own unique opportunities and is becoming increasingly important in various industries. \n\nYou can always explore other fields later, but having strong skills in CV will serve you well.",
        "Yes! Massively growing. Most notably in manufacturing, robotics, distribution, retail and defense. \n\nI’ve been working in CV for a decade now and there is not enough people in it. \n\nCV is just not the headline these days",
        "I've been in computer vision for a couple of decades, and lately, it feels like the field is in decline. I just attended a local tech business conference, and all the buzz was about generative AI and LLMs. No one had any interest in what my computer vision group had accomplished or what we had to offer. It reminds me of the trajectory speech processing and recognition took—it's becoming largely \"solved\" and commoditized.\n\nI've had a good run in this field, but I'm starting to see the writing on the wall that everything's pivoting toward LLM-related work. The issue I have with generative AI and LLMs is that they seem rather boring. It feels like it's just API calls, maybe setting up a local model, some fine-tuning with LoRA, or crafting prompts, RAG, and vector databases. Is everything just prompts wrapped around calls to OpenAI, pretending to be something new and innovative?\n\nI also want to add, if you do pursue computer vision, focus on photogrammetry, camera models, and the principles of geometry related to vision. That will put you ahead of a lot of people who think computer vision is just about running YOLO models.",
        "it isn’t solved yet, whatever the transformers tell you",
        "I don't know anything, but it strikes me that when androids are finally being deployed, working on new computer vision solutions will become way more pressing. LLM is only important now because we already have keyboards and can throw infinity compute at the problem\n\nI'd expect them to be come much more 'mixed input', like adding thermal as an input to isolate the humans in the room. Also, where is the mug with the hot coffee\n\nTerahertz radar applications, going to be super interesting for obscured object detection. Also, Defence departments will want missiles that can still operate in 100% signal jamming environments, see the failure of the Excalibur precision artillery round in Ukraine for why. Getting stuff like this working with finite compute is an issue. All this used to be a signal processing field, now it's more object detection, and also, \"what are we expecting to see here and is there anything out of place\", so being able to understand context appears to be a neglected area",
        "1. Computer vision will always be important. It is needed for many applications, it's not a solved problem, and it's unlikely to be solved anytime in your lifetime.\n2. Having any sort of research experience is valuable if you're hoping to get a research position. You can always switch domains.\n3. Talk to your professors. Pursue the the areas where they can help you.\n4. The LLM bubble will probably burst before you're on the job market anyway. (This is more of a guess, but the fact is it's only going to go so far, and the venture capital firms pouring money into this stuff are starting to get frustrated.)",
        "CV has less opportunities. NLP/LLM is where every company is pouring money into right now. And it's also more widely applicable than CV because almost all companies have some form of text data where NLP/LLM can be potentially useful.\n\nThe above wouldn't matter if you actually like CV. You wouldn't care about the opportunities or the lack thereof. But if you are more concerned about the job opportunities and if that's your primary motivation, then as of the state of \"AI\" currently, you probably will want to look at those other areas. But then again, everyone and their mother is doing that, so expect stiff competition.",
        "Computer vision is very multidisciplinary, so you will learn a ton of valuable skills which you can use anywhere.",
        "You need to understand that tech fields have cycles and trends.\n\nA few years ago was \"all hands on CV, nobody cares about NLP\". ChatGPT comes out and everyone is like \"all hail to NLP\". With the next big thing the hot topic changes.\n\nStudy what you like.",
        "Just now Hailo realized CV chips, and Sony their AI (CV) camera",
        "I'd say be practical analyze the job market in your country. See which jobs are more in demand and which jobs are within your reach (qualifications wise masters, phd, experience). Depending on that learn the skills. There are plenty of AI jobs in US, but not everywhere.",
        "I would say that different subfields of modern ”AI” has some degree of similarity in terms of approach and architecture. Generally, it’s about breaking down the mode of interest (text, vision, audio, something else) into vector data, choosing/developing appropriate architecture and toolkit, training a model, validating results, and putting the model in production. Transferrable knowledge is generally high. What I’m saying is basically, go with whatever your more passionate about, you won’t have a super hard time hopping around different ai applications. Passion and motivation will give you strong foundations.",
        "In terms of applications, it is exploding right now. Interesting things are also happening on the research front, so if computer vision is something you enjoy there should be plenty of things to do. Also, the current advances such as ViT and diffusion are relevant to other ml-domains as well.",
        "Yes. Those generative models are trained on data gathered by computer vision. Not only is it relevant it's foundational.",
        "Computer vision is a growing field, but research increasingly requires some work with multimodal models (IME) and tangentially related technologies (like SAM for segmentation).  \n\nI work with an old CV colleague (I'm more on the engineering side, but take up paid research contracts with him) on government-funded CV projects, and there are a lot of them. But they also increasingly include evaluation of multimodal models (they can do decent scene analysis) and SAM, depending on the project.\n\nThis is applied research though, so you're mostly a user of these technologies and just have to understand how and when to use and evaluate them.",
        "Look up Neural radiance fields and Guassian splatting",
        "Yes ofc. Generation, multimodal learning, segmentation, etc. are all *very* active areas of research.",
        "Vision is unequivocally harder than text. The general consensus is that vision is growing, relatively immature and yet to have its transformer moment",
        "Long term - yes \nshort term - maybe not (except for generative approaches)\n\nthe truth is you never know what picks up fire what in the AI tech industry. Learning anything core is valuable.\n\nAdvice - go for it at uni level.",
        "I was reading through the AI server company Gigabyte's blog and they have a bunch of computer/machine vision case studies old and new, from self-driving technology developers (not stated outright but heavily implied to be Mobileye: www.gigabyte.com/Article/constructing-the-brain-of-a-self-driving-car?lan=en) to oil rigs (www.gigabyte.com/Article/geological-science?lan=en) So I think this field is definitely still going strong. If anything there's still tons of work to do.",
        "I mean, it is growing but in able to get in, you need a PhD degree. To get into a PhD program in CV, you need several first-author top conference papers. I won't suggest any undergraduates get into CV coz it is way too competitive.",
        "not all the buzz, yet. will be huge in the future, think AR, VR.",
        "There is still a big part of the chamber to explore. However if your concern is if at some point it saturates, I still advice you to engage on it but combined with other topics (I think multimodal learning will be dominant in the following years if it is not now).",
        "LLM now. So, CV.\nLL models are on the hype right now because the computational viability and business potential. \nSomeday they will need eyes hahaha\nTo be honest, I see more potential in CV than in NLP in the future.\n(I'm working with that by the last 3 years, the image processing science is more complex than audio processing, for example. They will need that people).",
        "Generative models are included in CV",
        "Maybe doing this project would help me to have stronger skills in CV. Could you tell me somewhere to learn? Actually i'm reading the Adrián Rosebrock starter's book Deep Learning for Computer Vision. But maybe it's outdated. What you think?\n\nAnd other question, do you think Should I spend a lot of time learning CV or should I spend more time researching and learning about other things?",
        "What do you mean by the last sentence exactly?",
        "You guys got any entry level openings for masters grads coming up? 😅",
        "I'm glad to hear that, but in which country is there so much demand?",
        "Biology for pharma as well",
        "Thanks for this helpful response, I am also looking to start learning and hopefully, applying computer vision",
        "That's very helpful,thank you for suggestions",
        "But the real question is, is more worth study other field of ai?",
        "An if i would learn more CV, could you tell me somewhere to study? Right now I'm reading the beginner's books on Deep Learning for Computer Vision by Adrian Rosebrock, but I think the book is from 2016.Maybe it's too outdated, what are the current trends in CV, which ones work best? And which ones should you spend more time learning and understanding? From where?",
        "NLP/LLMs are also at the peak of the hype cycle, many companies are burning cash with no hope on the horizon, there's no way they can all survive in the market.",
        "[removed]",
        "It's not just about job opportunities, is about I'm practically new in this of AI (I'm finishing my degree this year) and I want to inform me about the fields in AI, the opportunities. And then research by myself which field do I like the most. What you think about this? Out of curiosity, based on what you mentioned about job opportunities, what country are you from?",
        "My advice: all areas of the AI market will be great if you like it and do it well. IoT, CV, LLM, big data, etc.\nYou need to find out what you want to work with and apply yourself. IMO the data career is the most safe path at this moment. This will be more and more useful with the AI advance.",
        "Check out Hartley and Zisserman's Multiple-View Geometry. I consider it a litmus test to filter people that are actually interested in CV.",
        "I'd say stop reading books and start reading SoTA research papers and tinker with their models.",
        "Computer Vision: Algorithms and Applications by Richard Szeliski is also considered as one of the best resource to learn CV",
        "I wish I could offer something, it’s hard coming out of school looking for the first role.",
        "United States",
        "what's your objective function?\n\nits interesting, and the career outlook is as rosy as you please",
        "I really don't know much, but every time I come across a paper on the subject I would wish my Maths was better",
        "Deep learning specialization by Sir Andrew NG",
        "\"Computer vision from the first principles\" on Coursera is an amazing course. It will help you understand what \"vision\" really is and how computers see the world - deep learning is a very small part of it",
        "Foundations of Computer Vision by Torralba et al is probably the most recent serious CV textbook, while Deep Learning for Vision Systems by Elgendy is also worth a close look.",
        "Because you're dealing with a lot more data some solutions that exist in the LLM space are just not viable yet in the CV space. Try throwing a high res video at a vision transformer and see how fast you get an OOM.",
        "First of all, thank you very much for your response and experience. Out of curiosity, where are you from?\nAt this momment I'm finishing my career in Computer Science, and I want to do a Masters degree, but I'm undecided which one to do, which one would you recommend?",
        "That book is not a great filter unless your job focuses on multiple-view geometry. Multi-View (or even single-view) Geometry is useful, yes, but is more ancillary in some subfields",
        "Why do you think so? I’m using it right now for a class of mine.",
        "\"The career outlook is as rosy as you please\"\n\nCan you tell me how it can workout over an entire career?\nIf I work for a company that was to make inspection systems,\nIf I implement an object detector/classifier/segmentation model in a production system which gives me a good accuracy and good inference time, what else can I do?\nSure, the projects may be different and may require a bit of preprocessing, but how would the career trajectory look like? (This is assuming I'm working at the same company for most of my life)",
        "Im from Brazil but I work remotely. To be honest, I joined my current job as intern, and when I was effectively contracted, I dropped my graduation because the routine :p\nIf you have a really strong experience, I believe this will be just a detail. But most of the jobs \"in theory\" requires at least a graduation and a master in some cases... I want to get a master degree btw, because I want to publish some research in this area.",
        "Just gatekeeping for comedic purposes. My old 3DCV prof had a saying: if you haven't waved a chessboard in front of a camera to calibrate it you haven't done true CV.\n\nJokes aside, CV is more than just fitting a DL model to image/video inputs and outputs for perception/generation tasks; there's tasks out there other than perception e.g. reconstruction, localization, mapping, scene intrinsic/geometry understanding which require a really solid understanding of geometry.",
        "How do you know the things you do today? If the answer has at least some part self-directed learning and initiative you’ll do alright. It’s just as much art as it is rigor, a problem-solver’s career. Even if, someday you tire of it, as long as you’re in it for the challenge you could end up doing a bunch of different things you didn’t foresee and never regret any of it.",
        "I think the master degree is very individual because it's about very specific areas. You will research, study and practice this same subject everyday for some years...it should be something that you really want to find out the answer kkkk"
    ]
},
{
    "submission_id": "1fyufuk",
    "title": "How do I hardware sync stereo cameras? For a RPi 5 (I don't have cameras yet)",
    "selftext": "Is there a board or something I can buy? I found this supplier which seems to sell cameras with a sync board: https://www.imagequalitylabs.com/digital-camera-modules/sony-starvis-imx335-5mp-ff-raspberry-pi-camera (although I need a monochrome + fisheye setup)\n\nDo I need to buy global shutter cameras instead?\n\nMy intended application is indoors, tracking a person moving around a room. The system will be mounted to a wall, and won't move. 30fps+, low latency inference (<50ms)",
    "created_utc": "2024-10-08T00:48:44",
    "num_comments": 5,
    "comments": [
        "Here's information on setting up and sync'ing multiple Intel RealSense stereo cameras - [https://dev.intelrealsense.com/docs/multiple-depth-cameras-configuration](https://dev.intelrealsense.com/docs/multiple-depth-cameras-configuration)",
        "Arducam might have what you're looking for",
        "I haven’t finished my system 100% yet but I’m using global shutter 1mp ov9281 cameras from Inno-maker and it’s pretty simple to sync them up, they just need a 3.3v high signal on their sync pin which will be generated by an an ESP32 microcontroller. I’m using an IMU too so it’s responsible for triggering an interrupt on the MCU at 200Hz and then every 10 cycles the sync pin gets toggled on and off. Presumably you could just use the GPIO on the RPi to do this too, idk how precise it will be but at least you could get timestamps that are synced to the RPi’s clock. Look in the data sheet of your camera for how exactly the external sync works some need a lower logic level, in that case you can use a resistor divider.",
        "Note: You'll want to run ROS2 Jazzy on Ubuntu 24 (Raspberry Pi 5). Here's the instructions:\n\n[https://docs.ros.org/en/jazzy/Installation/Alternatives/Ubuntu-Development-Setup.html](https://docs.ros.org/en/jazzy/Installation/Alternatives/Ubuntu-Development-Setup.html)\n\nLibrealsense2 and Pyrealsense2 for Ubuntu 24 are still on Intel's development branch. You'll need to clone this branch and follow the install from source instructions here:\n\n[https://github.com/IntelRealSense/librealsense/tree/development/wrappers/python](https://github.com/IntelRealSense/librealsense/tree/development/wrappers/python)",
        "ah thank you for the website! another supplier to check out (i'm not enjoying shopping for cameras lol - what a PITA it is)\n\nand thank you for the suggestions, i'll have to dig into them when i get two cameras. gonna mess around with a single one for now while i get familiar with it all"
    ]
},
{
    "submission_id": "1fyubtg",
    "title": "Creating boxes using lines",
    "selftext": "So I have  for example 8 lines which I found and are correct on a image . And they are making a box in the image but the problem is lines are not intersecting each other because they are just short of intersection. Is there algo which can group lines and find the intersection .",
    "created_utc": "2024-10-08T00:39:43",
    "num_comments": 2,
    "comments": [
        "You can use the hough transform to find lines as rho,theta in the image, then you can find the intersection of those lines.\n\nhttps://stackoverflow.com/questions/46565975/find-intersection-point-of-two-lines-drawn-using-houghlines-opencv\n\nhttps://docs.opencv.org/3.4/d9/db0/tutorial_hough_lines.html\n\nYou might get multiple lines due to the algorithm picking up the thickness of the lines, so you would need to find a way to pick the \"average\" line in a given locality.",
        "One approach is to calculate the distance between the endpoints of the lines and then see if they’re within a certain threshold to consider them as intersecting.\n\nYou could also try clustering the lines based on their positions and angles to group them more effectively. There are libraries out there, like OpenCV, that can help with line detection and manipulation. It might take some tweaking though."
    ]
},
{
    "submission_id": "1fyr5jr",
    "title": "Real time processing",
    "selftext": "I love sports in general and I'm a system engineer with some experience but 0 experience with computer vision.\n\nI have a few ideas, I would like to integrate both. So asking the experts here. Is computer vision ready to do real-time processing?\n\nIf so, how could I get started on this? \n\n",
    "created_utc": "2024-10-07T20:59:43",
    "num_comments": 20,
    "comments": [
        "What do mean by realtime processing?",
        "It depends on your use case. Big neural networks can be slow, but yolo has been used for real time applications\n\nHowever “real time” is a particularly subjective concept. It isn’t the same if your application has a few us to respond than having several seconds. In the first case, it will be quite hard to achieve such a fast computation and probably an rtos will be needed to ensure that results are provided always on time (as well as acting safely if that isn’t the case). On the other hand, if several seconds are available for computing the result, a python script on windows will be enough",
        "I work in this field, so I can give you some details about what is easy to do in real-time and what is difficult.\n\nThe problems are different depending on the sport, but let's assume that your sport requires \"common processing\" (see [SoccerNet Game State Reconstruction: End-to-End Athlete Tracking and Identification on a Minimap](https://arxiv.org/pdf/2404.11335).\n\n* detection can be done in real-time\n* Tracking can also be done in real-time, but improving it offline may be possible (this mainly concerns tracks REID).\n* Field registration can be done in real-time, but doing it offline means you can use more information to avoid errors).\n* Player identification is difficult to do in real-time.",
        "By real-time you mean atleast 30fps right? With right hardware it could be realtime. But depends on so many facotors.",
        "Overall, the level of technology (I mean the processing speed of GPU/NPU devices) allows performing tasks such as object detection in a frame, segmentation, human pose estimation, and more at a frequency of 30 frames per second or even higher on devices like the Raspberry Pi 5, using Hailo-8 modules.",
        "Yes it certainly is and this has been true for probably 5 to 10 years at least.\n\nThere are many models that process video in realtime with lag of less than a tenth of a second or better. Just Google for “ball tracking” or “player tracking” and PyTorch and you should see some demos. ",
        "Analysis of video from the field, match etc etc",
        "To start with, sounds like offline processing and then consider something like near real time or real time",
        "I'm talking about 1 minute delay or few seconds like 30 45 seconds",
        "What are good resources to get started with this\n, frankly speaking, I'm just learning this will be the first step",
        "If you need to achieve real time operation, I would consider those requirements from the beginning. Otherwise, you can end up with a solution that works offline but can’t operate fast enough for real time operation\n\nHard real time is quite difficult to achieve (it might require using a special operating system). Even the chosen programming language can have an impact (for example opencv in python sometimes has problems with multi threading)\n\nEdit: what do you consider real time? What’s the maximum amount of time available for performing recognition on a single frame?",
        "You can get object detection pipelines to run a lot faster than that. Realistically 300 FPS is fairly basic speeds for a production computer vision pipeline. https://paulbridger.com/posts/tensorrt-object-detection-quantized/",
        "Basically, you need knowledge of OpenCV and model preparation for execution on NPU. From my side, I can recommend solutions from Hailo.ai. They have materials on GitHub regarding model preparation for use on Hailo devices. These devices work with various SoCs, such as RBP, Radxa, OrangePi, and others\n\nIf you are interested in a deeper understanding of computer vision and deep learning, I can recommend 'Computer Vision: Algorithms and Applications' by Richard Szeliski and [https://d2l.ai/](https://d2l.ai/).",
        "I would like to analyze athletes and the game in general, so in general I think a few seconds should be enough I believe but I wouldn't be able to answer this question because I haven't experimented this technology yet so I'm not aware of the current challenges and opportunities",
        "In that case, you’ll be probably able to achieve it with normal libraries (check if yolo could be used). In some sectors real time means having to respond to a stimulus in less than 10ms (100fps) or even less time. Having more than one second is quite slow",
        "I will try yolo, I also saw there's a company/repository called roboflow, have you heard anything about it?",
        "Yes, they provide software for building datasets, but I’ve never used it",
        "Roboflow is one of many companies that provide hosted services for computer vision and related datasets. \n\nSearch for “image annotation platforms” and you’ll come up with more. "
    ]
},
{
    "submission_id": "1fyotgr",
    "title": "Hours Needed to write an OpenCV Script to Measure Fruit",
    "selftext": "Need to know how long it'd take someone who is an expert in OpenCV to write a script which measures the length and width of different fruits with an item for measure reference. I was told the hard part is measuring fruits that are irregular, like a banana. Want to ask the pro's here what would be a fair time estimate to have someone make a script that does this.\n\nI want to use this link as reference: [https://pyimagesearch.com/2016/03/28/measuring-size-of-objects-in-an-image-with-opencv/](https://pyimagesearch.com/2016/03/28/measuring-size-of-objects-in-an-image-with-opencv/)",
    "created_utc": "2024-10-07T18:53:38",
    "num_comments": 17,
    "comments": [
        "lol",
        "What exactly would be the input? Do we first need to detect fruits and reference in a complex scene? Do we have all camera parameters? When you talk about the ‘hard part’, what would be an ideal readout?",
        "It depends a lot on specific requirements and data, if you need high/low precision, if you have already a camera setup or you have to design also that. Let's say in the easiest setup I do it in one hour, in the worst case scenario can be months because you need a deep model.\n\nEdit: Actually I just thought if I would have a telecentric lens I take like 15 mins in the easiest setup.",
        "Hi. I do not want to advertise my company here but you can try AugeLab Studio from https://augelab.com/. It's free and we provide full version for academics and students.\n\n  \nYou'll need no coding and take much less time to measure areas.",
        "14.67 hrs",
        "I did something almost exactly like this just last week. Assuming the input image is exactly as shown, the setup shouldn't take more than a day, maybe around 4-6 hours? However, deploying it in a production setting with many more requirements would take many more days for optimizing and testing.",
        "Is it the banana bounding box you want or some diagonal? I’d look up the ISO standard for banana measurement to get a good start.",
        "Oops, apologies for not being specific enough. I want to upload a photo from my phone (nothing fancy) taken vertically from the fruit with a quarter on the same plane. Looking to get in a 1/4 inch range of accuracy.... again, I was told the curve of a banana will make this hard. I'm a computer vision noob.",
        "Using aruco markers as a reference would be a good approach. I recently made a video on that if you want more info",
        "answered above :) - ideal readout is a measurement of length and width within 1/4 an inch",
        "nothing fancy! just upload a pic from iPhone taken vertically from the fruit with a quarter on the same plane. Looking to get in a 1/4 inch range of accuracy",
        "please specify it's standard deviation with at least 4 decimals.",
        "oh? do you mind sharing your script or whatever you used? I want something that can measure length and width from an iPhone photo taken directly above the fruit with a quarter on the same plane, in the same image. need it to be \\~1/4 inch of accuracy, room for .5 inches.",
        "yes, please!",
        "I want something that can measure length and width from an iPhone photo taken directly above the fruit with a quarter on the same plane, in the same image. need it to be \\~1/4 inch of accuracy, room for .5 inches.",
        "Corrected, 14.67 +/- .285 hrs",
        "I did that for a client so I'm afraid I can't"
    ]
},
{
    "submission_id": "1fyo9f7",
    "title": "Street Parking Computer Vision Project",
    "selftext": "Hi all! I'm new to CV and would like to build a program that detects open street parking spaces from a live video feed. If you all have any recommendations on how to build the project, I'd love to hear them! Thank you. ",
    "created_utc": "2024-10-07T18:24:58",
    "num_comments": 2,
    "comments": [
        "Look into opencv cascade classifiers. I used that with some manually defined bounds to do this at my old work to find the best time to show up for the good parking XD",
        "You could use yolo to detect cars"
    ]
},
{
    "submission_id": "1fynxlh",
    "title": "Overview of modern Edge boards for CV + guide on how to choose",
    "selftext": "I write a long article about choosing an edge board for Computer Vision (or LLM/VLM) + overview of the most popular ones.  Hope it can help you in your next project - [https://medium.com/@zlodeibaal/cookbook-for-edge-ai-boards-2024-2025-b9d7dcad73d6](https://medium.com/@zlodeibaal/cookbook-for-edge-ai-boards-2024-2025-b9d7dcad73d6)",
    "created_utc": "2024-10-07T18:08:39",
    "num_comments": 3,
    "comments": [
        "Thank you good review, but I also was hoping to see inference times for models you've tested/requirements, like nano 8 gb will work for this but not for this, and so on.",
        "Nice work.",
        "In my previous article, I tried to do this. I even still update the table with some basic measurements - [https://docs.google.com/spreadsheets/d/1BMj8WImysOSuiT-6O3g15gqHnYF-pUGUhi8VmhhAat4/edit#gid=0](https://docs.google.com/spreadsheets/d/1BMj8WImysOSuiT-6O3g15gqHnYF-pUGUhi8VmhhAat4/edit#gid=0)\n\nBut the main problem is its super misleading characteristic:  \n1) Different networks perform differently (Board \"A\" can be x3 faster for a network \"N\" but x2 slower for a network \"M\")  \n2) Different boards require different amounts of CPU usage for NPU inference. Even video encoding|decoding can change speed dramatically  \n3) Hard to compare different format inference (int8/fp16)  \n4) Hard to compare different connections for accelerators (PCIe, USB, M2)  \n5) Hard to compare multi-device cases (Jetson has 1 GPU and 2 DLA, and RK2588 has 3 NPU).  \n6) Different batchsizes optimisation\n\nAnd a lot more problems that will make every test biased. I am still trying to append everything in the table I showed. But I am not sure it's worth:)"
    ]
},
{
    "submission_id": "1fym8gv",
    "title": "Compare and match face in an image",
    "selftext": "I am new to computer vision. Can anyone please share suggestions on libraries/APIs that I can leverage for face recognition/filtering in this workflow:  \n--User uploads an image (showing their face)  \n--The application (web-based) receives the image  \n--The application searches/filters through different images already stored on the server  \n--The application returns only the image(s) where the face (from the uploaded image) appears",
    "created_utc": "2024-10-07T16:45:04",
    "num_comments": 1,
    "comments": [
        "Look for facial recognition algorithms like deepface or mobile facenet. On each upload of the image these algos compute face embeddings. You need to keep a store of these embeddings, using any vector darabases, there are plenty available now. When user uploads a new image, compute its face embeddings and search for embeddings in your vector store using a similarity search. Map those returned embeddings to the faces that you have sorted and return them. The classic case is how attendance system works. check that out"
    ]
},
{
    "submission_id": "1fygwyv",
    "title": "Wich raspberry pi and camera for object classification? ",
    "selftext": "Hello, I am making a small robot that can recognize waste and classify it. The idea is to put waste on a conveyor belt. When the piece of waste moves past a sensor, the conveyor belt stops and the waste comes to a specific coordinate. A camera then recognizes the type of waste and a robot arm picks it up and throws it into the correct bin. I think that it's best to use a raspberry pi for this project because I am using visual studio code for programming in python and tensorflow. But i'm wondering wich specific raspberry pi and camera i should use. I also never really used a raspberry pi so if you have any tips for the project let me know :)).",
    "created_utc": "2024-10-07T12:54:52",
    "num_comments": 7,
    "comments": [
        "This doesn't answer the question on which Raspberry Pi to get, but it might be helpful in general. Note that the tests are a bit laggy due to running using remote desktop:\n\nhttps://youtu.be/VeWBYRpodnI?si=TCR3NS0-E_CDCtFR\n\nI also have other videos on my channel including a brief video of the latest Raspberry Pi AI Camera. You could look into it. Basically the camera runs the neural network inference within the camera itself allowing the Raspberry Pi to not have to perform the hard work. This could be helpful if you choose to get an older Raspberry Pi. Note that the AI camera is much more expensive than the basic Raspberry Pi cameras and also hard to find right now (out of stock).",
        "There is not enough detail here to say what model RPI will work. But in general, any model RPI will work for simple detection tasks.",
        "Also on this topic, a RPi is a standalone computer. So how do I develop the program on the RPi if I have my own computer? \n\n\nDo I try remote access in to the RPi from my computer to set it all up? \n\n\nOtherwise I'd need to buy a separate screen, mouse, and keyboard right? ",
        "I'm probably going to use a dataset with around 5000 images. The idea is to train this dataset for recognizing and putting it in the right categorie.",
        "You can remote in or use a usb keyboard and connect a monitor via hdmi.",
        "How's the latency when remoting in?\n\n\nI was thinking via Ethernet, as I expect via USB to have higher latency? ",
        "Haven't done ethetnet. Only wifi. It can be very crappy at times. Ethernet should e much better I assume."
    ]
},
{
    "submission_id": "1fy6lu2",
    "title": "Paper implementation review",
    "selftext": "I have tried to implement a CV paper, can you please review it? Any kind of feedback is welcome.\n\nThe paper and code both given in the repo.  \n[https://github.com/Yagna24/DVF-Implementation](https://github.com/Yagna24/DVF-Implementation)\n\n",
    "created_utc": "2024-10-07T05:39:22",
    "num_comments": 0,
    "comments": []
},
{
    "submission_id": "1fy4f4i",
    "title": "Segmentation Fault with Open3D in Python",
    "selftext": "I am working with Open3D to visualize human stick figure motion. I am using the SMPL skeleton structure. My animation is working smoothly but after it loops for some 14-15 times the window closes and on the vscode terminal I get segmentation fault (core dumped). I cannot seem to figure out why its happening.\n\nAlso another issue is that when I close the render window, the program execution does not end on its own, I have to press ctrl+C to end the execution.\n\n    import numpy as np\n    import open3d as o3d\n    import open3d.visualization.gui as gui\n    import open3d.visualization.rendering as rendering\n    import time\n    \n    SKELETON = [\n        [0, 2, 5, 8, 11],\n        [0, 1, 4, 7, 10],\n        [0, 3, 6, 9, 12, 15],\n        [9, 14, 17, 19, 21],\n        [9, 13, 16, 18, 20]\n    ]\n    \n    def load_data(file_path):\n        try:\n            data = np.load(file_path, allow_pickle=True).item()\n            return data['motion'], data['text'], data['lengths'], data['num_samples'], data['num_repetitions']\n        except Exception as e:\n            print(f\"Failed to load data: {e}\")\n            return None, None, None, None, None\n    \n    def create_ellipsoid(p1, p2, radius_x, radius_y, resolution=50):\n        p1 = np.array(p1, dtype=np.float32)\n        p2 = np.array(p2, dtype=np.float32)\n        direction = p2 - p1\n        length = np.linalg.norm(direction)\n        mid=(p1+p2)/2\n    \n        # Create a unit sphere\n        sphere = o3d.geometry.TriangleMesh.create_sphere(radius=1, resolution=resolution)\n        transform_scale = np.diag([radius_x, radius_y, length/2, 1])\n        sphere.transform(transform_scale)\n        \n        z_axis = np.array([0, 0, 1])\n        direction = direction / length  # Normalize the direction vector\n        rotation_axis = np.cross(z_axis, direction)\n        rotation_angle = np.arccos(np.dot(z_axis, direction))\n        \n        if np.linalg.norm(rotation_axis) > 0:\n            rotation_axis = rotation_axis / np.linalg.norm(rotation_axis)\n            R = o3d.geometry.get_rotation_matrix_from_axis_angle(rotation_axis * rotation_angle)\n            sphere.rotate(R, center=[0, 0, 0])\n    \n        sphere.translate(mid)\n        sphere.compute_vertex_normals()\n    \n        return sphere\n    \n    \n    def create_ground_plane(size=20, y_offset=-0.1):\n        mesh = o3d.geometry.TriangleMesh.create_box(width=size, height=0.1, depth=size, create_uv_map=True, map_texture_to_each_face=True)\n        mesh.compute_vertex_normals()\n        mesh.translate([-size/2, y_offset, -size/2])\n        return mesh\n    \n    def create_skeleton_visual(frame,joint_color=[0, 161/255, 208/255], bone_color=[50/255, 50/255, 60/255]):\n        geometries = []\n        \n        # Create spheres for joints\n        for joint in frame:\n            sphere = o3d.geometry.TriangleMesh.create_sphere(radius=0.05)\n            sphere.paint_uniform_color(joint_color)\n            sphere.compute_vertex_normals()\n            sphere.translate(joint)\n            geometries.append((\"sphere\",sphere))\n        \n        # Create bones\n        for group in SKELETON:\n            for i in range(len(group) - 1):\n                start = frame[group[i]]\n                end = frame[group[i+1]]\n                \n                #determining the size of the ellipsoid depending on the area it is located on the human body\n                if (group[i]in [0,3,12]): #pelvis and stomach and head\n                    radiusx=0.04\n                    radiusy=0.04\n                elif (group[i] in [7,8,9,13,14]): #feet,chest and shoulders\n                    radiusx=0.05\n                    radiusy=0.05 \n                elif (group[i]==6): #chest joint\n                    radiusx=0.02\n                    radiusy=0.02\n                elif (group[i] in [16,17,18,19]): #hands\n                    radiusx=0.06\n                    radiusy=0.06\n                else:                   #thighs and calf\n                    radiusx=0.1\n                    radiusy=0.1\n                \n                bone = create_ellipsoid(start, end,radius_x=radiusx,radius_y=radiusy)\n                bone.paint_uniform_color(bone_color)\n                geometries.append((\"bone\",bone))\n                \n        return geometries\n    \n    class SkeletonVisualizer:\n        def __init__(self, motion_data, title):\n            self.motion_data = motion_data\n            self.title = title\n            self.frame_index = 0\n            self.last_update_time = time.time()\n            self.frame_delay = 1.0/20   # 20 FPS\n    \n            self.window = gui.Application.instance.create_window(self.title, width=1920, height=1080)\n            self.scene_widget = gui.SceneWidget()\n            self.scene_widget.scene = rendering.Open3DScene(self.window.renderer)\n            self.scene_widget.scene.show_skybox(True)\n            # self.scene_widget.scene.set_background([0.2, 0.2, 0.2, 1.0])\n            self.window.add_child(self.scene_widget)\n            \n            self.setup_camera()\n            self.setup_materials()\n            self.setup_lighting()\n            self.add_ground_plane()\n    \n            self.current_geometries = []\n            self.update_skeleton()\n    \n            self.window.set_on_tick_event(self.on_tick)\n            self.window.set_on_key(self.on_key_press) \n            \n    \n        def setup_camera(self):\n            all_positions = self.motion_data.reshape(-1, 3)\n            min_bound = np.min(all_positions, axis=0) - 1\n            max_bound = np.max(all_positions, axis=0) + 1\n            self.center = (min_bound + max_bound) / 2\n            initial_eye = self.center + [3, 3, 10]  # Assuming your initial setup\n    \n            self.camera_radius = np.linalg.norm(initial_eye - self.center)\n            self.camera_yaw = np.arctan2(initial_eye[2] - self.center[2], initial_eye[0] - self.center[0])\n            self.camera_pitch = np.arcsin((initial_eye[1] - self.center[1]) / self.camera_radius)\n    \n            bbox = o3d.geometry.AxisAlignedBoundingBox(min_bound, max_bound)\n            self.scene_widget.setup_camera(60, bbox, self.center)\n            self.update_camera()\n    \n        def update_camera(self):\n            eye_x = self.center[0] + self.camera_radius * np.cos(self.camera_pitch) * np.cos(self.camera_yaw)\n            eye_y = self.center[1] + self.camera_radius * np.sin(self.camera_pitch)\n            eye_z = self.center[2] + self.camera_radius * np.cos(self.camera_pitch) * np.sin(self.camera_yaw)\n            eye = np.array([eye_x, eye_y, eye_z])\n    \n            up = np.array([0, 1, 0])  # Assuming up vector is always in Y-direction\n            self.scene_widget.look_at(self.center, eye, up)\n            self.window.post_redraw()\n    \n        def on_key_press(self, event):\n            # if event.is_repeat:\n            #     return  # Ignore repeat presses\n            if event.key == gui.KeyName.RIGHT:\n                self.camera_yaw -= np.pi / 90 # Rotate by 10 degrees\n            elif event.key == gui.KeyName.LEFT:\n                self.camera_yaw += np.pi / 90 # Rotate by 10 degrees\n    \n            self.update_camera()\n            \n        def setup_lighting(self):\n            # self.scene_widget.scene.set_lighting(self.scene_widget.scene.LightingProfile.MED_SHADOWS,(-1,-1,-1))\n            self.scene_widget.scene.scene.add_directional_light('light1',[1,1,1],[-1,-1,-1],3e5,True)\n    \n        def setup_materials(self):\n            self.joint_material = rendering.MaterialRecord()\n            self.joint_material.shader = \"defaultLit\"\n            self.joint_material.base_roughness=0.1\n            self.joint_material.base_color = [0, 161/255, 208/255, 0.5]  \n    \n            self.bone_material = rendering.MaterialRecord()\n            self.bone_material.shader = \"defaultLit\"\n            self.bone_material.base_metallic=0.1\n            self.bone_material.base_roughness=1\n            self.bone_material.base_color = [0/255, 0/255, 120/255, 0.5]   \n    \n            self.ground_material = rendering.MaterialRecord()\n            self.ground_material.shader = \"defaultLit\"\n            self.ground_material.albedo_img = o3d.io.read_image('plane.jpeg')\n            self.ground_material.base_color = [0.55, 0.55, 0.55, 1.0]  \n    \n        def add_ground_plane(self):\n            ground_plane = create_ground_plane(size=50)\n            self.scene_widget.scene.add_geometry(\"ground_plane\", ground_plane, self.ground_material)\n    \n        def update_skeleton(self):\n            for geom in self.current_geometries:\n                self.scene_widget.scene.remove_geometry(geom)\n            \n            self.current_geometries.clear()\n            frame = self.motion_data[self.frame_index]\n            geometries = create_skeleton_visual(frame)\n            \n            for i, (geom_type, geom) in enumerate(geometries):\n                material = self.joint_material if geom_type == \"sphere\" else self.bone_material\n                name = f\"{geom_type}_{i}\"\n                self.scene_widget.scene.add_geometry(name, geom, material)\n                self.current_geometries.append(name)\n    \n            self.frame_index = (self.frame_index + 1) % len(self.motion_data)\n    \n        def on_tick(self):\n            current_time = time.time()\n            if current_time - self.last_update_time >= self.frame_delay:\n                self.update_skeleton()\n                self.last_update_time = current_time\n                self.window.post_redraw()\n    def main():\n        file_path = r\"results.npy\"\n        all_motion, all_texts, all_lengths, num_samples, num_repetitions = load_data(file_path)\n        example_number=8 #take this input from the user\n        motion_data = all_motion[example_number].transpose(2, 0, 1)[:all_lengths[example_number]] * 2 #scaled for better visualization\n        title = all_texts[example_number]\n    \n        print(f\"Loaded {len(motion_data)} frames of motion data\")\n        print(f\"Number of motion examples= {len(all_texts)/3}\")\n    \n        gui.Application.instance.initialize()\n        \n        vis = SkeletonVisualizer(motion_data, title)\n        \n        gui.Application.instance.run()\n        gui.Application.instance.quit()\n        \n        \n    if __name__ == \"__main__\":\n        main()\n\n",
    "created_utc": "2024-10-07T03:29:59",
    "num_comments": 0,
    "comments": []
},
{
    "submission_id": "1fy30j4",
    "title": "What does a Computer Vision team actually do in a daily basis ?",
    "selftext": "I'm the scrum master of a small team (3 people) and I'm still young (2 years of work only). Part of my job is to find tasks to give to my team but I'm struggling to know what to do actually.\n\nThe performances of our model can clearly be improved but aside from adding new images (annotation team's job), filtering images that we use for training, writing preprocessings (one time thing) and re-training models, I don't know what to do really.\n\nMost of the time it's seems our team is passive, waiting for new images, re-train, add a few pre-processings.\n\nCould you help know what are the common, recurring tasks/User stories that a ML team in computer vision do ?\n\nIf you could give some example from your professional work experience that would be awesome !! ",
    "created_utc": "2024-10-07T01:45:27",
    "num_comments": 45,
    "comments": [
        "Gosh, this post is what’s wrong with many tech companies. They gave someone with no CV background job to find tasks for a 3-people CV team? It’s no offence to you OP, company put you on a stupid mission. List of tasks should be coming from intersection of the team itself and management (business needs). \nIf you are already in the place where you are and can’t do anything about it, try to learn exactly what is expected business need and communicate what’s the best way of achieving it.",
        "the short answer is... it depends.  \n1) are you reusing preeexisting models which you retrain on something else?  \n2) or are you developing your own network from scratch?\n\n  \nA part of a CV engineers job should be researching (iEEE for example) for papers that may or may not fit your project's needs and try to prototype them.  \nYou as well if you find something interesting, you can pitch it to them and have them prototype it.  \nThat's one task.  \nPresent results, reach a mutual agreement for the feature to be included in the main network.   \nThat's another task.\n\nResearch a bit into multitask learning. It's more efficient to have a network with multiple heads than multiple networks.   \nLook into cuantizaiton methods to further reduce memory usage.  \nPresent results, patent results, write papers based on results.  \nI'm really not sure how your team is passive when there's loads of stuff that can be done in this domain!",
        "Read papers. Prepare datasets (curating, cleaning, exploring, synthesizing, etc). Train. Test. Review results. Plan experiments. Create inference. Writing code to accomplish the above. Repeat until done \n\nThis is basically the ML work I generally do. Traditional CV is a bit different",
        "What is your AI Roadmap? Are you working on new products? What are your goals in the next 1-2 years? All of this should be defined and should basically be used for all your epics and will control the planning and tickets. Just planning random research with a passive team waiting for new data doesn't sound exactly efficient.\n\n>The performances of our model can clearly be improved\n\nWho is saying it should be improved? Would improving it lead to a) more revenue, b) more customers, c)????\n\nAny of the research that goes into a new model (multi-task learning, few-shot learning, prompt learning) should be discussed and based on actual targets and requirements. You might not even need fancy new research, could just need better training schemes, better data etc.\n\nIf I had a team of bored/passive CV engineers, I would tell them first to do the following\n\n\\* Write documentation and make sure everything is documented properly  \n\\* What does the codebase look like? Are we set up for reproducibility? Is everything optimised?\n\nthen figure out how our models can be optimised, what do we need to improve them etc.",
        "Depends on the current requirment   \nif i have to develop a new model, then i research about the existing ones which can be trained later on or train a custom one which can take upto 4 to 6 months (data gathering, annotations and training)  \nwhen it is done then its time to write scripts so that the valuable information has been extracted in real life  \nthen after that deploy on cloud and write apis on lambda so that the actual user can also use it",
        "A lot of my time is spent on: (1) researching models and choosing the best candidates that fit our use case & specs; (2) benchmarking the performance of each of them, observing pros & cons, and making a choice; (3) analyzing the errors and making improvements (this might require customization in the model, sometimes even architectural changes, which is the most exciting part of my job). Repeat 2 & 3 until satisfied (or deadline!); also we might need to go back to adding/cleaning/balancing data.",
        "As scrum master, go talk to business and actively look for projects outside of your squad current, passive attitude. No offense. If you struggle it's obviously because the squad (as a whole) lacks meaningful objectives. Get into hot subjects au your upper hierarchies layers and, idk, talk about yourself and your team and what your team is able to do. They should provide at least general ideas if they really need a CV team ?",
        "Data creation is a large issue. Right now I am creating tools that will help clients create data for their models easier. I also explore non deep learning solutions which requires having some domain knowledge about the image content (medical). So I spend a lot of my time reading and prototyping. That also includes finding new technologies that help streamline model creation or deployment. As far as deliverables though, it really depends on your goals. Are you creating models only or an actual product? Are you supposed to generate income directly from your work or is this research focused?",
        "Do you have a Product Manager?\n\nIf not, it sounds like you need to morph into that PM role. Sit down with business decision makers and the CV research team and whiteboard business needs and if CV applications are possible with the data you have.\n\nIn those meetings don't drive the conversation, make the business side demand features. And they say the priorities and timelines. Second meeting with CV only is to figure out of the tasks are possible. If not, another round of meetings with the big group. Repeat until products fall out of the ideas. Build demos and POCs to help business people understand the concepts, more than likely they have no clue what they want or what data they have.\n\nPush the CV team to map the business needs to actual models they can build",
        "I don’t blame you for feeling a bit lost. This sounds like bad management from above. But also, this sounds like a great opportunity to learn, and take some initiative. First I would try and get to a place where you have a few focused problems you are trying to solve for. You may need to collaborate with the product people for this. \nI’ve been in these situations where I feel left high and dry as far as what direction a project should go. For me, collaborating with people has been super successful. When things seem ambiguous I try and get as many people from different departments together and lead a discussion or even an exercise. Any user feedback you c-n get could be valuable.\nThis is a really hard question to answer without more details. But don’t feel like you need to figure it out all by yourself. If there isn’t any clear direction, then start experimenting, trying new architectures, and try and up skill as a CV team. Defining some key metrics to hit would be good:\n1 new architecture experiment a quarter, improve accuracy by x% each quarter, improve dataset quality.",
        "Currently as a team of 1 person, I spend most of time looking in  libtorch document.",
        "Have you considered how you'd deploy and manage the model in production?  \nWhat your target HW and inference speed is?  \nWhat your update policy is? When / how to push new models to production?  \nBuilding/training a model is one thing... making it useful is a totally different effort!",
        "I'm very junior but most of my day is filled with tests with with different datasets, lots and lots of tests, I change a single opencv parameter by 0.1 and the whole test results can change. maybe the experts can also say something while they are here.",
        "My scrum master, is it you?",
        "I am curious to know why there is even a small team for \"our model\", is that for a single model?  What type of model is it?",
        "Seriously?  SCRUM is a management tool. It is used only after there is a real design for the software.  That software must be doable.  A block diagram must be developed for the project with well-defined steps and times to completion per block. \n\nThe steps must have a sequence of the order the steps must be designed in. You cannot write the blocks out of sequence if one has to be completed before another. \n\nTalk to me if you need help.",
        "> (annotation team's job)\n\n  \nIt would be your job, ideally. Aim at automating the data annotation process, e.g. via SAM/SAM2. Manual annotation is far too ineffective to make a difference when you're approaching larger datasets with hundreds of thousands or millions of examples.",
        "My thoughts exactly. There are many cases in which scrum masters aren't the ones who create and assign tasks, but rather ensure everything flows well. But to make a CV-newbie responsible for both tasks? Wtf?",
        "I actually was the first recruited, and they hired 2 more people. I agree that hiring a senior profile would have been their best buy but that's also twice the money, which is hard for a small company.   \n  \nI also agree that tasks should come from business need and business need is \"improve the model\".  Therefore my question, what do we do to that ? \n\nAnd I mean that, with working 2 years as a DS but I'm pretty sure there still things that a senior would do differently, and I'm here for those advice.",
        "Quantization",
        "Hey, u/TransylvaniaRR thanks for the insight ! How often do you find something from papers actually useful ? I rarely see the benefit compared to changing something in the data for example.  \nMultitask learning and model ensembling is definitely something we should improve yes, thank you !",
        "Thank you for sharing your experiences ! How does the definition and planing of data cleaning/curating usually goes ? Meaning, do you have an educated guess that more cleaning is needed from testing or it come from something else ?",
        "Very insightful, particularly the part on why it should be improved. We have new products coming but before tackling them it seems that need of the business is to make our AI as good as a human to detect the particular thing we are detecting. No deadline, just work on this project only.\n\nMainly the team is not bored, just doesn't really know what to do to improve the current model because of lack of expertise i guess.\n\nBut everything you said is really helping me, thank you.",
        "very insightful, thank you so much for your answer ! Do we agree that analysing the errors is a manual task, right ? How much would you value, cleaning more your data, compared to tweaking the model ?",
        "So we have hard problems to solve, that's not the problem. We have goals in terms of accuracy on certain classes. Mainly we know that more images and of better quality is helping reaching this higher accuracy. Other kind of pre and post processings too.\n\nBut we mostly lack ideas on what we can try, aside from what we we have already tried. As other pointed out, we clearly lack expertise for that in the team but we're a small team and lack ressources to go for a senior/expert profile",
        "That's very insightful thank you ! I'm helping creating a product that use CV. This is very production oriented",
        "Thanks for this very clear answer ! As we're a small company we currently, and unfortunately, don't have a PM yet. CV application is clearly possible we already have a decent accuracy.\n\nBut my CV Team lacks expertise and they're unable to tell what result they might actually achieve, but challenging this seems indeed very important !",
        "Thanks for the understanding, I indeed posted here to learn from other. Being in a small company it's easy to have a lot to figure by yourself since other are busy with other important things too but i'll definitely involve them a bit more in the definition of the objectives.",
        "That is very true. This part is a bit better understood and better manage than the \"how to improve the model\" part so that's why I didn't mentioned it.",
        "Use case is way to specific to be solved by SAM, we tried it for some time it perform quite poorly. I think this work better with cleaner environment and more common objects",
        "I think the advise to him is not to manage and find task but to create an environment where the experts in cv is the one proposing what else they can do to improve it.  Nothing wrong with a non cv expert scrum master to do that just that he might have either misunderstood his job scope of his manager put him up to fail.",
        "YES. I work in automotive, part of 170-people team.\nThere's always something to try with trabsformers, encoders, stixelnet, cuantization, foundation models, sparse queries, migration to pytorch, migration to ssd from yolo, sensor fusion.\nIn this domain, Tesla and China are our main competitors and it forces us to stay in shape.\nI myself have written 6 patents in various stages of publication, 2 of which quantum.\nSo to answer your question, yes, there's plenty.",
        "Well there are papers they confirm that data is usually more important than network architecture ;)\n\nIt really comes down to your company’s goals. If they want to be the best in the world at whatever task they’re doing then it’s usually worth using the most recent models even if it only improves things by a fraction of a percent.  ",
        "Testing/data exploration. This is a very problem and dataset specific question. Are you using some publicly available set or are you labeling it yourself. For the latter it might be gathering specific examples to submit for annotation. \n\nFor example. At the start maybe it's just a random sample of all the images I have. But as my models get better maybe I'm submitting things I know are edge cases or trouble areas. Maybe my model is good enough and I'm using it to bootstrap labeling and the annotator needs to just clean up mistakes.\n\nMaybe there are no labels and the entire set is synthetic, in which case my tweaking and iteration on how those look.\n\nMaybe it's a standard set and there's little for me to do except look at results and plan next moves.\n\n  \nHonestly a workflow where someone else is telling me what to do makes no sense to me. My \"manager\" should tell me what outcomes I need to produce. My model/algorithm needs to produce some result, the specific tasks are mine to decide and are governed by iterative results. I'm against agile for this kind of work when it's really down to the minutiae of task because the ideas I have today might be thrown out by tomorrow. I currently work under 1-2 stories and maybe some specific bugs that last multiple sprints and that's all I have in JIRA lol",
        "Unless you have meaningful CV/DL experience, you shouldn't be finding tasks per se.\n\nTalk to your tech leads, figure out what they are building, why and what their performance targets are for deployment/shipping. You should definitely be setting requirements for these algorithms to constrain R&D and focus it appropriately. Also think about the infrastructure - are experiments reproducible, are datasets versioned and traceable, how do you deploy algorithms?\n\nUse all of this information to effectively manage the project through epics and stories (I fucking hate how I am now speaking in Agile as a researcher)\n\nIf your CV engineers don't know how to improve the model then that's a bigger issue tbh. Generally it should be the opposite, they have tons of ideas and stuff they wanna do but are constrained by both time, scope and requirements.",
        "Figure out the metrics the business want to improve, get CV people to run the models and return iterations on the benchmarks. If the results are poor, push CV team to research deeper into models that have worked in similar domains.",
        "I understand you're new, and that it's not your fault you're in this position. To almost every single question you're asking, the answer is \"it depends\". It depends on many things: what's your ultimate goal for your sponsor/customer? What are the time constraints? Compute constraints? Accuracy/other metrics requirements? Difficulty of changing each aspect (data vs model)? Long term developmental goals?\n\nThere's so many things that you need to understand deeply, before you can create a priority list.",
        "This is also my working style/communication type, so not important to do it exactly my way. It could be a Slack thread, a poll, or anything. Start with your CV team and ask them “What do you think we could improve on the most?” “What opensource model would you like to try to implement?” “If we had to replace on model, which would it be?” Then if they don’t have any good answers, you’ll know you are fugt 🤠",
        "the difficult thing is we don't have like a expert profile to propose how to improve, only profile with 2-3 years of exp. max. We all know in the company that hiring someone with 5-6 years is essential for us, we just don't have the money to do so.",
        "Okay thank you ! Do you feel this logic of going that deep into research is bound to big teams with very separate roles or it can apply to small teams too (where due to lack of ressources one could be doing both a bit of DS and a bit of MLE)",
        "That's very insightful, thanks so much for the honest answer. It surely helps",
        "okay yes I get it, of course I see how all these questions deeply impact the roadmap. Thanks for bringing visibility on this",
        "Hire an experienced CV consultant temporarily (maybe 2 weeks, or 1 week then 1 day a week for a few months) to help you draw a roadmap and suggest pipeline improvements. This is a lot cheaper than getting a senior in full time and can get you out of your stuck situation.",
        "Oh well then in this case reading up extensively is the only way although it is probably really hard. One way is to if budget allow look for experience guys and see if they can provide guidance at fraction of a full time staff cost. Some might do it out of interest and not mind not charging full value for 2 hours a month kind of commitment.",
        "when it's a smaller, you have to be more picky as prototyping a paper is indeed a time-consuming task.  \nSo maybe further filtering the research towards specifically your domain of activity and you project needs.  \nLet me give you an example (I didn't always work on a large team, rather started in a team of 5 at another company).  \nWe had resnet15 (pytorch) and did inference over 10 images.  \nAt a time budget of 500ms, the process as a whole took 450ms.  \nAnd then in came a paper that if those 10 images you were to concatenate as a single 10-channel image, you could reduce drastically.  \nFast prototyping in python => exec time reduced to 50ms  \nDeploy to main, use OpenCV in C++ for doing inference on device and total time reduced from 450 to 70ms (average) as observed on device.  \nYou can be as innovative as the budget allows.  \nIf it's high you can take into consideration architectural changes.  \nIf it's lower, aim for optimizations.  \nFor the small company I was working for, optimization like the one described was more than enough.  \nFor my current employer, it's changes from yolo to ssd (now) as later will be transformers.  \nIn essence, research multiple approaches, estimate time cost, align with budget."
    ]
},
{
    "submission_id": "1fy2dh2",
    "title": "Amazon SageMaker",
    "selftext": "I’ve been working as a deep learning engineer for a startup for almost two years. We’ve been using OVH to train our models (mainly YOLO and a few classifiers). Our monthly expenses with OVH are around $200, but we’ve become dissatisfied with their service.\n\nRecently, my manager suggested two alternatives:\n\n1. Buying our own machine with a high-performance GPU (approximately $4,000).\n2. Using AWS SageMaker.\n\nI’m unsure which option would be more beneficial.\n\nTo provide some context, we train two YOLO models and about 12 small classifiers each month, along with a few additional models for testing or new projects. It’s also worth mentioning that this would be the startup’s first high-performance machine, so neither the team nor I have much experience in managing a server or handling its maintenance.",
    "created_utc": "2024-10-07T00:55:17",
    "num_comments": 10,
    "comments": [
        "AWS sucks at Machine Learning. SageMaker is supposed to be their answer but it’s a mess and seems like it was built with the sole purpose of getting people to spend more money on AWS\n\nI would suggest Lambda Cloud availability can be hit or miss. Or just spinning up EC2 containers so you don’t get sucked into the nightmare that is Sage Maker\n\nAlso $4000 would maybe buy a desktop with a 4090 your not getting much of a shared resource with just $4k",
        "i've found sagemaker expensive, effective, and annoying. i haven't used ovh so i can't speak to that, but if you're worried about long-term costs, sagemaker does have a free trial period where you can see how much you would be spending, and work from there. training basically anything other than premade sagemaker models requires you to create custom docker containers and manage them with sagemaker, which is a skill on its own. if you're doing that, you might have an easier time running your own server.\n\ni'd suggest going as low-commitment to sagemaker as possible, so you can pull the plug if the projected expenses and training environment prove unsatisfactory, and if that happens to be the case, invest in an on-prem server. \n\nanother option is of course just running a big ec2 instance and training on that. it's pretty transferable to an on-prem server if it comes to that, and it should be a little cheaper than sagemaker if you power it down when not in use. that option requires fairly minimal server maintenance skill, while giving direct experience managing something not dissimilar to what an on-prem server would look like.",
        "How about Roboflow? They seem really great",
        "Sagemaker is unnecessarily cumbersome for this. You can easily run a small classification model serverless-ly on [beam.cloud](http://beam.cloud)",
        "Look at RunPod.io for training, not sure for hosting. Can you run the small classification models server less?",
        "Thank you, so the main solution here is to use a \"big\" ec2 instance for training right, this is what everyone is saying in the comments I beleive?",
        "I'd strongly advise against building an op-prem server, especially since OP says they're not experienced in this area. It can be a huge time sink with all unexpected/unplanned issues.\n\nTraining a few CV models and small classifiers monthly is not going to be expensive, I'd definitely go with cloud (e.g. AWS), just not sagemaker.",
        "Thank you so much 🙏🙏🙏",
        "I want a flexible solution, a cloud provider ...",
        "Correct and Lambda Labs is a better. Its cheaper easier to use more transparent"
    ]
},
{
    "submission_id": "1fy1x0s",
    "title": "Camera recommendation for CV",
    "selftext": "Hi there!\n\nI hope that someone has more experience and can recommend me a well working camera suitable for highway applications. My goal is to mount two cameras on the car roof and do object recognition and distance estimation of objects on the road. The vehicle would be moving at speeds between 80 and 130 kmh.\n\nThanks!\n\nEDIT: \n\nAdditional info:\n\n* 2 cameras\n* waterproof\n* not too big so that it can be installed on the roof\n* can be powered by external power supply (car battery or whatever)",
    "created_utc": "2024-10-07T00:19:30",
    "num_comments": 26,
    "comments": [
        "The TWO absolute MUST haves for STEREO is to make sure they have an EXTERNAL TRIGGER for capture sync and GLOBAL shutters.  \n1 - When moving at speed if the images aren't captured at the same time they'll be at different spatial positions so the calibration done when stationary will be useless.  \n2- And the global shutter for accurate measurements - look up rolling shutter artifacts if you're not sure why this is important.",
        "Check out Luxonis, they have an IP67 industrial grade cameras that can run inference on device. [https://shop.luxonis.com/collections/oak-cameras-1/?q=filter\\_tag%3dextra-features-wr-waterproof+rating&sort\\_by=manual](https://shop.luxonis.com/collections/oak-cameras-1/?q=filter_tag%3dextra-features-wr-waterproof+rating&sort_by=manual)",
        "For distance estimation, you would probably need some stereo camera for good accuracy.",
        "Gopro  cameras? you can use a usb cable to power it without battery",
        "Could look at the ZED Head. Based out of France. I've had the opportunity to play with one of their stereo cameras. And the have a whole suite of software on Github",
        "Arducam has some interesting products",
        "For your highway application, I recommend the **RouteCAM\\_CU25**. This camera is specifically designed for capturing fast-moving scenes and would be a great fit for your requirements. Here are some key features that make it suitable:\n\n# RouteCAM_CU25 Features:\n\n* **Full HD Global Shutter**: Captures images at 60 fps, minimizing motion blur and ensuring clear object recognition, even at high speeds.\n* **Compact and Lightweight**: Designed for easy installation on the car roof without adding excessive weight.\n* **Waterproof**: Built to withstand outdoor conditions, making it reliable for highway use.\n* **GigE Interface**: Allows for video data transfer over long distances (up to 100 meters), which is ideal for your setup.\n* **External Power Supply Compatibility**: Can be powered by a car battery or similar external sources.\n\nWith its high frame rate and synchronization capabilities, the RouteCAM\\_CU25 will help you achieve accurate distance estimation and object recognition on the road.\n\n[https://www.e-consystems.com/gige-cameras/ar0234-global-shutter-full-hd-gige-camera.asp](https://www.e-consystems.com/gige-cameras/ar0234-global-shutter-full-hd-gige-camera.asp)",
        "Expect to burn through cameras as you figure this out, and once figured out that dang camera is going to be exactly where people want to price cut. So, assuming you do this and get all the way to aa product, start now with using low end cameras. A low end camera today is at minimum 1280x720p and 30 fps, and under $30. That price can get under $10 each in volume. The image quality is fine for object and motion detection.\n\nI recommend low end cameras because once you have a product, market forces will basically demand low end cameras are used, but that does not mean low end cameras are used for demos. Having a solution that works with low end cameras will also be a system that works incredibly better with a higher end camera. I see far too often CV team start with good quality hardware, and then scramble to get their kit working when marketing informs them the actual cameras to be used are no where near the quality of what they developed using, and that often means recreating / re-acquiring of a significant portion of their training data.",
        "Choosing the right camera is an important choice. I would advice going deep into the reflection about what you need:  \n- Waterproof  \n- resistant to what heat and what cold level  \n- what resolution do you need  \n- Can you plug it to the car battery or does it need to be autonomous\n\nThis will surely guide you to some specific camera",
        "Those are two very good insights!\n\nCould you maybe recommend me some camera models or at least manufacturers?\n\nThanks a lot!",
        "> 1 - When moving at speed if the images aren't captured at the same time they'll be at different spatial positions so the calibration done when stationary will be useless.\n\nI know about stationary calibration , but dont get your point.  \nCan you please explain this a little more?",
        "At what speeds is a global shutter required? \n\n\nMy application is tracking a person moving about a room. Can I get away with rolling shutter and hardware sync? ",
        "They mention they're going to use two cameras ",
        "I am not developing a product, but only a PoC for R&D project. This means that I am not limited with the price, but I would try to keep the costs low. With cheap hardware I worry that the shutter speed would not be sufficient and that the image would be too blurry when driving fast(er).",
        "This is good advice and something I needed to hear \n\n\nI'm struggling to find an available camera to suit my exact specs, so I'll just get started with a cheap camera and work from there ",
        "Thanks for the comment! I have added it to the description! :)",
        "Any of the big names [Cameras | Browse USB, Gigabit Ethernet, & More | Edmund Optics](https://www.edmundoptics.co.uk/c/cameras/1012/) should do the job nicely, just spec it based on your needs/cost. There's a camera for nearly all use case.  \n[The KITTI Vision Benchmark Suite (cvlibs.net)](https://www.cvlibs.net/datasets/kitti/setup.php)",
        "Camera calibration tells you the spatial relation between the cameras so you can do triangulation of depth map to real world depth.   \nIf the cameras do not capture at the same time, but rather left\\_t0 and right\\_t0+dt because they're not synchronised, then the position of one camera is left\\_s0 whereas the other is right\\_s0+(v\\*dt)",
        "so you dont get distorted images at high speeds.",
        "That's something you'll have to calculate. Find out the update rate of the rolling shutter, the lens FoV, the speed of the object and how this speed maps to pixels traversed / sec.  \nYou can kinda mitigate it in some scenarios if your camera has rolling shutter global reset. So while the top and bottom of the images are captured at slightly different times... at least both camera scan lines should be in sync.\n\n...but of course this also depends on the direction of the scan lines, I'm assuming they always run top>bottom but this might not be the case.",
        "Exactly! Thanks for the comment!",
        "As far as blur with lower frame rates, use something like Lidar to get a more accurate real time read while recording video at 30fps, getting motion blur. Your system will train with the motion blurred frames and operate just fine. And to be honest, you really ought to train with imagery captured at variations of high to low frame rate, enabling your system not to care about frame rates.",
        "You may want to be aware of \"ELP cameras\" they are an industrial camera manufacturer that many AI systems use, and you can get high frame rate versions at good prices - if you can find them , they sell out immediately when available.",
        "> **If** the cameras do not capture at the same time  \n\nYes , I missed the above one.  \nBut if they capture at same and moving at high speeds , that should not be an issue right?",
        "Yes, that wasn't my question... ",
        "sure, that's fine if they capture at the exact same time - but don't rely on the USB bus to assume they've actually captured at the the same time, you'll find there's often an offset. That's why you need an external trigger, they both capture on the same trigger signal precisely."
    ]
},
{
    "submission_id": "1fy19yy",
    "title": "ultimate graphics/game engine, is it an AI problem or a graphics problem?",
    "selftext": "tldr : Should I do PhD in graphics or computer vision(AI) if I want to build AI-powered graphics engine?\n\nMy dream is to build an ultimate graphics/game engine where people can make blockbuster movies or AAA game with ridiculously cheap budget (less than $1000) and in a short time using AI. I was fascinated by text-to-3d stuffs and also NeRF/3DGS etc(inverse rendering) and diffusion model. It seems modelling will part of the problem could be solved in the near future.\n\nThen the remaining part of the problem is animation/simulation/vfx since rendering is almost solved as well. It seems a lot of work is going on with regards to replacing mocap with video+deep learning.\n\nShould I do graphics PhD or AI PhD if my goal is to solve the last missing pieces of this puzzle and buld the graphics engine I want?\n",
    "created_utc": "2024-10-06T23:31:36",
    "num_comments": 5,
    "comments": [
        "I don't really get the question here: what's a graphics PhD Vs an AI PhD?",
        "One thing I know it's impossible to do computer graphics without a CS major. So if you already have one and have deep knowledge of c++, you can go for it. If not, it's going to be very very hard in you.",
        "I did my Msc in computer graphics.\nI thought about moving forward to a PhD in computer graphics.\nBut to be honest the job market for computer graphics is tiny outside of game development.\nAnd I mean absolutely tiny .\nAnd I did not want to be pigeon hold. \nThat's just my advice based on my experience at the time .\nAlways always be thinking about jobs and careers, because at the end of any fancy piece of paper is the need to make $$ to have a comfortable life.",
        "Thank you for your reply! But doesn't Meta/Adobe/Autodesk/Blender foundation and other VR/AR companies etc also hire a lot of graphics PhD as well?",
        "So 4 companies ? \nAndroid and iOS , has about 1 million jobs openings a year."
    ]
},
{
    "submission_id": "1fy0xyb",
    "title": "How long it takes to get review from TIP(Transaction on Image Processing)?",
    "selftext": "Hi everyone, I submitted my paper to *Transactions on Image Processing (TIP)* in July 2024, and the SAE was assigned two weeks ago. I’m wondering when the review process typically starts. It would be great if you could share your experiences.\n\nI’m concerned that if the TIP review process takes too long, I might miss my chance to submit this paper to ICCV or other conferences.",
    "created_utc": "2024-10-06T23:07:15",
    "num_comments": 0,
    "comments": []
},
{
    "submission_id": "1fy0sul",
    "title": "How to sort result in EasyOCR?",
    "selftext": "I have a document with paragraph that detects nicely but the order of the result is not in order. the first word detected is last at the resulting array my code is: \n\n        reader = easyocr.Reader(lang_list=['en'])\n        result = reader.readtext(image=im, decoder='greedy', paragraph=False)",
    "created_utc": "2024-10-06T22:57:22",
    "num_comments": 3,
    "comments": [
        "By default, readtext returns a list of tuples sorted by y, x coordinates.  \nYou may want to sort them by x or y. This is a post-processing step that doesn't depend on the OCR.  \nYou can simply sort by x coordinates or merge words into lines and sort lines first and words into lines",
        "What is the default? And I forgot to mention, it will sometimes return a float bbox and it messes up the sorting. What is causing the decimals?",
        "EasyOCR sorts bboxes from top to bottom, left to right.\nThe floating point values don't make any difference. You can cast them to int.\nPython easily sorts lists with ints and floats. It's not a problem at all"
    ]
},
{
    "submission_id": "1fxsmnb",
    "title": "Are Detection Transformers well suited for small objects detection (FOD detection on airport runways)?",
    "selftext": "Hi, I am an ungrad student. I am currently working on my final year project that is related to Foreign Debris (FOD) Detection. The detection has to be real-time. I am considering using RT-DETR and YOLO v8 models. I have heard RT-DETR is not good for small objects detection. For now, I have only tested RT DETR and YOLO v8n on the FOD A dataset and RT DETR gave superior performance (more accuracy). Should I stick to RT-DETR or consider other YOLO models and DETR models? Which architecture is more preferred for FOD detection in real-time?",
    "created_utc": "2024-10-06T15:24:11",
    "num_comments": 3,
    "comments": [
        "RTDETR in my experience performs better with small objects.",
        "I had the same experience with RT-DETR too , but you can also try some of the Yolo-nas models which are specially trained to detect small objects.\n\nAlso , ultralytics released Yolov11 recently. So, it is good to give it a try.\n\nIf you are not getting satisfied results with any of the object detectors.. you can employ below techniques to improve the overall performance \n\n1. Copy paste augmentation\n2. SAHI",
        "Thanks for the suggestions. I'll def look into the newer models and techniques shared."
    ]
},
{
    "submission_id": "1fxrirx",
    "title": "Coral USB-Accelerator working in recent kernels?",
    "selftext": "For a Raspberry Pi 5 project (non-profit...) I had to get a cheap AI-accelerator. I got a used Coral USB-Accelerator for under 50bucks.\n\nBefore I go into the deep-dive I tried to install everything on my local Kali Linux. Despite all efforts I can't get it running under kernel 6.x. I checked and even the latest Raspi OS is running on 6.6.... even ChatGPT was again in a loop of telling me how to bug-track this, but couldn't deliver a working solution.\n\nAny way to use the thing on any 6.x kernel? Right now I even can't test it... what a bummer.\n\nAny suggestions? Thanks alot!",
    "created_utc": "2024-10-06T14:33:16",
    "num_comments": 0,
    "comments": []
},
{
    "submission_id": "1fxqoeg",
    "title": "Revolutionizing Car Dealerships: Seeking Computer Vision Expert (Equity Opportunity!)",
    "selftext": "We're on a mission to **transform how dealerships showcase their inventory through cutting-edge virtual showrooms.** Think [AutoFox.ai](http://AutoFox.ai), but better. We're looking for a rockstar computer vision expert to join our team and lead this innovation. With 10 major dealerships already lined up, the demand is there!.\n\nThis is your chance to disrupt the auto industry and earn equity in a fast-growing startup.If you're passionate about AI, computer vision, and cars, let's talk!\n\nhttps://preview.redd.it/9br0xy0297td1.png?width=1710&format=png&auto=webp&s=595fc43c86c1a8727b692161f49bd49540274395\n\n",
    "created_utc": "2024-10-06T13:56:10",
    "num_comments": 6,
    "comments": [
        "Whenever I hear the word \"rockstar\" I immediately start thinking that this must be how \"tech bros\" talk to each other.",
        "Are you planning to offer market-rate pay?",
        "talk about money first if you want a rockstar",
        "Do you have a LinkedIn/web page of the company that you could share ?",
        "Of course. + Substential equity might be interesting for a lot of experts out there",
        "Nice, i will reduce my snark level accordingly. Good luck!"
    ]
},
{
    "submission_id": "1fxotmn",
    "title": "Similarities Between Images",
    "selftext": "Hello. I’m new to computer vision realm. \n\nI have 2 goals:\n1) Understand how similar are 2 images to each other. \n2) Classify objects between images that are similar\n\nThe data set I have is going to be relatively small say 100-500 samples same width and height. It’s going to contain a variety of thumbnails per category that could be vastly different in art style, objects, and colors. \n\nFor question 1 I was thinking of using SSIM (Mean Structural Similarity). I just want something simple that doesn’t care too much about details. \n\nFor question 2) I was thinking Histogram Oriented Gradient to parse out different objects in picture. Compare objects between pictures and use some clustering technique to show what objects are similar or not. (Probably use PCA to project onto a 2-D space for a visualization check). \n\nAny thoughts on how to best go about this problem or some additional resources you recommend to get more familiar with CV.  ",
    "created_utc": "2024-10-06T12:36:07",
    "num_comments": 6,
    "comments": [
        "For 1), you can look at extracting embeddings from each image using DINO, CLIP or SimCLR and use those embeddings to compute a similarity metric (usually cosine).\n\nFor 2), this looks like a single-shot/few-shot objrct detection task. Look at how open-vocab object detection models do this.",
        "İ use mobiilnet with cosine similarities algo. İ convert to vector from image with mobilnet and use them cosine similarity",
        "I would advice to use SIFT or ORB and compare number of feature matches. But be careful because all these techniques will always be sensible to light changes (can be handled with detecting light changes and darken those areas but it's not 100% efficient)",
        "1. Check Nvidia FLIP https://github.com/NVlabs/flip. Alternatively you can look on training an Auto encoder as well.\n2. Train an object detector for the images. Using histogram equalization won't work out in all cases.",
        "My company just released an open source tool for plotting out embeddings in 3D space using dimension reduction. You can find clusters, identify outliers, and the actual filter out mislabeled data. You may be able to massage it for your needs https://www.gud-data.com/post/tool-for-cleaning-image-classification-datasets",
        "GitHub link is in the article, hmu if you have any questions."
    ]
},
{
    "submission_id": "1fxlr5b",
    "title": "Vision-Based AIs Racing in Unity",
    "selftext": "",
    "created_utc": "2024-10-06T10:24:52",
    "num_comments": 0,
    "comments": []
},
{
    "submission_id": "1fxl2jd",
    "title": "Apple Depth Pro",
    "selftext": "I was really excited to read about Apple's Depth Pro model especially seeing the examples of fine detail snd how it compared favorably to DepthProV2 (which I think is already amazing), and with the addition of massive speed gains over these other models - but in reality I've found it incredibly inconsistent, often completely wrong, and exactly the same speed as DepthProV2. I'm just wondering if other people have found similar experiences? There's not a great deal in the way of settings so I don't think I can be doing much wrong but perhaps it's the quality of the original images not being high enough?\n\nAs examples, it does often get pin sharp details for lines and some areas of someone's coat or clothes, but I often see a \"halo\" around a subject that simply isn't an area of different Depth to the background. I am also mainly interested in using it for stereoscopic imagery and when converting Depth map + 2D image to a stereoscopic image this reveals massive holes and areas that are completely inconsistent or wrong. Perhaps the model is mainly designed for different purposes such as robotics or image detection though as well? Even viewed simply as a depth map I csn see I'm not getting results comparable with the original authors, however.\n\nI'd be interested to hear how other people are finding it!",
    "created_utc": "2024-10-06T09:55:43",
    "num_comments": 3,
    "comments": [
        "It’s probably designed for photo defocus and depth of field",
        "Ah, interesting! It does mention predicting focal length. I believe the halo I'm seeing and fact that it has still largely over and underestimated some depths would probably still.. be an issue for whatever it's for I'd imagine though?"
    ]
},
{
    "submission_id": "1fxgjkc",
    "title": "Comic-style pictures ",
    "selftext": "Hello everyone!\n\nCould you please tell me if there is any service that allows generating comic-style images in 3-5 slides using prompts, while maintaining consistency in character drawings? Is there a way to do something like this without fine-tuning models? Even if it’s a subscription service.\n\nI checked the Comixify service, but it’s closed.",
    "created_utc": "2024-10-06T06:30:51",
    "num_comments": 2,
    "comments": [
        "try [https://www.reddit.com/r/StableDiffusion/](https://www.reddit.com/r/StableDiffusion/)",
        "Pixton focuses on comic creation and lets you adjust characters to keep them consistent throughout your slides.\n\nIf you’re okay with a bit of experimentation, some AI art generators, like Midjourney or DALL-E, can produce comic-style images."
    ]
},
{
    "submission_id": "1fxfwpo",
    "title": "Are IEEE/CVF the top conferences for CV/Image Processing?",
    "selftext": "As the title say, are IEEE/CVF to CV what ICLR, ICML, NeurIPS are to AI?",
    "created_utc": "2024-10-06T05:58:40",
    "num_comments": 5,
    "comments": [
        "[https://gprivate.com/6dltp](https://gprivate.com/6dltp)",
        "In my experience CVPR is the big one.",
        "😂",
        "💀",
        "It is literally what I did and how I found out, I just want someone in the know to confirm, lmao."
    ]
},
{
    "submission_id": "1fxaees",
    "title": "False possitive result on generalize method ",
    "selftext": "I've been training three models (animals, danger detection, and PPE detection) on YOLOv8m, and they work well when I run them in parallel. By parallel, I mean that if the video is for PPE detection, only the PPE model runs.\n\nHowever, when I run the models sequentially or in a generalized manner (all three models at the same time), I start seeing false positives. For instance, people might get classified as monkeys in the output. I've tried a few things to fix this, including using SAHI for small object detection, but the issue persists.\n\nCould this be caused by my dataset, or is there another underlying issue?",
    "created_utc": "2024-10-05T23:35:53",
    "num_comments": 4,
    "comments": [
        "The first question would be why do you have separate models?\n\nBecause that's exactly one of the problems with having multiple models. If you had a single model for all of them, then it would have learnt the distinction from the totality of the dataset.\n\nEven if an image doesn't contain one of the classes you want to detect through a model, it's still providing information as to what NOT TO detect. By splitting them into multiple datasets and models, you're diluting that information.\n\nSo the animal detection model hasn't learnt to ignore humans which it could have learnt had you also included the PPE detection dataset in the training for that model.",
        "Because the project i handled on, my PM wants 1 camera just detect some objects in PPE, that's why i think it better on separate models.\n\nSo i'm just wondering, if i had those 3 dataset, and train it into 1 models, can i do that? So my model can learn from those 3 datasets\n\nEdit: so the better use case/SOTA of my case is, i use only 1 models?",
        "If you want to save compute and reduce false positives, yes.\n\nOtherwise, you will need to sample some images from the other datasets and add them to the specific dataset. So sample 500 images from PPE and 500 from danger detection and then add them to the animal dataset without any labels and train the animal model. Same goes for the other models.\n\n> Because the project i handled on, my PM wants 1 camera just detect some objects in PPE, that's why i think it better on separate models.\n\nYou can do this with a single model that detects all the classes too. Just remove the labels you don't need in the postprocessing.",
        "After some trials and error, what you suggested to me already works, thank you!\n\nWell yes, while doing some A/B testing singular model works perfectlt rather than i use some separate models."
    ]
},
{
    "submission_id": "1fxabx6",
    "title": "How I want to combine my passion for soccer with data analysis and AI - your opinion",
    "selftext": "Hello everyone,\n\nI have been working as a freelancer in the field of data analysis for about a year now and during this time I have intensively acquired Python and Langchain, with which I have already implemented some smaller projects. My professional background, however, is in soccer, where I worked for many years as a youth coach and video analyst for professional teams.\n\nRecently, I have been thinking hard about how I can combine my passion for soccer with my current skills in data and AI. I find computer vision projects in soccer particularly exciting, for example for tactical analysis, player development or training optimization.\n\nThe areas of application in this field are extremely diverse and I am convinced that there is a growing market and strong demand for this. However, as I am still new to this area, I would be very happy to receive your feedback and assessments - especially regarding the current market and possible entry points.\n\nBest regards from Brazil/Germany\n\nPhilipp",
    "created_utc": "2024-10-05T23:30:43",
    "num_comments": 7,
    "comments": [
        "I work in this field, I know about 3 or 4 relatively big companies similar to mine. It's very cool stuff",
        "RemindMe! 3 day",
        "Oh cool, thank you for your answer. May I ask which country you work in? How do you rate the chances of positioning yourself as a freelancer in this market?",
        "I will be messaging you in 3 days on [**2024-10-10 04:53:22 UTC**](http://www.wolframalpha.com/input/?i=2024-10-10%2004:53:22%20UTC%20To%20Local%20Time) to remind you of [**this link**](https://www.reddit.com/r/computervision/comments/1fxabx6/how_i_want_to_combine_my_passion_for_soccer_with/lqqefhl/?context=3)\n\n[**CLICK THIS LINK**](https://www.reddit.com/message/compose/?to=RemindMeBot&subject=Reminder&message=%5Bhttps%3A%2F%2Fwww.reddit.com%2Fr%2Fcomputervision%2Fcomments%2F1fxabx6%2Fhow_i_want_to_combine_my_passion_for_soccer_with%2Flqqefhl%2F%5D%0A%0ARemindMe%21%202024-10-10%2004%3A53%3A22%20UTC) to send a PM to also be reminded and to reduce spam.\n\n^(Parent commenter can ) [^(delete this message to hide from others.)](https://www.reddit.com/message/compose/?to=RemindMeBot&subject=Delete%20Comment&message=Delete%21%201fxabx6)\n\n*****\n\n|[^(Info)](https://www.reddit.com/r/RemindMeBot/comments/e1bko7/remindmebot_info_v21/)|[^(Custom)](https://www.reddit.com/message/compose/?to=RemindMeBot&subject=Reminder&message=%5BLink%20or%20message%20inside%20square%20brackets%5D%0A%0ARemindMe%21%20Time%20period%20here)|[^(Your Reminders)](https://www.reddit.com/message/compose/?to=RemindMeBot&subject=List%20Of%20Reminders&message=MyReminders%21)|[^(Feedback)](https://www.reddit.com/message/compose/?to=Watchful1&subject=RemindMeBot%20Feedback)|\n|-|-|-|-|",
        "dm me"
    ]
},
{
    "submission_id": "1fx7rep",
    "title": "How to setup environment correctly when renting GPU to train model",
    "selftext": "Hi everyone,  i need to train a CV model. The model and train script has a lot of dependencies, which are all set up perfectly on conda env on local machine. I want to rent a GPU to train this using cloud such as EC2 instance. But i don't know:\n\nCan i mount the instance to local machine env? Or i have to setup all env from scratch on the instance ?\n\nIf not, any recommendation to archive my goal ?\n\nThank you.",
    "created_utc": "2024-10-05T20:40:05",
    "num_comments": 3,
    "comments": [
        "For EC2, you choose an AMI, which is like a base environment. If you choose a PyTorch environment, then PyTorch will come installed. But you will have to set up the other dependencies.\n\nSome other services, like vast.ai, allow you to specify any custom docker image. So if you can turn your environment into a docker image and upload it to Docker Hub, you can use that to launch an instance, in which case you would not need to set up the dependencies.",
        "Docker",
        "Making a custom container image and uploading to docker hub is probably the best way of preserving the environment on your local machine, but bear in mind that what runs perfectly on your local machine might not run perfectly in the cloud. \n\nPresumably if you are looking to use a cloud instance it is because your local GPU isn't good enough, and the cloud based ones are likely to be quadro/Datacenter cards which might require different drivers and CUDA versions to perform optimally. \n\nSo although moving your exact setup might seem a good plan, setting up a fresh one could also be beneficial. \n\nI used Hyperstack because they have an image with the best drivers, CUDA, and container toolkit pre-installed and then I can just build my stack on top."
    ]
},
{
    "submission_id": "1fx23ax",
    "title": "Why do a lot of people write their CV code in Notebooks?",
    "selftext": "I’ve just entered the realm of CV so forgive my ignornance, but I’m trying to learn CV and I’m finding a lot of tutorials are giving links to these notebooks like “colab.research.google.com”. What is the point of this? I’d much rather be doing this locally on my machine in python, so what am I missing?",
    "created_utc": "2024-10-05T15:29:59",
    "num_comments": 20,
    "comments": [
        "What people are glossing over here is that notebooks provide an easy way to connect to remote kernels without having full SSH access to a server. Colab uses this, and since Colab gives free access to GPUs, it has influenced the ecosystem.\n\nAlso, its easier to freeze and document the results in a notebook, which makes it easy for others to follow along.",
        "It can provide a clean slate in a container of sorts to make sure you have all the dependencies necessary to run the code. If someone says to run some command it might not work with the wrong versions of things. The notebook should include commands for installing all the necessary requirements. You should look into virtual environments. When you want to try many different projects they may require different versions of cuda, python, etc.",
        "Notebooks are much easier to share than local code, many people do not have the hardware required to run things fast locally, many online Colab type services have free plans making the barrier to entry much lower on that front as well.",
        "It makes the iterations easy, you don't have to run the whole program again and again, you don't have to load model again and again, you can visualise result again, quick experimentation, quick feedback loop.",
        "As someone who works mostly on embedded machine vision solutions, I have come to the conclusion that there are two factors at play here:\n\n1. Notebooks are easy to set up, easy to collaborate on and everyone shares the same working environment. Packages are easy to install. Additionally you can profit from computing power that may not be available on your local machines.\n2.  Unfortunately, many CV people know very little about software development or do not have an answer to \"how do I write good and maintainable code?\". I have met so many scientists who were great with math and theory but they were completely lost when it came to working with more complex development environments. I don't judge, many do not actually need to work outside their note books.\n\nUltimately it is up to you. There's nothing wrong with using notebooks.",
        "Notebooks are very convenient when writing visual pipelines because they combine code editing with a visual debugger. \n\nYou write some code, visualize the output to verify you get what you expected, if not, you fix it in place until it's perfect, then continue to the next step.\nSince the context is always in memory, you don't have to run steps 1-8 each time you test step 9. \n\nIt's a big upside when developing, as long as you are careful to keep it reproducible, meaning not overriding any state from previous steps",
        "You don't need to run a notebook on a remote server. You can also use the notebook based development on your own PC, for example with VS Code and jupyter notebooks.\n\nNotebooks can be useful even when used locally in that it allows you to easily experiment keeping the state of previous steps in the memory. You don't need to restart and execute the beginning of your program again and again all the time but instead you can adjust one step in your \"pipeline\" and re-execute as many time as needed. By other words it can allow you to experiment and iterate much faster.",
        "It's easier to distribute tutorials as colab notebooks. ML engineers/researchers who have access to private GPU servers don't use colab for their day to day work, but they do use notebooks running locally on those servers. Notebooks are a really great way to iterate on code and see results right there.",
        "You can also run notebooks locally - e.g. install anaconda (free for noncommercial last I checked)\n\nThe advantage is it's a nice middle ground for prototyping between an interactive shell and a full script or application. There's a clear relation between plots/output and the code that produced it.\n\nAnd that's if you're working on a local machine, on  remote machine it's even more practical.",
        "It’s quick and easy but super messy. I try to avoid doing that for my YouTube videos on computer vision for that reason. Better to have your code in vs code and have a requirements file to make it repeatable and clean. But sometimes it’s nice to run things on collab if your hardware is limited",
        "I don't use notebooks to write codes. I use them to test and design  small functions. After the function passes the first tests, I implement it in my Code. I just wish we have something like them for c++. Would have made production way easier.",
        "Because those notebooks have remote kernels with gpu compute allowing you to reproduce their result by just running that or a copy of that notebook. You will never really be using your own laptop to run models or anything that requires gpus because even 4090s are like baby gpus compared to what you can get and will need for most work.",
        "Largely its due to a lot of people in ML not being real programmers and are unable to setup/configure their local workstation with the required software stack.  Python being the mess of a programming language is responsible for this difficulty due to it's versioning and dependency problems, package management, and gigabytes of wheel files needed to get a working environment running.",
        "[deleted]",
        "Plotting from a terminal is a huge pain",
        "Also just because someone used a notebook doesn't mean you have to use one. You can copy their code and run it locally, you'll just have to figure out what all needs to be installed on your own (which is a useful exercise, but may not be the skill you're immediately trying to learn).",
        "The true benefit of a notebook is that you can experiment much faster. If you want to change one part in your code, you don't have to run the whole code from the beginning every time. You can only run the one cell that you adjusted in the notebook. This allows a very fast feedback loop to adjust one part of the algorithm at a time.",
        "\"Someone uses a tool that I don't like, this means they're not real professionals like I am\". I could argue that everyone who thinks like that is less of a professional.\n\nNotebooks are convenient to test segments of code, verify outputs at every step, visualize or modify something, etc. Basically it's an easy tool for discovery and experimentation. After a solution is ready, the code is moved to non-notebook files because they're easier to version control and productionize.",
        "[deleted]",
        "True, it's an advantage!",
        "mv ~/ /dev/null"
    ]
},
{
    "submission_id": "1fwxcev",
    "title": "Help: Thin Plate Spline & Lanczos Interpolation ",
    "selftext": "Hey everyone, I am running into two issues right now with my project regarding aligning astronomical images nonlinearly:\n\n**1. By looking at these plots post interpolation, you'll see I got the image aligned, but the values of the warped image is almost twice as high as the source and target images. Why is that? Here's the following code I used for it:**\n\n    # Scikit-image\n    tps = ThinPlateSplineTransform()\n    \n    tps.estimate(dst_pts, src_pts)\n    \n    warped_img = warp(src_img, tps, order=5)\n\n[Source](https://preview.redd.it/cv6j0xddgzsd1.png?width=576&format=png&auto=webp&s=890c0d3e48b8fc6226f06067c72165633f7e7ff1)\n\n[Target](https://preview.redd.it/xfammwddgzsd1.png?width=576&format=png&auto=webp&s=1e289e54f03d663044dc72401994b0df0d844fab)\n\n[Warped](https://preview.redd.it/x13f0wddgzsd1.png?width=576&format=png&auto=webp&s=ead0f48668acf013512e1b5d46ee87a93b310376)\n\n**2. I wanted to convert from Scikit-image to OpenCV, but was having issues getting similar results to the above. The outputted image looks very twisted and incorrect from the following code:**\n\n    tps_transformer = cv2.createThinPlateSplineShapeTransformer()\n    \n    matches = [cv2.DMatch(i, i, 0) for i in range(len(src_pts))]\n    \n    tps_transformer.estimateTransformation(src_pts.reshape(1, -1, 2), dst_pts.reshape(1, -1, 2), matches)\n    \n    warped_img_opencv = tps_transformer.warpImage(src_img, flags=cv2.INTER_LANCZOS4)\n    ",
    "created_utc": "2024-10-05T11:47:40",
    "num_comments": 0,
    "comments": []
},
{
    "submission_id": "1fwjdhi",
    "title": "How to read car plates",
    "selftext": "I'm just starting out in the world of computer vision with the idea of an application to read license plates. I got 2,000 images and set up a dataset, I tried using tesseract and easyocr, but none of them got more than 20% of the reads right. Can anybody give me an idea of alternatives to get better results?\n\nhttps://preview.redd.it/fdblxbp0jvsd1.png?width=453&format=png&auto=webp&s=1818c023717b564682fc2ea0c4ed880525f0a382\n\n",
    "created_utc": "2024-10-04T22:32:34",
    "num_comments": 6,
    "comments": [
        "You will need to rotate the image to align the text on a straight line before you apply OCR to get better results.\n\nYou can use YOLO to detect license plate regions in an image, then crop the plate out.    The cropped image can then be feed to LPRNet or PPOCR for character recognition.",
        "https://huggingface.co/spaces/GanymedeNil/Qwen2-VL-7B \n\nI added your image (cropped so the title doesn't show), with the prompt \"transcribe this license plate\". \n\nResponse:\nThe license plate in the image is:\n\nFQK 4583",
        "It all depends on your goal. Do you want to get the service working or just try different things to get used to it.\n\nThere are several projects on github, one of them is: https://github.com/Mann1904/Automatic-Number-Plate-Recognition. Just plug and play\n\nFor the second part, I would recommend using YOLO from the ultralytics package for license plate recognition and EasyOCR for text recognition. Optionally, you can add car detection as a first step and license plate alignment after cropping.",
        "You're going to want to give a lot more information and better failure images if you want any constructive response. I'd say almost everyone here could get the right reg out of the provided image.\n\nHow you are pre-processing your images?",
        "try yolo",
        "Yes make sure to impose and extract invariant features from ur dataset such as rotations etc."
    ]
},
{
    "submission_id": "1fwi2bc",
    "title": "Annotation Tools not appearing in Roboflow",
    "selftext": "https://preview.redd.it/sjtfo0074vsd1.png?width=1919&format=png&auto=webp&s=6349dea7f1e9649d4750e81e847d36802f9fe6e9\n\nGood day! May I ask why is the annotation tools not appearing on upload image? It has been an issue for me since last night. Hoping for an immediate solution. ",
    "created_utc": "2024-10-04T21:09:00",
    "num_comments": 5,
    "comments": [
        "What project type is it? Classification, or Object Detection?",
        "*What project type is*\n\n*It? Classification, or*\n\n*Object Detection?*\n\n\\- Proud-Rope2211\n\n---\n\n^(I detect haikus. And sometimes, successfully.) ^[Learn&#32;more&#32;about&#32;me.](https://www.reddit.com/r/haikusbot/)\n\n^(Opt out of replies: \"haikusbot opt out\" | Delete my comment: \"haikusbot delete\")",
        "Hello! it is a object detection project.",
        "Actually, looks like others are reporting it too:\n\nhttps://www.reddit.com/r/roboflow/s/COI8cug955",
        "Yeahh it is a recent issue"
    ]
},
{
    "submission_id": "1fw796n",
    "title": "The finding reference process: how to analyze key frames of YouTube/Vimeo videos based on specific keywords?",
    "selftext": "Before we start developing anything, I'd like to test the ground a little bit on what's currently possible and available. Are you aware of any SaaS tool, or even a Chrome extension, that could analyze and understand key frames of YouTube and Vimeo videos based on specific keywords to identify specific objects or themes?\n\nLet me explain.\n\nThis is about the process of finding references. Currently, this is an imprecise task. Video creators often don’t add all the \"right\" keywords in their descriptions, on platforms like Vimeo or YouTube.\n\nSo, how might we use AI, ML, and computer vision to make the search for those references within the video itself easier? This would be similar to what you can do on Google Photos, when typing in the search bar, looking for a set of pictures.\n\nFor example, by searching for keywords like \"cars\" and \"highway,\" this algorithm/tool/feature would look for any videos with frames of a car or a highway and report them back.",
    "created_utc": "2024-10-04T12:13:59",
    "num_comments": 3,
    "comments": [
        "I'm not sure what exactly you are looking for. I assume you want to automatically segment the video chapters and give them a title? I assume the way YouTube does it is by analysing the transcription of the video and combining it with something like CLIP (or more likely something like SigLIP)",
        "Nope I want to scout trillions of videos and find the ones that contains keyframes of the keyword of my interest",
        "Then you could still use something like CLIP"
    ]
},
{
    "submission_id": "1fw4fst",
    "title": "Datasets of food images",
    "selftext": "I'm developing a project that will provide data about food on a plate from a picture, can anyone recommend any large datasets of food images that I could use to train a model?",
    "created_utc": "2024-10-04T10:13:45",
    "num_comments": 3,
    "comments": [
        "Also , checkout huggingface\nFor ex -EduardoPacheco/FoodSeg103",
        "Check out Kaggle",
        "Anything on kaggle in particular?"
    ]
},
{
    "submission_id": "1fw2q3j",
    "title": "where can i see the yolo functions like .xyxy .xywh ",
    "selftext": "And all those stuff I can't find it on the there website",
    "created_utc": "2024-10-04T09:02:15",
    "num_comments": 5,
    "comments": [
        "Try browsing their GitHub repository",
        "The xywhtoxyxy methods are here. \nhttps://github.com/ultralytics/ultralytics/blob/b89d6f407044276b1f54753ef98c719e89928631/ultralytics/utils/ops.py#L412",
        "I asked there AI in their website and i got it [https://docs.ultralytics.com/reference/engine/results/#ultralytics.engine.results.Boxes](https://docs.ultralytics.com/reference/engine/results/#ultralytics.engine.results.Boxes)",
        "Depending on what you wanted to get, I guess?\nI thought you might be looking for the function themselves, their implementation within the library",
        "Yeah that's what the link I provided is"
    ]
},
{
    "submission_id": "1fw04oa",
    "title": "SAM2 on a 2x T4 gpu",
    "selftext": "Did anyone fine tune SAM using 2 T4 gpu(kaggle)? \nIf so can you share how did you do it?",
    "created_utc": "2024-10-04T07:12:26",
    "num_comments": 0,
    "comments": []
},
{
    "submission_id": "1fw03ya",
    "title": "How to Measure Object Distance from Reference Points in a Side-Angle Video?",
    "selftext": "Hey everyone, I'm working on a project where I have the distance between known reference points in a video. However, I need to measure the distance of objects from these reference points, and perspective correction is required since the camera is positioned at a side angle. Any advice on how I can go about this? Thanks!",
    "created_utc": "2024-10-04T07:11:33",
    "num_comments": 5,
    "comments": [
        "If the camera is not moving, then you cannot measure the distance to any unknown point, because from a single position, you can only get the orientation to that point. Unless the scene you're looking at has a special structure, like being completely planar, for instance.\n\nif you know the 3D positions of the reference points in some reference frame, and your camera calibration, you can run PnP to find your camera's position throughout the video by tracking the reference points. If you track the unknown points, you can now triangulate them because you know the camera's position and orientation at each frame.\n\nIf you only know the pairwise distances between the reference points and you need their 3D positions, in any frame: the reference point j, seen at (u\\_j,v\\_j) is along the ray d\\_j = K\\^-1\\[u\\_j;v\\_j;1\\], so its 3D position is lambda\\_j \\* d\\_j, with lambda\\_j>0. For 2 reference points j and k, we know the distance D(j,k), D(j,k)²=||lambda\\_j\\*d\\_j - lambda\\_k\\*d\\_k||². For many such pairs of points j and k, you get a system of quadratic equations which should be solvable, at least by gradient descent, except in some degenerate case.",
        "You’ll need to use image calibration to flatten out the image then you’ll be able to measure distance. A skewed image can be corrected with a calibration grid \n\nhttps://docs.opencv.org/4.x/dc/dbb/tutorial_py_calibration.html\n\n\nThings to consider. Camera cannot move and lighting needs to be consistent",
        "Hi,it is a camera at fixed position,I know the width of the surface the objects are on,the only problem is perspective difference as the left corner of the video is wider and right corner is narrower.",
        "Thanks",
        "ok, if what you mean is that all the objects present themselves on a single planar surface, then that's good: that means it's a classical problem: there is a homography between the surface and the image plane, and you can estimate it. With it, you'll be able to compute the distance to any point on the surface (that won't be correct for points slightly above, or below,, for instance). So just look up \"homography\" and you should find tutorials on what you need to do."
    ]
},
{
    "submission_id": "1fvzh4y",
    "title": "Computer vision research engineer ",
    "selftext": "Hello everyone as the topic says I have an interview scheduled 4 days from now, I'm a fresh graduate, I have done projects on both 2D and 3D\n\nThe thing is I can't seem to find interview questions for computer vision research engineer.\n\nAny websites would be helpful \n\nHere's the small description of the job\n\nSome of our problems areas include Image Restoration, Image Enhancement, Generative Models and 3D computer vision. You will work on various state-of-art new techniques to improve and optimize neural networks and also use computer vision approaches to solve various problems. \n\nI'll study the projects once again and I have 3 rounds\n\nFirst Technical Round (All Basic concepts)\nSecond Technical Round (Skill based)\nLead Round (Advanced Skill based)\n\nAnything to refer would be really helpful \n\nThank you!!!",
    "created_utc": "2024-10-04T06:43:16",
    "num_comments": 9,
    "comments": [
        "Make sure you understand and you can explain every term used in your resume. Make sure you can explain your projects/thesis very clearly. \nI think you will be expected to demonstrate a basic understanding of ML concepts, and you will also be expected to show understanding of Computer vision. I would advice you understand (theoretically and mathematically) the loss functions and evaluation metrics in common CV tasks such as Object detection (mAP), Segmentation (IoU), super resolution (SSIM/PSNR). For 3D CV, understand how Camera Calibration works, PnP, Homography, Epipolar Geometry, Bundle Adjustment as a part of classical 3D CV. From recent SOTA, see if you can get time for NERF, Gaussian Splatting, Implicit Neural Representations.",
        "IMO, just be yourself. An interview is 80% a personality test and 20% technical. They know your experience level and they know what you learned, they can see it all on your resume. They just want to see if you are a good person and have the capability to learn how to do the job. \n\nPro tip, enthusiasm goes a long way. Show your smile and how much you are looking forward to working on their project. Show your previous work, talk about hard problems that you solved.\n\nIf you don’t know or understand something, just say you don’t know. Just be honest.\n\nI’m telling you this from experience, I’ve been hiring engineering teams for a decade now.",
        "I've collected some resources over the years\n\nhttps://old.reddit.com/r/RoumenGuha/comments/n32eg0/computer_vision_deep_learning_interview/\n\nhttps://old.reddit.com/r/RoumenGuha/comments/mzi88m/senior_computer_vision_interview_at_toptier/\n\nhttps://old.reddit.com/r/RoumenGuha/comments/n21e9l/rcomputervision_resources/",
        "Hi OP . All the best for your interview. would you mind sharing the resources you followed ?",
        "I think OP is based in India, and the job is by Fynd .",
        "Hey OP, do let me know how the interview goes! Which company is this? I’m also an aspiring CV Research Engineer",
        "Write a code that receives a set of 2D points and sort them in such a way that drawing a line feom the first to second, second to third, etc. will form a convex polygon.\nOBS: it is garanteed that the points do indeed form a convex polygon.\n\nI got rejected in my first ever interview bc i didnt know how to do this.",
        "Thanks !!!! I'll be on it now!",
        "Explain your experience with a smile and enthusiasm. Nicee.\nThanks!!"
    ]
},
{
    "submission_id": "1fvxksh",
    "title": "How to modify YOLOv8 or any YOLO architecture to better fit my needs?",
    "selftext": "Hey guys, I'm currently using YOLOv8 for a commercial project of mine, I want to tweak the model architecture to better fit my needs. I had a couple of different ideas, they include:\n\n1. Modifying the backbone architecture, change it to some sort of untrained or pre-trained ResNet, EfficientNet, GoogleNet and VGG (Please also advice me on how to pretrain and load that model in the backbone)\n2. Modifying the neck (To improve accuracy or decrease inference time, I will fuck around and find out)\n3. I'm happy to listen to any other suggestion you guys may have, I'd love hear any other ideas that you guys may have\n\nPS: The objects I want to detect are not extremely small, they're probably around the size of a football (soccer ball for the americans). \n\nThank you so much for taking the time out to read and respond to this!",
    "created_utc": "2024-10-04T05:08:56",
    "num_comments": 12,
    "comments": [
        "Couple of observations here:\n\n1. The size of the object is irrelevant. Is it a football photographed from 1000 meters away or a pinhead photographed from a centimeter away? \n2. Did you buy an Ultralytics commercial license, or are you publishing all your application code? Show us the code and we can gladly help. Otherwise, ask Ultralytics since you are paying them :)\n3. What benefit do you expect from modifying the model? What specific problem are you trying to address?",
        "Just use the Ultralytics library to make the prototype, take the model parameters and recreate your own model. Rip out the features that are critical, then retrain your own custom model",
        "I think experimenting is cool but the typical Darknet backbones used in YOLO series I believe are the result of neural architecture search so it seems somewhat unlikely to me you will do better with an off the shelf ImageNet trained backbone like you describe.",
        "I mean if you don't really know how to modify it and you are asking others to tell you how, then you are probably biting more than what you can chew.\n\nThere isn't any guide out there showing you how to modify a library source code to fit your use case. That's something you do on your own and the expectation with that is also that you know how to search and read code to find the relevant parts you need to modify, in which case you wouldn't be needing to ask how.\n\nThe answer to the question you're asking is not even simple enough to fit a reply.",
        "1. It's similar to a football photographed from 2-5 meters away\n\n2. No, I don't have a commercial license for Ultralytics, I want to develop something of my own, I'm taking inspiration from the architecture of YOLOv8\n\n3. The main benefit I'm looking for is to have an object detection model that is sort of specific to the kind of objects I'm dealing with (The object that is the size of a football photographed from 2-5 meters), and with it to have a model of my own that I can use without any restrictions",
        "Could you please elaborate a little more on what you mean by ripping out features that are critical?",
        "Oh okay, that makes a lot of sense, but other than playing around with the backbone, is there something I can customize in terms of the neck or head, that you think I may be able to change so as to improve the model, and even if not improve, I'll need to change something to make sure I'm not infringing on the license as the license used for these models stops me from deploying it into a commercial product without making the model open source",
        "The only thing I'm looking for is any help/advice, I am aware that what I'm asking right now is out of my scope, was just looking for any person who has experience with this, so that I can atleast understand what to start with and where to look for",
        "Ah I see! Good call to develop your own model. \n\nCant help with specifics but I do know that the newer versions of yolo can sort of self configure certain parameters to target objects of different dimensions. Maybe if you can look at thet code it would help you figure out how to configure your own network? ",
        "Just copy all of the parameters that you need to rebuild your own model",
        "What are people supposed to reply with?\n\n\"Go to file X, edit line Y and add the module, which needs to be added to `__init__.py` inside folder Z, but you also need to edit file A at line C, and add the module name so that it can be used which will break method M in file H, so you have to edit that to fix it\".\n\nThen you reply with \"I got some error / didn''t find that specific line. Can you help?\"\n\nEven if you told exactly what module you wish to add (different modules would have different quirks), it's still something people can't help you over a Reddit thread with.\n\nThere's no feasible way someone can tell you to do what you have to do through a thread like this. There's no guide to editing source code. You just know how to read the code and go from there. People edit source code and open pull-requests all the time. They don't need a guide. If you need a guide on what and how to edit, then you shouldn't be editing it to begin with.",
        "lol that’s an epic reply that should be used for half the questions in most Reddit coding subs!\n\nBut to the OP’s question specifically I think they’re looking for model architecture advice and just didn’t phrase the question that well.\n\n\nSomething like this: “skip connections should come from the layers with the strongest activation at a given object size.” I just made that up and have no idea if true…but if it was it would give the OP an idea of how to make their own architecture that’s different than YOLOv8 but still works as well or better. "
    ]
},
{
    "submission_id": "1fvx2z2",
    "title": "Blog post: Use cases of Artificial intelligence and computer vision implementation in education",
    "selftext": "",
    "created_utc": "2024-10-04T04:41:26",
    "num_comments": 0,
    "comments": []
},
{
    "submission_id": "1fvw0nn",
    "title": "How to do data augmentation on a YOLO annotated dataset?",
    "selftext": "Hey guys, I'm working on this project, the dataset I'm dealt has multiple classes. I want to build a model using the YOLO architecture so that the model can detect (both class and bounding box) the targets. The dataset I'm given is very imbalanced, how can I perform data augmentation in this case? This project is for commercial use and the data I'm dealing with is confidential, please suggest some tools that I can use locally to perform the annotations (So that the data isn't uploaded/stored in any other platform)",
    "created_utc": "2024-10-04T03:34:40",
    "num_comments": 14,
    "comments": [
        "Also, ultralytics has built in augmentation on their yolo training tools. It does a really good job, but you’ll need to evaluate if their licensing methods work for you. It’s great for prototypes and you can quickly learn a strategy, then just copy what works best for you.",
        "Albumentations is your solution.",
        "PyTorch transforms is an option built into the framework https://pytorch.org/vision/stable/transforms.html",
        "I've been doing augmentation (for yolo box COCO annotated synthetic chessboard datasets that I've made) with [keras_cv](https://keras.io/keras_cv/), I did try the albumentations the other commenters talked about but at least for me found this one easier to get going.\n\nBit of a plug but I talk about making and augmenting the dataset in a YouTube video (it shows the keras_cv augmentations and some alternatives I considered, should help you get ideas on how to augment your annotated dataset) https://youtu.be/eDnO0T2T2k8",
        "I like to use Roboflow, they have a nice UI that provides a preview so I can see what things look like. ",
        "I use albumentations, which returns both the modified image and modified labels. \n\nAugmentations are usually done on the fly while leading each batch during the training loop. You don’t need to save themz",
        "Albumentations... I'd like to decouple the training part from the augmentation step and save them for future references as long as they are not too many. You can do various ops (around 15) on the annotated data and get both the images as well as their corresponding annotations. Just make sure you consider all of your annotation methods if you've used more than one type of annotation (mainly for Segmentation).",
        "if u did annotate yet try roboflow and its augmentation methods",
        "As you've mentioned, their licensing is something that makes it unusable (during deployment) for my case, so I'm looking at replicating the same architecture with minor tweaks that suits my use case or to go with some other object detection architecture with a license more favorable to me",
        "I did come across Albumentations , but can I augment images after they're annotated, and for the new augmented images, will I get the corresponding annotation text file?",
        "Will this option take in already annotated images and for the new augmented images give me the new annotation text files?",
        "After annotations? I think yes. It accepts the bbox parameter. It does not give annotation text file but bbox of augmented. \n\nhttps://albumentations.ai/docs/getting_started/bounding_boxes_augmentation/",
        "Yes, it works with annotated images and transforms both the image and the bounding box using an object called a TV tensor.",
        "Thank you so much, I really appreciate your help!"
    ]
},
{
    "submission_id": "1fvv66b",
    "title": "Train using one Skeleton then Input Multiple Skeleton into that Classifier(Activity Recognition)",
    "selftext": "1. Pose estimation to Collect keypoints of a human hand skeleton.\n\n\n2. I Train a Classifier (LSTM) with keypoints to Recognise Some activities (Waving, Clapping) using Training videos of one person Waving, Clapping\n\n\n3. Recognise Skeleton (hands) of multiple people realtime in a video through pose estimation.\n\n\nHow do I feed these multiple skeletons simultaneously to the LSTM to identify the person who is waving?...Is there an example I can look up?\n\n\n\nAlso is possible to recognise wave from a different  video angle than training data using this method?",
    "created_utc": "2024-10-04T02:34:50",
    "num_comments": 0,
    "comments": []
},
{
    "submission_id": "1fvv41t",
    "title": "Post processing techniques",
    "selftext": "Hi guys.\n\nI'm working on a medical binary semantic segmentation task. The output of my model is clearly a mask with 0 pixels indicates background and 1 pixels indicates lesion (the latter not too much big).\n\nAre there, in your opinion, some post processing techniques to further enhance my predictions?\n\n  \nThanks a lot for the help.",
    "created_utc": "2024-10-04T02:30:22",
    "num_comments": 4,
    "comments": [
        "I do not know if it will answer your question clearly. But my binary segmentation model always had sigmoid at final layer and I had to apply elementwise thresholding like if an elemeny is greater than 0.5 then its 1 else 0. And mostly there would be some holes in the mask so I used to apply morphological operations. Then I would put info  like: mask, contour, contour ratio in a dataclass.",
        "You can use Conditional Random Fields",
        "Yes the same as you but I didn’t get what you mean with contour, contour ratio ecc. Thank you",
        "I meant to extract contours from the mask. Which we can later draw on the original image. And contour ratio could be the area of contour divided by the product of mask height and width. It is not widely used though. But I use this to discard very small contours relative to input image."
    ]
},
{
    "submission_id": "1fvuqcx",
    "title": "Game event detection",
    "selftext": "I was given a project \"Highlight game event\", the aim is to recognize events in video games. I plan to do so in Valorant, recognizing headshots and vulnerable enemies. Is this possible with YOLO and how can I train it. Please help me, thank you.",
    "created_utc": "2024-10-04T02:00:52",
    "num_comments": 2,
    "comments": [
        "I have same idea, but with League of Legend!   \nI want to know how to start from scratch.",
        "I'd do it with text. I don't know the game but you'll probably find that there's some text somewhere on screen that appears for a bunch of interesting events."
    ]
},
{
    "submission_id": "1fvuivg",
    "title": "OCR evaluation metrics suggestions",
    "selftext": "Is there any other open source library like jiwer which is used for evaluation of OCR models ? \nAny metrics than (precision, recall, F1 score ) that we can build ourselves to compare response vs GT ?\nCurrently I have implementations for wer, CER etc",
    "created_utc": "2024-10-04T01:44:49",
    "num_comments": 6,
    "comments": [
        "AFAIK, it’s edit distance all around, regardless of whether you do it on per character basis or per word basis. \n\nYou can also calculate accuracy (i.e., how many full matches you have), but I can’t really say it’s a great metric to go by.",
        "CER is the most popular metric for model training.\n\nYou can add your own metrics depending on your business pipeline: WER on specific fields or calculate the whole text match rate. You can add text correction if you know the source text and measure cer/wer after it or ask llm to do it.",
        "WER (Word Error Rate) is basically a word level accuracy",
        "Great, can you explain a bit more on the text correction aspect ?",
        "Check your dates. For example, you only recognize dates in the mm.dd.yyyy format.\n\nYou may want to add heuristics to improve the results. Replace all Os and Ds with 0, I with 1.\n\nEven if your application didn't recognize the delimiter, you can recover the data based on the number of digits, another option is to check the possible dates: month can only take 12 different values and can only start with 0 or 1. If you have 9 digits and the first digit is not 0/1, you can probably delete the first digit.\n\nIt takes a considerable amount of time to learn more about your data patterns, common model errors, and to construct the heuristics.",
        "Sure thing"
    ]
},
{
    "submission_id": "1fvu9sc",
    "title": "Training yolo to recognize a specific cat?",
    "selftext": "I live in a neighborhood with many cats, and have to open the door for my cat every time she wants to get in or out. Unfortunately, a cat door is not an option in the house I live in. What I instead would like to do, is set up a camera, and use a raspberry pi running YOLO to detect if my cat is outside. However, a bunch of cats tend to wander in front of this specific door, and I would only want to detect MY cat. Is this possible? How big of a dataset would I need?\n\nThanks for any insights.",
    "created_utc": "2024-10-04T01:25:17",
    "num_comments": 6,
    "comments": [
        "Use yolo to detect any cat and then use a Siamese network (a “contrastive learning” model often used for facial ID) to identify your cat. \n\nBonus points if your cat is a Siamese cat 😂 ",
        "How big of a dataset you need usually depends on how hard the problem of detecting your cat.\n\nDoes your cat look very similar to other cats? Is the lighting around your area good enough to make it easier to visibly spot your cat? These are some factors that contribute to the \"difficulty\" of the problem. Unfortunately its hard to quantify difficulty. \n\nSince you want to differentiate your cat with other cats, its important to include other cats in your dataset as well. Eg take the images of other cats and NOT annotate any bounding boxes on these. This will let the model differentiate between your cat and other cats. \n\nI'd start with probably 50 to 100 images of your cat in various lighting conditions. Then see the performance. Keep adding images until you're happy with the results.",
        "I have done this and run an image classification model using EfficientNet Lite.  Our two cats though have distinct markings and patterns on their coat of fur so I can detect them with accuracy.    However if you have an all black cat for example your not going to be successful at all comparing it to another black cat in the neighbourhood.\n\nIf your wanting to detect in the dark then you will need IR flood flight and NOIR camera to pick them.\n\nYou could get away with as little as 100 images of your cat from various angles and lighting to get a model trained.  As another user writes you also need other cats in the dataset to classify those as not your cat.\n\nCreating[ cat door bells](https://github.com/search?q=cat%20doorbell&type=repositories) is actually quite a popular project and can be done using a variety of methods;  proximity sensors, RFID, via sound/microphone listening for your cat meowing, and image classification.",
        "Can't you put something like an RFID tag on your cat's collar? ",
        "Take a video of your cat staying still, this will give you more frames to use in your data set. Then repeat the video from different angles. Make your bounding box tight and consistent (don’t chop off features in some that are included in others)\n\nThis will create a large biased data set which should nearly over fit, which is what you want for your cat",
        "Hey guys. Do you think it would be possible (by using any method, even if it's hard) to train a model that would recognize more or less any random cat compared to their picture in a database ? Doesn't need to be absolutely precise, but at least might be for narrowing down/filter and the final evaluation can be done by a human. If you know any tools or if you have any method you can think of that would allow to approach that, don't hesitate, even if the answer is very technical. For now all I can find is training for binary cat/not cat, OR for only ONE specific cat."
    ]
},
{
    "submission_id": "1fvu8ph",
    "title": "8x Faster TIMM Vision Model Inference with ONNX Runtime & TensorRT Optimizations",
    "selftext": "I wrote a blog post on how you can take any heavy weight models with high accuracy from TIMM, optimize it and run it on edge device at very low latency.\n\nAs a working example, I took the eva02 large model with 99.06% top-5 accuracy, optimize it and made it run at about 70+ fps.\n\nFeedbacks welcome - [https://dicksonneoh.com/portfolio/supercharge\\_your\\_pytorch\\_image\\_models/](https://dicksonneoh.com/portfolio/supercharge_your_pytorch_image_models/)\n\nhttps://reddit.com/link/1fvu8ph/video/8uwk0sx98psd1/player\n\nEdit - Here's the Hugging Face repo if you'd like to reproduce the video above. You can also run it on a webcam. \n\nModel and demo on Hugging Face.\n\nModel page - [https://huggingface.co/dnth/eva02\\_large\\_patch14\\_448](https://huggingface.co/dnth/eva02_large_patch14_448)  \nHugging Face Spaces - [https://huggingface.co/spaces/dnth/eva02\\_large\\_patch14\\_448](https://huggingface.co/spaces/dnth/eva02_large_patch14_448)",
    "created_utc": "2024-10-04T01:23:06",
    "num_comments": 16,
    "comments": [
        "Wow. This is such a nice article with all the goodies. I really like the trick to merge processing as part of onnx. Does the merging help on jetson devices as well which has unified memory?",
        "This is amazing. For someone(me) who is deploying vision model for the first time on edge device. Thank u very much for posting this for others!",
        "interesting. thanks",
        "I think you should see even more of a boost if you use the [onnxruntime_extension](https://github.com/microsoft/onnxruntime-extensions) library rather than merging the torchscript yourself.",
        "Chad",
        "Are you a PhD student or you do this for fun",
        "I don't have a Jetson to confirm, but it should in theory. I can provide the merged model if you'd like to play around with it and confirm if it helps with the inference latency on Jetson",
        "Here's the model on Hugging Face - [https://huggingface.co/dnth/eva02\\_large\\_patch14\\_448](https://huggingface.co/dnth/eva02_large_patch14_448)",
        "I didn’t know that processing could be embedded in ONNX, but that opens really interesting possibilities. Being able to update preprocessing with the rest of the models simplifies updating them a lot when major changes happen",
        "It's my pleasure. Please share your results too when you have them!",
        "Great idea. Thanks!",
        "Done with phd, mostly for fun at this point 😁",
        "Thank you for providing this. I will try it in jetson and let you know. Can I reach out to you over DM for questions on this article?",
        "Yes. The post processing can also be embedded into onnx. I did not do that for this post. But that may reduce the latency more. In fact you're not limited to pre/post processing. Any operator that onnx runtime supports can be added into onnx. This opens up a lot of possibilities",
        "Sure",
        "Embedding all the required stages in the ONNX file seems a great opportunity for deploying models as ONNX runtime is available for many languages (not just python). I’d love to see more research on this direction"
    ]
},
{
    "submission_id": "1fvsq1e",
    "title": "Yolo for multi class object detection?",
    "selftext": "I need to be able to detect and classify multiple objects. Is YOLO a good way to do this or should I opt for something like tensorflow instead?",
    "created_utc": "2024-10-03T23:25:59",
    "num_comments": 6,
    "comments": [
        "Sorry, you have not clue what you are asking right now.\n\nTensorFlow is a library that allows you to create and train AI models.\n\nYOLO is actually a model that is good at detecting and classifying objects in pictures. \n\nSo to answer your question, yes, you need YOLO (do it PyTorch, PyTorch is another library to create and train models like TensorFlow).",
        "Go for yolo.for yolo when you do detection you have already done classification",
        "At this link you can find anything you need to work with yolo: https://docs.ultralytics.com/#where-to-start",
        "Yeah, I figured I probably wasn’t making much sense. Thanks for the info!",
        "Thank you!",
        "Thanks!"
    ]
},
{
    "submission_id": "1fvox4h",
    "title": "Where I can check the WACV workshop list for 2025?",
    "selftext": "It is not on the website and supposely the deadline for workshop papers are apporching\n\n",
    "created_utc": "2024-10-03T19:29:15",
    "num_comments": 3,
    "comments": [
        "I cannot find it also",
        "Finally released [https://wacv2025.thecvf.com/workshops/](https://wacv2025.thecvf.com/workshops/)"
    ]
},
{
    "submission_id": "1fvmpri",
    "title": "UAV Small Object Detection using Deep Learning and PyTorch",
    "selftext": "UAV Small Object Detection using Deep Learning and PyTorch\n\n[https://debuggercafe.com/uav-small-object-detection/](https://debuggercafe.com/uav-small-object-detection/)\n\nSmall object detection is a real challenge for deep learning models. Most deep learning models, although capable of performing well when detecting large objects, perform relatively worse on small objects. Even more so, when we start to fine-tune an object detection model on a new dataset. In this tutorial, we will carry out UAV Small Object Detection. In short, we will train an object detection model on high-resolution aerial imagery which contains very small objects. This will be a nice challenge considering that we will deal with a very unique dataset.\n\nhttps://preview.redd.it/zc32lz26xmsd1.png?width=1000&format=png&auto=webp&s=bc613f0921a552e02f60c74561bc97e88fbf53fd\n\n",
    "created_utc": "2024-10-03T17:33:39",
    "num_comments": 0,
    "comments": []
},
{
    "submission_id": "1fvlyfy",
    "title": "Recommendations Good book for computational photography ",
    "selftext": "Same as title ",
    "created_utc": "2024-10-03T16:56:27",
    "num_comments": 0,
    "comments": []
},
{
    "submission_id": "1fvh83w",
    "title": "Roast my resume, skills and experiences. ",
    "selftext": "https://preview.redd.it/toq1dpornlsd1.jpg?width=1242&format=pjpg&auto=webp&s=7f62f75237421fc48ade2514435a57e07169946c\n\n\n\nhttps://preview.redd.it/sy4rxzytnlsd1.jpg?width=1242&format=pjpg&auto=webp&s=84c84bd3b2d994e5a9e6addcc33fdad1d0cb64e3\n\nI invite you all to take a sit, relax, and roast the hell out of my resume, skills and experiences.  \nPS : The novel FER model mentioned in Audience tracker, is one of the results from my upcoming paper.",
    "created_utc": "2024-10-03T13:20:55",
    "num_comments": 7,
    "comments": [
        "Roast:\n1. I don’t fully understand what is the first page, is it notable projects? Who are they done with? Their descriptions are written in different styles.\n2. “Over 100 ML projects” between 23-24 raises an eyebrow.\n3. I just don’t find here what are you interested in and good at. You clearly have a lot of ML experience, but it’s a vast field. Imagine that I have a company that is doing X. And I need a person that has experience with X. What is that X for you? I have to read quite an essay here to get a feeling of your skills.\n4. Don’t use a “progress bar” for English skills. Just write “professional level” or something similar.\n5. (might be controversial) I know it’s a popular advice to show effects of your work (“achieved 150% accuracy on cat model”), but TBH I would rather read about how were you involved in a project and scope of your work rather than read metrics on a dataset I don’t know anything about.\n\nYou have nice experience, I just think you need to boil it down a bit to the essence.",
        "Well thank you. 1- it's not. It's the second page. Reddit decided to reverse the order. About the projects, the name of the employee is just right below the project title. I think it doesnt catch the eye the way it should. I must change it.\n\n2-hmm, it's not just in ML as it says, and also I needed money very badly in those years but you are right that can be suspicious. How would you suggest I put them?\n\n3- oh, I thought the courses and projects indicated computer vision. But I am also very interested in production and deployment. How would you suggest to mentiom these?\n\n4- Got it. \n\n5- Very good. How about the others? Any suggestion on how to fix it?\n\nAgain, thank you .",
        "2. I get that. You’ve done a lot of work, gained a lot of experience and want to show this. I just think you need to focus on highlighting and describing in more detail work that is relevant to what you aspire to. I would try to limit number of words spent on projects that are less relevant, or remove them at all.\n\nComment for 2 and 5: try focusing on what technologies you worked with. Examples: have you used Kubernetes? AWS (or different) for training or data storage? Docker? Have you deployed your models somewhere? These kind of information are more interesting than “95% accuracy”. Using Random forests, SVMs or others usually involved 1 call to a library (or if it doesn’t, it should). I think everything “around” it is more interesting.",
        "What I’m finding sometimes useful is to add a very short description about yourself: I am a MLengineer/student interested in <x> with experience in <more concrete x>.",
        "Thanks dude. I'll change it according to your suggestions.",
        "Good luck, I think you have good experience and can land any job you want!",
        "Good luck to you too. And thanks again. You've been a great help."
    ]
},
{
    "submission_id": "1fvghk3",
    "title": "How to OCR a little known language?",
    "selftext": "The alphabet is Latin-based, with many diacritics. I've tried fine-tuning Tesseract models but the result were unsatisfactory. Any advice would be helpful!\n\nhttps://preview.redd.it/v1z0hrvoilsd1.png?width=1870&format=png&auto=webp&s=6d5a9788103782fae6715a0cab641dba4c743372\n\n",
    "created_utc": "2024-10-03T12:49:04",
    "num_comments": 0,
    "comments": []
},
{
    "submission_id": "1fvet5z",
    "title": "Industry grade lighting",
    "selftext": "I'm not sure if this is the right subreddit, but canyou recommend some good manufacturers when it comes to industry grade lighting equipment?\n\nI've mostly been using videography lights and they tend to work fine. But I'm currently working with a CV application in a quite dusty environment and I'm afraid consumer grade videography lights won't work.\n\n I need a quite powerful light 100W+, with a narrow FoV (30-50 deg) and that's either passively cooled or has fans that wont clog up from a dusty environment.",
    "created_utc": "2024-10-03T11:37:08",
    "num_comments": 1,
    "comments": [
        "Banner and Cognex both sell great lighting. Use for ip69k environments regularly."
    ]
},
{
    "submission_id": "1fvekcy",
    "title": "Where should a beginner start with computer vision?",
    "selftext": "Hi everyone, I’m a Java developer with no prior experience in AI/ML or computer vision. I’ve recently become interested in computer vision, and while I know its definition, I haven’t explored the field yet.\n\nI’ve watched a few YouTube videos on using OpenCV, but I’m wondering if that’s the right starting point. Should I focus on learning the fundamentals first, or is jumping into OpenCV a good way to get hands-on experience? I’d appreciate any advice or recommendations on where to begin. Thanks in advance!",
    "created_utc": "2024-10-03T11:26:50",
    "num_comments": 26,
    "comments": [
        "Start with general mathematics. I'm not kidding.",
        "there is a channel on YouTube called first principles of computer vision. That will explain you the basics. Then you can start studying deep learning things. If you don't understand some concepts, chatGPT is your friend. Also you will learn a lot from this subreddit.",
        "If I started learning about computer vision again and in particular image recognition (you mentioned AI in your question), I would start learning how to properly structure a project and the overall methodology that should be followed. A bad dataset can cause a model to fail in production, so it is important to capture and label data properly before training models. Once you have that, it is also important to keep a record of all preprocessed images, extracted features, tested models, etc to avoid having to reprocess them or losing a model that worked fine. It is also important to know how to properly test models",
        "1. Maths + classical computer vision ( don't go too deep) \n2. Maths + CNN ( all basics) \n3. Maths + Pytorch/tensorflow ( able to edit or right custom network) \n3. Hardware understanding for inference\n4. Parllel programing ( start with openmp easy to learn the  Cuda) \n5. What ever projects you like. \n\nThat's all",
        "Fast.ai",
        "Have a project in mind? I've found that having a project idea (or even something for work) can help motivate the learning process considerably. I also found the OpenCV tutorials very helpful as well.",
        "Use YOLOv11 to run object detection against static image files or an RTSP stream from a security camera",
        "cs231n andrej and coursera andrew . deep learning with pytorch book",
        "Start here. You would need to brush up your calculus too.\nhttps://szeliski.org/Book/download.php",
        "It depends what you are trying to learn. If you want to get good at CV, you will need to have a strong background in linear algebra and calculus. On top of this, for the more modern techniques, you will need to understand machine learning and especially deep learning.\n\nIf you just want to do some fun projects but don’t really care about building a deep understanding, you can find some online tutorials for training vision models like yolo or FasterRCNN.\n\nEither way I think once you are ready to learn CV, the best way is to find a project that interests you and learn by doing. Reading books or taking courses can help with some of the prerequisites, but I truly believe projects are the best way to build practical skills.",
        "Ancient secrets of computer vision on YT. This course was taught by Joseph Redmon - main author of YOLO at uni of Washington.",
        "Just learn PyTorch and come up with something you want to solve, and implement an end to end solution. You don’t really need the math , at least not on a deep level.",
        "Depends how deep you want to go. Math background isn't required if you're just using and training existing models",
        "thanks for the advice. Should i put a pause to openCV videos and first go through the introduction?",
        "This is honestly it. I've been trying to break into machine learning for almost 2 years now (I'm 17 now) and I got drowned in so much theory and math that I haven't really done what I want to do..............build real cool machine learning systems for a wide range of applications. Next year I'm gonna just do what I want and piece stuff along the way instead of trying to learn pre-requisites separately",
        "Link: https://youtube.com/playlist?list=PLjMXczUzEYcHvw5YYSU92WrY8IwhTuq7p&si=yYUJwS-N0r7HVlgm",
        "I know but isn't the job of an engineer to dive into designing as well?",
        "I think so. Those videos on YouTube are very good, start from there",
        "It really depends on what they're trying to accomplish. Some engineers will build neural networks from scratch, and many more will just use what others have built, and integrate it with other systems.",
        "Thanks for the advice",
        "OK so they are not cv engineers. They are software engineers at best. I'm not undermining software engineers, just pointing out what cv engineering is.",
        "Even if you are using existing models, you need to understand what distributions are, how they can be normalized, and what assumptions can be made surrounding these distributions to be successful in selecting/training/deploying existing models",
        "Forgot to add, those videos assume that you know a bit of math",
        "What level of math? Advanced?",
        "to understand how a camera works you need to know some basic linear algebra: matrices, homogeneous coordinates, linear mappings. Also you will need basic calculus things like derivatives and limits."
    ]
},
{
    "submission_id": "1fve35x",
    "title": "Cv Comments",
    "selftext": "Hi, please evaluate my resume.",
    "created_utc": "2024-10-03T11:06:52",
    "num_comments": 16,
    "comments": [
        "please don't list all the yolo versions, they are all the same ... also RNN and LSTM are the same thing",
        "You have more libraries listed there than I have after 9 years of computer vision. Trim it and be concise.",
        "looks like you've googled all libraries for each field and then pasted them there. I'm not saying it's the case, I'm saying it looks like that. and it's not a good impression",
        "Hmm, seems like you are very new in CV and just googled some stuff. for example, arc face is a face recognition system and not a detection system. There are numerous duplicates in CV section, generally, object detection contains any object, and recognition means classifying the images based on some labels. You don't have to separate different detection or recognition problems. They are all the same. Darknet is just a backbone, its not a detector. So if i were the employee you would be rejected instantly. I suggest you pick a field and study that field deeply. Also explain a little bit about what you did on those projects and learn DIP. Its very clear that you have 0 knowledge in that regard. Wish you the best and  Good luck.\n\nEdit : I just saw you experiences, looks as fake as a Lamborghini with Huttube.",
        "Reduce the skills section (it doesn't tell me anything. Are you an expert at every singe one of them or have you just heard of them?), and pick out a few important projects. This way you should get it to one page.",
        "I see a lot of names of somewhat related libraries but no indication that you actually know what you're doing with any of them\n\nIt's also impossible to know which of your list of projects you substantially contributed to, or what they did, or if they actually completed a task\n\nIf I saw someone with 10+ projects over 2 years without any really specific details like that, I'm just going to assume that you didn't have a chance to dig in and really understand what you were doing in them",
        "don’t list, but contextualize. I personally don’t count tools/skills/etc on someone’s resume unless they describe where it was employed. Bonus points if projects link to some proof of completion (eg github, youtube video, paper, etc.) So many people make up stuff per job application it is the only way to quickly judge if the person actually knows the required/desired skills",
        "Too many projects. I dont want to see list of example projects. I want to see whole finished max 5 apps with websites or mobile apps, or even a video showcase. None of them even have a name, a project should have a name right?\nToo many technologies. Knowing a “language” and being expertise on it is different things. We want expertise in few fields. Not all fields with little expertise. Sen beni anladın, başarılar.",
        "One page max - if you’re gonna include projects you need to show measurable results on then",
        "Mm no they are not?",
        "Hmm, what if some projects are under NDA ? because in most cases you cannot publish the source code anywhere.",
        "What? The YOLO versions? He is probably using them as a black box, they are all the same. Ultralytics versions don't even have a paper to study",
        "If there is public material (even marketing), share it. If not, describe what you can. Even resumes I’ve seen have projects describing US DoD projects in a sentence, so that is always feasible if you work with your (previous) employer on what can be publicly said",
        "RNNs and LSTMs are not the same thing, you paesant",
        "Can you please take a look at mine?\nhttps://www.reddit.com/r/computervision/s/eLUJmgfkLc",
        " both recurrent architectures and obsolete now, and he has it's own right, it's not like is there someone that uses darknet"
    ]
},
{
    "submission_id": "1fv6xsy",
    "title": "Benchmarking vision language models for document data extraction",
    "selftext": "Hi all, I’m a deep learning engineer working on vision language models exclusively for document data extraction, so I decided to test 6 VLMs (3 open source, 3 closed) and see how they perform.\n\nYou can check out the full study here - [https://nanonets.com/blog/vision-language-model-vlm-for-data-extraction/](https://nanonets.com/blog/vision-language-model-vlm-for-data-extraction/)  \nand the source code here - [github.com/nanonets/hands-on-vision-language-models/](http://github.com/nanonets/hands-on-vision-language-models/)\n\nModels tested : Qwen2, MiniCPM, Bunny, GPT-4o mini, Gemini 1.5 flash,  Claude 3.5  \nI have chosen these models because they fit on a consumer GPU (less than 24GB VRAM)\n\nDatasets used: SROIE and CORD receipt datasets - i’ve explained why these in the blog, but tldr is that they are \n\n1. standardized (and hence simpler to benchmark)\n2. complex enough to present a challenge to all the VLMs\n3. no MLLM benchmarks focussing on extracting structured data from documents.\n4. Both fields and tables are present in the datasets\n\n Takeaways \n\n* QWEN is the superior OS model, and Claude is the best among closed source. However these were also the most expensive among their subsets. \n* Both model types are closely matched in SROIE which is a simpler dataset, but closed source CLEARLY outperforms on CORD which is more complex.\n* My personal opinion is that high accuracy, high cost is more likely to benefit customers vs low accuracy, low cost - in the latter case cost of error handling is likely to exceed any $ savings\n* Also discovered that google has a free api for upto some large number of API calls per day which is so cool!\n\nhttps://preview.redd.it/5lt0xpsjfjsd1.png?width=1500&format=png&auto=webp&s=0f61c6942cb3e76f7c32cc8fa5e7e7955e57184c\n\nPS: I could have used VLMEvalKit but there were no benchmarks exclusively for structured document data extraction. Closest benchmarks I found were DocVQA and OCRBench but they had like one question per image. What I’m looking for is to extract something like a json of the entire image with one prompt. Let me know if you know any such datasets and I’ll benchmark on them as well and update the blog in my freetime.\n\nPPS: got feedback from the community on my earlier posts that they seemed like ads because I talked about my company, so I’ve been careful not to do that this time. Feedback is much appreciated!",
    "created_utc": "2024-10-03T05:49:44",
    "num_comments": 0,
    "comments": []
},
{
    "submission_id": "1fv5xcv",
    "title": "“Foundations of Computer Vision by Torrabla, Isola & Freeman” useful? ",
    "selftext": "I’m not new to CV and know basics of neural nets and transformers and from Andrew Ng courses, I have theoretical understanding of CNNs. \n\nThe book is really interesting, has wide variety of topics from basics of neural nets to generative models, but my dilemma comes from topics such as geometrical representation of images, Hemographies, Multiview geometry, camera modeling, learning depth in the images, understanding motion…\n\nI don’t have much experience in current CV industry, so I’m not sure if these topics are useful for the current industry (I’m preparing for job interviews, that’s why). \n\nNot shying away from learning basics or reading theory, just don’t want to jump into a rabbit hole at this moment. ",
    "created_utc": "2024-10-03T04:56:38",
    "num_comments": 1,
    "comments": [
        "For some positions it's crucial and for others you'll need ML only"
    ]
},
{
    "submission_id": "1fv53iz",
    "title": "I Just Developed an MRI Brain Tumor Detection App! 🧠",
    "selftext": "",
    "created_utc": "2024-10-03T04:06:55",
    "num_comments": 34,
    "comments": [
        "That title was a short rollercoaster.",
        "love it, but why a phone app?",
        "Yes this is equal to the titanic dataset.",
        "That is definitely not a meningioma.",
        "This is dumb.",
        "there is a huge amount of work in this direction, if you want to read more about it was generally called \"radiology clinical decision support\" and \"radiology reporting support\" or at least thats what it was called when I was working on it at philips in 2012",
        "Thanks for the input! I'm actually planning to take this project further by integrating a (complete diagnosis report) feature. This would include not just tumor detection, but also detailed analysis and insights based on the scan, potentially providing doctors with a more comprehensive tool. I’m also thinking about how to launch this app more widely. Does anyone have ideas on the best platforms or regions to target first? Should I focus on specialized healthcare app stores, or partner with hospitals and clinics directly? Would love to hear your thoughts on where and how this could make the most impact globally!",
        "But what’s the interest of your application?\nMedical image is rarely present on phone and above all medics doesn’t storage sensitive data\nBut good project it’s impressive and interesting :)",
        "Cool project. Not to discourage you but this is probably not a one-person job unless it’s only for 3rd world type countries where there are few medical regulations.\n\nIn other words, hospitals with MRI machines aren’t going to buy or even allow an app developed by someone like you to be used by their doctors and patients. They’re going to spend $500,000 per year on a software application license  from some big medical technology company that has a dozen PhD’s and massive amounts of private datasets. \n\nImagine the lawsuits if someone gets a wrong diagnosis and it turns out the doctor used some random app they found online. ",
        "Where did you get the data for this?\n\nI've been dying to do something on a different illness similar but I've not had the time to source open source datasets...",
        "Guys i just make it in video explanation [https://youtu.be/BHacXkcQzVU](https://youtu.be/BHacXkcQzVU)",
        "You developed it using react native or flutter?",
        "Haha, I guess it was! Tried to keep it short and to the point, but it seems like it might have taken you on a little ride. Thanks for sticking with me through it! 😄",
        "thanks for your feedback.  \nim adding more features to it , like generating complete diagnosis reports and other medical services.\n\n its initial plan later we can move on web also.  \ni would like to talk more on it . lets get some more ideas",
        "Yeah, it's an ok project, but it's just an object detection running on mobile app (why even mobile? nobody uses phones for mri scans). Using a real-time video feed from phone's camera and running inference would make more sense and be more fun.",
        "Thanks for pointing that out! The app uses a machine learning/cv model that’s still being fine-tuned, so it might not always be 100% accurate at this stage. I’m constantly working on improving the model’s accuracy with more data and better training. If you have any suggestions or feedback, I’d love to hear them — I’m always looking to make improvements!",
        "I understand this project might not be for everyone, but it was a great learning experience for me and serves a specific purpose. If you have any suggestions for improvement, I’d be happy to hear them!",
        "thanks. i would like to listen to you,  \nlet's get in touch",
        "If you’re serious about this, you HAVE to get some clinical insights/partner with a clinician at least semi-officially.\n\n1. As mentioned in other comments, you’ll need them to ascertain proper ground truth (for model training, and evaluation).\n\n2. Imaging modality. Not all MRI data are equal, there are plenty of specifics that affect MRI image quality or usage. How sure are you at identifying the domain shifts/or that you’re using the right type of MRI protocol?\n\n3. MRI is often evaluated in for example a DICOM viewer, where a clinician makes their opinion based off multiple plane views, across multiple slices. What is the value of a single slice diagnostic app?\n\n4. Data security. Hospitals might have varying (usually higher) data security requirements. Are they allowed to even export images from their current ecosystem? Can you integrate your software effectively? \n\nThe above questions, and more, only a clinical expert can guide you through.",
        "thanks",
        "I completely agree that this kind of app isn’t meant to replace high-end, regulated medical software from established companies. The project was more of a proof of concept to demonstrate the potential for AI in medical diagnostics, and I’m aware that clinical-grade applications would require strict regulation, validation, and large-scale datasets. The goal was to create something accessible, especially for educational purposes or in low-resource settings where advanced tech might not be available. I appreciate your perspective — it’s always important to consider the larger regulatory and legal context!",
        "roboflow",
        "no.  \nits in kotlin and java.  for easy ai inetegration",
        "I like the energy, is it for a course you are doing all of this?\n\nBecause if it's to be useful for a user you have to think about how they will be using it.  \nNot too sure a lot of people have MRI scan on their phone.",
        "Thanks for your feedback! You’re right that running object detection on mobile isn’t typically the way MRI scans are handled in practice, but the goal here was to make an easy-to-use mobile interface for healthcare professionals to quickly analyze images in a portable format. While real-time video feeds would be interesting, in medical scenarios, static high-resolution MRI images are standard. I went with mobile for accessibility and on-the-go use, particularly in environments where a desktop or server setup might not be feasible. Definitely open to exploring real-time applications in the future though — great suggestion!",
        "Typically when you showcase an application, you show it working properly, not failing",
        "Thank you sir, my best wishes to you and all your future endeavors.",
        "It's more of a personal project to experiment with AI in healthcare diagnostics, not really intended for everyday users to upload MRI scans from their phones.",
        "If you think using Chatgpt to write your replies isn't obvious to people reading, think again.",
        "im running emutlater",
        "ah then great work keep it up! :)",
        "Haha, I promise it's all me! Just trying to keep things clear and concise. But hey, if ChatGPT can help, maybe I'm doing something right! 😄",
        "Do you think I am an idiot? Your comments are a mix between poorly punctuated mess to perfect journalist english with chatpgt-isms staring down my eyes.",
        "I aim to communicate clearly, but I understand it can come across differently"
    ]
},
{
    "submission_id": "1fv1et6",
    "title": "Messy Alternative ",
    "selftext": "Suggest me some opensource project that matches the meshy-4 (meshy.ai) quality for image to 3d task. ",
    "created_utc": "2024-10-02T23:24:22",
    "num_comments": 2,
    "comments": [
        "That's not computer vision, but still here's the actual answer :\n\n[https://github.com/MrForExample/ComfyUI-3D-Pack](https://github.com/MrForExample/ComfyUI-3D-Pack) offers multiple solutions, try what's best for you. For me instantmesh and era3d were the easiest to set up but that may have changed.\n\n[https://www.reddit.com/r/comfyui/comments/1ehyj23/comfyui\\_now\\_support\\_stable\\_fast\\_3d/](https://www.reddit.com/r/comfyui/comments/1ehyj23/comfyui_now_support_stable_fast_3d/)",
        "its not pure computer vision but understanding an image is computer vision and generating 3d model is computer graphics. comfyui-3d-pack has a lot of models like instant mesh, unique 3d, etc. I have tried them already but they aren't good enough. \n\nstable fast 3d aint good as well. I have clearly asked for an open source model that matches the level of meshy.ai"
    ]
},
{
    "submission_id": "1fv0ns5",
    "title": "Benchmarking vision language models for document data extraction",
    "selftext": "Hi all, I’m a deep learning engineer working on vision language models exclusively for document data extraction, so I decided to test 6 VLMs (3 open source, 3 closed) and see how they perform.\n\nYou can check out the full study here - [https://nanonets.com/blog/vision-language-model-vlm-for-data-extraction/](https://nanonets.com/blog/vision-language-model-vlm-for-data-extraction/)  \nand the source code here - [github.com/nanonets/hands-on-vision-language-models/](http://github.com/nanonets/hands-on-vision-language-models/)\n\nModels tested : Qwen2, MiniCPM, Bunny, GPT-4o mini, Gemini 1.5 flash,  Claude 3.5  \nI have chosen these models because they fit on a consumer GPU (less than 24GB VRAM)\n\nDatasets used: SROIE and CORD receipt datasets - i’ve explained why these in the blog, but tldr is that they are \n\n1. standardized (and hence simpler to benchmark)\n2. complex enough to present a challenge to all the VLMs\n3. no MLLM benchmarks focussing on extracting structured data from documents.\n4. Both fields and tables are present in the datasets\n\nhttps://preview.redd.it/mphpcn8d9hsd1.png?width=1500&format=png&auto=webp&s=8034e06085b285d97ec8b63117cf81e59494c50f\n\n Takeaways \n\n* QWEN is the superior OS model, and Claude is the best among closed source. However these were also the most expensive among their subsets. \n* Both model types are closely matched in SROIE which is a simpler dataset, but closed source CLEARLY outperforms on CORD which is more complex.\n* My personal opinion is that high accuracy, high cost is more likely to benefit customers vs low accuracy, low cost - in the latter case cost of error handling is likely to exceed any $ savings\n* Also discovered that google has a free api for upto some large number of API calls per day which is so cool!\n\nPS: I could have used VLMEvalKit but there were no benchmarks exclusively for structured document data extraction. Closest benchmarks I found were DocVQA and OCRBench but they had like one question per image. What I’m looking for is to extract something like a json of the entire image with one prompt. Let me know if you know any such datasets and I’ll benchmark on them as well and update the blog in my freetime.\n\nPPS: got feedback from the community on my earlier posts that they seemed like ads because I talked about my company, so I’ve been careful not to do that this time. Feedback is much appreciated!",
    "created_utc": "2024-10-02T22:31:16",
    "num_comments": 1,
    "comments": [
        "ad!"
    ]
},
{
    "submission_id": "1fuqxws",
    "title": "Question about Processing stack of Radar Images",
    "selftext": "Hi, I'm fairly new to computer vision, and I'm currently working on a problem of Synthetic Radar (SAR) images from satellite Sentinel S1 imagery data. I was trying to reproduce some SoTA work and I'm somewhat confused about the pre processing steps on this following workflow from this paper I was reading: \"Detection of Temporary Flooded Vegetation Using Sentinel-1 Time Series Data\".\n\nhttps://preview.redd.it/d9ar981mlesd1.png?width=826&format=png&auto=webp&s=b8dd7f9f92cb3d6fad44724d9fe7910bb43f5814\n\nI dont understand quite understand how does the workflow function, particularly goal of the clustering and thresholding. \n\nConsidering I have 100 images (350x350) with two polarizations (two stack as show above), then I have the two following numpy arrays sized (100x350x350). The pixel-based approach is quite straightforward as I just apply the Zscore through time (axis=0) for the two time-series stack. However, I'm quite confused about the point of Multi-temporal and spatial clustering. The authors use Kmeans with k=10 and k=5 for temporal and spatial clustering, respectively. I dont quite get the point of using both types of clustering, or how to use it in the next step.\n\nFinally, I don't quite understand how the hierarchical thresholding is done using random forests. I have a labelled reference image, but I dont get what data should I make predictions on to predict to predict one of the four classes.\n\nThanks in advance!\n\n",
    "created_utc": "2024-10-02T14:00:21",
    "num_comments": 1,
    "comments": [
        "So after a quick skim of the paper I think I've got the gist of it. They're not totally clear which doesn't help. \n\n\nThe multi temporal and spatial clustering is to differentiate between what's always water and what's sometimes water - you couldn't do this with spatial alone, and temporal alone might struggle to distinguish water from other temporal features (e.g. shadows).\n\n\nThe hierarchical classification simply means that they perform sequential classification rather than a single step - e.g. do class a if x, then class b if y, then class c if z. They don't appear to be using random forests for this - it looks like they use a RF to assess feature importance, then use a decision tree to identify classification thresholds for the most important features, and use those in the hierarchical classification process. It's essentially a manual decision tree, and it would have been useful of them to illustrate it.\n\n\nIt's not clear from my skim exactly how they combine the pixel based and object based legs; I see they present them separately, then as a combination. Presumably it happens somewhere in the hierarchical step, but again they aren't explicit on what they do here \n\n\nHope that helps - if you're new to CV then starting in SAR will be difficult, and remote sensing is often different to other CV applications due to historic differences in the research groups (anything with a geospatial component tends to be very different in my experience). I'd recommend starting with RGB and ground/air based to get some of the basic concepts if you want to focus on CV."
    ]
},
{
    "submission_id": "1fupw65",
    "title": "Useful receipt readers in Python?",
    "selftext": "Hello , I have been working with tesseract in Python to try to form a catch all receipt reader , for things like hotel receipts , rental car receipts , taxi receipts , and pretty much all kinds of different receipts, so I can consistently and accurately read them and pass them to Python . Is there a product I can install locally on my PC that has already solved this problem ?",
    "created_utc": "2024-10-02T13:15:58",
    "num_comments": 2,
    "comments": [
        "I've used [PyMuPDF](https://pymupdf.readthedocs.io/en/latest/) to parse column data in PDFs with good success.  It might be worth giving it a try on a few of your receipts, to see if it can make sense of them.",
        "https://huggingface.co/docs/transformers/model_doc/trocr\n\nTROCR was trained on receipts and does well but requires single lines at a time.  You can also try EasyOCR, which runs fast."
    ]
},
{
    "submission_id": "1fuoeiq",
    "title": "What is the best way to detect events in a football game.",
    "selftext": "Was wondering if I wanted to detect the number of tackles, shot, corners, free kick per game, what's the best models and datasets to use. Should I go for a video classification model or an image classification model.\n\nIdeally I want my input to be a 10 min long video of a football sequence and from the sequence, classify/count the occurence of each event.\n\nAny help or guidance for this would be greatly appreciated. ",
    "created_utc": "2024-10-02T12:12:33",
    "num_comments": 8,
    "comments": [
        "RemindMe! 1 day",
        "You could detect poses of each player, apply some initial filters like distance between players or team label and finally threat poses of 2 players from last n frames as I put to your model, probably some kind of RNN. Trigger word problem could be similar.",
        "The proper way to do it is using  2d trajectory football data. That's all you need.  I have done it. \n\n\nif you want to start from the video, map the video to 2d trajectory data.I have seen some projects on this sub doing it with one camera. ( It's difficult) \n\n\nThe pose estimation is mainly needed for the offside detection, otherwise you don't need it.",
        "Not entirely joking, voice > text > llm on audio (presenter's comments) might be feasible.",
        "\n>Was wondering if I wanted to detect the number of tackles, shot, corners, free kick per game, what's the best models and datasets to use. Should I go for a video classification model or an image classification model.\n\nNeither. This isn't a classification task. It's a programming task, where you have models to help you along. You will have to build a system. You can have models for player detection, pose estimation etc. But the models are not going to detect the relevant statistics for you. That's something you program.",
        "I will be messaging you in 1 day on [**2024-10-03 19:31:19 UTC**](http://www.wolframalpha.com/input/?i=2024-10-03%2019:31:19%20UTC%20To%20Local%20Time) to remind you of [**this link**](https://www.reddit.com/r/computervision/comments/1fuoeiq/what_is_the_best_way_to_detect_events_in_a/lq0witx/?context=3)\n\n[**2 OTHERS CLICKED THIS LINK**](https://www.reddit.com/message/compose/?to=RemindMeBot&subject=Reminder&message=%5Bhttps%3A%2F%2Fwww.reddit.com%2Fr%2Fcomputervision%2Fcomments%2F1fuoeiq%2Fwhat_is_the_best_way_to_detect_events_in_a%2Flq0witx%2F%5D%0A%0ARemindMe%21%202024-10-03%2019%3A31%3A19%20UTC) to send a PM to also be reminded and to reduce spam.\n\n^(Parent commenter can ) [^(delete this message to hide from others.)](https://www.reddit.com/message/compose/?to=RemindMeBot&subject=Delete%20Comment&message=Delete%21%201fuoeiq)\n\n*****\n\n|[^(Info)](https://www.reddit.com/r/RemindMeBot/comments/e1bko7/remindmebot_info_v21/)|[^(Custom)](https://www.reddit.com/message/compose/?to=RemindMeBot&subject=Reminder&message=%5BLink%20or%20message%20inside%20square%20brackets%5D%0A%0ARemindMe%21%20Time%20period%20here)|[^(Your Reminders)](https://www.reddit.com/message/compose/?to=RemindMeBot&subject=List%20Of%20Reminders&message=MyReminders%21)|[^(Feedback)](https://www.reddit.com/message/compose/?to=Watchful1&subject=RemindMeBot%20Feedback)|\n|-|-|-|-|"
    ]
},
{
    "submission_id": "1fun3hs",
    "title": "Seeking Guidance for a Raspberry Pi-Based Football Performance Analysis Project",
    "selftext": "I'm developing a football performance analysis project using a Raspberry Pi 4 and Pi Camera 3. The goal is to capture shooting practice sessions and provide real-time feedback on player performance using computer vision.\n\nThe project involves capturing video footage of players during practice, processing the video with OpenCV to detect the ball and analyze player movements. Key features will include tracking shooting accuracy, identifying successful and missed shots, and assessing player movements for technique improvement.\n\nSince I don't have a laptop with an external GPU, I plan to leverage cloud tools for any intensive processing tasks. I'm seeking guidance on best practices for integrating OpenCV with video analysis, efficient methods for data labeling, and tips for enhancing the system's accuracy and user feedback mechanisms. Any suggestions or resources related to this project would be greatly appreciated!",
    "created_utc": "2024-10-02T11:17:47",
    "num_comments": 0,
    "comments": []
},
{
    "submission_id": "1fulkkz",
    "title": "Need Help with 2D to 3D Facial Model Conversion",
    "selftext": "Hi everyone,\n\nI have no prior experience in 3D modeling and need to convert 2D facial images into a 3D model based on user facial image input. Could anyone suggest the best tools, libraries, or methods to achieve this?\nSo this is a dynamic automatic 2d to 3d model conversion and shouldn't be done manually. \n\nThanks in advance!",
    "created_utc": "2024-10-02T10:15:28",
    "num_comments": 5,
    "comments": [
        "What are the requirements? What will the models be used for?",
        "Customizability like changing face features and tts.",
        "What about accuracy and resolution?  Real-time or offline? Tolerable lag? What equipment might you have?",
        "Will try to mimic realistic details.   \nThe user's pics will be sent to the service (2d to 3d facial model) through api from android device. The service will generate and send the model back to user. The model will be rendered inside the app.\n\nEquipment/Resources:  \nrtx 4060 8gb. 32gb ram.",
        "Use photogrametry if possible, which needs more photos but can be more accurate. Monocular depth estimation as a fallback. Use the phone’s lidar or built in depth information if available. \n\nHere are some to get you started. https://github.com/awesome-photogrammetry/awesome-photogrammetry\n\nhttps://github.com/choyingw/Awesome-Monocular-Depth\n\nI also came across some repos specifically mentioning face 3d. Will leave it to you to find those by searching GitHub…there were at least a few. "
    ]
},
{
    "submission_id": "1fujbln",
    "title": "Is a Raspberry Pi 5 strong enough for Computer Vision tasks?",
    "selftext": "I want to recreate an autonomous vacuum cleaner that runs around your house. This time using depth estimation as a way to navigate your place. I want to get into the whole robotics space as I have a good background in CV but not much in anything else. Its a fun side project for myself.\n\nNow the question, I will train the model elsewhere but is the raspberry pi 5 strong enough to make real time inferences?",
    "created_utc": "2024-10-02T08:44:18",
    "num_comments": 32,
    "comments": [
        "Have you looked at the AI-kit for the raspberry PI 5?https://www.raspberrypi.com/products/ai-kit/",
        "I would get something with hardware acceleration. CPU inference on a raspberry pi is going to be slow. \nYou could get something like a Jetson Nano or probably some kind of pci addon for a raspberry pi with a GPU or AI accelerator.",
        "I have some experience working with rsp boards (version 4). Few things to keep in mind :\n\n1- All of them have both video cores and vector extensions that require you to build your project in C++.\n\n2- There are also some Python wrappers for these special hws, in that case, you need to build these tools on your board.\n\n3- I highly recommend you stick with C++ rather than Python because:  3.1- Arm processors are not as nearly strong as intel and x86 family, so wrapper overhead might be a burden (so don't trust the inference speed on those). 3.2- The level of parallelism in preprocessing and postprocessing achievable using multithreading is unavailable in Python due to GIL 3.3- and poor memory control in Python is definitely going to be an issue.\n\n4- Try model compressing if you are using DNN in any sort of way, it can help much.\n\n5- Since those devices' memory and bus architecture are rather simple, pay attention to how you are reading and writing data. This can affect performance a lot.\n\n6- You can use some libraries like MPI for message passing to use all available cores.\n\nYou can dm me whenever you need help. I would be happy to lend a hand.",
        "Jetson nano, I was getting up to 60fps inference using tensorRT… nvidia has a ton of guides and resources",
        "Have you thought about offloading the processing to the camera itself? Something like realsense or stereolabs zed camera.  \nOtherwise the Pi5 is a little weak for CV on its own. The AI-kit (linked in the other comment) is ok, but you have to fight with converting any models that aren't on their zoo - this is not fun...  \nSomething with a bit more power like the Radxa X4 might be a good middle ground. Or again as already mentioned the Jetson range.",
        "Depends! Are you considering running ai on device for monocular depth estimation - cause then I think you need something else than the compute available on the pi. But if you’re getting depth from   stereo cameras and doing something light like object detection you might be alright!",
        "I think it could work. \n\n Just so you know the monocular depth models are not as accurate as a hardware based depth sensor, a category that also includes stereo cameras. If you’re trying to generally understand the scene they’re good but if you want to know for example “I can move forward 9 inches before I hit the couch” prepare to be disappointed by monocular depth estimation. Especially if you’re running the “mini” version of those models on limited hardware.",
        "Check this guy: [https://www.youtube.com/@JeffGeerling](https://www.youtube.com/@JeffGeerling)\n\nhe did some work on getting AI co-processors like Hailo working\n\nalthough depth estimation with vision is difficult, that type of device tends to use Lidar or microwave radar",
        "MobileNetV2 + tensorflow lite runtime works fine, but it’s pretty constrained",
        "You might want to use a USB-connected accelerator. The Ultralytics YOLOv11 model is by far the best computer vision model out there. I believe they support depth estimation tasks.\n\n[https://docs.ultralytics.com/guides/ros-quickstart/?h=depth#depth-step-by-step-usage](https://docs.ultralytics.com/guides/ros-quickstart/?h=depth#depth-step-by-step-usage)\n\n[https://docs.ultralytics.com/guides/coral-edge-tpu-on-raspberry-pi/](https://docs.ultralytics.com/guides/coral-edge-tpu-on-raspberry-pi/)",
        "I built a simple dog camera to see if our dog gets out a few frames behind but not awful with a simple yolo8 tracker. Forgot mention https://www.amazon.com/Libre-Computer-AML-S905X-CC-Potato-64-bit/dp/B074P6BNGZ/ref=asc_df_B074P6BNGZ/?tag=hyprod-20&linkCode=df0&hvadid=693308325727&hvpos=&hvnetw=g&hvrand=10272951388999482131&hvpone=&hvptwo=&hvqmt=&hvdev=m&hvdvcmdl=&hvlocint=&hvlocphy=9009689&hvtargid=pla-593018028874&psc=1&mcid=53c799494fac312299df2c36b2d506b4 is the board I used.",
        "Only as a toy, just get a jetson so you enjoy your project",
        "I’d go on Raspberry Pi + Hailo (or the Sony camera, which was just announced, but maybe frees the HAT for you)\n\nNot just would - that’s what I’m doing.\n\nI also have Google Coral on stick and an old Jetson, but they are old, stuck on Python 3.7 and not fun to work with, to say the least.",
        "If your are planning on using object detection algortihms I can recommend nanodet.You have to export your model to ncnn and then use it via ncnn api. I got 60+ fps with base model. You do not need ai accelerator. I am not sure about depth estimation though, because usually those models require strong backbones.",
        "Yeah, there is special camera also announced for interference: https://www.raspberrypi.com/products/ai-camera/\n\nMaybe someone knows, if these components complement each other to boost overall system speed",
        "The first one coming to my mind was Google Coral 😋",
        "Can’t you do C++ or C for that then the low processing in Python?",
        "At what resolution?",
        "Yolo v6 is still superior in mAP. v11 has mAP of \\~55. yolo v6 is 57.2.",
        "In most cases, I would say they don't complement each other, but solve tasks of different complexity. \n\nThe Sony chip is weaker and has less memory, but has the advantage of being directly on the same silicon as the camera, allowing very low latency at very low price. \nThe AI hat is more powerful and hence can solve more complex tasks (deeper networks, etc.)\n\nBut I'd say there are some cases, they can complement each other, by building a cascade. Where you wait for some detection by the camera or set a low confidence threshold on the detections of the camera and then only look at the filtered frames / objects with a more complex network to either get a better overall accuracy or solve additional tasks.",
        "They wouldn’t supplement, but work separately.  \nI have Hailo and waiting for the camera to arrive (a month? 2?)\n\nIf Hailo could work on a stream on 3-5 models with relatively imperceptible speed, the camera would be redundant, in my opinion.\n\nIf not, they could very well handle different tasks working in tandem.",
        "Coral is stuck on Python 3.7, bare in mind.  \nThe Jetson can definitely do computer vision.",
        "I listed 6 items, which one are you referring to?",
        "It’s been a while since I worked on that project but it was def more than 512x512",
        "But that's at imgsz 1280 (vs. 640 in YOLO11). You could do the same with YOLO11 if you don't care about speed. The YOLOv6L6 at 1280x1280 consumes 673.4 GFLOPs over 3 times more than YOLO11x at 640x640 (194.9 GFLOPs).\n\nIt's not an apple to apple comparison.",
        "All of them - any “heavy lifting” is better done with C/C++.  \nBut any simple wrapping logic that uses it - Python is fair game.  \n\nThat’s why loading an LLM is fair game in Python because all heavy lifting is actually C, and the conversation loop is really low weight",
        "You are right, I didn't take that in mind. 1 or 2 % more or less doesn't matter that much.",
        "Yes but that's not how it works in production. Production is totally a different game.",
        "I believe you.  \n\nMy field is innovation - get things done fast, working reliably as possible, suggest improvements for the handling team.  \n\nA low effort, smartly implemented POC, before any high grade production scale.",
        "Well in that case stick with python to Create prototypes.",
        "Thank you, will do :)  \n\nBut it’s always good to learn from pros in their domain, so thank you for your insights!"
    ]
},
{
    "submission_id": "1fuhmzy",
    "title": "Table cells extraction",
    "selftext": "Hey everyone,\n\nI'm working on a project where I need to crop specific sections (or cells) from a document image, similar to the attached image.\n\n[https://ibb.co/4Wc3889](https://ibb.co/4Wc3889)\n\nAny insights or recommendations for models I can try out on Hugging Face would be awesome!\n\nThanks in advance for any help!",
    "created_utc": "2024-10-02T07:33:43",
    "num_comments": 2,
    "comments": [
        "You can try this one https://huggingface.co/spaces/Keemoz0/my-table-transformer-structure-recognition . Btw, I would annonymize personal data while uploading sensitive documents in social media."
    ]
},
{
    "submission_id": "1fuendj",
    "title": "OpenCV On Web now supports OpenCV's object detection module!",
    "selftext": "",
    "created_utc": "2024-10-02T05:10:59",
    "num_comments": 3,
    "comments": [
        " What would you like to use OpenCV On Web for? What features would you need?\n\nFace detection demo ➡️ [https://opencv.onweb.dev/#/project/fb60a950-316a-5a3d-ba86-005ecd2044f6](https://opencv.onweb.dev/#/project/fb60a950-316a-5a3d-ba86-005ecd2044f6)  \nCanny edge detection demo ➡️ [https://opencv.onweb.dev/#/project/2b77cd94-b731-539d-988c-b8e49d8800b6](https://opencv.onweb.dev/#/project/2b77cd94-b731-539d-988c-b8e49d8800b6)",
        "This is not a knock on OpenCV, but why would you use it for object detection when there are already so many capable options? \n\nHonestly asking because I do like the idea of a single package that does anything and everything vision related!",
        "I tend to agree. Unless you have extremely limited resources, I don't know why you wouldn't use something like Yolox instead."
    ]
},
{
    "submission_id": "1fuehj4",
    "title": "What's the best way to extract features of a video? Is it better to use I3D or something else?",
    "selftext": "This is for something like video captioning on the charades dataset. Is there any pre-trained model that's better than others?",
    "created_utc": "2024-10-02T05:01:49",
    "num_comments": 0,
    "comments": []
},
{
    "submission_id": "1fubymz",
    "title": "Trying to draw boundary boxes on objects in an image/ video",
    "selftext": "I want to draw boundary boxes around humans and dogs on a image, and then move on to drawing boundary boxes on videos. \n\nThe approach I used was contour detection to detect objects, HOG to extract features, SVM for classification and drawing the boxes. \nIt seems absolutely bad since the boxes were here n there, and just overall not accurate. \n\nThe potential issue might be coming from using contour detection where it either captures too much details or too less and just not good for this type of object detection. \n\nI only can use non neural methods. \n\nAre there any better approaches that I couldnt implement that would provide some promising results?",
    "created_utc": "2024-10-02T02:16:47",
    "num_comments": 4,
    "comments": [
        "why the non-neural network limitation? is it homework or some other constraint about processing power?",
        "Can you grab a background image with no humans or dogs? If so you can use background subtraction to better locate the humans and dogs.",
        "Its a limit that they set, yes it is something similar to homework."
    ]
},
{
    "submission_id": "1fuaryc",
    "title": "Resnet101 for Counterfeit-Nike-shoes. Not sure if it will work",
    "selftext": "There is an object detection dataset available on roboflow - https://universe.roboflow.com/default-kupxs/counterfeit-nike-shoes-detection\n\nI plan to separate the images into two classes, fake and authentic based on the annotations. Then I am thinking of utilising transfer learning using a resnet model. \n\nNow should I first crop out the bounding boxes from the object detection dataset and then use the cropped images for classification model or go ahead with the images as they are in the above link? \n\nMy main concern is the quality of the dataset. Can any experienced person check it out and let me know if the classification model will work? ",
    "created_utc": "2024-10-02T00:45:03",
    "num_comments": 2,
    "comments": [
        "Keep in mind that if you are cropping you’ll also need to detect each shoe in the first place. If you use object detection classes then it will simultaneously detect and classify each shoe. In theory that should work as well as a separate classifier. \n\nImpossible to say how well it will work without trying, but I assume if someone took the time to upload a dataset they were able to train a model that worked better than random chance. ",
        "Oh and it’s probably NOT going to generalize to other types of shoes. "
    ]
},
{
    "submission_id": "1fua2r5",
    "title": "Resume review",
    "selftext": "Hey guys! I had transitioned to computer vision after my undergraduate and has been working in vision for the past 2 years. I'm currently trying to change and hasn't been getting any calls back. I know this is not much as I havesn't been involved in any research papers as everyone else, but it's what I've been able to do during this time. I had recently joined a masters program and is engaged in that in most of my free time. And I don't really know how else I could improve it. Please guide me how I could do better in my career or to make my resume more impressive. Any help is appreciated! Thanks. ",
    "created_utc": "2024-10-01T23:51:06",
    "num_comments": 38,
    "comments": [
        "Solid resume. A few points:\n\n* since you're not a recent grad, I'd change the section ordering (descending by importance): experience, education, everything else\n* if you have at least 1 uni/college degree, high school is no longer relevant; drop it\n* relevant coursework shouldn't be a separate section, move it to education\n* in education, primary focus should be on the degree (bold, mentioned first), the institution is secondary (non-bold, mentioned after). Clarify the level of your degrees (bachelor, master, etc). Check the spelling (in my country usually these variants are used: \"master's in X\" or \"master of science in X\" or \"MSc in X\")\n* you mentioned freelance projects. If those projects were paid work, definitely mention them as work experience\n\nEDIT: regarding my first point, I didn't see that you're still studying. Maybe keeping education first is better",
        "is this the nVidia course ?\n\nhttps://learn.nvidia.com/courses/course-detail?course_id=course-v1:DLI+C-AC-01+V1",
        "Damn that's a solid resume. If you don't get any callback then we are screwed. That Cuda skill alone weighs more than 99% of resumes I've seen in past 3 months (I'm currently located in Iran, maybe that has something to do with it).\nYet I think there is always room for improvement. Maybe do some projects in cv and publish them on Github. I'm not any better than you in this except I have some freelance projects. Perhaps that can help you because you possess the skills, but there is nothing to prove it.",
        "All lists are ordered from recent to old, but the certificates are the other way around.",
        "This is an insanely solid resume. You should be proud!",
        "Very good resume. I would echo, having CUDA is a skill that is unfortunately quite rare to see from people soon to graduate, and would be enough for me to give an interview for most positions.\nThe only concern I would see is not showing (or having) experience in ROS or similar, based on your robotics experience, and it is the first key word i search for when I get resumes. But this is only relevant if you want to work in robotics.",
        "I would actually remove the certificate part besides the Nvidia one (IMO). Your experience already shows that you have solid knowledge",
        "Are you doing OMSCS or on campus?",
        "Peaks my curiosity ahem!?",
        "Add a link to your Github",
        "Georgia tech ke liye kitna CGPA or kitna GRE score chahiye ?",
        "Got it. Thanks for your advice. I'll get rid of the high school, clarify and highlight the degree and add projects to it.",
        "https://learn.nvidia.com/courses/course-detail?course_id=course-v1:DLI+S-AC-04+V1\n\nThis is the one that I did. One of my clients at my company was a member of the Nvidia Inception program and added me as an engineer under him and so I got this course for free. Otherwise it is $90.",
        "Oh, thank you. To  be honest, I wasn't expecting a positive response. I too have done a few freelance projects, but wasn't sure if I should add it to my resume.",
        "Hadn't noticed it, Nice catch. Thanks. \nI'm only going with 3 certificates after other comments, but will correct the order for the ones that I'm keeping. Your username checks out!",
        "Thanks. I was worried that something was wrong with my resume as I wasn't getting any calls from recruiters.",
        "OMSCS",
        "Thanks for the reply. I actually have it on top but masked it out while I masked out my other personal details.",
        "I don't speak hindi",
        "I would if i were you. That increases the chance of something catches the employee's eye. Like let's say I am hiring for medical image processing, I see nothing in that regard in your resume but maybe some of your projects are close to it.\n\nI don't know what's the current situation in US, but in Iran every single fking resume I see used pytorch, tensorflow on a some shitty toy dataset. Only a few have even any project near real life problems. That's the first filter. The second filter is if any of those project are related or close to the position we have available.\n\nAnd also they go a long way. For example having a project in a field means the applicant is familiar with required pre and post processing techniques which may be useful for our project or may indicate that he/she posses enough theoretical knowledge to attack the challenges we are facing.\n\nAnd again, this is only the second filter. You also have to pass the technical interview.",
        "Your resume looks good for the YoE you have. Other than the sorting issues that others have highlighted, just keep in mind that luck plays an important role in hiring…\n\nNowadays some companies post openings that don't plan to fullfill (or they do with internal candidates). The job market favors the patient, especially if you still have a job",
        "How proactively are you networking?\n\nMy strategy is to skip the recruiters. Use \"InMail\" to message engineering managers at companies you want to work at. Attach your resume, make the message short and to the point. Don't flat out say \"I want a job\". Instead, simply say you're interested in learning more about what they are up to at their company and ask if they're available next week for a short call, then give a few scheduling options.\n\nI've had a lot of success from this approach and landed quite a few interviews this way.",
        "Nice. I graduated from that back in 2020",
        "How much CGPA AND GRE score do you need for applying for the college",
        "\"in Iran every single fking resume I see used pytorch, tensorflow on a some shitty toy dataset.\" could you elaborate on this",
        "Thanks for the details. I'll add the freelance projects to the projects section. Since others have pointed out I could remove a few details that would provide enough space for this I guess.",
        "I have been networking to get engineers working in companies to refer me, but I rarely had any success there. Your approach seems more effective, I'll try that way. I hadn't taken the LinkedIn premium till now though, I'll see to that as well.",
        "Woah. Didn't see that coming. Since I experienced the rigor of the course, I respect you. What specialization did you go with? Also was it any easier to land jobs after completing the course?",
        "You don't need GRE. 3.0 cumulative CGPA in undergraduate, but they'll still accept if CGPA is lower if you can show your experience in programming.",
        "Like (read it in a fun tone) \"I developed a novel neural network to classify  numerical digits\". He used keras to stack some conv  layers and loaded mnist then called model.fit().",
        "There is nothing wrong with 2 pages resume.",
        "ML. When I started it was just something I wanted to do but now I feel like an MS is almost necessary for a lot of jobs so I'm very glad I did it. I came in already working in CV and did a bit of basic ML.",
        "lmaoooo",
        "I heard too much advice asking not to go for a 2 page resume. Even if I did, I could only fill half of the second page. Will that be alright?",
        "Cool! and true, most of the job descriptions for CV jobs mentions masters as a minimum requirement. I've just started it, a long way to go now.",
        "As far as I know 2 pages resumes are OK. In this case you can provide more details.",
        "Got it. I'll try it out and see how much details I can fill in the second page.",
        "^^^ This is the problem with doing more than one page, you end up adding fluff. If you need more than one page you're not condensing good enough. \n\nIf you want to add something I suggest taking something away"
    ]
},
{
    "submission_id": "1fua0k1",
    "title": "How feasible is doing real time CV over a network",
    "selftext": "I’m a computer science student doing my capstone project. We need to build a fully autonomous capable of navigating and aiming a turret at a target. The school gave us these nvidia jetson nanos to use for GPU accelerated computer vision processing. We were planning on using VSLAM for the navigation system and open CV for the targeting. I should clarify, all of us on this team have little to no experience in CV, hence why I’m here. \n\nHowever, these jetson nanos are, to put it bluntly, pieces of shit. They’re deprecated, unreliable pieces of hardware that seemingly can only run a heavily modified EOL version of Ubuntu. We already fried one board by doing absolutely nothing and we’ve spent 3 weeks just trying to get them to work. We’re ready to cut our losses. \n\nOur new idea is to just use a good old raspberry pi, probably a model 5 8GB. Our idea is to have the sensors feed all of their data into the raspberry pi, maybe do some light processing locally, send the video feeds and sensor data to a computer over a network. This computer will be responsible for processing all of the heavy stuff and sending the information back to the rpi for how it should move and such. My concern is that the added latency of the network will be too slow for doing real time navigation and targeting. Does anyone have any guesses as to how well this sort of system would perform if at all? For a system like this, what sort of latency should be acceptable? I feel like this is the kind of thing that comes with experience that I sorely lack lol. Thanks!\n\nEdit: quick napkin math: a half decent wireless AP should get us around a 5-15ms ping time. I can maybe even get that down more by hardwiring the “server”. If we’re doing 30hz data, that’s 50ms we get to process each frame. The 5-15ms isn’t insignificant, but that doesn’t feel like the end of the world. Worst comes to worst, I drop the data rate a bit. For reference, this is by no means something requiring some extreme amounts of precision or speed. We’re building “laser tag robots” (they’re not actually laser tag robots, we’re just mostly shooting stationary targets on walls)",
    "created_utc": "2024-10-01T23:46:28",
    "num_comments": 24,
    "comments": [
        "assuming identifying the target uses something like yolo， jetson nano would still be your best bet because of its gpu",
        "Not going to happen with PI over network, latency will be too big for any navigation.",
        "Ping is not how long it would take to send all your data it is how long it takes to send a ping packet. You still would have to account for how long it would take to transfer all the data you will be using.\n\nI think it depends on what you want to focus on. As an embedded person it’s a terrible idea, and if you’re interested in embedded applications it won’t  be a good look that you gave up on using a Jetson nano. Your complaints are just how the Nvidia modules work. The Jetton is old but it should still work fine.\n\nIf you just want to focus on the algorithm it’s probably fine for a capstone project. You will have  all kinds of latency issues if you use WiFi though",
        "if you already have a pre-trained model for Object detection, inferencing should be lightweight enough. But again, it is hard to suggest without knowing the specifics of the workload and saturation metrics. Pragmatic approach would be to exhaust the resources on the Pi first and augment the Pi with a GPU accelerator (May be Coral).",
        "Make your own tiny pc with small motherboard and cheap nvidia GPUs. \nOr do a lot of optimization, quantize the model, size image on your board then send to cloud (use mqtt protocol). There are more things to do…",
        "Welcome to the realisation club :)\n\nI tend to use various x86 mini-PCs these days - no ARM headaches and way more power than a Pi. Sure, they're generally not as outright as powerful as the Jetson devices but find the right model and optimisation and it's way easier.\n\nJust waiting for my Radxa X4 to arrive. Want more power - try something like [https://lattepanda.com/lattepanda-sigma](https://lattepanda.com/lattepanda-sigma)\n\nOn the network latency issue... it's possible, make sure to downsize the video steam to the model input size - no point sending more data than needed. And of course you need enough power to ensure fast enough encode and serving over something like RTSP.",
        "Can you use a USB webcam hooked up to your laptop for the camera and send wired or wireless position updates to the PI controlling the servo turret setup?\n\nOr forget the PI and just place the turret rig on top of the laptop with the lid closed. Use another laptop to ssh in there and start/stop commands.\n\nA lot faster to send a string of information than send an entire image frame wireless if you can avoid it.",
        "Depends on what you mean by “real time”. Almost nothing can truly be done in real time, not even on device ops, especially if doing ML. When you add in network latency it gets even worse, as a web call can take at least 150-200 ms.\n\nMost people that do video processing establish a pseudo “real time” in which things can process in. For example maybe you only process every 3rd frame instead of every frame. That gives you the time to process and still be perceived as real time to anyone watching the feed.\n\nFor best speed, processing/inference should be done on device as slow/missing internet connection or network latency will hurt more than they help. There are several options. The Jetson series of devices offer on board GPU’s which can supercharge processing, but they have gotten really expensive over the years, like starting at $500. Another option (if problem is small enough) is to use a Raspberry Pi with an AI accelerator such as a “Neural Compute Stick” to actually do the inference. The NCS is really nice since it is an ASIC purpose built for this work, works well with video processing on edge devices, and only takes 1W of power so won’t cause battery or charging issues.",
        "also, What's the autonomous platform?",
        "The problem is, it doesn’t work. Every single day it’s a new issue. First no video, then it wouldn’t boot from an SD card, then no video again, then weird data corruption, then the whole fucking board died and we got a new one. Then we discovered it’s running Ubuntu 18.04 which is EOL and it’s an absolute pain getting any software to install. But that’s not a problem anymore because we can’t download anything! Our network card broke",
        "Budget concerns unfortunately. Our prof basically blew it all on these non-functional boards",
        "Wdym?",
        "that's jetson nano for you, as a matter of fact, you could probably get the orin with those 2 units, but software would still be shit to deal with.",
        "Embedded is hard, although if you are getting a bunch of weird issues like that and failing hardware, I would double check that your power supply is good quality and is delivering enough current.",
        "Do you need real time ? Rover on mars works with huge lag :)",
        "you are mounting the turrent on some kind of vehicle right?",
        "Probably not really lol. I mean it would be nice, but some lag would be fairly acceptable. No more than maybe .25s",
        "Oh ya, sry overthought ur question. Yes it’s most likely going to be 3D printed since half the parts they gave us are rusted together",
        "I would suggest that you guys try to get yolo or whatever you'll be using for objects recognition working on the nano first.\n\nuse the pi5 for slam, cpu performance on the nano is worse than pi4 btw.",
        "I’ve never used vslam in my life I just know it exists, hell I’m not even on the software team, I’m on hardware. Would vslam run better on CUDA or is it CPU only?",
        "Also using both boards is something I’ve considered. Use the raspberry pi for basically everything except GPU processing which just gets offloaded locally to the jetson",
        "does the turrent do anything? or just follow targets?",
        "Points a laser at fixed and possibly moving targets",
        "if all else fail, there's this ai extension board for pi5, haven't tried it but it's on YouTube. seems to work fairly well.\nhttps://youtu.be/olaSVKmt9YI"
    ]
},
{
    "submission_id": "1fu91n4",
    "title": "How to detect and crop particular regions from picture?",
    "selftext": "Hi, I have an image with vertical contour lines drawn on it. The contour lines are basically drawn along the boundaries where there is a transition between white and black colors.\n\nI want to identify the areas where the contour lines are closer together than in the rest of the regions and place red boxes around these areas.\n\nBelow is an example. This is the original picture\n\nhttps://preview.redd.it/c7uqgsk25asd1.jpg?width=345&format=pjpg&auto=webp&s=71109d5fcd2d5d0771e94dda652e364130e20215\n\nThe script detects the area under the red box has contour lines that are closer together.\n\nhttps://preview.redd.it/xzlzobu35asd1.jpg?width=345&format=pjpg&auto=webp&s=db54dd8b89229a52d262159a023762e41738c63f",
    "created_utc": "2024-10-01T22:36:15",
    "num_comments": 1,
    "comments": [
        "Looks like you already have the computer vision part down, and just need to interpret the results now. \n\nIterate over both contours together in a top-down manner and calc the distance at each “level”. Store that in a list and analyze the list to find what you would consider to be the “closer together” portion. "
    ]
},
{
    "submission_id": "1fu682g",
    "title": "What camera will I need for real-time tracking of the human body?",
    "selftext": "Very new to this area. I have a project that involves tracking the gestures and body movements. I was wondering if I needed a specific type of camera or will a regular webcam suffice. The devices recommended to me were Intel Real Sense and Kinect cameras, however, these are very costly. Any help appreciated.",
    "created_utc": "2024-10-01T19:46:59",
    "num_comments": 3,
    "comments": [
        "It depends on your needs. A regular webcam will probably be fine unless the project needs extremely high accuracy analysis on small, fast-moving stuff. In that case you’ll probably need a really expensive camera.\n\nEdit: for 3d stuff you’ll want 2 (ideally identical) regular webcams for stereo vision. Otherwise the 3d cameras you mentioned are good options",
        "imx219",
        "There's a new raspberry pi AI camera with human body posture recognition. It works only with pi zeros and pi 5"
    ]
},
{
    "submission_id": "1fu5qyz",
    "title": "What groundbreaking computer vision use cases could emerge in the next few years?",
    "selftext": "In the last few years, the cost of AI-capable hardware has dropped dramatically, and computer vision models have become both cheaper and more powerful. This trend looks set to continue.\n\nWith these advancements, what exciting new computer vision applications do you think we'll see soon?  \n   \nWhether it's in healthcare, retail, transportation, the environment, or something entirely new, I'd love to hear your thoughts on the most promising possibilities. Any specific real-world problems or industries you think could be transformed by this tech?",
    "created_utc": "2024-10-01T19:21:56",
    "num_comments": 22,
    "comments": [
        "Mass unwarranted surveillance, turning pedantic aspects of an otherwise casual existence into a performance sport at the request of a handful of insurance companies and enforcement arms. \n\nFlappy bird 2 in augmented reality, maybe",
        "Bird detection",
        "Gaussian splats will be used to model more and more of reality. They will be augmented to include connectivity and physical parameters.\n\n\nThey've already done it for clothing and hair.\n\n\nGaussians as low level primitives will then be import to reasoning world models.",
        "Event based sensors. They can transmit at blinding speeds since it only sends data changes on the scene.",
        "Fire and forget military drones.\n\nHead East, if you don't see a red armband, fly home to recharge.",
        "a Queryable earth",
        "Sensors.  The  ability to take shitty sensors and turn them into extremely expensive ones. Or cameras and turn them into other kinds of sensors",
        "Perfect inspection. We're almost there. I've been doing a bunch of experiments on few-shot learning with multimodal LLMs and it's almost good enough to inspect \\*anything\\* accurately without needing to train models besides giving it a few examples. The examples can even be text descriptions in cases where you don't have data. Wild.",
        "Security camera keeps track of my car keys.",
        "I think there’s too much money involved with the idea of groundbreaking for it to be shared by people who are in this subreddit. It’s too close to home and too close to competition. I have some ideas but I’m too afraid to share hi tree fear of people with more experience getting to it first",
        "There are 2 kinds of people.",
        "birds are not real",
        "They are also able to deal with some scenes where conventional cameras utterly fail. For example, [https://www.youtube.com/watch?v=cIrf4mpPXOw](https://www.youtube.com/watch?v=cIrf4mpPXOw)\n\nEdit: Forgot to point out that data speed relative to changes in the scene works as a reasonable assumption for certain conditions. If it is mounted on a platform that is moving (as our eyes are), this assumption is broken because the scene shifts in the sensor due to motion of the sensor. That said, this kind of camera fixed in place and pointed at a a scene makes moving object detection trivial (find the bright blob in the scene vs large ML model trying to comprehend frames.)\n\nHere's another fun example with a drone doing onboard processing to avoid a soccer ball the grad student throws at it : [https://www.youtube.com/watch?v=sbJAi6SXOQw](https://www.youtube.com/watch?v=sbJAi6SXOQw)",
        "That’s not a use case",
        "Can you provide a specific example. Sounds super interesting, but also too good to be true",
        "Tell me more about this please, sounds interesting.",
        "Well ideas are worthless. Execution is what matters.",
        "It’s a spy drones for sure!",
        "Insanely cool. Can't wait to see how people use these sensors next",
        "100%\n\nEven if ideas are shared almost no one will act on them",
        "someone already came up with Flappy bird 2 in augmented reality, so you are good OP"
    ]
},
{
    "submission_id": "1ftxbj8",
    "title": "Tips for improving the accuracy of reverse image search? My friend and I built AI glasses that reveal anyone's personal details—home address, name, social security #",
    "selftext": "",
    "created_utc": "2024-10-01T12:51:51",
    "num_comments": 17,
    "comments": [
        "Well luckily you seem to think that reverse image search can work to recognize people from a picture of their face, so your shitty creep glasses project won't go anywhere",
        "Ah ok, cool. Super ethical way to show off your hack there",
        "[deleted]",
        "Get ready for legal trouble.",
        "At the end you mentioned the description containing explaination for the technology. Can you please share that?",
        "Sounds cool by where is the hakster page or GitHub?",
        "This is cool, I can help.",
        "Omg great idea!! Haters gonna hate. I bet there are some more cool applications!!",
        "what do you mean? This project is not intended to go anywhere",
        "I mean nothing he did is impossible to replicate with a smartphone. You can take creepshots and use google reverse search and then do some looking up to achieve similar result as this douche.",
        "Yes!",
        "A 5 second glance at your post history makes it really obvious that this is an ad. And just from the claims you've made about it, it's just as obvious that you don't understand anything about what you're talking about",
        "Please DM",
        "Dude was spamming his projects everywhere. And this sub he was like uwu, it is illegal, just a hobby project here. Nothing to be concerned, at all."
    ]
},
{
    "submission_id": "1ftw9sr",
    "title": "2D human pose estimation APIs/Frameworks",
    "selftext": "I work on a project for uni (so noncommercial) and looking to integrate 2D pose estimation. The goal is to do pose estimation on synchronized frames (2-n different angles) and then, after getting the keypoints, triangulate 3D points.\n\nI stumbled across the common models like open pose, media pipe and YOLO and also checked out papers with code. I can't really see through what is best for my scenario. It seems to me, most are \"just\" the models and not really a library I could intertwine with my application (I mean I still could load it with OpenCV dnn etc. - but this seems a lot of work for my time constraint.  \nPreferably, I'm looking for a c++ solution, but python should also be fine - probably have to write my own bindings then.\n\nIs it open pose or media pipe - or what would you guys recommend to use?",
    "created_utc": "2024-10-01T12:08:48",
    "num_comments": 0,
    "comments": []
},
{
    "submission_id": "1ftvr0l",
    "title": "I need some cool projects suggestions ",
    "selftext": "Used to work with YOLO and UNets in the past, but then got diverted towards NLP, LLM and all. It’s been few years now that I’ve worked on any actual CV project. So I need some suggestions. \n\nHeres what I’m looking for: \n1. I don’t want to work on “API” ie just get some big model and apply it on data. Want to build something from my hands (to get that feeling)\n2. I’ve worked on basic projects/datasets before which I don’t want to repeat: YOLO object detection for cars, UNet for medical image segmentation (3D). \n3. Some work on SAM. I’m good with linear algebra, and comfortable with OpenCV. \n4. Not a total beginner, been working in industry for few years now, and have some research experience. \n5. This might be just hobby project so I don’t expect to gain any real world use out of it. Learning is more important for me at this stage. :) ",
    "created_utc": "2024-10-01T11:48:25",
    "num_comments": 6,
    "comments": [
        "Something involving low frame-rate tracking from video thst has instal measurements attached would be useful and challenging. Basically robotics stuff. ",
        "I have a project. Dm me. Let's talk",
        "Long range (90-120m) pose estimation",
        "Say a 'Hi in my inbox, I've got a project suggestion.",
        "Long range stereo matching.",
        "I am interested too"
    ]
},
{
    "submission_id": "1fttcgg",
    "title": "25 new Ultralytics YOLO11 models released!",
    "selftext": "We are thrilled to announce the official launch of **YOLO11**, bringing unparalleled advancements in real-time object detection, segmentation, pose estimation, and classification. Building upon the success of YOLOv8, YOLO11 delivers state-of-the-art performance across the board with significant improvements in both speed and accuracy.\n\n### 🛠️ R&D Highlights\n\n- **25 Open-Source Models:** YOLO11 introduces **25 models** across **5 sizes** and **5 tasks**, ensuring there’s an optimized model for any use case.\n- **Accuracy Boost:** YOLO11n achieves up to a **2.2% higher mAP (37.3 -> 39.5)** on COCO object detection tasks compared to YOLOv8n.\n- **Efficiency & Speed:** YOLO11 uses **up to 22% fewer parameters** than YOLOv8 and provides up to **2% faster inference speeds**. Optimized for edge applications and resource-constrained environments.\n\nThe focus of YOLO11 is on refining architecture to improve performance while reducing computational requirements—a great fit for those who need both precision and speed.\n\n### 📊 YOLO11 Benchmarks \n\nThe improvements are consistent across all model sizes, providing a noticeable upgrade for current YOLO users.\n\n| Model           | YOLOv8 mAP (%) | YOLO11 mAP (%) | YOLOv8 Params (M) | YOLO11 Params (M) | Improvement |\n|-----------------|------------------------------|------------------------------|-------------------|-------------------|-------------|\n| **YOLOn**       | 37.3                         | 39.5                         | 3.2               | 2.6               | +2.2% mAP   |\n| **YOLOs**       | 44.9                         | 47.0                         | 11.2              | 9.4               | +2.1% mAP   |\n| **YOLOm**       | 50.2                         | 51.5                         | 25.9              | 20.1              | +1.3% mAP   |\n| **YOLOl**       | 52.9                         | 53.4                         | 43.7              | 25.3              | +0.5% mAP   |\n| **YOLOx**       | 53.9                         | 54.7                         | 68.2              | 56.9              | +0.8% mAP   |\n\n### 💡 Versatile Task Support\n\nYOLO11 extends the capabilities of the YOLO series to cover multiple computer vision tasks:\n- [**Detection**](https://docs.ultralytics.com/tasks/detect/): Quickly detect and localize objects.\n- [**Instance Segmentation**](https://docs.ultralytics.com/tasks/segment/): Get pixel-level object insights.\n- [**Pose Estimation**](https://docs.ultralytics.com/tasks/pose/): Track key points for pose analysis.\n- [**Oriented Object Detection (OBB)**](https://docs.ultralytics.com/tasks/obb/): Detect objects with orientation angles.\n- [**Classification**](https://docs.ultralytics.com/tasks/classify/): Classify images into categories.\n\n### 🔧 Quick Start Example\n\nIf you're already using the Ultralytics package, upgrading to YOLO11 is easy. Install the latest package:\n\n```bash\npip install ultralytics>=8.3.0\n```\n\nThen, load a pre-trained YOLO11 model and run inference on an image:\n\n```python\nfrom ultralytics import YOLO\n\n# Load the YOLO11 model\nmodel = YOLO(\"yolo11n.pt\")\n\n# Run inference on an image\nresults = model(\"path/to/image.jpg\")\n\n# Display results\nresults[0].show()\n```\n\nThese few lines of code are all you need to start using YOLO11 for your real-time computer vision needs.\n\n### 📦 Access and Get Involved\n\nYOLO11 is open-source and designed to integrate smoothly into various workflows, from edge devices to cloud platforms. You can explore the models and contribute at https://github.com/ultralytics/ultralytics.\n\nCheck it out, see how it fits into your projects, and let us know your feedback!",
    "created_utc": "2024-10-01T10:10:14",
    "num_comments": 32,
    "comments": [
        "\"Open source\" until you read the license.",
        "I lead a computer vision data science team and moved our company away from ultralytics Yolo models. From the not actually open source (and expensive!) for commercial use models alongside the shady chat-bot issue responses on github, we did not view these models as a viable option. The marginal performance boost between yearly new versions can be bested by just getting a better dataset anyways.",
        "You only look once, but you do it every year",
        "[Summarised] Another modded version with these features.\n- No published architecture just some random keywords thrown around for \"enhancements.\"\n- Marginal gains on an undocumented benchmark.\n- AGPL license just like all the other modded versions.",
        "Hmn, cool! I have a question, it seems most object detection models are usually trained on COCO and then also benchmarked against COCO however there are are other benchmarks like rf100 which could show if a model could generalize better beyond common objects. It would be interesting to see yolov11 benchmarked on rf100(or datasets other than COCO) and compared to eachother. How great is the difference in mAP when benchmarkining against datasets that are not COCO?",
        "Will you consider a MIT licenced Yolo in the near future? Will you consider at least a MIT license \"if trained from scratch\" like YoloNAS? \n\nAt least you would recover some lost karma out here ;)",
        "At a glance, this model is not as good as yolov10 or yolov9 at object detection.\n\nYolov11n is 1% \"better\" than Yolov10n and yolov9t, but uses 10% more params. Architecture seems nearly identical to yolov10n.\n\nAt the top end, yolov9e is better than yolov11x by 1.2% with only 2% more parameters.\n\nYolov9 is GPL or MIT as well.",
        "Let's go, YOLO 13.1.0.3.e.1 yay!",
        "How so?",
        "YOLO11 sports an official OSI-approved open-source license. See [https://opensource.org/license/agpl-v3](https://opensource.org/license/agpl-v3) :)",
        "The first think I do when people ask me for advice is to evaluate all their open-source options, not just Ultralytics. Happy to hear you found an option that works for you!",
        "u/darkerlord149 archicture is here [https://github.com/ultralytics/ultralytics/blob/main/ultralytics/cfg/models/11/yolo11.yaml](https://github.com/ultralytics/ultralytics/blob/main/ultralytics/cfg/models/11/yolo11.yaml)\n\nBenchmarking code to reproduce is here  \n[https://github.com/ultralytics/ultralytics/blob/main/ultralytics/utils/benchmarks.py](https://github.com/ultralytics/ultralytics/blob/main/ultralytics/utils/benchmarks.py)",
        "Yeah I'm curious if we are just hyper optimizing to a specific dataset. In practice, I often find little improvement on my more use case specific datasets compared to the tried and true.",
        "Yes great point! We benchmark on COCO because this is the reference standard in Object Detection. The only way to compare to past publications is by a single yard stick, even though today larger and more diverse datases exist like Objects 365 with 365 classes and Open Images v7 (650 classes).",
        "Great question! We chose AGPL to encourage open contributions and keep our research available for all. An MIT license isn't currently on the table, but we're always listening to feedback and open to discussions on how to improve access for the community. Thanks for sharing your thoughts! 😊",
        "You cannot use it commercially unless you make your code/model weights public too.\n\nBut they still like to take actual open source models, integrate them into their infrastructure, and make profits off of them (yolo world, yolov10...)\n\nThey also sneakily changed their license from GPL to AGPL\n\n  \nI recommend this thread: [https://www.reddit.com/r/computervision/comments/1e3uxro/ultralytics\\_new\\_agpl30\\_license\\_exploiting/](https://www.reddit.com/r/computervision/comments/1e3uxro/ultralytics_new_agpl30_license_exploiting/)",
        "In your view, is it ok to use an ultralytics model commercially as long as you make the new weights publicly available? \n\nI am not personally a fan of your business model, but i acknowledge your work in making sota models accessible for a broader audience.",
        "Why did you change the license of your YOLO repos from GPL to AGPL?",
        "An architecture should be published with detailed discussions on why it is innovative, what changed compared to the SOTAs and why those changes matter.\n\n\nAnd please help community members reproduce your results by DOCUMENTING the configurations, hyper parameters for your benchmarking.",
        ">to encourage open contributions\n\nYes, because it's fine to accept and integrate open source contributions and models to your for-profit business, without returning the favor by allowing actual commercial usage (you know it's not straightforward to use it commercially if we have to open source it, but it's fine if you use open source for profit.\n\n>keep our research available for all\n\nHow? You've never shared a single paper or technical report, only \"\"\"\"enhancements\"\"\" from actual open source models, and chatgpt-generated announcements\n\n>always listening to feedback\n\nRight, like using ChatGPT to answer instead of you under your issues on github...",
        "> You cannot use it commercially unless you make your code/model weights public too.\n\nIt would still be considered open-source with that restriction. The Linux kernel, arguably the largest open-source project, is GPL-3.0 licensed and has the same restrictions, i.e. if you distribute a custom Linux kernel with your software, the source code has to be released. That's what Android OEMs do when they release phones. They release the kernel sources.",
        "Hey, I always wonder, how enforceable are these licenses? I mean how would they know if you are using their model!\n\nThere are other open source projects with close models that I always keep seeing being used in different projects shared here.",
        ">They also sneakily changed their license from GPL to AGPL\n\nCould you explain what is the difference between the two?",
        "\n\n>You cannot use it commercially unless you make your code/model weights public too.\n\n  \nlike 90% of open source stuff",
        "Yes of course! If you open source your project then you can use the models for free commercially. This is the spirit of AGPL, maintaining work open and accessible to all.",
        "When Twitter went open-source under Musk, I noticed they chose an AGPL-3.0 license. Figured they probably knew what they were doing, so I decided to align our licenses with theirs.  \n[https://github.com/twitter/the-algorithm](https://github.com/twitter/the-algorithm)",
        "Full architecture is at [https://github.com/ultralytics/ultralytics/blob/main/ultralytics/cfg/models/11/yolo11.yaml](https://github.com/ultralytics/ultralytics/blob/main/ultralytics/cfg/models/11/yolo11.yaml), sorry if that wasn't clear before. Users can use these official models or start from them as the basis for their own customizations and improvements.",
        ">Over GPL, It has only one additional requirement: if you run a modified program on a server and let other users communicate with it, your server must also allow them to download the source code corresponding to the modified version in operation.\n\nsrc. [https://www.projeqtor.org/en/copyright/759-agpl-en](https://www.projeqtor.org/en/copyright/759-agpl-en)",
        "MIT or Apache based projects do not, no?",
        "Thank for answering!",
        "Two different directions.\nThey went open on their source written by their paid employers.\nYou went more closed after having \"aligned\" yourself with the brand and taken advantage of the opensource community for contributions.",
        "A yaml file isn't a paper nor a technical report, plus I hope you understand that it's clear you're cherry-picking what to reply about and avoiding what you prefer to avoid..."
    ]
},
{
    "submission_id": "1fts2ya",
    "title": "PaddleOCR putting random periods",
    "selftext": "python paddleocr  \n  \nI have a very simple image with a paragraph of computer text with a simple font. It reads the text properly, but after some words it puts a \".\"/period... (or double punctuation \"..\", \",.\"...)\n\n  \nhow can i fix this?\n\n    ocr = PaddleOCR(use_angle_cls=False, lang='en')\n    result = ocr.ocr(\"test.png\", cls=False)\n    paragraph_text = ' '.join([element[1][0] for line in result for element in line])\n    print(paragraph_text)",
    "created_utc": "2024-10-01T09:19:14",
    "num_comments": 0,
    "comments": []
},
{
    "submission_id": "1ftqv4o",
    "title": "Project Help: Footsteps Counter for Video Input – Looking for SOTA Models and Heuristics",
    "selftext": "I'm working on a project to count footsteps in an input video and have been experimenting with pose estimation methods like YOLOv8 and MediaPipe. My goal is to cover the following test cases:\n\n1. Only the upper body of the person is in the frame, but they are walking.\n2. Only the lower body of the person is in the frame.\n3. The solution should be occlusion-proof.\n\nHere’s the logic I'm currently using to count steps by calculating the distance between the left and right ankles:\n\n    def distanceCalculate(p1, p2):\n    \"\"\"p1 and p2 in format (x1, y1) and (x2, y2) tuples\"\"\"\n    dis = ((p2[0] - p1[0]) ** 2 + (p2[1] - p1[1]) ** 2) ** 0.5\n    return dis\n    \n    # Calculate distance between ankles (a crude approximation of taking a step)\n    if distanceCalculate(leftAnkle, rightAnkle) > 100: # Threshold for step detection\n    if not stepStart:\n    stepStart = 1\n    stepCount += 1\n    \n    # Append to output JSON\n    output_data[\"footsteps\"].append({\n    \"step\": stepCount,\n    \"timestamp\": round(current_time, 2)\n    })\n    \n    elif stepStart and distanceCalculate(leftAnkle, rightAnkle) < 50:\n    stepStart = 0 # Reset after a complete step\n\nHowever, this logic doesn't work for all videos. I'm looking for suggestions on state-of-the-art (SOTA) models and heuristic logic that can help improve the step detection, particularly for the scenarios mentioned above.\n\nAny advice or suggestions would be greatly appreciated!\n\nThanks in advance!",
    "created_utc": "2024-10-01T08:28:02",
    "num_comments": 0,
    "comments": []
},
{
    "submission_id": "1ftoptf",
    "title": "Pothole detection in farms",
    "selftext": "Hello everyone,  \nI am faced with the challenge of detecting potholes in farm like areas which have horse riding arenas in the farms. The traversable areas between the arenas have some potholes as shown in the images. We are building robots that navigate between these arenas to and fro and perform certain tasks. The robots in principle, need to navigate avoiding the potholes of course, which is why I need to detect these potholes. As a starting point, I trained yolov10 on a small scale pothole detection dataset. All the datasets that I could find are more or less related to urban driving scenarios with potholes. With this setup, I could not really detect all the potholes for my use case. Due to a lack of data and annotations too, I am stuck and not sure how to proceed. Annotation of my dataset is not feasible due to lack of resources and time. Your tips would be highly appreciated.\n\nhttps://preview.redd.it/i7u04tkjh5sd1.png?width=1280&format=png&auto=webp&s=796a3c4df7364fb2c2dfae2623769379e6fc0972\n\nhttps://preview.redd.it/a4frs9ljh5sd1.png?width=1280&format=png&auto=webp&s=73ac83a892ccabb4e6e3c8a07074b260096dd0d8\n\n",
    "created_utc": "2024-10-01T06:56:24",
    "num_comments": 7,
    "comments": [
        "Do you have lidar or stereo vision at your disposal?  It makes this task easier.  Water will generally produce voids or really noisy returns in lidar and stereo disparity calculations.  So you go for an approach of mapping out the drivable area into a cost-map, noisy non drivable spots will be potholes of at least puddles.  No machine learning needed that way.",
        "Okay so the very basics of ML is you need data. If annotation is not feasible then you’ve made a mistake in your project plan.\n\nYou will definitely have to transfer learn from existing weights and you will have to annotate some data. Your best bet would probably be try to use SAM or something to auto label your data to be somewhat helpful but most likely you will have to annotate a decent sized dataset cause that’s how ML works",
        "If you are interested in trying synthetic image data, send me a message. I think it is a great use case.",
        "u/CowBoyDanIndie I have a stereo camera as well as a lidar. However the point clouds seem to be normal and not noisy. As a work around, I have decided to detect puddles instead of potholes, as for potholes, the elevation map can be used.",
        "u/notgettingfined Thanks for your input. I understand this, but I was wondering if there are any approaches such as few shot object detection which could help me. Using transfer learning has been successful up-to some extent but as you said, it would require fine-tuning on my own dataset which is difficult to realize due to lack of annotations.",
        "u/syntheticdataguy message sent."
    ]
},
{
    "submission_id": "1fto0un",
    "title": "quantize a model",
    "selftext": "",
    "created_utc": "2024-10-01T06:24:32",
    "num_comments": 10,
    "comments": [
        "How can anyone help without seeing the error you are getting? And also tflite is only cpu compatible and as far as I know  cpus don't perform faster with fp16 (they don't support fp16 at all), neither most gpus. Quantization is only suitable in 3 scenarios:\n\n1- your model is so large that it cannot fit in the memory. So use quantization to make reduce the model size to half(fp16) of a quarter(int8).\n\n2- you have a hardware that supports fp16 (some gpus like h100) operations or int8 operations (almost every cpu and gpu).\n\n3- you tend to design a hardware which in that case you can design the register file, alu, bus etc. the way you want.\n\nSo in summary: 1- we cant help you unless you provide the error you are facing\n2- i highly doubt if quantization have any positive effect in your case since you are using a very light model.",
        "Hi, i am sorry i didnt put it more clearly\nBut the value error is\n\nValueError: `to_quantize` can only either be a keras Sequential or Functional model.\n\nPs: i put it within the code block i will change it",
        "my model have to be under **1 MB** for flash storage (ideally 300–800 KB for flexibility) for it to work on the himax",
        "I see, I don't know about this but I will provide you with a file later wich uses tflite to quantize and compress models.",
        "You'll be sacrificing accuracy.",
        "ah ok thanks. you see cause i need to run the model in a himax WE-I which doesnt have a lot of computing power.",
        "Yes im aware of that i have solve the quantization problem but it seems like the model is still too big for the himax",
        "Maybe don't use himax?",
        "Its my project to use himax",
        "Good luck then."
    ]
},
{
    "submission_id": "1ftmo5y",
    "title": "How to setup a good baseline in vision projects",
    "selftext": "Is it okay to use the same model on smaller dataset with class bias as baseline and then customize and improve data(by adding more data) to state the improvement over baselines with same model? What is the general practice in industries?",
    "created_utc": "2024-10-01T05:18:56",
    "num_comments": 3,
    "comments": [
        "I said this before in another discussion, avoid training as much as possible. It's very expensive, hard, and time consuming. Try fitting the problem in the same criteria as solved problems, if not successful then try to fine tune, if that fails too, try training. And in this scenario there is no defacto, do it the way you see fit.",
        "Let's say training cost isn't an issue. Fine tuning a CV model is one hell of a job. Single epoch can take hours and you need evidence to support and prioritise a hyperparameter. Now the question is how to design a baseline? Without baseline how one know if there is improvement or not?",
        "What you are asking is very case specific in my opinion. It varies project to project."
    ]
},
{
    "submission_id": "1ftlku9",
    "title": "Dataset class Distribution effect for model perf.",
    "selftext": "Does the class distribution of the dataset have a direct effect on the performance of the model? For example, the content of my datasets in figure 1 and figure 2 are the same, but when I combine the classes, 6,7,8 becomes 4 and 2,4,5 becomes 2. Actually, the most logical thing would be to try and see, but I wanted to ask if there is a paper-style study for this. \n\nI think that having too many of one class causes the model to learn that class excessively and not to learn other classes.\n\n[1](https://preview.redd.it/tv0xeejjo4sd1.png?width=640&format=png&auto=webp&s=f0840df458df15b9f52f09027370c34a0532ef53)\n\n[2](https://preview.redd.it/i4a8wv2qo4sd1.png?width=640&format=png&auto=webp&s=310db55e6bc8a3aa4fd824da25c0107db3053e2b)\n\n",
    "created_utc": "2024-10-01T04:17:04",
    "num_comments": 0,
    "comments": []
},
{
    "submission_id": "1ftjtc3",
    "title": "Key point Detections with instance segmentation",
    "selftext": "I have a task which I need to identify (predict/estimate) a specific part of an object even if it may be semi occluded. I thought the way to do this was to use keypoints as areas of interest, one for the top of the object and one for the bottom of the object. The problem with this comes as these \"objects\" I'm trying to detect are often tightly clustered and semi-occluded meaning with ordinary bounding boxes adds a lot of overlap creating a lot of unnecessary noise within my training dataset. Just for added context, these objects are far from square meaning normal bounding boxes just aren't suitable at all. The obvious solution to this would be instance segmentation to accurately draw masks around the objects and having two keypoints, one for the top of the object (not occluded) and one for the bottom of the object (flagged as occluded). Using the object in full view, and the available information of the semi occluded object to make a prediction of the bottom keypoint. In my head this is a solution which is suitable for my specific need but please correct me if I'm wrong or off the mark. Be aware I'm a beginner in computer vision and machine learning so my knowledge might be wrong.\n\nhttps://preview.redd.it/568fs4e234sd1.png?width=899&format=png&auto=webp&s=9ed90d15ddbda8834b84e701aab236f34db28fa4\n\n  \nPlease excuse the poor diagram i just threw it together quickly as I think it shows what im looking for better than i can describe with works. Anyway, I'm looking for a solution where I can train a model for a keypoint task or whatever, but uses instance segmentation masks rather than bounding boxes. I had a quick look on google and a lot of what I could find looked quite technical beyond my capabilities. So if theres any resources or guidence which can help me achieve this, this will be appreaciated.",
    "created_utc": "2024-10-01T02:16:43",
    "num_comments": 7,
    "comments": [
        "for occuled objects theres is an option like Visibility in annotators \n\npick that according to your need and your kp will be fine",
        "I would experiment with approach from the paper \"Objects as Points\" (https://arxiv.org/abs/1904.07850). It is similar to one-stage object detection, but instead of predicting bounding boxes it predicts other object properties. For this problem the locations and visibility of 5 keypoints could be predicted.",
        "Yeah I see with YOLO you can add the flags of 0,1 or 2 for visibility. My question is more about how can I use KP with instance segmentation rather than bounding boxes",
        "This is interesting thank you v much. I just had a quick scan through and it sounds promising, will look into this today. Struggling to get my head around how it works entirely as I'm fairly new to computer vision/machine learning but looks promising. Thanks again",
        "Just don't add bounding box during annotations",
        "Right, but what about the mask part? I still want to identify the object and the keypoints with it",
        "You might have to annotate both mask with key points \nOr in worst case you might train 2 different models"
    ]
},
{
    "submission_id": "1ftjp31",
    "title": "Recommendations Needed",
    "selftext": "Hello everyone, I have a few questions about the capabilities of this PC:\n\n* Can I train YOLO models on large datasets (around 150k images) without issues? Ideally, it should take less than a day! For context, we are training YOLO models to detect up to 53 car parts.\n* Is it possible to train large classifiers on this system?\n* Not a priority, but I’m curious—could I fine-tune large language models (LLMs) on this machine? (I don’t think it’s feasible, but I’m just asking out of curiosity.)\n* Any recommendations for a system within a $4,000 budget would be greatly appreciated!\n\nhttps://preview.redd.it/meago6t524sd1.png?width=1603&format=png&auto=webp&s=9191b5e1ab7e28bcda1e43cfd5fa4377fd3fc72e\n\n",
    "created_utc": "2024-10-01T02:07:51",
    "num_comments": 7,
    "comments": [
        "actually ı rent a rtx 4090 0.32 dolar for a hour, ımy dataset has 63k train image 10k val image. my model yolov8s took an epoch in 5 minutes.",
        "on 4090 RTX 8 GB it took 3 days on average for training 70k to 80k images   \ndetecting only 2 classes with max 5 key points",
        "Yolov8 is nearly 1.5-2x faster on training according to yolov7. I suggest you to get gpu with 16gb memory at least. For classification, i believe dinov2 would give you bothe accuracy and speed.",
        "Yes absolutely! You can train i.e. YOLO11n on Oiv7 (Open Images v7, 1M images) in 2-3 days on an 8x A100 server (default settings like imgsz=640).",
        "are there websites for renting GPU or did you find someone ?",
        "I used it in colab, but when the data set is large, it is a problem to mount the data from the drive.",
        "yes, there are many websites but I didn't want to write because it would be an advertisement, frankly"
    ]
},
{
    "submission_id": "1ftj9a1",
    "title": "GOT-OCR is the best OCR model so far",
    "selftext": "GOT-OCR is trending on GitHub for sometime now. Boasting of some great OCR capabilities, this model is free to use and can handle handwriting and printed text easily with multiple other modes. Check the demo here : https://youtu.be/i2ypeZA1_Yc",
    "created_utc": "2024-10-01T01:33:53",
    "num_comments": 16,
    "comments": [
        "Link to model so you don't have to watch a wannabe influencer https://huggingface.co/stepfun-ai/GOT-OCR2_0",
        "nice, might be a paddleocr competitor...",
        "This is great. Thanks for posting!",
        "could it be fine-tuned on custom dataset ? if yes , can you provide the guide ?",
        "could anyone help me on how train it on RTL langages ?",
        "could anyone help me in training got-ocr from stage1 to other langages ?",
        "Thanks!",
        "couldyou help me on how train it on RTL langages ?",
        "You must be fun IRL",
        "Paddle's pretty strong in OCR. Maybe more than anything else.",
        "couldyou help me on how train it on RTL langages ?",
        "couldyou help me on how train it on RTL langages ?",
        "Their github and huggingface has all you need.",
        "couldyou help me on how train it on RTL langages ?",
        "couldyou help me on how train it on RTL langages ?",
        "could you guide me troughout only the first steps ?"
    ]
},
{
    "submission_id": "1ftj46j",
    "title": "Help me understand validation metrics on the RetinaFace dataset",
    "selftext": "Hey everyone,\n\nI am trying to reproduce results from the [RetinaFace](https://arxiv.org/abs/1905.00641) paper, but it is unclear to me how they evaluate their method on the WIDERFACE dataset. They describe how they additionally annotate five facial keypoints, but their linked [repo](https://github.com/deepinsight/insightface/blob/master/detection/retinaface/README.md) only provides keypoint labels for the training set, not the validation set. Do they only evaluate the detection accuracy, or are the validation keypoint labels published somewhere else?\n\nEdit: additionally, it would be very helpful if someone could explain the data format of the RetinaFace dataset. If I understand correctly, the first four numbers represent the face bounding box, but I am not sure how the keypoints are represented. E.g., do they have a visibility flag, and ehat does a value of -1 mean? For context, I am trying to train a YOLOv8 pose model on the dataset to detect faces and the five facial keypoints.\n\nAny help would be greatly appreciated!",
    "created_utc": "2024-10-01T01:23:13",
    "num_comments": 1,
    "comments": [
        "Found [37 relevant code implementations](https://www.catalyzex.com/paper/arxiv:1905.00641/code) for \"RetinaFace: Single-stage Dense Face Localisation in the Wild\".\n\n[Ask the author(s) a question](https://www.catalyzex.com/paper/arxiv:1905.00641?autofocus=question) about the paper or code.\n\nIf you have code to share with the community, please add it [here](https://www.catalyzex.com/add_code?paper_url=https://arxiv.org/abs/1905.00641&title=RetinaFace%3A+Single-stage+Dense+Face+Localisation+in+the+Wild) 😊🙏\n\nCreate an alert for new code releases here [here](https://www.catalyzex.com/alerts/code/create?paper_url=https://arxiv.org/abs/1905.00641&paper_title=RetinaFace: Single-stage Dense Face Localisation in the Wild&paper_arxiv_id=1905.00641)\n\n--\n\nTo opt out from receiving code links, DM me."
    ]
},
{
    "submission_id": "1ftilsi",
    "title": "What background removal models are you using today?",
    "selftext": "I'm still using the good old RMBG-1.4, but it hasn't been working well for me lately. What are you using that has been the most reliable for you? I wanted to know if I'm missing out on something better on the market. I'm mostly using it for removing backgrounds from human images.",
    "created_utc": "2024-10-01T00:44:50",
    "num_comments": 3,
    "comments": [
        "SAM2 works really well",
        "It depends how fast the OP wants the model to run, and how much processing power they have, but overall, SAM2 is amazing. SAM2 requires a point or a bounding box to segment. If you don't have this, but you have words such as person/human, go for something that would detect this (groundingDINO, YOLOWorld, etc.), e.g. bounding box, and forward this to SAM2.",
        "Interesting, Do you have a comfyui workflow which I can try?"
    ]
},
{
    "submission_id": "1ftahpv",
    "title": "Open Source Tool for Cleaning Image Classification Datasets Using Embedding Visualization and UMAP",
    "selftext": "",
    "created_utc": "2024-09-30T16:57:10",
    "num_comments": 0,
    "comments": []
},
{
    "submission_id": "1ft91it",
    "title": "Stroke Width Transform w/Parallel Processing\n",
    "selftext": "Hey everyone!\n\nI’m excited to share my latest project: Stroke Width Transform (SWT), implemented in Python and optimized with parallel processing for faster text detection in images. The Stroke Width Transform (SWT) algorithm was introduced by researchers from Microsoft in a 2010 paper by Boris Epshtein, Eyal Ofek, and Yonatan Wexler.\n\n# Key Features:\n\n* Efficient text detection using SWT.\n* Parallel processing for improved performance.\n* Easy to use and fully open source.\n\nCheck out the project on GitHub: [https://github.com/vrlelif/stroke-width-transform](https://github.com/vrlelif/stroke-width-transform) ⭐ If you find it useful, I’d love a star!\n\nFeedbacks are welcome!\n\n# 1. What My Project Does:\n\nThe project implements the **Stroke Width Transform (SWT)** algorithm with enhancements, focusing on improving **text detection in natural images**. It adds **parallel processing** using Python's multiprocessing module to improve the algorithm’s performance significantly. The enhancements include modifications to improve noise reduction, more accurate text region detection, and overall faster execution by distributing tasks across multiple processors​.\n\n# 2. Target Audience:\n\nThe project is geared towards researchers and developers working in **computer vision** and **text detection algorithms**, particularly those who need efficient, high-performance text detection in images. While it can be a part of a **production** system, it also serves as a foundational or experimental implementation for those studying **image processing** algorithms​.\n\n# 3. Comparison:\n\nCompared to existing SWT implementations, this project distinguishes itself by:\n\n* Using **parallel processing** to increase the speed of the algorithm, especially on high-resolution images.\n* Improving text detection accuracy by applying rules for **noise reduction** and **stroke length limitation**, which help filter out irrelevant image features that are often mistaken for text​.",
    "created_utc": "2024-09-30T15:50:12",
    "num_comments": 0,
    "comments": []
},
{
    "submission_id": "1ft5x42",
    "title": "Converting Vertex-Colored Meshes to Textured Meshes",
    "selftext": "",
    "created_utc": "2024-09-30T13:37:01",
    "num_comments": 1,
    "comments": [
        "I’d rather it output a uv mapped obj with the texture and a mtl file."
    ]
},
{
    "submission_id": "1fsy52c",
    "title": "Keyframe extraction from a video",
    "selftext": "Hello! I did some research on the subject and learned a few popular methods (surf, sift, ssim, cm, etc.). So far I had the opportunity to try surf and ssim but they did not reach the performance I expected. Is there a method or paper you can recommend me? I would really appreciate it. \n\nThanks.",
    "created_utc": "2024-09-30T08:21:38",
    "num_comments": 13,
    "comments": [
        "Please explain what you are trying to do",
        "Are you talking about keypoint extraction or keyframe extraction? These are 2 different tasks.",
        "Maybe just extract fixed interval frames, then use an Image Embedding model with cosine similarity to filter out duplicates. Can also ask Vision Language model to determine bad / blurry frames",
        "I want to summarize a video with visual models. It should be able to tell in which frame certain scenarios start or at least summarize the video. For this I want to be able to select only the important frames.",
        "Actually, I'm trying to extract keyframes, but I used keypoint extraction methods. The more similar the extracted points are, the more I concluded that the frames are the same.",
        "The videos we are going to use can be hours long. So at this stage, instead of using a model, I should take a more traditional approach",
        "Define important.",
        "You can use them, but you don't need keypoints extractors in this case. Simple frame differencing will help you determine the amount of motion between frames.",
        "I'm working on a very similar project, about 90% the same. Could you explain why you're using a traditional approach? In my tests, a pipeline combining DataLoader and a TensorRT model can quickly extract embeddings from hundreds of thousands of images in a short time.\n\nDoes your video contain a lot of static frames? How much motion do you want to filter out? For example, imagine a sequence where someone is sitting still but moves their hand to reach for a coffee cup.\n\nIn my project, I’m working with a video of a news report. The general structure is: the news MC speaks, then the screen switches to actual news footage, and this pattern repeats. My approach is to cluster the embeddings to filter out all the MC frames. Within each cluster, consecutive frames (based on timestamps) that have very high cosine similarity are removed.",
        "If there is no movement or scene change in the video, I don't want to take more than one frame from that moment. Every frame that doesn't contain these things is important to me.",
        "Thanks. Speed is important to me, and I don't have a GPU. But I'll look into what you mentioned. The key point in my project is to find out at which second the scenarios begin, rather than summarizing the video. Regarding your approach, may I message you if possible?",
        "Probably video-based anomaly detection, if no movement or scene change corresponds with being a rare occurrence",
        "yes, feel free to DM me"
    ]
},
{
    "submission_id": "1fsx2ni",
    "title": "Phd in Computer vision about video game",
    "selftext": "I going graduate my master next years and I looking for PhD focus on AI game creation topic, specific computer vision in video game, related with 3d model/ character/animation generate. I not sure which school focus in that.",
    "created_utc": "2024-09-30T07:36:45",
    "num_comments": 6,
    "comments": [
        "McGill -> Prof Derek Nowrouzezaharai works in the intersection of DL and physics based rendering (monte carlo ray tracing, photon mapping, MLT etc) and deep learning, his students literally made nvidia optix what it is, amazing prof (really one of the best profs I have ever taken a course under. If differentiable rendering or similar fields is your goal then 10/10 recommend.",
        "maybe what you want to focus isn’t necessarily AI/computer vision but rather computer graphics. you’d have to do your own research on what professor would be the best fit for you",
        "look at Ai4 animations’s schools",
        "there's overlap but this is the wrong subreddit, you should maybe try r graphicsprogramming",
        "I studied computer graphic and CS in game development such as physics simulation , game engine tool programming during my bach",
        "yes somewhat overlap, it overlap with PCG too and somewhat related with GAN or 3d stable diffusion, 3d scan and more, is more on generative content side of computer vision than just sensing and detection alone.\n\n[the AI GRAPHICS: 인공지능, 캐릭터, 웹툰, 패션, 세계관, 디지털 디자인 | 김성완 - 교보문고 (kyobobook.co.kr)](https://product.kyobobook.co.kr/detail/S000214044095)"
    ]
},
{
    "submission_id": "1fstppp",
    "title": "Line/word segmentation for documents ",
    "selftext": "hello , is their any models or guide on how to build a script / model to do line to word segmentation of a document that contains both handwritten  and textwritten lines/words ? \ni've tried many approaches but a small need more adaptation / updates.",
    "created_utc": "2024-09-30T04:57:21",
    "num_comments": 7,
    "comments": [
        "You can use yolo segmentation or any segmentation model and train it on your dataset.",
        "exemples like ??",
        "Yolo segmentation",
        "work for both handwritten and text written ? \ncuz i am trying to do the segmentation then pass it to my ocr model.",
        "Any model can work on both handwritten and text written you just need to train it, unless you are asking for a pretrained model to segment text which I dont know about one.",
        "so for yolo i need it to label my data to train it ?",
        "Yes"
    ]
},
{
    "submission_id": "1fsslh6",
    "title": "Research opportunity ",
    "selftext": "Hello friends,\nI hope you are all doing well.\nI have participated in a competition in the field of artificial intelligence, specifically in the areas of trustworthiness and robustness in machine learning, and I am in need of 2 partners.\nThe competition offers a cash prize totaling $35,000 and will be awarded to the top three teams.\nAdditionally, in the event of achieving a top position in the competition, the results of our collaboration will be published as a research paper in top-tier conferences.\nIf you are interested, please send me your CV.",
    "created_utc": "2024-09-30T03:49:26",
    "num_comments": 1,
    "comments": [
        "Post the competition details (link, etc)"
    ]
},
{
    "submission_id": "1fsshtn",
    "title": "Anyone can recommend a library for Multi Camera Multi Object (Human) Tracking with Birds Eye View as final output (GitHub for implementation is a plus)",
    "selftext": "I thought of having multiple cameras to inference and do homography but I realise it might take abit of work… wondering if there was any working solution out of the box ",
    "created_utc": "2024-09-30T03:42:24",
    "num_comments": 7,
    "comments": [
        "There is none I'm afraid. You have to look in nuscense scoreboard, use camera only bev models and filter out any non pedestrian class, then use something like Hungarian algorithm or similarity learning for tracking. Also I think there is tracking benchmark in there, idk if there are any open source publications based on it.",
        "Whats the use case? How many cameras? Indoor or outdoor? Overlapping FOVs?",
        "Monoloco by Vita-Epfl. It’s not on a free license though",
        "I would suggest using OpenCV with supervision to get this sort of project done ✅ ",
        "This may be a use case for NeRF if compute and throughput aren't major requirements.",
        "Thanks for the reply! I’ll check that out, seems like. I can go paperswithcode and search of multi camera datasets for their SOTA",
        "I wanted to track movement in an area of 30mx30m indoor! Does overlapping FOV matter? Was thinking of getting 4 cameras places at 6-8m high"
    ]
},
{
    "submission_id": "1fsor98",
    "title": "Multi Subject Real-time Pose Estimation Model (50+ subjects)",
    "selftext": "I need to determine the Pose of Multiple Subjects (50+) in real time.\n\nI don't need too many variations. Just to know whether they are (walking, standing, lying down.)\n\n\nSomething lightweight I can run locally. Thanks!",
    "created_utc": "2024-09-29T23:01:08",
    "num_comments": 9,
    "comments": [
        "What subjects? Human, animals, objects?\n\nYou can try something like VitPose for human pose estimation but it's a bit heavy. May be try something from mediapipe?",
        "Human's. Is mediapipe lightweight?\n\nAfter browsing around the sub I saw one called RTMpose, Is that any good?",
        "RTMPose is pretty good and lightweight. But you want pose classification. RTMPose will provide you with the keypoints (keypoint estimation). But you will need to create a simple classifier to take in those keypoints and classify the pose.",
        "Thank you for the reply, Can you help me find the documentation for installation and use of Rtmpose? \n\nCouldn't find any detailed instructions in their repo \n\nAlso I would need a detector/tracker or whatever as well? When there are multiple subjects(who may overlap)\n\nEdit: I also found one called 'RTMO'. Which is based on mmpose as well. That claims to be faster than rtmpose in multi person scenarios?",
        "> Can you help me find the documentation for installation and use of Rtmpose? \n\nIt's on their repo:\nhttps://github.com/open-mmlab/mmpose/tree/main/projects/rtmpose\n\nAlso there's this library which makes it simpler:\nhttps://github.com/Tau-J/rtmlib\n\n\n> Also I would need a detector/tracker or whatever as well? When there are multiple subjects(who may overlap)\n>\n> Edit: I also found one called 'RTMO'. Which is based on mmpose as well. That claims to be faster than rtmpose in multi person scenarios?\n\nRTMO is end-to-end (no detector required), while RTMPose requires using a detector first and then running the pose estimation.",
        "Last question, Sorry I am very new to this, for the classifier like you said is LSTM the best option?\n\nOr CNN + LSTM is better?... From what I understand in that case CNN is used for feature extraction... But is that what These pose estimation models are doing?\n\nSo LSTM is enough to identify different activities?",
        "You don't need LSTM for this. Nor CNN. Just a simple neural network or even something like LightGBM or XGBoost. They have a Keras example of a classifier here. You can convert it to PyTorch using ChatGPT.\n\nhttps://www.tensorflow.org/lite/tutorials/pose_classification\n\nIgnore the MoveNet part. That's for pose estimation.",
        "Even if it's a sequential activity like throwing? I don't need an LSTM?\n\nThis also seems to be trained on images.. I was hoping to use videos as training data.... Like the process of walking, lying down and getting up etc?\n\nEdit: also different angles on the camera (different from training data)",
        "If it's sequential, yeah you need LSTM and the whole thing becomes a lot more complicated."
    ]
},
{
    "submission_id": "1fsmjo6",
    "title": "How do I determine a persons orientation?",
    "selftext": "So I'm using a kinect camera to extract a persons skeletal data, and I'm trying to code in visual studio on determining a person's orientation (sitting down, lying down, leaning left, leaning right, etc.) using mathematical operation. Any idea what mathematical method I should use? I've tried researching and what I've come up to now is determining the angle between the points of the hip relative to the torso using vector. I'm going to try it now, but I'm looking into seeing any more suggestions if you have any. ",
    "created_utc": "2024-09-29T20:39:19",
    "num_comments": 10,
    "comments": [
        "Hahaha bro I thought you were asking for help with a different kind of orientation from skeletal data, and was extremely interested to see your research.\n\nGood luck tho. No suggestions, just thought I’d mentuon",
        "If the number of positions you need to recognize is limited, I would consider training a simple (e.g. linear, random forest) classifier  starting from a reasonable number of labelled \"skeletons\".  Some limited data augmenting and normalization is possible to improve training",
        "I can't help, but you're trying to figure out their pose, not their orientation. Their orientation would be which direction they're facing.",
        "If you track key points on a person (head, neck, shoulders, elbows, hands, …) to get their pose, then obtain the poses for each of the orientations you intend to classify, you can track the current pose and compute the similarity of the pose vectors. Probably using cosines is fine.",
        "I'd just show the image to some vision LLM like gpt4-v. In case you want to scale it, I'd consider using it to create the labels.",
        "Seconded this OP.\n\nI'd probably be good to interpret why the position is working or not based on the skeletal data instead of plugging another neural net on top of the pose key points.\n\nPlus this would take less training data than some neural network solution which is really nice.",
        "Honestly, there’s a lot of ways to do this but it depends on the data you are using. Where is the camera? what is it capturing? are there multiple targets?",
        "Yeah so currently we're just doing it with only one person, and the camera is in front of the person. Right now we're just trying to figure out how we would mathematically get the pose of the person in front of the camera since it's also our laboratory task.   You said law of cosines, can you explain further?",
        "You are comparing the current pose captured by the camera and represented as a vector to each of the possible poses you are attempting to classify. This comparison can be done via cosine similarity between two vectors. The dot product divided by the product of the magnitudes. the resulting value is between -1 and 1. With values closer to the extreme with more similarity but potentially different orientation."
    ]
},
{
    "submission_id": "1fshi3x",
    "title": "Training 6DOF object pose estimation models…",
    "selftext": "Hello! I've been reading a lot about object pose estimation using only RGB images. Models appear to have achieved strong accuracy with this input only. What I haven’t heard much about is the pipeline to create your own dataset and how general can instance level methods be, for instance, if I have several objects with the same geometry but slightly different texture, will the pose be accurately estimated? Can someone share their experiences :)",
    "created_utc": "2024-09-29T16:13:45",
    "num_comments": 1,
    "comments": [
        "Latest work around 6DPE focused on single-shot/few-shot methods, where you can use the trained model on novel objects. NVIDIA's FoundationPose would be the best atm.\n\nTheory wise, read the FS-6D paper, see how they perform texture and geometry augmentation and perform downstream 6DPE by inputting a few images of the desired novel object with labelled GT poses to estimate the same object in the wild."
    ]
},
{
    "submission_id": "1fscudw",
    "title": "How to Classify Dinosaurs | CNN tutorial 🦕[project]",
    "selftext": "https://preview.redd.it/h5f2gjdcxsrd1.jpg?width=1280&format=pjpg&auto=webp&s=4f0354c6b3ac5fc18c3edd069702579f6c1c899b\n\nWelcome to our comprehensive Dinosaur Image Classification Tutorial!\n\n \n\nWe’ll learn how use Convolutional Neural Network (CNN) to classify 5 dinosaur categories , based on 200 images :\n\n \n\n- Data Preparation: We'll begin by downloading a curated dataset of dinosaur images, neatly categorized into five distinct classes. You'll learn how to load and preprocess the data using Python, OpenCV, and Numpy, ensuring it's perfectly ready for training.\n\n- CNN Architecture: Unravel the secrets of Convolutional Neural Networks (CNNs) as we dive into their structure and discuss the different layers—convolutional, pooling, and fully connected. Learn how these layers work together to extract meaningful features from images.\n\n- Model Training :  Using Tensorflow and Keras , we will define and train our custom CNN model. We'll configure the loss function, optimizer, and evaluation metrics to achieve optimal performance during training.\n\n- Evaluation Metrics: We'll evaluate our trained model using various metrics like accuracy and confusion matrix to measure its efficiency and robustness.\n\n- Predicting New Images: Finally , We put our pre-trained model to the test! We'll showcase how to use the model to make predictions on fresh, unseen dinosaur images, and witness the magic of AI in action.  \n  \n\n\n \n\nYou can find more tutorials, and join my newsletter here : [https://eranfeit.net/](https://eranfeit.net/)\n\n \n\nCheck out our tutorial here : [ https://youtu.be/ZhTGcw0C3Dk&list=UULFTiWJJhaH6BviSWKLJUM9sg](%20https:/youtu.be/ZhTGcw0C3Dk&list=UULFTiWJJhaH6BviSWKLJUM9sg)\n\n \n\n \n\nEnjoy\n\nEran",
    "created_utc": "2024-09-29T12:41:36",
    "num_comments": 2,
    "comments": [
        "Nice",
        "Thanks"
    ]
},
{
    "submission_id": "1fscmw7",
    "title": "Object detection with NAS",
    "selftext": "I want to develop real time object detection model that will run on edge devices like Nvidia Jetson nano or RPi 5. I was looking into neural architecture search. Has anyone tried something like that and was successful? I know I can try with some predefined models like Yolos but I want the model to be as efficient as possible\n\nThanks!",
    "created_utc": "2024-09-29T12:32:36",
    "num_comments": 3,
    "comments": [
        "Consider something from rockchip like the rk3588. You’ll find the yolo series in the examples found in the rknn_model_zoo. I have had success with v4, v8, v10, and yolo world. I can get 40ms inference time for Yv8m using c++ and a quantized model. Quantization led to only a minor reduction in metric performance.\n\nI got what I needed out of those libraries above but there is also the repo edge-yolo which is said to be optimized for edge devices. You might like to investigate",
        "İ use the rpi ai kit, it has 13 tops ai performance and using hailo 8l ai chip,",
        "Yolo v6 already did that. Just use their models.\nAnd for speedup;\n\nJeston nano: tensor rt\n\nRpi: arm nn or pyarmnn"
    ]
},
{
    "submission_id": "1fsclsh",
    "title": "How long does it take for you to read and understand a typical paper?",
    "selftext": "It takes me quite a long time to fully understand a typical computer vision paper.  I usually need to revisit sections multiple times and research different topics to absorb everything.\n\nI’m curious—how long does it take for others? Does your experience in computer vision or related fields affect how quickly you grasp these papers? Share how you approach them and how long it takes you!",
    "created_utc": "2024-09-29T12:31:14",
    "num_comments": 21,
    "comments": [
        "Reading every third word of the abstract, then I discard the paper :) Takes 2 Minutes at most.\n\nMost papers don't do anything new and for ever paper you read, there a two new ones published.\n\nAnyway,  if they seem interesting, I skip to the results or conclusion section. If they become even more interesting, I fully read them. This can last up to 1-2 hours, depending on the knowledge I already have.\n\nDue to the fast moving and changing world I had to adopt and developed this way of keeping up to date with research. Ideally, I would like to dig deeper into some of the papers, but unfortunately I do not have the time todo so.\n\nThat being said. Keep on reading papers and develop you own way of handling them. Enjoy the time to fully read, some of them. \n\n(Btw. I doubt, reviewers of maybe even top tier conference fully read the papers, they are supposed to do ... But this is my own impression)",
        "I dont actually read most papers all the way through.\n\nNew paper: read abstract, skim a few figures while scrolling to the conclusion, read conclusion.\n\nIs it a typical mid paper? Done in a few minutes.\n\nIf it seems interesting, I'll go back to the figures and actually read them.\n\nNow it's about 10 min. Is it still interesting? (Rare) Then just a quick skim of the whole paper. Not actually trying to understand it all, just soak up the main points.\n\nNow it's about 20-30 min. Still interesting? (Super rare) \n\nThen skim their github page. Is it gonna be a bitch to implement? Do I really need to implement this? If yes, then read the whole paper and try to understand it.\n\nReaching this point only happens for mostly foundational models like vits, clip, dino, etc...\n\nThen it takes anywhere from 4 hours to god knows how many hours.\n\nTake this with a grain of salt though, I'm just in grad school. Haven't been forced to apply these papers to an actual prod environment yet.",
        "I don't spend time fully understanding every single paper. I start out with a quick skim read and try to get the gist of what they're doing - maybe a minute or two focusing on the abstract, conclusion, and figures.\n\n If I'm interested I'll read through fully, but still don't bother understanding every single aspect. I go straight through without going back - from front to back without stopping or slowing down, usually that means not really looking closely at equations and tables. I'll highlight and markup along the way (for example if something didn't make any sense I'll put a question mark)\n\nAfter that, I'm usually good and know enough about the paper to be able to describe it at a high level to someone.  Now if I need to fully 100% understand something, I'll go back to specific sections or equations and review them until I understand them. But it's rare for me to feel the need to do this since it's so time consuming. I only do this if it's something I need to implement or something I think is really worth fully understanding.",
        "People just skim through papers. I have found the average reading comprehension even from reviewers is sadly very low. I totally understand the need for this (too many papers out there, too much effort to properly parse the information), but I'd suggest at least trying your best with the key seminal papers when you're learning a new topic.\n\nAnd as someone else pointed out, reading the source code is a huge advantage when it's available. Might obscure the reasoning sometimes, but explains what was actually done down to the fine details better than any text can.",
        "Noob question but what does reading papers mean and why do ppl do it?",
        "Abstract -> Figures -> Results/Conclusions.\n\nSome are more organized than others. But this is how I typically read papers. I'm reading them to go about implementing something else similar. So this is really all I care about.",
        "I use pdf gear and chat with paper. I don't like to read papers, it is completely non-friendly",
        "For papers I find interesting: I hate reading them because its like Alice in Wonderland where I don't know how deep the rabbit hole goes: \n\n  \nPaper A -> *cites* -> Paper B\n\nPaper B -> *cites* -> Paper C\n\nAnd so on. And I can quickly end up too deep / lose track of time. \n\nFor papers I read out of sense of duty, its simpler: Abstract, key highlights, conclusion, done.",
        "Recent learning has been that , apart from skimming the abstract and architecture sections , Also look for references from popular sources like FAIR or Google etc and most of the times I find that the paper I set out to read is a minor variation or even a repurpose of same model to different data+task. Also the ref papers from big names usually have a git repo , hugging face models etc that come along which is lot lot easier than just trying to crack at  a random paper with not much resources.",
        "A few days to understand but I’m still learning the basics. It would take me months to fully inpliment one at this point! ",
        "Probably 30 mins to an hour. Used to take like 4 hours. Who knows, maybe I'll get it down to 15 minutes eventually.",
        "That’s great but what if you are reading a paper on a topic that you’re new to and you’re using this paperfor a project, how would you approach that?",
        "I came here looking to make a snide comment but you nailed it. Hunt the good stuff and then onto the next.",
        "Also underrated how common it is to find that the authors cherry picked or fibbed.",
        "I'm not sure if you're serious, but I'll answer your question. The basis of science, what schools teach as the scientific method, is to ask a question, obtain a hypothesis, formulate an experiment that is repeatable, perform said experiment, and then report on the result. The result is reported on the aforementioned papers. \n\nIn terms of computer vision, you'll have papers introducing new methods and showing how their method is better than previous methods (a.k.a. state-of-the-art or SOTA). There are also the occasional surveys, which are comprehensive compilations of methods or models to date.\n\nWhy do people read papers? Because no one wants to reinvent the wheel. Researchers build off of previous research to develop their own ideas. Companies will use papers to build their applications.\n\nI'm not sure where your interests lie, but there are many papers published daily on ArXiv. As a word of warning, papers are full of jargon since they're usually meant to be read by others of the same field. But you'll understand with some time.",
        "Imo, if you're trying to just understand the model itself, the least confusing way is to take an hour reading the source code of their the model implementation. Papers always leave out the important intricate details such as: \"What does their transform pipeline do?\", \"How the fuck is the loss actually calculated?\", \"how can I access the embeddings to do stuff with it?\"\n\nThe one thing I've never once seen yet is how papers generate those attention heatmaps for ViT architectures. They don't really talk about it in papers, and the code to generate them are never in the repo. If anyone has good resources on this, please let me know! I want to generate some of those myself.",
        "Papers are written for people that know about the topic. If you are not then you have to take a step back and read a tutorial/guide/course on the topic and then go back to the paper. That's what I do at least.",
        "Good one, I will try that. thanks",
        "What you want to search for is xai or explainable AI. There are a few techniques with varying performance on different inputs. Something like integrated gradients is relatively simple though.",
        "A new topic but not a new field, for example I have never worked with 3d reconstruction models, but I do have general knowledge in computer vision and deep learning so I’m able to understand what’s in the paper it just takes a long time, do u get me?",
        "Papers always have a related works. Read some of the papers in that section. You don't have to go too in depth, but enough so you understand some references or get some more context for decisions"
    ]
},
{
    "submission_id": "1fsartn",
    "title": "Package for correcting fisheye distortion in an image",
    "selftext": "#optics #cv #fish\\_eye #cameras Just found an interesting package for correcting fisheye distortion in an image\n\n[https://github.com/duducosmos/defisheye](https://github.com/duducosmos/defisheye)\n\nhttps://preview.redd.it/yjcn6r5ghsrd1.png?width=1290&format=png&auto=webp&s=4f3677aaa3586356800500d5c33c2c0066e02b83\n\n  \n",
    "created_utc": "2024-09-29T11:12:51",
    "num_comments": 0,
    "comments": []
},
{
    "submission_id": "1fs7mc2",
    "title": "Has anyone achieved accurate metric depth estimation",
    "selftext": "Hello all,\n\nI have been working mainly with depth-anything-v2 but the accuracy seems to be hit or miss. I have played with the max-depth and gone through the code and tried to edit parts that could affect it but I haven't achieved consistently accurate depth estimations. I am fairly new to working in Computer Vision I will admit so it's possible I've misunderstood something and not going about this the right way. I had a lot of trouble trying to get Metric3D working too.  \n  \nAll my images will are taken on smartphones and outdoors so I admit this doesn't make it easier to get accurate metric estimations. \n\nI was wondering if anyone has managed to get fairly accurate estimations with any of the main models out there? If someone has achieved this with depth-anything-v2 outdoors then how did you go about it? Maybe I'm missing something or expecting too much of the models but enlighten me!",
    "created_utc": "2024-09-29T08:57:10",
    "num_comments": 30,
    "comments": [
        "Metric depth estimation from single images is fundamentally intractable in the general case. There is no difference from the point of view of a camera between a scene and the same scene scaled down 10x or a picture of a picture of the same scene. All can be made to render as approximately the same pixel values.\n\n\nIf you constrain the problem by adding extra information like assumptions about the image being taken in a certain context you can get in the ballpark of accurate but even the state of the art models are not close to centimeter or even decimeter accuracy most of the time. I doubt they ever will be. That they work as well as the do is really cool. And if all you care about is relative positioning they work fairly well. \n\n\nMost cases don't need accurate estimations, even humans rely on tools to be accurate but our general inaccurate estimations helps us handle a lot of situations anyway.\n\n\nSo no, no one has figured it out yet.",
        "Well, even though there are some good methods out there for depth estimation, you have to accept that it nevel will be accurate given 2 dimensional coordinate system. And the reason is a concept  called \"perspective projection\". You are projecting 3 dimensional space into a 2 dimensional and a lot of are lost in this projection. Depth happens to be one of them.",
        "I got fairly good results with depth anything v2. I used kitti weights and played with max depth. Even in indoor scenarios kitti performed well, I just need to decrease max depth. I didn't get good results from the other dataset which is supposed to be indoors.",
        "Apple has also open sourced depthpro which seems a big step forward: https://huggingface.co/apple/DepthPro",
        "Hey Metric3Dv2 and Unidepth are having the best results on Benchmarks. Metric3Dv2 has also a Huggingface page to test it. My Results weren‘t bad.",
        "Its been some time since i worked with depth estimation models but i remember MiDaS from to be quite good..",
        "I’ve gotten pretty good results from Google street view images with depth anything v2.  I’m using the 640px tiles api and found a fov that works reasonably well.",
        "I mean no one has figured out monocular depth estimation, but stereo and structured light depth estimates are reasonablely accurate (depending on many factors of course)",
        "And if you are on mobile you can often take advantage of stereo depth estimation since many phones have multiple camera. Especially on iPhone. Even though the baseline is often very small it can help a lot.",
        "Yeah this is what I suspected, it really is amazing tech.",
        "is it possible to get more accurate measurements if you have multiple cameras from different angles?",
        "Metric3Dv2 and UniDepth tried to solve the Problem by adding the focallength in the Model.",
        "I found that decreasing max depth worked only for certain distances. So for example I would decrease the max depth to 30 metres and then items 1m away were roughly accurate but something 5m away was way off. I could find a max depth that worked for the reverse too but not one that was consistent.",
        "the relative depth results are good, but have you tested for actual metric depth? like gathered ground truth data with metric depth information and tested it?!",
        "Thanks, I have seen that name when I've been researching. I'll give it a go!",
        "Not OP but could you elaborate more struggling with a similar task",
        "A good clarification, I thought it was implied that the question was not about that.",
        "Especially on iPhone? Why especially?",
        "Yes absolutely, though was you usually do is offset two camera from each other along the x-axis but pointing in the same direction. Then you use what you know about their relative positioning to compare pixel placements in the produced frames and use that to generate depth estimations. Look up stereo depth estimation and stereo cameras.",
        "In a certain range, it's fairly accurate. But you're right, for distant objects it's way off. It depends on the use case as well. I used it for object avoidance in robot navigation, so I only care about nearby objects.",
        "They tested the ground-truth metric depth in some benchmarks in their paper.",
        "Apple is currently putting a lot of work into their spatial computing meaning they put a lot of work into depth estimation on iPhone. Both stereo and for the pro models ToF.",
        "I tested on GT from around my area, standard outdoors, the results were not reliable at all, it seems that these researchers tend to fit their model on the evaluation benchmarks",
        "Are you talking about Lidar?",
        "Okay, which model worked better for you?",
        "Which camera did you use and which focallength in pixel did you use?",
        "Both. The non-pro versions of iphone don't have lidar but do depth estimation using stereo. The pro versions have lidar and do depth estimation based on a combination of lidar and image frames.",
        "Depthanything has the better looking depth maps but the individual depth values are way off\n\nMetric3Dv2 has slightly worse depth maps, individual depth values are better than DepthAnything, but still very incosistent from scene to scene and cannot be used\n\nfor an image with gt of 2m I got 1.3, 1.4. 1.6 in one scene, in another image with gt depth of 2m I get 0.8, 0,6, waaay to inconsistent to be used where accurate metric depth is needed",
        "My phone camera, I tried with ƒ=3000 (which is what I got from calibrating) and 2000, 1000, 500, 250 and the authors suggested 707 for metric3D,\n\nall couldnt produce consistent results because focal length is only used to scale the models result after they are predicted, so if they are off for a batch they will remain off",
        "Interesting, I was unaware of the non-pro ones. Thank you so much!"
    ]
},
{
    "submission_id": "1fs5p05",
    "title": "Transparent Filament",
    "selftext": "Hi! What computer vision is best for tracking transparent filament? We’re making a filament out of PET that’s why it’s transparent",
    "created_utc": "2024-09-29T07:32:19",
    "num_comments": 4,
    "comments": [
        "wut\n\npost example images",
        "I'm guessing you're wanting to check tolerances?  I'm not a CV expert but I think edge detection is probably easiest.  Or just setting boundaries",
        "You might want to look into using OpenCV for your computer vision needs. It’s pretty versatile and has tools that can help with tracking transparent objects. You could try contour detection or background subtraction techniques to enhance visibility.\n\nAnother option could be using depth sensors if you have access to them; they can sometimes pick up on transparent materials better than regular cameras.",
        "also post literally any additional information"
    ]
},
{
    "submission_id": "1frzrtb",
    "title": "Help with Implementing Face Authentication in Web App",
    "selftext": "Hey everyone, I’m currently working on my final college project and need to implement face authentication in a web app using FastAPI. However, I have no background in Python, AI, or Machine Learning and I’m struggling to figure out how to get started.\n\nMy goal is to build two functions:\n\n1. Face Registration – This will detect a user’s face, capture it, and save it in a folder.\n\n\n2. Face Authentication – Here, the user presents their face, and it will be compared with the saved face data from when they registered.\n\n\n\nI’ve been researching computer vision, but it feels too overwhelming without a proper background in these technologies. I’m not sure what tools or libraries to use for face detection and recognition, or how to go about saving and comparing the face data.\n\nDoes anyone have experience with a similar project or any advice on how I can implement this? Any tips on which libraries are beginner-friendly or tutorials to get started would be super helpful.\n\nThanks in advance!",
    "created_utc": "2024-09-29T01:29:35",
    "num_comments": 3,
    "comments": [
        "there are loads of projects doing this on github - start there",
        "Search deepface Github.\nOther than there is litteraly no one that can teach all these topics to you in a reddit comment. You must know most of it to able to do anything.",
        "man forgot to add the link"
    ]
},
{
    "submission_id": "1frvkur",
    "title": "reCamera on-board! The first Ultralytics YOLO11 native support AI camera for everywhere",
    "selftext": "",
    "created_utc": "2024-09-28T20:40:24",
    "num_comments": 14,
    "comments": [
        "The open-source, programmable AI camera featuring a modular design with 1 TOPS AI performance, reCamera is easy to swap camera sensors and interface boards for expanded functionality, and also comes with a custom Linux OS and a built-in web UI for quick setup. Everything at: [https://www.seeedstudio.com/recamera](https://www.seeedstudio.com/recamera)",
        "specs? is it faster than raspberry pi 5?",
        "Is the training of the model (e.g. yolo) possible with camera? \nE.g. Shooting some pics and training the model (not from scratch)?\n\nI mean running yolo_n is possible on a potato - training is harder",
        "Any benchmarks yet? The most important information is missing.",
        "Can it see Uv spectrum?",
        "How is this different than the OAK camera from Luxonis?",
        "Super excited about this!",
        "It is more than just a plug-and-play camera; it's a developer's playground, designed to empower and inspire!  \n✅Open-Source both in HW & SW, make it truly your own.  \n✅Equip it with Ultralytics YOLO native support and licensing for advanced target identification and computer vision analysis.  \n✅Built-in Node-RED no-code workflow, easy to call camera API and utilize NPU to load models directly onto the device.",
        "How does it compare to Raspberry Pi AI Camera that was just announced, based on Sony’s camera?",
        "Feel free to check out specs on GitHub: [https://github.com/Seeed-Studio/OSHW-reCamera-Series](https://github.com/Seeed-Studio/OSHW-reCamera-Series), or reCamera page: [https://www.seeedstudio.com/recamera](https://www.seeedstudio.com/recamera)\n\nWe deliver on-device 1 TOPS AI performance with video encoding 5MP30FPS. Actually reCamera is a totally different system with pi5: pi5 is powered by a quad-core 64-bit Arm Cortex-A76 @ 2.4GHz, while reCamera's SG2002 Soc is 1core C906@1GHz + 1core C906@700MHz.",
        "What goal are you looking to achieve, training a model on a camera? \n\nMost applications would look to do inference on the board enclosed with the camera.",
        "So no?",
        "Something like the Kendryte K210 (huskylens) but more powerful (faster recognition).",
        "yeah.. for chip performance, it's not at the same level... While, chip is also not the only point to determine hardware choice right? depending on what application scenarios you're going to deploy, and the cost."
    ]
},
{
    "submission_id": "1frv922",
    "title": "Autonomous Driving Research Project",
    "selftext": "I am pursuing Masters in AI and taking Computer Vision as a course this sem. We are required to do a research project which basically entails improving/enhancing an existing (recent) top research paper from conferences like CVPR, ICCV (and such). My project partner and I wanted to pursue something related to Object Detection, Depth Estimation, Optical Flow, or Lane/Edge Detection in Autonomous Driving space. However, after going though some 20-30 papers (out of 1000s of papers) we saw that all the papers were using large datasets like nuScenes, KITTI, Waymo etc. They also used to train on high end GPUs like A6000 (or higher) .. or if they used A3090, then they would use 3-4 of those GPUs .. We have only 1 A4050 at our disposal.. is there a way where we could make this work? We really wanted to pursue something in this space but seems like we would have to give up on it.",
    "created_utc": "2024-09-28T20:20:57",
    "num_comments": 11,
    "comments": [
        "I've been working on autonomous driving with semantic segmentation with A100, would love to collaborate on some conference paper \nResume - https://drive.google.com/file/d/1G-sLe0Od_ptwqWGSUo_jbr6bZuHCOyb3/view?usp=drivesdk\nujjwalpardeshi@gmail.com",
        "The big and/or multiple GPUs are mostly for productivity. Especially when doing research you need to do a lot of experiments and don’t want to be waiting forever. You can still do most with less gpu though.\n\nI don’t know what an A4050 is but if it’s the RTX 4050 on a laptop, it only has 6 GB of video memory which is not very much at all. Your main challenge would be using lightweight architectures and hoping your results extrapolate to bigger GPUs for better accuracy. \nPerhaps you could research fine tuning methods which need less memory. Basically these take an existing model and try to fine tune it for a different dataset or task while leaving most of the parameters alone.  ",
        "I read a case study once about a university that built a high-precision traffic model with a single Arm server: www.gigabyte.com/Article/gigabyte-s-arm-server-boosts-development-of-smart-traffic-solution-by-200?lan=en Of course the server had slots for two A100s: www.gigabyte.com/Enterprise/GPU-Server/G242-P32-rev-100?lan=en But maybe an expert like yourself can get something out of this? I'm not well-versed in this field but I thought the case study might be an interesting reference for you.",
        "A simple gpu with more than 12 GB of memory is quite efficient. But you have to put a lot of time. Like up to 10 days in training. And also you better be careful with hyper parameters. And always remember, stars are born under the heaviest pressures. Just kidding you are cooked😂.\n\nPS. My thesis is also around 3d object detection so i am quite experienced in this regard.",
        "Cool..I will email you in couple of days. Would love to collaborate with you on a research project.",
        "yeah..rtx 4050..my bad on that..but yeah, that was the option discussed, but couldn't find any smaller datasets..the thing is, with datasets like nuScenes or KITTI, if we have to just trim the dataset, we wouldn't know how the authors of the paper annotated the data since the images are interconnected, and direct reduction of dataset wouldn't be feasible?",
        "Thanks .. I got a measly A16 so I dropped autonomous driving and I'm now doing 3D cinemagraphs",
        "Sent u a DM",
        "Done 🤝\nujjwalpardeshi@gmail.com",
        "Hi Ujjwal .. sorry for the late reply .. I've emailed you .. from a ch......11@gmail.com id. Can you look at it and give a reply?",
        "Heyy pl check your mail :)"
    ]
},
{
    "submission_id": "1fruct9",
    "title": "Teach your VLM Pose Estimation with PoseText",
    "selftext": "",
    "created_utc": "2024-09-28T19:28:54",
    "num_comments": 0,
    "comments": []
},
{
    "submission_id": "1frmmso",
    "title": "SAM2 with no CUDA",
    "selftext": "Could I use the SAM 2 (Segment Anything Model 2) in CPU with no CUDA? I don't have a GPU but a have to run some tests.\n\nThank so much, if someone can help me.",
    "created_utc": "2024-09-28T12:53:39",
    "num_comments": 3,
    "comments": [
        "Technically you can, I did it with SAM but you will need some RAM, 16 GB could be enough. \nIt’s written with PyTorch therefore CPU inference is nothing special but it will be very slow.\nOn my system CPU inference for a single image took 2 minutes, with GPU 4 seconds. \nEnjoy!",
        "try [https://github.com/yas-sim/openvino-segment-anything-interactive-demo](https://github.com/yas-sim/openvino-segment-anything-interactive-demo)",
        "Ok, thanks for the help! I will try"
    ]
},
{
    "submission_id": "1frldo6",
    "title": "Recommended workshops at ECCV 2024? ",
    "selftext": "Any good workshops or lecturers to find in this years eccv? ",
    "created_utc": "2024-09-28T11:56:23",
    "num_comments": 0,
    "comments": []
},
{
    "submission_id": "1frjp7i",
    "title": "Issues getting desired result",
    "selftext": "Hello, i'm following a tutorial but its explanation is a bit vague so I can't quite achieve the results i'm looking for.\n\nIt goes from the first image (Grayscale image with blackhat filter) :\n\n[Image with blackhat filter](https://preview.redd.it/ke45lkzc6lrd1.png?width=826&format=png&auto=webp&s=4ba6e0e02d935350880209ecd1c6c84518e6c0a0)\n\nTo this image:\n\n[Image i want to achieve](https://preview.redd.it/j9fd8ywl6lrd1.png?width=752&format=png&auto=webp&s=8559d81ae67c5102b1c6a47ec4026aca960bd132)\n\n With this explanation:\n\nWe must do a series of operations that highlights a rectangular blob. Then, we can apply morphological operations to join together blobs filling in gaps between closely spaced objects.\n\nI imagine they used some kind of edge/gradient detector such as sobel and then some kind of blur, but i cannot manage to achieve this rectangular blobs in my image. Does anyone here have any idea about how they might have done it? Thanks!!",
    "created_utc": "2024-09-28T10:40:31",
    "num_comments": 2,
    "comments": [
        "As far as I remember you need to use contouring to capture enclosed adjacent pixels. Which in your case is the plate.",
        "Try a larger kernel for the blur filter and increase the number of convolutions (passes)."
    ]
},
{
    "submission_id": "1frj2hm",
    "title": "OpenCV On Web",
    "selftext": "My most recent side project is OpenCV On Web: a browser-based IDE for developing image processing applications. Unlike Jupyter Notebook, it runs entirely in the browser, eliminating the need for server infrastructure.Try out the edge detection demo: [https://opencv.onweb.dev/](https://opencv.onweb.dev/)",
    "created_utc": "2024-09-28T10:11:31",
    "num_comments": 9,
    "comments": [
        "This is some cutting edge stuff :P",
        "Why use this instead of colab?",
        "what’s the pricing?",
        "how did u do it? any code",
        "Almost uncanny isn't it?",
        "Google Colab runs on the GCP platform which means you have limited CPU/GPU resources or have to start paying a significant amount of money for it at some point.",
        "It is free, at least a basic version of it will remain free. Perhaps I add a paid version if people are interested in private projects or C++ support.",
        "Ba dum tss :D",
        "So, a sheep, a drum, and a snake, fall down uncanny valley ..."
    ]
},
{
    "submission_id": "1frg1pn",
    "title": "I made an open source gaze tracking model in python (GitHub in comments)",
    "selftext": "",
    "created_utc": "2024-09-28T07:54:43",
    "num_comments": 8,
    "comments": [
        "[GitHub](https://github.com/ck-zhang/EyePy)\nCheck it out, it comes with a cool interactive demo and is super straightforward to implement in your projects! Any suggestions are greatly appreciated.\nShare this to other subreddits if you want, I don't have enough karma to post elsewhere .",
        "Weird question.... Would I be able to use this to play videogames with my eyes?",
        "Hmm. How’s it handle someone with amblyopia?",
        "Wow that’s cool, you could work on making the tracking smoother and maybe add user interaction via eye blinking patterns",
        "Most definitely! That's definitely something I'll work on, I've never wrote virtual drivers before but theoreticlaly it should function like any other eye tracker :D. \n\n-edit: Just did some research, that's really hard, I'll be integrating with opentrack",
        "If the game allows it, sure\n\n\nThere's the Tobii eye tracker which does this for certain games. Also does some things on Windows too",
        "short answer: \nnever tested it but it should work\n\nlong answer:\nthe model behind this is very simple, it's a ridge regression model that takes in relative pupil position and head orientation to predict coordinates on the screen. If someone has amblyopia and the movement of that pupil follows some other pattern or just stays still, it should still work. That's one of the benefits of using a simple model. Still, hasn't tested it so can't say for sure.",
        "Good idea, I'll be working on blink detection"
    ]
},
{
    "submission_id": "1fre4hp",
    "title": "Help me understand the YOLOv9 Confusion-Matrix",
    "selftext": "Hello everyone,\n\nI'm currently using YOLOv9 for a university project, but I don't fully understand the provided confusion matrix. Why are there so many false predictions for the background images? It seems like none of the background images are predicted correctly.\n\nhttps://preview.redd.it/zxwoh9m8wjrd1.jpg?width=878&format=pjpg&auto=webp&s=063566694d28abe19f655d0c486ed8ba26536d3b\n\n",
    "created_utc": "2024-09-28T06:18:55",
    "num_comments": 10,
    "comments": [
        "The columns are normalized. All the columns are going to sum up to 1, regardless of how few false positives or true positives there are.\n\nThe background column indicates \"out of all the false positives, what fraction of them belong to a particular class?\" False positives here means that the model detected an object in an area where there's no object. Or it produced more than 1 prediction for the same object.\n\nSo 0.23 in the first cell of the column means, 23% of all the false positives were from that class (the class corresponding to the first row).\n\nMeanwhile, the bottom row indicates \"out of all the objects of that particular class, what fraction were not detected, i.e. were false negatives?\"\n\n> Why are there so many false predictions for the background images? It seems like none of the background images are predicted correctly.\n\nThese metrics are not image-level. They're object level. The background column or row isn't just referring to background images. They are referring to objects that were classified or misclassified as background. You would have a background column and row even if you had no background images.",
        "Is it object detection or classification?",
        "Thank you very much, i finally got it",
        "It's both. I'm trying to detect and classify different animal groups",
        "Let me try to reformulate it. Are you doing boxes or trying to classify an image as whole?",
        "Im doing boxes",
        "Then I guess it makes sense the way it is. The matrix is not about the images it’s about the boxes. \n\nFor that reason, the number of background samples would be near infinite (all possible false boxes in the dataset). Which is why the bottom right corner is empty. The „True background“ column on the right is then probably all false positives summed up over all classes.",
        "> The „True background“ column on the right is then probably all false positives summed up over all classes.\n\nActually that box is always empty, since there isn't any feasible way to determine true negatives in object detection (You would have to be able to get IoU between background region and a non-existent box which corresponds to background, which is not possible).",
        "Yes that’s what I meant with infinite. The column on the right has to be the sum of all „wrong“ predictions.",
        "Thank you, i got it."
    ]
},
{
    "submission_id": "1frd5u3",
    "title": "Exporting YOLOv8 for Edge Devices Using ONNX: How to Handle NMS?\n\n",
    "selftext": "",
    "created_utc": "2024-09-28T05:26:54",
    "num_comments": 2,
    "comments": [
        "If your target device can run Python you could use the Ultralytics library with the exported model.\n\nOtherwise you're probably going to have to reimplement the pre- and post-processing steps yourself (including the NMS).",
        "Might look into this nms from torchvision:\nhttps://pytorch.org/vision/main/generated/torchvision.ops.nms.html\n\nSeems to run fast enough for my application. Not sure how suitable it is on edge devices, goodluck!"
    ]
},
{
    "submission_id": "1fr7ny4",
    "title": "Minimalist Vision with Freeform Pixels",
    "selftext": "A minimalist vision system uses the smallest number of pixels needed to solve a vision task. While traditional cameras use a large grid of square pixels, a minimalist camera uses freeform pixels that can take on arbitrary shapes to increase their information content. We show that the hardware of a minimalist camera can be modeled as the first layer of a neural network, where the subsequent layers are used for inference. Training the network for any given task yields the shapes of the camera's freeform pixels, each of which is implemented using a photodetector and an optical mask. We have designed minimalist cameras for monitoring indoor spaces (with 8 pixels), measuring room lighting (with 8 pixels), and estimating traffic flow (with 8 pixels). The performance demonstrated by these systems is on par with a traditional camera with orders of magnitude more pixels. Minimalist vision has two major advantages. First, it naturally tends to preserve the privacy of individuals in the scene since the captured information is inadequate for extracting visual details. Second, since the number of measurements made by a minimalist camera is very small, we show that it can be fully self-powered, i.e., function without an external power supply or a battery.",
    "created_utc": "2024-09-27T22:53:16",
    "num_comments": 1,
    "comments": [
        "See [web page](https://cave.cs.columbia.edu/projects/categories/project?cid=Computational+Imaging&pid=Minimalist+Vision+with+Freeform+Pixels)\n\nOr [youtube video](https://www.youtube.com/watch?v=KC8s30clJSY)"
    ]
},
{
    "submission_id": "1fr7n8u",
    "title": "Looking for people to write CV Projects with",
    "selftext": "Hello! I have done research in 3D vision and wrote a couple of papers during my undergraduate studies. As I am currently not working on any major projects, I am looking to collaborate on machine learning and computer vision, particularly in 3D vision areas like NeRF, Gaussian splatting, and diffusion models. If you have experience in these fields and are looking to work on exciting projects, please feel free to reach out! I’m always open to learning new techniques and collaborating with others to push the boundaries of this field...",
    "created_utc": "2024-09-27T22:51:56",
    "num_comments": 3,
    "comments": [
        "No experience in these topics. \nWould love to start expanding on these.",
        "I have worked in indoor 3D Computer Vision \n\n-3D Reconstruction from a single RGB Image [Metric 3D, Zoedepth]\n\n3D goes hand in hand with 2D. Have done a lot of fine tuning using yolov8.\n\n-Scene Understanding [Detecting walls, floor, ceiling and furniture in the room]\n\nthis is to give you an idea what I have worked on so far, I'm happy to work together building some product related to anything computer vision, llm, and vlm.",
        "Would love to collaborate on some paper together \nResume - https://drive.google.com/file/d/1G-sLe0Od_ptwqWGSUo_jbr6bZuHCOyb3/view?usp=drivesdk\nEmail - ujjwalpardeshi@gmail.com \nWork - up6276@srmist.edu.in"
    ]
},
{
    "submission_id": "1fr7ix4",
    "title": "Project ideas for a fresher to land a job in Computer Vision Domain",
    "selftext": "Hey all, I am 2024 grad ECE, past 2yrs i have done projects in the domain of low vision systems, deflaring, defogging. But it is not helping me land jobs even though i have publications. So can u guys please suggest some good prjects which looks fair for the employers to hire me ? I desperately need a job. \n\n",
    "created_utc": "2024-09-27T22:43:32",
    "num_comments": 2,
    "comments": [
        "Create a library that gets 10k+ stars. That ought to attract attention.",
        "Find the holes in the area and try to fill or cover them"
    ]
},
{
    "submission_id": "1fr11kq",
    "title": "So, YOLOv11 just got announced",
    "selftext": "",
    "created_utc": "2024-09-27T16:25:24",
    "num_comments": 33,
    "comments": [
        "Another week another unofficial YOLO variant...",
        "Part of me wonders if some of these advancements as far as accuracy is concerned might be research groups managing to tailor their algorithm development towards the target dataset. I have been using primarily YOLO for work applications since YOLOv4 and have really noticed almost no practical difference in accuracy of results since then. Performance wise I like that they are able to make similar models run on (presumably) lower cost hardware.\n\nI’m just hoping that the python use options aren’t wildly different than what they currently are for the ultralytics package. I just got used to yolov8 lol",
        "Their whole process of publishing, licensing, etc is v shady tbh",
        "I think object detection applications shouldn't be considered to run on high performance devices because the model architectures we have should not be cared to spend that amount of power. After working on this area, i think they satured the amount of work and performance in object detection. All they do is yeah we are 2% faster than the previous model etc. it is like apple announcements these days.\n\nWhen new models are proposed, they should aim running on low power devices. Although most of the work done in that area is used in military applications.",
        "YoloV11 Patch notes:\n\nAdded a foojimaflip between the MLP layers and a whatsit as an activation function.",
        "Does anyone know when we might get to use it? They say it's gonna be 2% quicker than v10 and the m is gonna have 22 million less parameters without sacrificing performance - want to see what it looks like",
        "For the first time it seems that there is no improvement in performance and/or latency. It seems that there improvements in performance but as I understand it cannot be faster than v10.\n\nI wrote a github issue to have their answer: [https://github.com/ultralytics/ultralytics/issues/16566](https://github.com/ultralytics/ultralytics/issues/16566)",
        "did anyone understand if v11 is really faster than v10? considering that v10 does not need NMS. They are discussing it here https://github.com/ultralytics/ultralytics/issues/16566.\n\nAlso, looking at the plot showing the inference times, it looks like only yolo-medium is faster. From that plot, the other sizes seem to have the same speed, or even slower than v10. Also, as far as I understand the plot does not include NMS.",
        "Is there a white paper?",
        "One of the authors of the original YOLO, Ali Farhadi, started a company and recently announced Molmo, a state-of-the-art open-source VLM.\n\nhttps://molmo.allenai.org/blog\n\nTwo of the original authors of YOLO worked on it, Ross Girshick and Ali Farhadi. Ross Girshick has been part of many pioneering works in computer vision, including Fast and Faster RCNN, YOLO, RetinaNet, SAM and SAM2.",
        "Which of them are official ?",
        "Hey, can I ask you a question since you mentioned you are working with Yolo models? , after version 4, I've seen no paper or technical report that explains how to match labels or encode labels during training. So do they follow the same pricnciple of Yolo v3? I've had a very hard time understanding the labels encoding and assignments after v3.",
        "Im 80% sure they are again gonna create a matrix of circular dependencies to challenge anyone who dares to look into their \"open\" source code.",
        "I wonder the same, although there are some videos on YouTube comparing the models where you can clearly see the difference. Speed is important too",
        "I choose to stay away from them. Yolov4 + good mlops data engineering is more than enough.",
        "'like apple' is the whole point here, exactly 💯 \nI worked with yolo V9, V10 and now working with V11, they are not really different from each other, they are only pushing with improving their base 'pre-trained' model actually,\nThe improvement with V11 and V9 is like they saw some flows in some areas and fixed that, (still not acceptable as community view) and they also trained their base model more.(Or adding new pre-order classes)\n.\nThe Main concern is non-truly \"OPEN COURSE\" and bad documents (we need more details!)",
        "It will be released on Monday.",
        "They said it's 2% quicker than v8. It's not better than v9 on the high end or v10 at the low end. The model architecture is nearly identical to v10. They just increased parameter counts to get \"better\" results.",
        "Yeah I saw their response, almost looks autogenerated and reeks of meaningless  corporate jargon",
        "Can't seem to find any",
        "Molmo is nice!",
        "Until v4 as far as i remember. Other versions starting from v5 are from different groups and ultralytics is the main group and they started to earn money from these models",
        "You don't have to take my word for it, but I thought I would chime in. The \"YOLO\" model was originally authored by Ali Farhadi and Joseph Redmon (as mentioned above). AFAIK, they worked on YOLOv1 through YOLOv3. YOLOv4 was done by Alexey Bochkovskiy and team. YOLOv5 was the first Ultralytics iteration and moved the codebase to python.\n\nI'm open to correction here because I honestly don't know, but AFAIK there is no broad \"official\" YOLO model. We say \"official\" about our models, but try to ensure we say \"Ultralytics YOLO\" when referring to them. There are lots of iterations, each developed by various sources, each of which could be considered \"official\" in some capacity for that specific organization or person. That's where the distinction of the \"official Ultralytics YOLO\" models makes a bit of a difference, but I don't think there is any singular model considered \"official\" since the original author (Joseph Redmon) has left the industry and given full usage rights to anyone and there is no governing body that determines what is or is not official.\n\nI'd bet there are lots of opinions of what is the \"official\" for anything. I won't argue on that, as everyone is welcome to their opinions. My take is that \"official\" shouldn't really matter unless you want to support a specific group. Instead go with what works for you or whatever you like most/best!",
        "YOLOv6 paper is quite nice with a lot of details that other papers leave out. It also has a section dedicated to label assignment.\n\nYOLOv8, YOLOv9 and YOLOv10 use TaskAlignedAssigner, which was found as performing the best by the YOLOv6 paper compared to other strategies.",
        "I use edge devices by Nvidia which require TensorRT format to work efficiently and till date I have tried v8, v5, v9, v10 and inception bet, and v8 works best \nV8m specifically gives the best balance between fps and performance",
        "I think yolox paper talks about it",
        "Was there an official announcement orrrrr",
        "So out of v8, v9, v10, and v11, which is the best?",
        "Sorry about that, I'll try to do better next time, but I honestly was trying to answer without any BS. The answer from Laughing-Q is the one to pay attention to, he's the one who knows best what's what, I'm just trying to help within the bounds of what I understand (I'm only 2 years into this myself).",
        "I see, thanks",
        "Well, do they follow v3 in that matter?",
        "Yes. That's the official date.\n\n\nhttps://x.com/ultralytics/status/1839606294019502533",
        "I don’t think so. Yolox uses FCOS I believe which is anchor free"
    ]
},
{
    "submission_id": "1fqpq3t",
    "title": "Blog post: Use cases of Robotics implementation in Agriculture",
    "selftext": "This [blog post](https://www.opencv.ai/blog/robotics-and-agriculture) explores how robotics and agriculture are collaborating and what startups are creating cutting-edge solutions in this sphere. It is not a technical post, but it can be useful for starting a thread. If you have more interesting Use cases or projects, please add them to the thread. It would be very useful to me. Thank you.\n\nhttps://preview.redd.it/aevgbsdi9drd1.png?width=910&format=png&auto=webp&s=fd20ae19bb99cdfcb3e8af80ed9812add581319e\n\nhttps://preview.redd.it/dpa6kxqo9drd1.png?width=910&format=png&auto=webp&s=10b309baaadf1efdd6f45ae2685ca425674b4a10\n\nhttps://preview.redd.it/xc742xqo9drd1.png?width=910&format=png&auto=webp&s=fd830d21d0630d364bbd61e7bb6865b80c1ace6c\n\n",
    "created_utc": "2024-09-27T07:58:29",
    "num_comments": 0,
    "comments": []
},
{
    "submission_id": "1fqpk9v",
    "title": "How is the scale determined in camera calibration",
    "selftext": "In [Zhang's method](https://www.microsoft.com/en-us/research/wp-content/uploads/2016/02/tr98-71.pdf), camera focal length and relative pose between the planar calibration object and the camera, especially the translation vector, are simultaneously recovered from a set of object points and their corresponding image points. On the other hand, if we halve the focal length and the translation vector, we get the same image points (not considering camera distortions). Which input information to the algorithm lets us determine the absolute scale? Thank you.",
    "created_utc": "2024-09-27T07:51:22",
    "num_comments": 8,
    "comments": [
        "The real-world spacing between the calibration target points gives us scale.",
        "if you halve the translation and the point positions, you get the same thing... not the focal length.\n\nyou can scale *a posteriori* using the real world measurement of the target points, but it's important to understand that scale is completely irrelevant: nothing in the image gives you any information about scale. This information can only come from real-world measurement, not from the images (inb4: that is, if you only care about the geometric aspect of the problem)",
        "Scale is determined by a known real world object distance. You will need to reference an object of known size to the number of pixels it covers. \n\nSimply, scale = pixels/mm (or whatever unit of linear measurement you are using)\n\nThe reason the example you refer to in open cv uses multiple images is because it’s calculating the lens distortion factor (the amount the image is “bent” compared to the flat object (grid). Once you get this you can “flatten “ the image and once you have the flat image you can calculate linear distance using the distance between the points on the grid (which are of known size)",
        "Thanks. Could you please elaborate more on which real-world spacing determines the scale? Using opencv routine (https://docs.opencv.org/4.x/dc/dbb/tutorial\\_py\\_calibration.html) as an example, is it the distance between corners on one view, or the distance between different views of the calibration pattern? At least in opencv, the distance between different views is not part of the input.",
        "Ah. Thank you. I guess my statement is true only when the planar calibration object is parallel to image plane. Another view with different orientation can resolve the scale ambiguity. Thanks",
        "Ah.  So no, you cannot independently determine scale of an object with just the camera calibration.  However if you independently know a real-world physical measurement on the object you can determine scale.\n\nSo if you have a object with an additional known measurement such as the spacing between the points you can recover scale.  Otherwise you can't.",
        "No, no additional image can resolve the scale ambiguity, only an external measurement. An example: in stereo, we can have metric measurements by measuring the distance between our 2 cameras, and then scaling our reconstructed scene so that it matches the real world measurement.",
        "Thank you. I think a tilted view of the planar calibration object is needed to disambiguate the scale, in which case the physical measurement of the distance between corners provides the information."
    ]
},
{
    "submission_id": "1fqow7m",
    "title": "Detect Water, Snow, Ice on a road surface",
    "selftext": "Newbe Alert!\n\nI've been asked to develop a computer vision application that detects how much water, snow, ice is on a road surface.\n\nImagine a pickup truck (with a camera mounted on top) driving down a road, we want to display the road surface and identify where any water,snow,ice is on the image.\n\nIs it best to use a custom CNN or is it better to start with a yolo pretrained model and train with a custom dataset?\n\nAny suggestions are appreciated.\n\n",
    "created_utc": "2024-09-27T07:21:49",
    "num_comments": 9,
    "comments": [
        "Instead of a normal RGB camera, other vision based sensors might be a better option coz it might be difficult to detect the said ice/water if it's transparent...",
        "This sounds more like a segmentation task than YOLO which only predicts boxes. \n\nIt would almost certainly mean training with a custom dataset, unless you find a segmentation model that was already trained on those specific categories. \n\nDo you have a bunch of video already recorded that you ca annotate and then train on? ",
        "Last comment before I have to get back to work.\n\nTraining data is absolutely where you should focus most of your development efforts. There a great rule of thumb for any ML project but especially ones like this where you’re trying to identify fairly subtle things like black ice. \n\nAlso, if you want to build a custom model you could take into account extra data like the air temperature, or even additional imagery from a thermal camera. This would probably help boost performance a bit but not a huge amount. ",
        "Something like this https://iwais2019.is/images/Papers/071_IWAIS2019_junfeng.pdf\n\nOr this\n\nhttps://www.mdpi.com/2075-5309/13/3/767?type=check_update&version=2\n\nYou should be able to find some datasets or even trained models from papers like these. ",
        "I agree, but I really don't know what other vision based sensors to use. I guess for now, I'll keep it simple and see how it goes. Thanks!",
        "I thought YOLO could be used to do segmentation.\n\nYes, we have images that can be used for training purposes.",
        "Both links appear to be exactly what I'm looking for. Thanks so much.",
        "YOLO as a name is pretty ambiguous nowadays. Originally it was a bounding box model, but people have developers versions with segmentation heads.\n\nIn any case don’t worry about the name of the model…and keep in mind licensing if this is for commercial use. The most popular version of YOLO is from Ultralytics which has a free license not necessarily compatible with commercial work. ",
        "No problem, and good luck! \n\nI just googled for “snow and ice segmentation” and there are a bunch of hits. "
    ]
},
{
    "submission_id": "1fqn845",
    "title": "https://zoom.us/webinar/register/WN_JFxgAvQ0RQqCF4re06rYcA#/registration\n",
    "selftext": "",
    "created_utc": "2024-09-27T06:04:02",
    "num_comments": 0,
    "comments": []
},
{
    "submission_id": "1fqmzj6",
    "title": "Datumaro CLI",
    "selftext": "Hello everybody , is attribute name modification possible with Datumaro CLI ? Thank you",
    "created_utc": "2024-09-27T05:52:18",
    "num_comments": 1,
    "comments": [
        "https://openvinotoolkit.github.io/datumaro/stable/docs/get-started/introduction.html\n\n>\nAnnotation conversions, for instance:\npolygons to instance masks and vice-versa\napply a custom colormap for mask annotations\nrename or remove dataset labels"
    ]
},
{
    "submission_id": "1fqm4od",
    "title": "Best realtime Object Detection algorithm for Jetson Nano ",
    "selftext": "Hello everyone \nI'm a student who is working on a project with autonomous vehicles. \nI have to build a realtime object detection model that detects 6 different classes.\nI have the labelled dataset.\n\nI have implemented yolov10 but I was facing 8-9 sec delay on jetson nano\n\nI'm up for all the suggestions. And if you have made something similar to it I'd love to hear about your experience. \n\nAlso if there is something already built and is available on internet please share the links",
    "created_utc": "2024-09-27T05:06:37",
    "num_comments": 11,
    "comments": [
        "you should export the model for tensorrt",
        "did you check gpu utilisation?",
        "Are you certain you're inferring on the GPU and not the CPU? You can also get about a 2x speedup if you implement your inference in TensorRT and C++.  There are also smaller, faster models than YoloV10.",
        "Are you working on Formula Student Driverless?",
        "https://letmegooglethat.com/?q=%22jetson+nano%22+object+detection+benchmark",
        "I read about it in many places but I couldn't figure out how",
        "I tried the yolov8 too\n\nAnd can you explain me the process for tensorRT and C++",
        "We are building our own project for that I need realtime object detection",
        "thats wonderful :D",
        "This is what you're looking for: https://github.com/wang-xinyu/tensorrtx",
        "TensorRT is an API for fast inference on NVIDIA devices.  Basically, you convert your model and weights into a TensorRT \"engine\" (a binary entity), which is then loaded and used for inference by your code. You can invoke TRT in Python, but doing so in C++ is faster.\n\nThe actual process of converting a model (for example, in PyTorch) and its weights to an engine can be as simple as exporting to ONNX and then generating the engine from that intermediate representation. Or it can be very complex -- if you have custom operators, for example.\n\nLuckily for you, someone has already done the hard work for you: https://github.com/hamdiboukamcha/Yolo-V10-cpp-TensorRT/\n\nI have not personally tried this repo, but at a quick glance it looks complete and easy to use.  Try it on a desktop (not the Nano) first to get the hang of it, and then once you understand the build and deploy process, install it on the Nano.\n\n**Important Note:** TensorRT engines are specific to the type of GPU used to generate them!  You cannot generate the TRT engine on your desktop using a 4090 (for example) and then expect it to run on the Nano. It won't.  You **must** generate the engine on the same type of GPU."
    ]
},
{
    "submission_id": "1fqigbg",
    "title": "Help IoU keras",
    "selftext": "Hi everyone!\n\nI'm trying to understand why Keras calculates the IoU value aggregating the images batches into a single one image and then calculate the value instead of calculates the IoU value for each image and average it by the number of images in the batch.\n\n  \nI'm noticing this by checking the documentation [https://www.tensorflow.org/api\\_docs/python/tf/keras/metrics/IoU](https://www.tensorflow.org/api_docs/python/tf/keras/metrics/IoU), in particular the source code.\n\n  \nThanks for any help.",
    "created_utc": "2024-09-27T00:46:53",
    "num_comments": 9,
    "comments": [
        "Are you aiming to compute IOU for segmentation or detection? I think these two are different. I say that since in segmentation you are dealing with each individual pixel as a prediction reference rather than a set of processed boxes. Hence, accumulating makes sense in this scenario because every pixel must be evaluated (and the link you provided explicitly mentions the segmentation), and per any input, a fixed number of outputs (masks) exists. So really doesn't matter if we accumulate or not, but the accumulation increases parallelism. On the other hand in object detection, each input has a different number of labels so the accumulation doesn't make sense.",
        "Just do it yourself using PyTorch if you want to really understand and have full control. Shortcuts like Keras get in the way. ",
        "Hi. Thanks for your answer! I'm performing segmentation task. Btw I checked and the results are quite different.",
        "I didn't quite catch what you mean by \"the results are quite different\", but what i can suggest is to write your own IOU function. Keeping your development in a specific platform criteria is often hard and time-consuming. You'll be better off writing your own utility functions.",
        "Yes sorry. I had the same problem with the Dice loss. I implemented my own Dice coefficient and consequently my own Dice loss averaging every single Dice coefficient of my images.\n\nFor instance I explain you what happened with the best epoch in the validation set. With the version batch-wise I obtain a Dice coefficient of 0.49 while with my version in which I calculated the Dice coefficient for every image and averaged by the batch size I obtained 0.43.",
        "Do you think is it better to train my model with my custom Dice Loss function (same thoughts also for IoU)? Thank you very much",
        "First of all, given the numbers you mentioned, i think you did a quite good job defining your loss. Second, don't expect to get the same result every time you run your code, the weight initialization is usually random and if only you have 1M parameters, each fp32, there are over (2\\^32)\\^1M possible settings, initialization plays a vital role in model convergence, even the dataset shuffling affect this (look up chaos theory). So I wouldn't worry as long as I get an acceptable metric evaluation. And yes I believe its always better to use your custom functions since you can modify and maintain them correctly.",
        "Thank you so much! I explain you even better sorry :). I trained the model (with the standard Dice loss, so aggregation of all the images) and saved it. Then I loaded the model and I perform two different actions:\n\n1. y\\_pred = model.predict(...) and then passed y\\_pred to my custom Dice coefficient calculation one image at a time and then I did the average -> this resulted in 0.43\n2. dice\\_coeff = model.evaluate(...) -> this results in the dice coefficient used in the training (the aggregation one) and resulted in 0.49\n\nAfter that discrepancy I decided to implement also my custom Dice loss for the training.",
        "Glad I could help."
    ]
},
{
    "submission_id": "1fqbu2r",
    "title": "Workout Recognition using CNN and Deep Learning",
    "selftext": "Workout Recognition using CNN and Deep Learning\n\n[https://debuggercafe.com/workout-recognition-using-cnn/](https://debuggercafe.com/workout-recognition-using-cnn/)\n\nDeep Learning and computer vision have immense potential in the field of exercise and workout analysis. It can recognize whether someone is doing an exercise wrongly and suggest changes according to the situation. But for this, the deep learning model first has to recognize a particular exercise. To tackle that, in this blog post, we will **train a CNN based deep learning model for workout recognition**.\n\nhttps://preview.redd.it/vu03d8nx29rd1.png?width=1000&format=png&auto=webp&s=1c190027533b10dc31c4067889844f932171324c\n\n",
    "created_utc": "2024-09-26T17:56:54",
    "num_comments": 0,
    "comments": []
},
{
    "submission_id": "1fq4lik",
    "title": "Is there a way to have SAM2 track the same player across scenes with no manual re-tagging?",
    "selftext": "",
    "created_utc": "2024-09-26T12:24:45",
    "num_comments": 18,
    "comments": [
        "Natively? No.\n\nYou can use some off-the-shelf Re-ID model to Re-ID the desired person and use the result to reprompt SAM-2 for different scenes.",
        "Train a classifier or encoder to recognize players by their bounding box (extracted using generic person detector). At the start of each video, find the player using the classifier/encoder and pass this to SAM2",
        "If you just want to tag him once have him tracked over a couple frames you could also reuse the eroded mask from the previous frame. If you just want a quick hack",
        "Collect few frames of the user. Mark the points for prompt. Use this information on all the frames.",
        "I think meta released detectorn2 along with SAM2 , maybe check that out?",
        "why not just use a face detector and see which face is most similar to the original player and then based on that you select the mask of that character.",
        "Combine pose keypoint detection with SAM2. Find krypoints in the 1st frame of the videk and use those as prompts for SAM2. After every X frames of the same video, get the updated keypoints by pose model and check what a keypoints  are already tracked by SAM 2 masks. Those that are't tracked get a new ID. \nThis method may be bit slow but the accuracy is good.",
        "I would say use a face detector / recognizer.",
        "Thanks for the feedback! I'm ok with not doing it natively and will look into Re-ID.",
        "Thanks! Training is something I'm willing to do, after checking if there's something off-the-shelf first.",
        "Could you expand on \"reuse the eroded mask from the previous frame\"?",
        "I think I follow the first two steps of this, but could you elaborate on what you mean by \"use this information on all the frames\"? \n\nMy understanding of steps 1 + 2:\n\n1. Collect a few frames where the player is in the shot manually  \n2. Mark an X/Y point within the player's mask on each frame  \n3.  ?",
        "Will do, thanks for the suggestion!",
        "What would be the process using detectorn2 ? I am not familiar with the model",
        "I think what they're saying is: SAM provides you a mask for the player in frame N.  Erode that mask a little and use it as a seed for segmentation in frame N+1.  This assumes the player doesn't move very much between frames.",
        "In sam2, I believe there is a prompting frame and then they track based on that. I think in the official demo you can prompt on the first frame and they track based on that. \n\nNow for every video you want to track a specific person\nYou use the manually collected data for prompting or start the process. Then you track based on this. Make sure the initially collected data for annotating points contains varying poses and backgrounds \n.For the first step you might need to change the code a bit.\n\n1. Collect data\n2. Annotate the data\n3. Before tracking read the data and initialize the model\n4. Track normally using the initialized model.",
        "TIL: dilate/erode have specific meanings in video. \n\nThis method wouldn't work when there's a scene cut though right? E.g. the player teleports to a completely different position in frame N+1 that doesn't overlap with their mask in frame N (or maybe doesn't even appear in frame N)",
        "Yes"
    ]
},
{
    "submission_id": "1fq2tr1",
    "title": "Learning image analysis ",
    "selftext": "Hi all,\nFirst and foremost I just want to apologise for any inconvenience.\nTo keep things short, I need to learn image analysis within the next few month. I have no prior knowledge of image analysis or any of that sorts.\nIf you can, i kindly ask that any of you please share any resources (beginner friendly) or advise.\nThank you",
    "created_utc": "2024-09-26T11:09:35",
    "num_comments": 2,
    "comments": [
        "I would suggest this github repo - https://github.com/bnsreenu/python_for_microscopists you can work your way from using basic filters to advanced computer vision projects. It also has youtube tutorials.",
        "Thank you for sharing. Skimmed through the repository, and it looks promising."
    ]
},
{
    "submission_id": "1fq2rzj",
    "title": "Gaussian Splatting to map indoor scenarios from FPV Drone footage",
    "selftext": "Hello everyone,\n\nI was thinking of using Gaussian Splatting from the original paper \"3D Gaussian Splatting for Real-Time Radiance Field Rendering\" to map out large scale indoor scenes like say a hotel lobby or a mall lobby from FPV drone footage.\n\nHeres what I am trying. I get photos from the videos after say about 15 frames. Then I am using the default arguments from the repo. The model seems to sort of localise to only one section from the video like in case of a lobby then the model will localise near the reception area or the sofa.\n\nI have been working in computer vision for quite some time by now but i don't have much knowledge in the computer graphics side of things so i cant wrap my head around what i am doing wrong.\n\nIf someone has worked on a similar project DM me.\n\nEDIT:  One thing that I forgot to mention is that some of the videos are like 3-4 minutes long.",
    "created_utc": "2024-09-26T11:07:27",
    "num_comments": 7,
    "comments": [
        "I assume you want to learn about how 3DGS works, which is why you're using the original paper? If your goal is to have a nice 3D reconstruction, I recommend using Postshot.\n\nThe model should be able to localise anything that you throw at it, as long as the camera poses are there. I assume you ran `convert.py` to get the camera poses? How many images do you have in total and how many images did COLMAP (from `convert.py`) match?",
        "With the method that i was using to sample images from the video I was getting around 20,000 images and after using `convert.py` the COLMAP would match hardly 20-30 images of only one particular scene from the video.",
        "Colmap doesn't work so great when there isn't sufficient overlap between the images, such that the features can be matched. How far/close to the walls and objects is your drone? If you could post your video or at least a few frames of your video, that would be ideal.\n\nAlso, gotta ask again: Why not use Postshot?",
        "From a video you shared, it seems as if the footage features the camera moving straight (i.e. without turning left or right), which might be a problem, from my experience. I've once tried to reconstruct a scene of a suburb while standing almost in the same spot, rotating the camera around myself, and the results were awful :) (around 5 images matched from the set of 500+, less than 50 points)\n\nThe reason is in the structure from motion internals: to estimate camera poses and points, there better be more than 3-5 photos of the same part of the scene (a set of future 3D points) from different angle. When there are two frames of the same part of the scene but with minimal relative camera \"displacement\", it's harder to establish the point coordinate and the camera poses accurately, since they are too close.",
        "I am trying to learn and understand how these models work thats why i went with the original paper.  \nFor now i was trying it on indoor drone videos for malls like this one.\n\n[https://www.youtube.com/watch?v=JMt9vzxAtvg](https://www.youtube.com/watch?v=JMt9vzxAtvg)",
        "Ya that makes sense but the idea is that i want to use footage from fpv drones to create the 3D representations.. and i am guessing that in case of a fly in scenario it wont be able to get the type of images that you are talking about.. Any idea how I can achieve that",
        "The thing you are trying to do is IMPOSSIBLE. You can’t create novel views from just one view at the input.  Straight linear flight of drone is not enough for high quality structure from motion (camera poses estimation) and for 3DGS reconstruction as well. \n\nYou can get very limited reconstruction only in case if you will use sequential sfm method (available in metashape and colmap, you need to run colmap locally and set this parameter manually, postshot doesn’t accept it) and then you will reconstruct only the frames which are having same camera poses with the initial ones.\n\nThe best way to understand what I mean - take a look Luma Flythroughs."
    ]
},
{
    "submission_id": "1fq0gnc",
    "title": "Simplest way to estimate home quality from images?",
    "selftext": "I'm currently working on a project to predict home prices. Currently, I'm only using standard attributes such as bedrooms, bathrooms, lot size, etc. However, I'd like to enrich my dataset with some visual features. One that I've thought of is some quality index or score based on the images for a particular home.   \n  \nIdeally, I'd like some form of zero-shot approach that wouldn't require finetuning the model. If I can use a pre-trained model for this that would be awesome. Let me know your suggestions!",
    "created_utc": "2024-09-26T09:31:49",
    "num_comments": 6,
    "comments": [
        "Try give a multimodal LLM eg QWEN2-VL some examples with ratings from 1-10 then ask for a rating 1-10 on your input image",
        "Pyimagesearch has a tutorial with a dataset. Probably the same as this: https://www.kaggle.com/code/amir22010/house-price-estimation-from-image-and-text-feature\n\nBasically I would just do it that way where you don’t try to extract certain features, but you just feed the entire photo or photos directly into the model along with your other data. Extract specific features during training as a form of self supervision if you want. That might help avoid overfitting and could guide the model to what you as a human think is important, but it will still let it consider other deeper features that you as a human can’t identify, like the subtle texture of finishes for example. The whole point of DL is to avoid feature engineering decisions. ",
        "This is a super good idea! You can do similar things with Molmo or feeding closed foundation models (openai, claude, etc) a series of prompts to look for whatever is helpful to you (wood cabinets y/n, wood floors y/n, bathtub y/n, type of exterior material, cracks in driveway, peeling/chipped paint, etc etc etc). They will do a very good job at getting you the right answers so as long as you, the human, know the things you're looking to identify, you can outline those for the model to spot.\n\nHope to hear how this goes for you!",
        "I actually did this with GPT-4o mini and the performance was satisfactory!",
        "I am currently using a Random Forest Regression model to predict the prices based on metadata. Do you know if I could incorporate this method into my existing pipeline? I lean towards using rf because it's fairly interpretable with libraries like dalex",
        "I suppose you could train an image classifier to infer the value from photos alone, and then remove the final classification head and feed the feature vector into your random forest. \n\n "
    ]
},
{
    "submission_id": "1fpzazv",
    "title": "Denoising IR images for Instance Segmentation",
    "selftext": "I have aligned depth and IR images from a public dataset, PKU MMD, where I'd like to perform instance segmentation on the IR images with a model trained on RGB. The quality is not great, as shown in the picture below, obtained with clamping followed by min-max normalization code here: [https://pastebin.com/3e9t48k9](https://pastebin.com/3e9t48k9)\n\nhttps://preview.redd.it/knhufgwpb6rd1.png?width=512&format=png&auto=webp&s=2864836f2879e9884257ea323e0582425e3456e0\n\nI'm wondering what obvious denoising I should apply. At the moment the results are surprisingly ok despite the terrible noise and domain shift from RGB to IR, but I'd like to get the best output possible, where the legs seem to be the biggest issue.\n\nThings I have tried:\n\n* light Gaussian blur\n* median filtering (3 by 3, 5 by 5)\n* temporal median filtering (3 by 3, applied pixel-wise)\n\nAny other ideas very much appreciated.",
    "created_utc": "2024-09-26T08:43:04",
    "num_comments": 2,
    "comments": [
        "You can try non local means denoising https://docs.opencv.org/4.x/d5/d69/tutorial_py_non_local_means.html. I've had good results applying that to noisy camera sensor data.",
        "I've had the best luck using a guided filter using the image to be denoised as both the guide and the source. I typically use a kernel size of one, but you may want to use a larger kernel. The epsilon value will need to be found through trial and error.\n\nDivide the image by 2^16-1 after converting to a float32 array (if you're getting a 14/16 bit depth frame from it), denoise with the guided filter, *then* normalize it\n\nAlso you shouldn't need to be clamping the image *and* normalizing. To normalize subtract the minimum pixel value from the image (which will set the min to 0) then divide it by the max value times 255 (for an 8 bit result)"
    ]
},
{
    "submission_id": "1fpz6uu",
    "title": "Which computer vision model (or LLM) for segmentation?",
    "selftext": "I'm new here. I have nearly 2,000 bee images with hand annotations of masks of the full body. I want to automatically generate \"head\", \"thorax\" and \"abdomen\" from this full mask annotation. I tried different tools for it. At first, I tried the segment anything model 2 (SAM 2), but it didn't work well since it tried to generate the whole body mask instead of the head (the mask of the head is ambiguous and has high overlap with the thorax without any edge between them). Because of the lack of edges, comparing two points for segmentation also didn't work (I tried both SAM and SAM 2). Then I tried CLIP and gave text prompts for separating \"head\" from the body. But it returned nothing! I also tried manually finding the smallest contours in the full segmentation which can be its head. But It didn't work as the head often has no separate contours. I tried a very simple approach, like \"take the first 20% of the full body mask as the head\", but I am also finding it hard to detect the orientation of the bee as well, since different images have different orientations. Please suggest me any algorithm or any AI tool which I can use to separate \"head\", \"thorax\" and \"abdomen\" from the bee full body segmentation. Note that sometimes the shape of the full body segmentation is very ambiguous, and it just looks like a cylinder, but the colors of different areas may help.",
    "created_utc": "2024-09-26T08:38:11",
    "num_comments": 8,
    "comments": [
        "Annotate some images with the segmentations you want and train a segmentation model using deeplabv3\n\nYou’re not going to find AI to magically convert full body segmentations for you ",
        "Have you tried GroundingDINO (with appropriate prompts) with subsequent SAM for generating segmentation masks from the boxes?\n\nAlso if CLIP didn't work, did you try lowering the decision threshold?",
        "What's your inference speed target? how about the target hardware for deployment?",
        "Hand annotate more. So let’s say 10 then train a simple segmentation model using whatever method is most convenient. Then use that model to auto-annotate another 50 images. Fix those manually. Train a second model and am have it annotate another 100. Fix those, and train a third model. You probably get it by now!\n\nThis works best if you train each model for a particular “photo shoot” for example if you have a video of a bee flying around, use one model for that entire video. Use a different model for other videos.\n\nI’ve seen this done in annotation tools and have seen it called “micro models”. ",
        "how long the annotation dataset should be? can I use something like few-shot learning? Or do I need a large dataset?",
        "Grounding dino with sam2 is really powerfull. If you use it for camera streams or videos, the track functionality speeds up estimations!",
        "And the final step is of course to combine all these semi automatically annotated images and train a single model. \n\nIf you want to continue using SAM or similar tools that’s still useful. Could save time by only having to manually split the body parts with a few drags of the mouse. At least no need to outline the entire body manually.",
        "Start with 10 then iterate from there "
    ]
},
{
    "submission_id": "1fpynz8",
    "title": "How do I learn about NeRF in order to do research on it ?",
    "selftext": "I know that is a very broad question but I am new to it and have to start somewhere.\n\nI see the latest works on nerf and try to check out their githubs (if available), but all of them are implemented in very different ways which makes it difficult for me to understand what piece of code does what.\n\nAny advice would be greatly appreciated.",
    "created_utc": "2024-09-26T08:16:44",
    "num_comments": 16,
    "comments": [
        "[https://arxiv.org/pdf/2003.08934](https://arxiv.org/pdf/2003.08934) is the original paper.\n\nNeRF doesn't really do reconstruction explicitly, it's a representation technique, with which you can do renders. There is a trick that allows you to extract the depth too, but it's usually of somewhat lower quality than what the renders suggest.\n\nThe main idea is using an MLP to encode the scene, given localized cameras, after having applied a Fourier embedding on the coordinates.",
        "Like some of the other comments said, NeRF is a technique for scene representation. I personally feel that the much more difficult part is in fully understanding how to propagate accurate camera measurements given a scene representation. The simple case is ray marching, it gets more complicated. \n\nHaving a solid understanding of your imaging system’s forward model can make it much easier to try arbitrary scene representation approaches, and back propagate through them to achieve desired results. I’d guess that non-NeRF papers do a much better job at explaining how to implement ray marching and similar algorithms. You’ll just need to write that in a way that’s differentiable, and backprop through your parameterized scene representation (i.e. the NeRF).",
        "Along with already great recommendations by others. I found [this](https://www.youtube.com/live/nRyOzHpcr4Q?si=VMN-L6fRcy2LQL-e) 2020 lecture video by Jonathan T. Barron ( one of the authors of the original NeRF paper) is very helpful. Have fun learning.",
        "My suggestions is to understand the following concepts:\n1. Understand how camera model work\n2. Understand how rendering works in computer graphic.\n3. Understand the different lighting models (you can look at how lighting is performed on video games, actually quite interesting).\n4. Start reading NeRF original paper and most relevant followings ( Gaussian splatting)\n5. Start focusing on your research work",
        "Check out this course \nhttps://www.udemy.com/course/neural-radiance-fields-nerf/",
        "Start with reading their respective papers. It's easier to trace which functions does what after having an abstract idea of the overall reconstruction pipeline.\n\nAddendum: an outdated NeRF survey paper https://arxiv.org/abs/2210.00379\n\nAddendum 2: [a professor with a lot of recent NeRF-related publications.](https://scholar.google.ca/citations?hl=en&user=7hNKrPsAAAAJ&view_op=list_works&sortby=pubdate)",
        "I recently completed a research project on enabling real time NeRF/Gaussian Splat training and rendering for robotic systems. \n\nMy advice is to start with NerfStudio. Make your own NeRFs and play around with the different models they offer. NerfStudio is open source, so you can see how they implemented things. \n\nOnce you’re comfortable generating NeRFs, it’s worth asking yourself what part of it interests you in particular. I focused on the image + pose estimation pipeline to enable real time NeRF training while a robot explores a space.",
        "Someone created this playlist. Some of the presentations are from the original authors. It helped a lot to gain the background and understanding which papers to read.\n\nhttps://youtube.com/playlist?list=PLL_ozUoLD18pm689k4RKCEVvtIirlKcjR&si=wfdz-h4vWbpKZaH9",
        "You can play around with implemented NeRF models with Nerfstudio. You can even write your own version and see how they compare. The best way to learn is to learn by doing.",
        "I do have a pretty good theoretical grasp of what NeRF does and how it does it (you know the underlying mathy stuff). \n\nIts the implementation thats the biggest hurdle for me atm.\n\nBut thanks for the suggestion tho.",
        "The first few units of this course have helped me gain a background. Such as Camera models, Perpective projection, generating rays and optimization.",
        "Thanks, I will check these out.",
        "There is a models section in the NerfStudio documentation. It has various models that one can study.",
        "Which paper/part are you trying to understand then? the official nerf github from Ben Mildenhall even has a notebook that isn't hard to follow. [https://github.com/bmild/nerf/blob/master/tiny\\_nerf.ipynb](https://github.com/bmild/nerf/blob/master/tiny_nerf.ipynb)",
        "\nI see you've posted a GitHub link to a Jupyter Notebook! GitHub doesn't \nrender large Jupyter Notebooks, so just in case, here is an \n[nbviewer](https://nbviewer.jupyter.org/) link to the notebook:\n\nhttps://nbviewer.jupyter.org/url/github.com/bmild/nerf/blob/master/tiny_nerf.ipynb\n\nWant to run the code yourself? Here is a [binder](https://mybinder.org/) \nlink to start your own Jupyter server and try it out!\n\nhttps://mybinder.org/v2/gh/bmild/nerf/master?filepath=tiny_nerf.ipynb\n\n\n\n------\n\n^(I am a bot.) \n[^(Feedback)](https://www.reddit.com/message/compose/?to=jd_paton) ^(|) \n[^(GitHub)](https://github.com/JohnPaton/nbviewerbot) ^(|) \n[^(Author)](https://johnpaton.net/)",
        "Not any specific paper just the more recent ones with public codebases from herehttps://paperswithcode.com/method/nerf ."
    ]
},
{
    "submission_id": "1fptzvp",
    "title": "Deploy Llama 3.2 Vision with LitServe ",
    "selftext": "",
    "created_utc": "2024-09-26T04:34:56",
    "num_comments": 0,
    "comments": []
},
{
    "submission_id": "1fptrc8",
    "title": "Undergraduate seeking CS/CV journals for first publication before graduation ",
    "selftext": "Hello fellow researchers and academics♥️,\n\nI'm a final-year undergraduate student majoring in computer science, with a focus on computer vision. I'm seeking advice on publishing my first paper before graduation. Here's my situation:\n\n1. I've completed a research paper in computer vision👀\n2. I'm in my last semester of undergraduate studies\n3. I'm looking for journals that are:\n   - Suitable for undergraduate-level computer vision research\n   - Open to first-time authors🥹\n   - Have a relatively fast review and publication process\n   - Indexed in SCI (Science Citation Index) if possible\n\nI understand this might be challenging given my undergraduate status and the time constraint, but I'm eager to try.\n\nI'd greatly appreciate any recommendations, personal experiences, or advice on navigating this process as an undergraduate. Are there any journals known for being more open to undergraduate research in CS/CV?\n\nThank you all in advance for your help and insights!🥰",
    "created_utc": "2024-09-26T04:20:30",
    "num_comments": 2,
    "comments": [
        "Ask your PI",
        "they let me figure out it myself😮‍💨"
    ]
},
{
    "submission_id": "1fptij6",
    "title": "Camera and Software for object sequence recognition and alert",
    "selftext": "",
    "created_utc": "2024-09-26T04:05:08",
    "num_comments": 0,
    "comments": []
},
{
    "submission_id": "1fpqqyl",
    "title": "Hello. I have a question.",
    "selftext": "Hello everyone how are you doing.\nI would like to ask you. I m learning computer vision with pytorch, and my question is what do I need to know to call my self a computer vision engineer? at least a junior.\nDo I need model deployment to s3?\nDo I need MlOps to get started in computer vision?\nThanks allot.",
    "created_utc": "2024-09-26T00:39:40",
    "num_comments": 8,
    "comments": [
        "Pytorch is not a computer vision platform. It's a machine learning platform which happen to be used in computer vision often.\nAnd start with basics. I don't care if someone has swallowed the entire pytorch, if you don't have any dip knowledge, you are not a computer vision engineer or scientist or researcher of whatever you want to call it.",
        "I'd say to call yourself a computer vision engineer you'd need a job with that title, lol. Good luck on your learning and make sure to study theory too, I'm also learning computer vision :)",
        "Id say it comes down to your ability to solve real world problems. Gain some experience deploying applications in the field and this will give you the credibility you’re looking for. \n\nI’ve worked in robotics and CV for 15 yrs, hired engineers for CV work. It comes down to your ability to find a solution using the tools available.",
        "Thanks 😊",
        "Thanks 😊",
        "Thanks 😊",
        "what is dip knowledge",
        "Dip knowledge of neural networks and math concepts I think 🤔."
    ]
},
{
    "submission_id": "1fppivl",
    "title": "3D point cloud making using 2 CCD cameras with know camera parameters ",
    "selftext": "I am a MTech IOT student, working on a robot project where I am using Monocular vision for making 3D map of surroundings.whole system is working on Respberry pi 4B (4 GB RAM).\n\n\nI am looking for existing image processing methods that i can use here.",
    "created_utc": "2024-09-25T23:08:11",
    "num_comments": 3,
    "comments": [
        "Look at ORB slam: https://github.com/raulmur/ORB_SLAM2 .  This will build a sparse feature map.  I don't have any immediately useful suggestions for a dense map with a monocular camera... if you can add an RGBD sensor then you can use the depth information from that registered with the trajectory information from ORB slam.\n\nI don't know if it will run on a Raspberry Pi, but I would highly recommend just getting something working first then focusing on speeding things up later.",
        "Side note, there are some stereo cameras available that can do this off the shelf if you are interested. Check out Luxonis and Intel Realsense. Great IOT devices with all the tools built in",
        "I appreciate your help and the products you mentioned, are very good at this but they cost from 140$ to 600$. \n\nSince this is my Minor project I want some methods which can be built from scratch or near to it. Like the triangulation method with template matching."
    ]
},
{
    "submission_id": "1fpphfo",
    "title": "Update with yolov5 yolov8",
    "selftext": "",
    "created_utc": "2024-09-25T23:05:07",
    "num_comments": 2,
    "comments": [
        "actually i havent  examined this code but you should find yolov5 section and you write yolov8 code here. i think it shoudl work.",
        "thank you very much"
    ]
},
{
    "submission_id": "1fpon38",
    "title": "Mlops tools running in colab",
    "selftext": "Hello i am using the google colab for training my ai model and i need  the mlops tool running in colab. Do you know",
    "created_utc": "2024-09-25T22:06:33",
    "num_comments": 1,
    "comments": [
        "i found, comet is work :D"
    ]
},
{
    "submission_id": "1fpng0n",
    "title": "Models to convert 2D floor plans to 3D designs",
    "selftext": "Are there any models available that is a able to generate 3D house/building designs from it's floor plans. If there isn't one, how would I go about creating one? What kind of data should I try to collect for training such a model? Any help is appreciated. ",
    "created_utc": "2024-09-25T20:52:57",
    "num_comments": 6,
    "comments": [
        "RemindMe! 24hours",
        "https://floorplancreator.net/",
        "I have built one part of it (the ai part). another part needs to be merged is computer graphics, someone who knows WebGL can do it.",
        "I will be messaging you in 1 day on [**2024-09-27 08:06:44 UTC**](http://www.wolframalpha.com/input/?i=2024-09-27%2008:06:44%20UTC%20To%20Local%20Time) to remind you of [**this link**](https://www.reddit.com/r/computervision/comments/1fpng0n/models_to_convert_2d_floor_plans_to_3d_designs/lozlxh8/?context=3)\n\n[**CLICK THIS LINK**](https://www.reddit.com/message/compose/?to=RemindMeBot&subject=Reminder&message=%5Bhttps%3A%2F%2Fwww.reddit.com%2Fr%2Fcomputervision%2Fcomments%2F1fpng0n%2Fmodels_to_convert_2d_floor_plans_to_3d_designs%2Flozlxh8%2F%5D%0A%0ARemindMe%21%202024-09-27%2008%3A06%3A44%20UTC) to send a PM to also be reminded and to reduce spam.\n\n^(Parent commenter can ) [^(delete this message to hide from others.)](https://www.reddit.com/message/compose/?to=RemindMeBot&subject=Delete%20Comment&message=Delete%21%201fpng0n)\n\n*****\n\n|[^(Info)](https://www.reddit.com/r/RemindMeBot/comments/e1bko7/remindmebot_info_v21/)|[^(Custom)](https://www.reddit.com/message/compose/?to=RemindMeBot&subject=Reminder&message=%5BLink%20or%20message%20inside%20square%20brackets%5D%0A%0ARemindMe%21%20Time%20period%20here)|[^(Your Reminders)](https://www.reddit.com/message/compose/?to=RemindMeBot&subject=List%20Of%20Reminders&message=MyReminders%21)|[^(Feedback)](https://www.reddit.com/message/compose/?to=Watchful1&subject=RemindMeBot%20Feedback)|\n|-|-|-|-|",
        "I meant some open source models that I could use, or any tips of creating one.",
        "Ah gotcha, didn't realize what subreddit I was in haha"
    ]
},
{
    "submission_id": "1fpnf7z",
    "title": "Need some suggestions in stitching images of a football match from two VEO camera feed to form a panaromic image view. The images are attached below along with the desired output.",
    "selftext": "https://preview.redd.it/8eqwqne1t2rd1.jpg?width=1600&format=pjpg&auto=webp&s=df7663b3ad2b1d59ac5f7d60217dcdaece1581e9\n\nhttps://preview.redd.it/91tu8122t2rd1.jpg?width=1600&format=pjpg&auto=webp&s=4ed025e6094740a134c7c9842c5351643c5ae792\n\nhttps://preview.redd.it/2jhg3fu2t2rd1.png?width=1080&format=png&auto=webp&s=94f668dc04c9017b7bcebf8b83bad76db4b5f1a5\n\n",
    "created_utc": "2024-09-25T20:51:37",
    "num_comments": 4,
    "comments": [
        "Perhaps CV might be an overly complex solution for this, might there be a simpler r/VideoEditing approach?",
        "Just images or video? This would be simple to do with something like PremierePro, or OpenCV if you need an automated process. Having used the Veo cameras before, waht's the intended use case here?",
        "Search for homography estimation in opencv. E.g. try to find examples of how the functions cv2.findHomography and cv2.warpPerspective can be used.",
        "Yeah I am intending to use it for videos, can you suggest me some ways to do it using open cv."
    ]
},
{
    "submission_id": "1fpf51a",
    "title": "Pokemon Card Detector Application",
    "selftext": "Hello!\n\nBeginner computer vision noob here looking to build a pokemon card detection app similar to a post mentioned previously in this subreddit: [https://www.reddit.com/r/computervision/comments/vvm36g/perceptual\\_hashing\\_match\\_pokemon\\_cards\\_through\\_a/](https://www.reddit.com/r/computervision/comments/vvm36g/perceptual_hashing_match_pokemon_cards_through_a/)\n\nI'm currently taking the approach of using perceptual hashing to match the cards and canny edge detection + find contours with opencv to extract the card out of the frame. This seems to work well when cards are put on a blank white/black background, but fails to extract cards in noisier backgrounds, if I'm holding the card up, or for cards inside a slab. I've attached some images to this post to what my progress is looking like with this approach, but was hoping for some feedback or advice to improve the extraction of these cards under these conditions. Would there be a better approach to take for these situations? \n\nThanks.\n\nhttps://preview.redd.it/5h2tkqqcr0rd1.png?width=772&format=png&auto=webp&s=e4794310d57751086e8a864c85ba10352c2daf18\n\nhttps://preview.redd.it/tly92wpcr0rd1.png?width=718&format=png&auto=webp&s=ebc9cbe877493e596c1588ccf0ae9a5001292c02\n\n",
    "created_utc": "2024-09-25T13:58:10",
    "num_comments": 10,
    "comments": [
        "Any reason you can’t train a neural network to detect pokemon cards in general (eg rotated bounding boxes or segmentation) and then do pattern matching to re-determine its pokemon and which exact card?\n\nYou will need to label a handful, but honestly you could likely make a decent synthetic data pipeline for card-annotation (eg superimpose the card on random backgrounds, rotating it in 3D space and changing the cards brightness/contrast/etc)",
        "I have a custom trained maskRCNN on XX,000 images including raw and slabs. I can provide an API that looks up against all Pokemon cards and provides real pricing data.\n\nWorks for Magic, Pokemon, Lorcana, and tons of other games",
        "Honestly it wasn't an approach I considered as I didn't have enough pokemon card  image data and having to label thousands of images didn't sound appealing either, so I took a simpler approach to this problem. I'm also a bit of a noob here so just trying to understand the approach you mentioned, do you mean create a neural network that would automatically detect the bounding boxes for a card in a frame, then passing that into the pattern matcher (I think perceptual hashing could still work here)? \n\nAnd thank you for the last point, looks like something I could try out too.",
        "This is interesting, got any thing on GitHub?",
        "Yes, exactly that. There are too many cards to make a YOLO model where each class is a card, as that would not scale or easily adapt, so a two-stage solution is good. I would find a way to make a vector of the segment/rotated-bbox (eg pretrained ViT activations used for linear probing) and either do a nearest neighbor or small classifier). I would stay away from more-traditional approaches here - as those thrive in highly repetitive environments and subjects which is not the case with pokemon cards and their environments (eg sleeves, top-loaders, etc on tables, shown on wall, etc.)",
        "Join our discord: Purplemana.com. Link at the top. We can chat this week.",
        "Alright thank you for the input! I've looked into YOLO briefly before, would training it on single class (a pokemon card) be a viable option, or would you recommend creating my own NN?",
        "Use YOLO OBB - you want to detect rotated cards so you need oriented bounding boxes",
        "I would recommend a recent YOLO approach (commercially licensable just in case), like this https://github.com/WongKinYiu/YOLO\n\n\nTry transfer-learning/fine-tuning on a segmentation output. I would first get your synthetic data pipeline working before YOLO. Get a pipeline where you have a script that can generate N images with a card with random augmentations in either a 2D or 3D (if you want to use Blender or similar). The important part is you need the segment mask for the card in addition to the image. Then you can convert these masks to the needed training format (YOLO, COCO, etc) and train a model where it can do well on other synthetic images and some of your real world images",
        "This won’t resolve the perspective warping if the card isn’t totally flat from the camera angle"
    ]
},
{
    "submission_id": "1fpc7i6",
    "title": "Medical report document parsing",
    "selftext": "Hey guys i am working on a project where i need to extract information from medical report image or pdf and convert it into json. I am currently doing it using qwen2 vl 7b model. Can anyone suggest a cheaper and less memory consumption approach",
    "created_utc": "2024-09-25T11:54:15",
    "num_comments": 6,
    "comments": [
        "Try llama3.2",
        "You might want to try using OCR tools like Tesseract. It’s open-source and effective for pulling text from images and PDFs.\n\nOnce you have the text, you can use simpler models or even regular expressions to extract the specific information you need.",
        "Have you tried Florence-2 model yet? You'll have to build few additional things around it if it works better",
        "I have just gone through the benchmark it is worse than qwen2 and also llama 3.2 vison model weights are larger than qwen2",
        "Not yet but will definitely look into to",
        "good to know, thanks for your reply"
    ]
},
{
    "submission_id": "1fpc4c2",
    "title": "What is the conventional way to convert temperature data to a typical thermal image?",
    "selftext": "Looking at typical IR camera images makes me believe they are **not simple isotherm images**. In other words, the thermal image pixels are not a simple translation from their temperature data. They are a sort of combination of temperature and edge enhancement.\n\nDoes anyone happen to know the typical way of converting a temperature map to a thermal grayscale bitmap?\n\nEdit \\[2024-09-25\\]:\n\nI already have the temperature map. For example, for an image with a resolution of 640x480, I have already obtained the 640x480 array of floating point temperature data in Celsius. They are quite accurate. I just need to convert this temperature 640x480 array to a typical 640x480 thermal image.\n\nI was able to convert the array to an isotherm grayscale image easily (i.e. map temperatures to brightness after normalizing) and it looked great. Again, it is not a typical thermal image that has edge enhancement. ",
    "created_utc": "2024-09-25T11:50:31",
    "num_comments": 4,
    "comments": [
        "There is no universal way. You need more specifics on hardware, run calibrations associating intensities of IR with temperatures, and then test these setup.",
        "Thank you for the comment.\n\nYour comment made me feel that my question was not clear, so I edited the question by adding the following:\n\nEdit \\[2024-09-25\\]:\n\nI already have the temperature map. For example, for an image with a resolution of 640x480, I have already obtained the 640x480 array of floating point temperature data in Celsius. They are quite accurate. I just need to convert this temperature 640x480 array to a typical 640x480 thermal image.",
        "If you have a temperature map you need to regularize (eg min-max -> 0-255) to make it a viewable image, if I understand the problem correctly? You have to select min and max values to clip your temperature map and then decide within those ranges which distribution makes sense (eg linear temperature to greyscale, log temperatures to greyscale, map temperature to distributions to greyscale, etc)",
        "Thank you. In light your comment, I edited my question again by making \"**not simple isotherm images**\" boldface and adding the following:\n\nI was able to convert the array to an isotherm grayscale image easily (i.e. map temperatures to brightness after normalizing) and it looked great. Again, it is not a typical thermal image that has edge enhancement."
    ]
},
{
    "submission_id": "1fpblc9",
    "title": "I want to do a project using RGB images and estimated depth maps with neural networks",
    "selftext": "Hello everyone,\n\nI’ve recently begun studying monocular depth estimation models, and the advancements in this field have been remarkable—particularly with models like Depth Anything, Marigold, and MiDaS. I’m planning to develop a proof of concept for a professor as part of my master’s in computer science, focusing on using estimated depth maps.\n\nI have experience working with PyTorch and am considering a project that either enhances RGB image-based neural network processes by incorporating depth data (RGB+Depth) or improves neural network systems that already utilize RGB-D inputs through more accurate depth maps.\n\nI’d appreciate any ideas or suggestions on how to proceed!",
    "created_utc": "2024-09-25T11:28:11",
    "num_comments": 4,
    "comments": [
        "RGB-D fusion isn't new. There are already old papers that tackle this task e.g. DenseFusion.",
        "Hi! \n\nFor monocular systems have seen some papers that tried combining fearures from segmentation with feautures from depth.\n\nMain reason is that when upsampling a low resolution depth map estimation, you can lose information near the edges. This is especially the case for mono-depth because with stereo-depth you have more info to work with.\n\nFor my project I needed real-time performance which was not possible using a depth segmentation refinement step.\n\nAlso one of the big problems is that Depth and Segmentation simtaneous requires datasets that have both annotated depth and segmentation masks. There are a few open source online, mostly synthetic or for autonomous driving. \n\nGood luck with your project!",
        "Check out some of the depth cameras available on the market today, there are some really great libraries to start building your work on top of. \n\nLuxonis, Intel realsense, etc."
    ]
},
{
    "submission_id": "1fpafz9",
    "title": "Looking for cameras that can transmit livestream data from a remote location (Without setting up a remote server)\n",
    "selftext": "I'm installing a camera to monitor a remote location.\n\nOne approach is to get PoE camera and connect it with a server on the same network running scrypted . I have some constraints and want to avoid setting up a server in the remote location.\n\nI am looking for a camera that doesn't need a server/mini pc setup.\n\n**This would ideally be a camera that has an inbuilt API/cloud that I can just query to get livestream data from anywhere.**\n\nMy end goal is to capture the livestream data and send it to my app/cloud and access it from any other network. I have access to a power point.\n\nReccos for cameras like these?",
    "created_utc": "2024-09-25T10:40:17",
    "num_comments": 4,
    "comments": [
        "You would need cellular data.\n\nYou could use a cellular hot spot any just about any cloud connected home security cam",
        "The smallest setup I know is a 4G router connected to a POE cable connected to a CCTV IP Camera.   \nIf you are less financially sensitive, Verkada cameras have essentially a mini video recorder built into each one",
        "Related Q. \"how do i access the feed in the camera from anothe device? (not on the same network)\"\n\nStream from the camera using RTP over internet.  Or push frames from the camera to a internet facing API.\n\nIf you dont want constant streaming make the client and server startup event driven (e.g. using message queues something like AWS SQS standard which is free for small use cases)\n\nI've not seen many products to do this and it generally needs a small computer like a Pi Zero to drive the camera\n\nThe code here is useful ... [https://www.reddit.com/r/msp/comments/1cs7hq9/best\\_way\\_to\\_stream\\_ip\\_camera\\_to\\_website\\_without/](https://www.reddit.com/r/msp/comments/1cs7hq9/best_way_to_stream_ip_camera_to_website_without/)",
        "gotcha - how do i access the feed in the camera from anothe device? (not on the same network)"
    ]
},
{
    "submission_id": "1fpa0vu",
    "title": "first time training YOLO for object detection",
    "selftext": "hello i am currently working on a school project and i am confused \n\nmy goal is to detect common road objects (e.g., pedestrians, vehicles) using a car-mounted camera. i will be using a pre-trained YOLOv8s model, accuracy n inference time is important\n\ni have the cityscapes dataset (5k images, converted to YOLO bounding boxes format) and found the Udacity Driving Dataset (19k images). I aligned the classes, but Udacity has fewer (e.g., trucks and buses are combined as \"trucks\").\n\ni will also collect my own dataset bcos there are vehicles here locally that are not in the available datasets\n\nhow can i approach this properly?\n\nshould I train the model on Cityscapes first, or my own dataset directly?\n\ncan I train on Cityscapes first, then add my own dataset for the new classes?\n\nis it okay to combine the cityscape and the udd then use it as a whole?\n\ni read about ‘freezing’ of layers but honestly i really dont get it that much. i am still learning also how and what fine tuning is.., im really new to this help is appreciated thank u so much in advance!",
    "created_utc": "2024-09-25T10:22:56",
    "num_comments": 6,
    "comments": [
        "Important thing: if you train on dataset A, then on B, the model will forget anything about A. This is called catastrophic forgetting and it's an open problem in AI. You have to combine all the datasets and align the classes if you want to train that model",
        "You will have to combine the datasets including your own into a single dataset if you want to keep the classes from all of them.",
        "Focus on your data set, it’s the most important part of your exercise. \n\nThings to consider:\n\nBalanced classes - if you have too many instances of a single class the model will learn that more and less of the other classes \n\nTight bounding boxes - don’t leave air gaps between the object and box\n\nTrain with the image size that you will run inference on\n\nConsistent bounding boxes rules, ie don’t cut of parts of the object in one example and include it in another example. I’ve seen this throw off models really bad \n\nLots of images, think 10k or more. \n\nCombine your data sets into a single DS, retraining with sequential DS will slowly skew you away from the previous data set \n\nBest way to think of training a model is just like fitting a line regression to a group of data points. If you change the data points the line will fit to the new DS. If you combine all the data into a single DS the regression is more likely to fit better\n\nSpending time with the data set is the least sexy part of AI but it’s the most important. Models can learn anything you give it, even garbage, so spend your time making sure all your data is clean. Think 80/20 with respect to effort, 80% of your time will probably be just reviewing your DS",
        "isn't this also fine tune? for example my have 2 dataset one has (a)general traffic sign label(square one, circle one ex) maybe 2 or 3 class, the other one (b)specific traffic sign(speed limit, stop sign, ex ) maybe 15  or 20 classes. if i using first a before b ,will my model be more successfully than just b.",
        "If you start from a pretrained model it's always a fine tune. But a fine tune doesn't mean that the model will be better, it means that it will take less time to train (if the data are in a similar domain). If you don't fine tune you eventually end up more or less to the same performance.\n\nIf you first train on A then on B it will take approximately the same time than just training on B to pair the performance. This is true if A and B are in the same domain, but in your case A and B are slightly different so it will take less time to train just on B.",
        "okay thx"
    ]
},
{
    "submission_id": "1fp9ee0",
    "title": "Understanding Machine Learning Practitioners' Challenges and Needs in Building Privacy-Preserving Models",
    "selftext": "Hello\n\nWe are a team of researchers from the University of Pittsburgh. We are studying the issues, challenges, and needs of ML developers to build privacy-preserving models. If you work on ML products or services, please help us by answering the following questionnaire: [https://pitt.co1.qualtrics.com/jfe/form/SV\\_6myrE7Xf8W35Dv0](https://pitt.co1.qualtrics.com/jfe/form/SV_6myrE7Xf8W35Dv0)\n\nThank you!",
    "created_utc": "2024-09-25T09:57:27",
    "num_comments": 0,
    "comments": []
},
{
    "submission_id": "1fp6e2w",
    "title": "I linked ComfyUI with Roboflow Workflows",
    "selftext": "",
    "created_utc": "2024-09-25T07:53:29",
    "num_comments": 1,
    "comments": [
        "I noticed a lot of computer vision people are great at theory and sometimes...lacking in actual coding and deployment.\n\n  \nSo I linked my two favorite drag & drop AI tools. ComfyUI + Roboflow  \n  \nOne generates images, the other analyzes.\n\nOpensource as always: [https://github.com/DareFail/ComfyUI-Roboflow/](https://github.com/DareFail/ComfyUI-Roboflow/)"
    ]
},
{
    "submission_id": "1fp5hul",
    "title": "Help dice coefficient calculation",
    "selftext": "Hi everyone!\n\nI implemented this version of Dice coefficient:\n\n    def dice_coef_per_batch(y_true, y_pred, smooth=1):\n        y_true = K.cast(y_true, 'float32')\n        y_pred = K.cast(y_pred, 'float32')\n        \n        y_true_f = K.batch_flatten(y_true)\n        y_pred_f = K.batch_flatten(y_pred)\n        \n        intersection = K.sum(y_true_f * y_pred_f, axis=1)\n        \n        dice = (2. * intersection + smooth) / (K.sum(y_true_f, axis=1) + K.sum(y_pred_f, axis=1) + smooth)\n        \n        return K.mean(dice)\n\nand then I'm using the Dice loss that is 1 - dice\\_coef\\_per\\_batch. At the end of each epoch during the  training there is the inference phase in which the dice coefficient on the val set is calculated. Then I save the model with the best weights and  I perform\n\n        y_pred_val = model.predict(X_val)\n        y_pred_val_bin = (y_pred_val > 0.5).astype('float32')\n\nand what I obtain is a validation Dice coefficient that is very similar to the best epoch in training phase but not equal. I think that the reason is that after training I round the labels while in training, the model uses the sigmoid output that is between 0-1. Is it a problem in your opinion? I'm thinking to use as a Dice coefficient the one with the rounded labels.\n\nThanks in advance.",
    "created_utc": "2024-09-25T07:15:12",
    "num_comments": 0,
    "comments": []
},
{
    "submission_id": "1fp58uh",
    "title": "Filtering Engagement Images using Computer Vision",
    "selftext": "[This project](https://github.com/AbdulHaseeb007/image_filteration_face_landmarks) has helped me a lot to solve my personal problem\n\nContext: I recently got engaged and my cousins were being ferocious about the photographer's images\n\nas they arrived they said \"send us our own specific images only, they are easy to download we cant download the whole album\" now i cant filter each image from 1500 to 2000 images  \nso i came up with a solution\n\nProject:  \n-> It takes a reference image - clear portrait photo   \n-> Source Directory   \n-> matched images More detail is in my Github\n\nNote: success rate of filtering images is aroung 90% (need refinement though)",
    "created_utc": "2024-09-25T07:04:16",
    "num_comments": 3,
    "comments": [
        "Very nice! Would it be improved by taking multiple reference photos? Or maybe that’s not needed. ",
        "Well that's a great idea for providing multiple reference images will try that soon\nThanks",
        "Well that's a great idea for providing multiple reference images will try that soon\nThanks"
    ]
},
{
    "submission_id": "1fp1muz",
    "title": "Which (& how) 3D deep learning framework to use for defective welding point detection?",
    "selftext": "Hi guys, I have a use case where i have to detect the defects (refer the images attached), I have a 3D laser scan camera from which I already have couple of point cloud images of these welding points, now I want to try implementing some 3d deep learning models , but I don't know how to annotate these images? how to implement open source models on that (like Point pillars). Most 3d open source models I found was mainly focused on Autonomous driving and works on Kitti dataset...\n\nAny Guidance would be really helpful ... Thank you 🤝\n\nhttps://preview.redd.it/2qa9sl3nsxqd1.png?width=198&format=png&auto=webp&s=06c91642c7fdd98cbbc5c53ee6895d6948421be6\n\nhttps://preview.redd.it/hdu9oqensxqd1.png?width=976&format=png&auto=webp&s=cd47cb0bf76932f7b73377266c373052161e627b\n\n",
    "created_utc": "2024-09-25T03:59:41",
    "num_comments": 2,
    "comments": [
        "I'm no export on 3D point cloud deep learning, but I can suggest you some alternatives:\n\n 1. Can you explain, why 3D? I can clearly see the difference in the 2D image. Maybe a 2D-approach is enough?\n 2. Do you have some objectively measurable requirements and tolerances to determine, what constitutes as a defect? If yes, an end-to-end DL-approach might be not optimal, since it will not ensure the tolerances are respected and only will rely on training data. Instead, you would probably need some classical methods or do segmentation of the PC with DL and then measure, the segmented point cloud.\n 2. Assuming you need the 3D information and have a PC of the whole joint: I (mind my 2D CV background) would convert it to a 2D problem (e.g. by doing a polar transform and then converting it to a height map). Then use anomaly detection, classification or object detection (depending on the defects and amount of variability) on the 2D elevation image. Deep learning on image data is a lot more developed and cheaper (e.g. annotation).",
        "For that scale, if you want 3D, take a look at Keyence. They have a laser based vision system that can pick up the detail you need. I think it’s the LJ series sensor"
    ]
},
{
    "submission_id": "1fp1kdz",
    "title": "Real sense r200 for £30 worth it?",
    "selftext": "Im looking to dip my toe in slam, mainly for localisation but also try some 3d mapping. \n\nThis is for a drone project where ill have rtk gps and the vision system combined for slam, maybe an imu as well. I want to track the location, this is the highest priority, and then suplement with mapping. \n\nNow i know the r200 isnt the best but for 30 quid its very minimal investment and i can try the software and learn. But i was wondering how well supported it is? Or if theres other options subtantially better and worth more investment? \n\n",
    "created_utc": "2024-09-25T03:55:14",
    "num_comments": 2,
    "comments": [
        "Here's a thread from a year ago saying it is dated but still very useful.\n\n https://www.reddit.com/r/intel/comments/yuzomp/does_it_worth_something_or_should_i_throw_away/\n\nSo, if it were me, I'd get it. It may even be the better idea  - spending £30 now and experimenting on a low budget - than immediately spending a lot more with so many unknowns.",
        "Thanks i got it for 25 quid. It looks like there's support in ros for it so should be interesting if it works."
    ]
},
{
    "submission_id": "1fp12h8",
    "title": "Issue with detecting car plates",
    "selftext": "Hey there,\n\nI've been working on an algorithm to read car plates, and it seems to do fairly well for plates that are nice and straight, but struggles a lot with ones slightly tilted (not to mention for pictures where plates have a change of perspective)\n\nMy approach is the following (what everyone seems to be doing on google/YT to be honest):\n\nResize image to 800x600, convert to grayscale, apply bilateral filter, detect edges with canny, find contours with cv2.findContours and then iterate trough the contours looking for the ones that have rectangle-like features (4 sides, right aspect ratio...). I also check wether the detected region contains blue, since it's a consistent characteristic in all European plates.\n\nI really cannot find anything different in the internet that improves the model (besides yolo/ DL models but i don't want to use that). What can I do differently/add to improve the algorithm?\n\nThank you so much!",
    "created_utc": "2024-09-25T03:22:02",
    "num_comments": 6,
    "comments": [
        "Well there is a reason they moved to DL. And that reason is exactly the problem you are facing. If something like Yolo is too heavy use mtcnn or some other single object detectors.\nBut if you insist on using old methods maybe use sift or HOG.\nAnd also image enhancements like CLAHE go a long way in these scenarios.",
        "can u please tell more about your use case ?\n\nlike from where are you getting your images, everything",
        "I had similar problems before, i used mmrotate to know and correct the object angle first",
        "Just a regular Yolov4 trained properly can detect almost any LP. All this magic related to alignment, color manipulation, etc is not required because modern OCR models (visual transformer-based) recognize LPs normally without image adjustments.",
        "I'll look into that, thank you!",
        "I took the images myself in the parking of my university haha. I'm doing this as a way to practice for a subject i'm taking this year (about computer vision ofc).\n\nThey are close up pictures of cars where you can clearly see the plate. I have 15 images where the plate is horizontal and 17 where it's at an angle (about 3/4). With the first, the algorithm works pretty well (even though for lighter cars it struggles a bit). As for the angled ones, I cannot manage to get it working properly. If want/need to see the code or images, i can send it to you or upload to git and share the link via dm if you prefer that. Thanks!"
    ]
},
{
    "submission_id": "1fp0j5i",
    "title": "Hiring Research Engineers -- Build the Next Generation of Expert Systems",
    "selftext": "Hey! We are Atman Labs. Our mission is to emulate human experts in software. To do so requires building systems that can comprehensively map knowledge in any domain and can explore that knowledge dynamically in service of a goal just like a human.\n\nHuman experts have spent thousands of hours looking at images, watching videos, and reading documents, implicitly forming extensive knowledge graphs in their brains which connect the various concepts they learn about. Today advancements in transformers combined with efficient compute and storage costs give us the opportunity to build large-scale knowledge graphs that connect any image, video, and piece of text on the internet. This serves as the knowledge foundation for an expert system. \n\nIf you are excited to research at the cutting edge of Computer Vision, NLP, and Knowledge Representation to realize this vision, as well as work in an interdisciplinary research team building the frontier of intelligent systems beyond LLMs, you'll fit well in our founding team. Let's chat :)  \n  \n[https://atmanlabs.ai/team/kr-founding-engineer](https://atmanlabs.ai/team/kr-founding-engineer)",
    "created_utc": "2024-09-25T02:45:10",
    "num_comments": 2,
    "comments": [
        "Hi, I’m an L4 engineer at Google. I’m extremely interested in ML (especially RL, I have completed David Silver’s course on RL on my own from YouTube and have attempted to implement AlphaZero on my own). I don’t have a masters or PhD but I’m a fast learner and very passionate about ML. Are there any open roles in your company could apply to?",
        "I think you should add founders’ background on your website. It’s quite important for a research role."
    ]
},
{
    "submission_id": "1fozdkr",
    "title": "Florence 2 model weights ",
    "selftext": "Hello,\n\nI want to use florence 2 but not as hugging face api. We have limitations to use external api in our company. Do you have any ideas? Please let me know any good alternatives. ",
    "created_utc": "2024-09-25T01:16:45",
    "num_comments": 2,
    "comments": [
        "https://huggingface.co/microsoft/Florence-2-large/tree/main you are welcome",
        "Thank you. Does this also have pretrained weights for inference?"
    ]
},
{
    "submission_id": "1foyw1v",
    "title": "Webcam for Optical Character Recognition and Object Detection",
    "selftext": "Hello currently we have a computer-vision based project. Our only problem left is that our webcam are cheap and not that good for text detection since the webcam is blurry. Do you have any suggestions on webcams you currently use in your setups? I'm eyeing on either Logitech C920 or Razer Kiyo Pro. We are using a Linux system so we need a webcam that is able to be used in Linux. Thanks! ",
    "created_utc": "2024-09-25T00:38:11",
    "num_comments": 0,
    "comments": []
},
{
    "submission_id": "1foy7q8",
    "title": "Whats the best Virtual Try-On model today?",
    "selftext": "I know none of them are perfect at assigning patterns/textures/text. But from what you've researched, which do you think in today's age is the most accurate at them?",
    "created_utc": "2024-09-24T23:46:45",
    "num_comments": 5,
    "comments": [
        "I was playing with this yesterday - it's trending on huggingface. Worked really well on the few images I tested :) \n\n[https://huggingface.co/spaces/Kwai-Kolors/Kolors-Virtual-Try-On](https://huggingface.co/spaces/Kwai-Kolors/Kolors-Virtual-Try-On)",
        "Zeekit and ARitize. They seem to handle the fitting pretty well, even if the details aren't always spot on.",
        "Not sure if it is the same implementation, sure looks similar if not the same as the virtual try-on implementation that is one of the \"spaces\" in stable diffusion forge UI: https://github.com/lllyasviel/stable-diffusion-webui-forge  I was playing with this last week, it works well.",
        "is it open source?",
        "Where could I try these models?"
    ]
},
{
    "submission_id": "1fox9k0",
    "title": "What evaluation metric for semantic segmentation can reflect the close but not overlapped masks? ",
    "selftext": "I was using mIoU as my metric for semantic segmentation earlier. But now I came across the following scenario.\n\nTo simplify the problem, I am using 1D vector as my example:\n\n    Ground Truth:    [0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n    Prediction 1:    [0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n    Prediction 2:    [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0]\n\nAlthough prediction 1 has no overlaps with ground truth, it is still obviously better than prediction 2.\n\nWhat metric should I use to reflect the success of prediction 1 and distinguish from prediction 2?  Thanks in advance.",
    "created_utc": "2024-09-24T22:37:55",
    "num_comments": 5,
    "comments": [
        "Hausdorff distance might be yhe one you are looking for.",
        "Maybe [Boundary IoU](https://arxiv.org/pdf/2103.16562)? I can recommend reading the paper - it has lots of very interesting information about image segmentation evaluation",
        "Probably something like surface distance or an other distance measure. \nAlternatively there are dice implementations with a threshold. \nFor some implementations see \nhttps://github.com/google-deepmind/surface-distance",
        "Not something I've seen in a paper but maybe truncated signed distance function from points from one mask to the boundary of another and then sum up the values and average by the contour length",
        "I don't think you want this metric unless you have a very strange task, and if it's that weird then you should probably provide context because it's not likely an efficient solution. \n\n Semantic segmentation isn't a task where it takes blobs and moves them around until they overlap the desired blob. It is determining the class of each pixel. It isn't truly better that it chose two incorrect pixels near the desired ones than two incorrect pixels far away."
    ]
},
{
    "submission_id": "1fouv3z",
    "title": "Can We Directly Generate 3D Models from 2D Images? Seeking Open-Source Solutions",
    "selftext": "Hello everyone,\n\nI am a novice in 3D modeling and want to discuss an open-source method for directly generating 3D models from input 2D RGB images. I have many textured 3D models in GLB format of buildings, as well as 2D RGB images of these buildings taken from different angles. This is an example from my dataset, and I only have RGB images without any camera intrinsic or extrinsic parameters. I’ve noticed that existing 3D reconstruction methods typically predict images from specific viewpoints and then compare those predicted images to the ground truth images to compute loss.\n\nSpecifically, is there an open-source method that can implement the following workflow: `pred_sence = model(image_list)`, and compute loss as `loss = loss1(pred_sence, trimesh.load('gt.glb'))`, or using PyTorch3D or other methods? I hope to compare the predicted 3D models with the actual GLB files, as this could allow for a more effective evaluation of the generated 3D model quality.\n\nThank you!\n\nhttps://preview.redd.it/x72dapxrgvqd1.png?width=3840&format=png&auto=webp&s=120d1c2af680a8e05c52f933721452621d6011dd\n\n",
    "created_utc": "2024-09-24T20:10:00",
    "num_comments": 12,
    "comments": [
        "This is an old field of study. Extracting 3d data from 2d images can be called photogrammetry generally, but there are a lot of flavors and expressions. There's plenty of open source libraries around this. Structure from motion is a good one to look into",
        "AFAIK there aren't many tools I'm aware of that build from unmatched images from different sensors.\n\n  \nI think there's a way to do it, and it might be possible purely through Deep Learning training, but I don't think it's going to be super simple.\n\n  \nI think you might be able to treat it as a 2.5D heightfield problem (most buildings are mostly concave for our purposes, ignoring bridges and outliers) and maybe extend from that. You could Use your existing models as training (if the license permits...) and use as input the various photos and maybe also extracted feature vectors from the photos. Providing time of day and light angle might also be useful inputs.\n\nI haven't looked to see if anyone has tried this yet, there might already be some papers about this.",
        "Thank you for your response. Could you please provide some open-source papers that directly generate meshes?",
        "Thank you for your reply. The issue that’s troubling me is that my input consists of only 4 RGB images, and as far as I know, heightfield images include an additional channel, which contains crucial data. Some large open-source projects like CARS, Danesfield, and S2P make use of this additional channel. I’m a newly enrolled student (starting this fall), and this project is for my thesis, with no commercial use. I will be mindful of the licenses, and most of them allow usage for research and learning purposes.",
        "try COLMAP or meshroom/aliceVision",
        "I'm not saying the INPUT images should have a Heightfield, I'm saying that you should make a neural network to OUTPUT the city as a (textured) Heightfield.\n\n\nYou could try to output it as a textured polygonal model but the neural network design to create topologically sane textured polygonal models is MUCH more complex that outputting a Heightfield with a color texture. \nIt sounds like you might need to learn more about the art before you'll be prepared to embark on a project of this complexity. ",
        "\\^ These are great free versions, but they did not do very well for my cases. If you are doing something for yourself (not business-related), you can look at RealityCapture and Agisoft. I found both of them to be great. Another way of generating 3D representation is through Gaussian Splatting. I found Jawset Postshot to be easy to start with.",
        "Thank you. I have used COLMAP, but I only have four satellite images from Google, Amap, GIS Online, and Tianditu. These four satellite images were taken at different times, from different directions, and under different weather conditions. In short, apart from the geographic coordinates, they have nothing in common. This makes it impossible for COLMAP to match any two images. In fact, COLMAP fails to find corresponding feature points in the first step of feature matching.",
        "Thank you for your feedback! I understand that outputting the city as a heightfield is a more feasible approach. I will further research the relevant technologies to better implement this project.",
        "Thank you for your response. I am a Ph.D. student, and I am struggling with my thesis (non-commercial use). I have noticed that RealityCapture and Agisoft are two well-established software programs, but they might not support training custom datasets. 3DGS is a very popular research direction at the moment, and many papers related to 3D presented at CVPR are about 3DGS. However, based on my understanding, 3DGS involves predicting images from specified angles, and these images are differentiable, which is fine. But to generate 3D models, most of these methods render the images to produce a 3D model. This rendering process disrupts the original gradients, making the predicted 3D model non-differentiable. As a result, the loss calculated between the predicted 3D model and the ground truth 3D model cannot be backpropagated. I am wondering if there is a network that can directly generate 3D models. Once again, thank you for your response.",
        "oh ok, your post said \"from multiples images\", sorry.\n\nYou can still try single image depth estimation (ex: Marigold: [https://github.com/prs-eth/Marigold](https://github.com/prs-eth/Marigold) ). That'll give you a depth map. You won't need extrinsics because it'll be in the camera reference frame, but you'll need the intrinsics to convert the map to 3D data.",
        "Thank you very much, I'll look into this repository"
    ]
},
{
    "submission_id": "1fou4hz",
    "title": "Product photography roadmap",
    "selftext": "Product photography model guide\n\nHi i am new to ai and generative ai field. I want to a feature where user upload a picture and describe the background and the model will generate a enhanced image with better lighting and background as per the prompt. Can nayone guide me on how can i build upon step by step or any open-source strating point",
    "created_utc": "2024-09-24T19:31:25",
    "num_comments": 2,
    "comments": [
        "you want it as a web based or like native apps as well ?",
        "Web app i just want to learn how the ai part works"
    ]
},
{
    "submission_id": "1fos0wa",
    "title": "Question about Yolov8, image size, and padding",
    "selftext": "I trained Yolov8 on my data.  \nMy images are usually of width \\~250 and height \\~100.  \nI used imgsz=640 as a training parameter.  \nThen, after the model was trained, I used imgsz=640, still, when doing predictions (\\`model.predict\\`).  \nBut when the prediction runs in verbose mode, the text log prints sizes such as \"353x640\".\n\nIsn't the image supposed to be resized to be 640x640 (using letterbox padding)? Am I doing something wrong?",
    "created_utc": "2024-09-24T17:43:27",
    "num_comments": 7,
    "comments": [
        "why would you train with padded images?",
        "This is due to how resizing is handled for the PyTorch model.\n\nhttps://github.com/ultralytics/ultralytics/issues/16339#issuecomment-2358362884",
        "My images don't all have exactly the same size. Also, I have seen people saying that Yolov8 is pretrained for images of size 640x640 so it makes sense to use that by default.\nWould you say it's incorrect?\n\nTo be clear, I didn't pad the images myself. From my understanding, Yolo itself pads images automatically if given a non-square image but passed a square ratio as imgsz parameter.",
        "I'm not sure I fully understand the implications.   \nSo in training, it resizes and then pads the images (to 640x640), but when making predictions (after training), the input this time isn't padded (the image is resized so its longest side is 640, but not the other side).  \nDoesn't have any negative effect on performance? Shouldn't the inputs during prediction use the same format as during training?",
        "if it were me, if the size of picture is small dont enlarge it beacues it softens the picture. i would do my picture resize same size for example 250x100 of instead this.  \nyou change yolo input imgsz this is a parameter",
        "Training uses augmentations such as mosaic, so it will usually not require any padding as the whole image is filled with actual pixels from images. But if you train with `rect=True`, it will behave the same way depending on whether the images in that batch have the same shape or not. It also behaves the same way in the last 10 epochs of training when mosaic gets disabled.\n\nThe difference is only in the padded area. You could either pad to make it 640x640 on both sides, but that's just adding more empty region just for the sake of it. Or you could just pad enough to make the shape compatible with the network input, saving some compute.\n\nIt will result in slight differences in confidence. You can export it to a different format such as torchscript if you want it to pad exactly to 640x640. It only does the minimum rectangle padding when using the PyTorch model.",
        "Actually, on your advice, I retrained it using 256 as a size instead of 640, and the performance didn't improve. It actually performed slightly worse (although the difference was small). In any case, it seems that for my case at least, size 640 works better."
    ]
},
{
    "submission_id": "1fooqa1",
    "title": "Need Advice on Computer Vision POC for Padel Tracking",
    "selftext": "Hey everyone,\n\nI’m working on a proof of concept for a project applying Computer Vision to the sport of padel, and I could really use some insights.\n\n**Here’s what I’m trying to achieve:**\n\n* Track the padel court.\n* Track players on the court.\n* Track the ball, including trajectories and bounces.\n* Identify and analyze the type of shot made by the player.\n\n**Constraints:**\n\n* Everything needs to be filmed from a single viewpoint on one side of the court behind one of the pair of players. (padel is played in a transparent-walled box).\n* The video input will come from a user’s smartphone, so the viewpoint will differ from the typical televised footage most models are trained on, much closer to the field and possibly with some angle difference.\n\n**What I've accomplished so far:**\n\nI don’t have a strong background in this area, so I’ve been relying heavily on Reddit, GitHub projects, and research papers (mostly tennis-related).\n\n* I’ve managed to implement YOLOv5 for player tracking, TrackNet (a CNN for ball tracking, often used in open-source tennis projects), and I’ve created a custom dataset/model for tracking the padel court.\n* Created a small project that brings all of this together. Given a clip of a padel game as input, the project outputs a video where the padel court is highlighted, the ball is tracked, and a minimap displays the real-time position of the players in relation to the court. It also shows the exact points where the ball touched the ground.\n\n**The problem:**\n\nEverything works reasonably well with TV-style footage, since both the external models and my custom dataset are trained on that. However, when using real-life footage from a phone at the side of the court, the entire setup breaks down.\n\nAs for shot recognition (9 different types in padel), I’m hitting a wall – it seems impossible with my current resources.\n\n**What I need help with:**\n\n* Is this feasible with limited resources (i.e., without a massive budget for data harvesting and labeling)?\n* If so, how would you approach this problem, especially considering the use of smartphone video?\n\nAny advice or pointers would be greatly appreciated! Thanks in advance for your help.",
    "created_utc": "2024-09-24T15:03:41",
    "num_comments": 2,
    "comments": [
        "start with training your model on data that mimics how your going to use the model. Sounds like you need to get out to the courts and play, I mean collect some real world data.\n\nFor the shot types you’re going to have to program a way to interpret the output of the model. I assume each shot has unique things about it, use those plus the way the paddle is swung or ball is struct to create an algorithm that can identify the shot type. Good Luck!",
        "had also the same idea today and I landed here :D\n\nI guess a wide view is necessary to track the court, players and ball at the same time. Maybe mounting the smartphone on the topcorner of a glass? Like with a suction holder like the ones in cars.\n\nHave you thought on how to discretize the data? Maybe the input of ball + player can give the serving side and then divide the data by points/serves?"
    ]
},
{
    "submission_id": "1foood0",
    "title": "Need Help with Object Detection on Small Objects",
    "selftext": "Hey everyone!\n\nI’m currently working on an object detection project and could use some advice. Here are the stats for my dataset:\n\n* **Trophozoite:** 15,838 (small objects)\n* **WBC:** 7,004\n\nThe challenge I’m facing is that the small objects often blend into the background or appear as random noise, which makes them really tough to detect. I’ve tried several versions of YOLO and even tested DETR, but despite using data augmentation and high resolution inputs, I haven’t been able to get past MAP50=0.6  for detecting Trophozoites.\n\nThanks!\n\nI've linked some images for reference with bounding boxes plotted for your reference \n\n[https://ibb.co/cvfbLPn](https://ibb.co/cvfbLPn)\n\n[https://ibb.co/58Rqsxr](https://ibb.co/58Rqsxr)\n\n[https://ibb.co/vBY6qD2](https://ibb.co/vBY6qD2)\n\n[https://ibb.co/vzkm0Lr](https://ibb.co/vzkm0Lr)\n\n[https://ibb.co/kQ2XMv2](https://ibb.co/kQ2XMv2)\n\n[https://ibb.co/2Mg2KjQ](https://ibb.co/2Mg2KjQ)\n\n[https://ibb.co/WkTGNdy](https://ibb.co/WkTGNdy)\n\n[https://ibb.co/NWpQMfT](https://ibb.co/NWpQMfT)\n\n[https://ibb.co/KhfSxLY](https://ibb.co/KhfSxLY)\n\n[https://ibb.co/r3w4fYD](https://ibb.co/r3w4fYD)\n\n[https://ibb.co/xHgzBxY](https://ibb.co/xHgzBxY)\n\n[https://ibb.co/Vxntr6J](https://ibb.co/Vxntr6J)\n\n[https://ibb.co/7zLVcZ7](https://ibb.co/7zLVcZ7)\n\n[https://ibb.co/5LzXVx4](https://ibb.co/5LzXVx4)\n\n[https://ibb.co/qRqRwQF](https://ibb.co/qRqRwQF)\n\n[https://ibb.co/y8PwTNk](https://ibb.co/y8PwTNk)\n\n[https://ibb.co/VVW8wSf](https://ibb.co/VVW8wSf)\n\n[https://ibb.co/TKN6KGV](https://ibb.co/TKN6KGV)\n\n[https://ibb.co/h8VpNZg](https://ibb.co/h8VpNZg)\n\n[https://ibb.co/ssRNJCd](https://ibb.co/ssRNJCd)\n\n[https://ibb.co/Sy0kPQW](https://ibb.co/Sy0kPQW)\n\n[https://ibb.co/ZHX0tBF](https://ibb.co/ZHX0tBF)\n\n[https://ibb.co/MnyG1B2](https://ibb.co/MnyG1B2)\n\n[https://ibb.co/x3hvwXJ](https://ibb.co/x3hvwXJ)\n\n[https://ibb.co/gVKS3Vp](https://ibb.co/gVKS3Vp)\n\n[https://ibb.co/55kfXpR](https://ibb.co/55kfXpR)\n\n[https://ibb.co/P5JF5wW](https://ibb.co/P5JF5wW)\n\n[https://ibb.co/BtJKwLF](https://ibb.co/BtJKwLF)\n\n[https://ibb.co/hmQx2fW](https://ibb.co/hmQx2fW)\n\n[https://ibb.co/WnvNGf5](https://ibb.co/WnvNGf5)\n\n[https://ibb.co/qm6HpG4](https://ibb.co/qm6HpG4)\n\n[https://ibb.co/jVKcXC1](https://ibb.co/jVKcXC1)\n\n[https://ibb.co/LS97xrt](https://ibb.co/LS97xrt)\n\n[https://ibb.co/QYD3gG0](https://ibb.co/QYD3gG0)\n\n[https://ibb.co/4KwCLjy](https://ibb.co/4KwCLjy)\n\n[https://ibb.co/YZcQnvV](https://ibb.co/YZcQnvV)\n\n[https://ibb.co/dQ9CWwN](https://ibb.co/dQ9CWwN)\n\n[https://ibb.co/7v0j20V](https://ibb.co/7v0j20V)\n\n[https://ibb.co/fDQ77rz](https://ibb.co/fDQ77rz)\n\n[https://ibb.co/yNnYYBF](https://ibb.co/yNnYYBF)\n\n[https://ibb.co/nQ0qF6c](https://ibb.co/nQ0qF6c)\n\n[https://ibb.co/tMvkbgY](https://ibb.co/tMvkbgY)\n\n[https://ibb.co/VQCqbWB](https://ibb.co/VQCqbWB)\n\n[https://ibb.co/6wwBFnW](https://ibb.co/6wwBFnW)\n\n[https://ibb.co/sC3K2Zv](https://ibb.co/sC3K2Zv)\n\n[https://ibb.co/BttxL38](https://ibb.co/BttxL38)\n\n[https://ibb.co/fMRxKMD](https://ibb.co/fMRxKMD)\n\n[https://ibb.co/k4zt6x2](https://ibb.co/k4zt6x2)\n\n[https://ibb.co/wKqCqWy](https://ibb.co/wKqCqWy)\n\n[https://ibb.co/H2Kg3Lg](https://ibb.co/H2Kg3Lg)\n\n[https://ibb.co/RTRXp7Y](https://ibb.co/RTRXp7Y)\n\n[https://ibb.co/P4HSMj6](https://ibb.co/P4HSMj6)\n\n[https://ibb.co/G91m0sz](https://ibb.co/G91m0sz)\n\n[https://ibb.co/L0gbsjm](https://ibb.co/L0gbsjm)\n\n[https://ibb.co/vx3c5t9](https://ibb.co/vx3c5t9)\n\n[https://ibb.co/6D4k29N](https://ibb.co/6D4k29N)\n\n[https://ibb.co/xzH7jBp](https://ibb.co/xzH7jBp)\n\n[https://ibb.co/LxvZjGv](https://ibb.co/LxvZjGv)\n\n[https://ibb.co/Dkyw81x](https://ibb.co/Dkyw81x)\n\n[https://ibb.co/g4M6bzG](https://ibb.co/g4M6bzG)\n\n[https://ibb.co/yVMfJy4](https://ibb.co/yVMfJy4)\n\n[https://ibb.co/5rMVqb5](https://ibb.co/5rMVqb5)\n\n[https://ibb.co/R4xKPP9](https://ibb.co/R4xKPP9)\n\n[https://ibb.co/VCnsBK3](https://ibb.co/VCnsBK3)\n\n[https://ibb.co/zVMCmsF](https://ibb.co/zVMCmsF)\n\n[https://ibb.co/4NgjY52](https://ibb.co/4NgjY52)\n\n[https://ibb.co/ZXSkLFC](https://ibb.co/ZXSkLFC)\n\n[https://ibb.co/KXz91fX](https://ibb.co/KXz91fX)\n\n[https://ibb.co/6ySCmh5](https://ibb.co/6ySCmh5)\n\n[https://ibb.co/Mfywqj0](https://ibb.co/Mfywqj0)\n\n[https://ibb.co/84sB2gW](https://ibb.co/84sB2gW)\n\n[https://ibb.co/N220QJH](https://ibb.co/N220QJH)\n\n[https://ibb.co/vjGtb70](https://ibb.co/vjGtb70)\n\n[https://ibb.co/rMzchP5](https://ibb.co/rMzchP5)\n\n[https://ibb.co/QCgTVDw](https://ibb.co/QCgTVDw)\n\n[https://ibb.co/k3qC8vN](https://ibb.co/k3qC8vN)\n\n[https://ibb.co/86JGQVB](https://ibb.co/86JGQVB)\n\n[https://ibb.co/hHTGW30](https://ibb.co/hHTGW30)\n\n\n\n",
    "created_utc": "2024-09-24T15:01:14",
    "num_comments": 14,
    "comments": [
        "Not sure if it's a problem of small objects as you said the data is hi res. I see 3 potential problems. \n\nFirst, your bboxes aren't good. A box should be as close as possible to object's borders, yours aren't specific enough. If you include too much background, your model will think that's what you're trying to find.\n\nSecond, make sure you annotate all necessary objects in every image. Some newbies make a mistake of labeling only part of samples, this confuses the model during training. \n\nThird, your problem inherently may be too difficult. I have no idea what is a trophozoite, but in many images they look too similar to other unlabeled (background) objects. The general rule is: if you can't properly detect things with your eyes, you won't be able to do it with a model too. You can try solving this with a bigger dataset if that's even possible.",
        "Are these detection boxes or annotation boxes ?  \nin both cases imporve your dataset and annotations",
        "Tiled inference and /or tiled training is a solution for this.\n\nHave you already tried SAHI ? If not look into it.\n\nAlternatively , try yolov8-p2 which is available in ultralytics.",
        "As you said, trophozoites are confused with noise. First, how can you tell the difference between the two? I'm zero expert on the domain and in some images I would 100% say that you missed some trophozoites, I really can't find the difference.\n\nIn any case, this is not a problem with the model, but a problem with the data. A bigger model wouldn't do better. \n\nWhat resolution are you giving the images to the model? Here you probably need to give the full resolution. Also check if there's some data augmentation that can introduce a domain shift (for example when a trophozoite is cropped)",
        "are you trying to detect on the high res images with yolo? im not expert but as far as i know yolo detects using 640x640 res images. what i did for my project is split up high res images in 640x640 res squares , detect and then reassamble the original image with labeled detection images (row and column indexes as names) , also gathering result data (detected objects , amount , etc.).",
        "Your source images are 1920x1080.    Your YOLO model inputs are probably 640x640.    Open up an image in photoshop/image editing program and scale the image down to 640x640 and you will see that those small objects are just noise.\n\nSo to deal with small objects you must retain the resolution of the source image.   To do this, simply crop the source image into regions of 640x640 and pass those regions into the YOLO model.",
        "Thank you for your detailed response, the data was annotated with \"experts\"\nfrom that domain, they really don't know (I believe) how they should annotate objects to help the model perform better, anyway now I have just to get better results with the same data ):",
        "I know the annotations are that bad 😞, I did not annotate these images, experts in medical domain did, and I have just to replicate their behavior with a model (I know that this is not a good approche but that's what they asked for)",
        "Can you provide me the link please can't find **yolov8-p2** on ultralytics!",
        "If you zoom enough you will see maybe the difference, in some cases I can see it without zooming, objects are with color degradation and splitted in two parts but not very clear, I'm confused now 😅, I just should get a little bit better results like 0,7 or something, I really know all the problems in the data and the annotations but can't do nothing about it",
        "this is what I would do but you need to be careful of the border cases - do overlapping sliding windows and reassemble the detections",
        "yeah i can understand even though annotations are easy to do but still this isnt their cup of tea  \nask them to annotate each and every possible instance they see or they can use auto annotators and later on you validate those annotations yourselves",
        "You can try loading the 'yolov8-p2.yaml'\n\nYou can also check out grounding dino if inference time is not an issue for you.\n\n\nCan you share the dataset with me so that I can also run some tests on it?",
        "What auto annotators do you propose?"
    ]
},
{
    "submission_id": "1fok6be",
    "title": "Best CV algorithm for Jetson nano ",
    "selftext": "I'm working on a autonomous vehicles competition and we are given a jetson nano(the one with Maxwell architecture and 4 gb ram)\n\nWe have to implement it on our vehicle.\n\nI ran my yolov10 on it but I was having a 7-9 sex delay. I haven't ran any stats yet but it's way below 30 fps \n\nAny recommendations for what should I do\n\nFor the competition we have to detect only 6 classes and I have the dataset ready for it",
    "created_utc": "2024-09-24T11:51:50",
    "num_comments": 12,
    "comments": [
        "Use DeepStream and TRT-optimizied quantised tiny models like YOLOv4, YOLOv10 is also fine but must be cooked properly.",
        "Take a look at tensorRT and ONNX",
        "secs != sex",
        "export models to tensorRT format - yolo allows you to export to .engine formats that is at least 3-4 times faster",
        "Guys! If you don't know why your software is slow... use a profiler! In any case, I don't have enough information to understand why your model is slow, but using tensor RT with the image preprocess in GPU without coping frames will likely run above 30fps even in python in a single thread.",
        "do PTQ and run with c++, very fast, tested with v4 can do 70+fps",
        "Depends on how quick you are",
        "If you want I can share it with you\n\nI didn't do anything extraordinary Im just using the coco weights and only enabling the classes I needed",
        "Can you tell me the steps in detail?",
        "🤣",
        "check out the yolo\n V10 official docs: https://github.com/openvinotoolkit/openvino_notebooks/blob/0ba3c0211bcd49aa860369feddffdf7273a73c64/notebooks/yolov10-optimization/yolov10-optimization.ipynb\n\nAnd tensorrt :https://docs.nvidia.com/deeplearning/tensorrt/developer-guide/index.html\n\nAlso I am certain that you can find some example project on GitHub!",
        "\nI see you've posted a GitHub link to a Jupyter Notebook! GitHub doesn't \nrender large Jupyter Notebooks, so just in case, here is an \n[nbviewer](https://nbviewer.jupyter.org/) link to the notebook:\n\nhttps://nbviewer.jupyter.org/url/github.com/openvinotoolkit/openvino_notebooks/blob/0ba3c0211bcd49aa860369feddffdf7273a73c64/notebooks/yolov10-optimization/yolov10-optimization.ipynb\n\nWant to run the code yourself? Here is a [binder](https://mybinder.org/) \nlink to start your own Jupyter server and try it out!\n\nhttps://mybinder.org/v2/gh/openvinotoolkit/openvino_notebooks/0ba3c0211bcd49aa860369feddffdf7273a73c64?filepath=notebooks%2Fyolov10-optimization%2Fyolov10-optimization.ipynb\n\n\n\n------\n\n^(I am a bot.) \n[^(Feedback)](https://www.reddit.com/message/compose/?to=jd_paton) ^(|) \n[^(GitHub)](https://github.com/JohnPaton/nbviewerbot) ^(|) \n[^(Author)](https://johnpaton.net/)"
    ]
},
{
    "submission_id": "1fodqcm",
    "title": "Easily process LLM video datasets",
    "selftext": "",
    "created_utc": "2024-09-24T07:23:35",
    "num_comments": 0,
    "comments": []
},
{
    "submission_id": "1foch6f",
    "title": "Is it good idea to buy NVIDIA RTX3090 + good GPU + cheap CPU + 16 GB RAM + 1 TB SSD to train computer vision model such as Segment Anything Model (SAM)?",
    "selftext": "Hi, I am thinking to buy computer to train computer vision model. Unfortunately, I am a student so money is tight\\*. So, I think it is better for me to buy NVIDIA RTX3090 over NVIDIA RTX4090\n\nPS: I have some money from my previous work but not much",
    "created_utc": "2024-09-24T06:27:48",
    "num_comments": 31,
    "comments": [
        "the ram feels like an issue,  \nam not sure how big SAM is, but imagine the training will take quite some time (days maybe) and many tries to find tune as well. This is not good on consumer GPUs to be honest. it's gonna stay hot for long durations which could eventually wear down the gpu.\n\nKaggle or  colab with google drive(to save your checkpoints and restart the training after 12 hours) are better options for you.",
        "OK I'm gonna break it down to you.\nIf you use SGD as optimizer, it stores 8 bytes per parameter, and let's say you have a model in which each layer have 5M activations and there are total of 10 layers.  Also somehow each layer have around 1 million  parameter(don't worry if the numbers don't match, pay attention to calculations). \nAlso consider batch size of 4 , and each input have 10,000 dimensions.\nSo total of 100 million parameters(10 layers *10 mil) , each parameter is fp32 so 4 bytes is needed to store one, optimizer stores 8 bytes per parameter so 12 in total, ~1.2 Gb only for parameters, also you have around 50M (10 layers 5M each) activations, which is fp32, so 200MB for each input and if you use batch size of 4 it results in 800MB for activations.\nYou can ignore the inputs their sum is around 160KB\n\nAround 2GB(1.2+0.8) of vram(ram if you use cpu) is required to train this model.\n\nThese numbers where mentioned for the sake of simplicity and for you to get the idea, given this calculations you can compute  how much ram is required for a specific model.\n** you may find these calculations to stand false in keras, which has something to do how keras manages memory but don't worry. They are valid.",
        "Why would you want to train a SAM model anyway. The kind of data you need to make it learn good representations is not something a single consumer gpu can do in a realistic amount of time. Either get 4 A100s or just use gcp or something. Buying a single consumer grade gpu to train anything more than toy models is a waste of money.",
        "As a person who stayed in this situation, having a GPU in your machine is a great thing. You can easily just write your code and debug and run. I bought one rtx 3090 ti, I have 64 GB ram and 8 core AMD CPU. If you want to train heavy models with large batch sizes, 24 GB GPU memory is not enough, you will need multi gpu machines in cloud. Bur single GPU will help you at least design, debug and test your model very easily.",
        "No it's not and I really wonder what kind of student would think this. \n\nIf you really need some compute, Colab has options. But you'll never need that kind of capacities as a student, this feels like a pro gaming config.",
        "A lot of projects need strong CPU, as the data size/model complexity may be too small for the GPU to make a real difference, and the transfer of the data between the ram and vram will take longer than the increase in computation speed.\n\nI recommend a strong CPU too.",
        "Aws ??  \nPay per hour. If there is no development of code required, 10-15$ you should be done noe?",
        "I would say that buy a descent laptop (Its ok even if u dont have gpu ) with good ram and ssd atleast 1tb .Beacuse in most of dl task we will be using cloud based notebooks for training not only because they provide gpu but easy to use and code. So i will suggest not spent ur hard earned money for laptop instead buy a durable good quality laptop.",
        "Seems like a solid build - my only suggestion is if you can get 64 go of ram and it would be perfect. I’ve reached over 92 gb but it saved me many times with my assignments and masters thesis.",
        "we are getting this AW model for a specific class based object detection model training and mock inferencing loads. \n\n\nprocessor\nIntel® Core™ i9 14900KF (68 MB cache, 24 cores, up to 6.0 GHz P-Core Thermal Velocity)\n\nvideocard\nNVIDIA® GeForce RTX™ 4090, 24 GB GDDR6X\n\nmemory\n64 GB: 2 x 32 GB, DDR5, 5200 MT/s\n\nharddrive\n4 TB, M.2, PCIe NVMe, SSD",
        "I read few CVPR papers which improve SAM, some claim to only need RTX3090 and under 1 day to train the model\n\nKaggle and Colab: can't leave it overnight. Colab Pro+ is one of the solution. But, I heard Colab Pro+ is very expensive",
        "Man, reading this right after studying neural networks and solving problems with like 3 dimensions hits me hard lol ",
        "If I am not mistaken, some CVPR papers that attempt to improve SAM use RTX3090\n\nMay I know your GCP's monthly bill?\n\nI intend to code via VS Code Remote Explorer, and when it's time to train the model, I will rent more expensive GPU (assuming it's possible to plug and play the GPU)",
        "Colab, Kaggle, 12 months free credits of cloud platforms, or OP can ask university for hardware support",
        "Runpod. afaik you have 3090s on there. Try for a few days and see.\n\nCheap CPU I’m not too sure considering you want fast data loading",
        "yes it only takes a day because there is probably a good cpu with good threaded data loader that handles loading/augmentation on the fly. a bad cpu will bottleneck your setup easily.",
        "Just one addition to this, you can leave it overnight unless you sleep more than 12 hours 😅",
        "$20/month is costly than buying a whole PC?",
        "Hahaha, still hits me everytime.",
        "Finetuning SAM makes more sense on a 3090, perhaps with a hiera small backbone. \n\nI don't pay for gcp my company does. But as long as you are only using compute occasionally for some projects and not full on like a business, cloud is cheaper.",
        "1. Colab and Kaggle: only support ipynb\n\n2. 12 months free credits of cloud platforms: I will try this, thank you\n\n3. OP can ask university for hardware support: They do provide it. However, 1) there are 11 students in this lab, 2) 4 workstations. 3 workstations are allocated to the Chinese students, 1 workstation is allocated to the 3 International students 4) yet, Chinese students use our workstation as well (so, you get the idea) 5) specifically for our workstation, we don't have access to the root. So, unzipping a dataset is a pain (I have to download it to my personal computer, unzip it, and then upload it via SFTP) 6) downloading is a pain (the download is throttled, no idea why) 7) I offered myself to reinstall the OS but got rejected\n\nIt's frustrating",
        "More like 40USD/month (I need the Colab Pro+, so can leave it overnight). Also, a Redditor said it only took 2 days to use all of the compute units",
        "If you plan to keep working ai long term, yes, Google didn't buy gpus to give you a deal, they did it to make money, which would be impossible if your payment to them didn't include profit, as he mentioned it would be 40/months, which is 480/year if you use it 3 years, you just buy google that pc Instead of yourself and now your monthly bill is due again",
        "Workstations in the lab are one thing. Many universities have clusters. You can use a 3090 pc for development and then when it comes time to train use a cluster. \n\nBut if it just comes to development on your local machine, you probably don't even need a 3090. So long as you're not doing the training on your machine, a mid tier gaming pc would be fine if you're tight on budget.",
        "If you're in China, you can use AutoDL and the GPU instances on it are dirt cheap.",
        "Oh, then they must have changed it. I remember using it in 2022 for $11USD /month, and I could train multiple times for around 12 hours continuously.",
        "To be fair, you get access to A100 with colab pro+, I have managed to get it pretty reliably (maybe due to timezone). For cheaper GPU's it maybe does make as much sense to rent. I think even if you work long term, the optimal solution depends how much you utilize the GPU.",
        "Google could be using COLAB as a loss leader. Give something away at below cost to get the customer hooked.\n\nIn other words, you use COLAB at low cost and then eventually you signup for more profitable Google cloud services, maybe even host a company on their cloud. ",
        "keep telling yourself whatever, those of us who ran the numbers know",
        "Telling myself what? That loss leaders are a thing? ",
        "Yes, they \"are a thing\" congrats, that's not what this is tho..."
    ]
},
{
    "submission_id": "1foa2sg",
    "title": "Computer Vision Made Simple with ReductStore and Roboflow | ReductStore",
    "selftext": "",
    "created_utc": "2024-09-24T04:23:15",
    "num_comments": 0,
    "comments": []
},
{
    "submission_id": "1fo8ngo",
    "title": "Unable to solve perspective distortion problem ",
    "selftext": "Dear computer vision enjoyers, \n\nI am trying to solve perspective distortion ([wiki](https://en.wikipedia.org/wiki/Perspective_distortion)) problem with my acA1300-30um camera ([datasheet](http://www.altavision.com.br/Arquivos/Basler/Manual/Manual_Ace_USB3-0.pdf)).\n\nI followed calibration steps tutorial ([tutorial](https://docs.opencv.org/4.x/dc/dbb/tutorial_py_calibration.html)), but still I am unable to correct my images from perspective distortion. \n\nI took 14 pictures with 5x5 chessboard pattern from variaous agles and distances. \n\nAfter receiving camera calibration and distortion coefficients, the undistorted images are almost the same as the distorted ones (I used image from calibration):\n\n[Distorted image](https://preview.redd.it/zoumooqeaqqd1.jpg?width=640&format=pjpg&auto=webp&s=8c08d51499203a0640e1acadf6a4057c75a1d75e)\n\n[Undistorted image usign cv2.undistort\\(\\) ](https://preview.redd.it/nizvqbnd7qqd1.jpg?width=640&format=pjpg&auto=webp&s=80081b09f33ba534a812d6836e17ddc2f0160723)\n\n[Undistorted image using cv2.remap\\(\\)](https://preview.redd.it/eg9qgcnd7qqd1.jpg?width=640&format=pjpg&auto=webp&s=e8e9e578713d4075c28b6944549a30a31b5147e4)\n\nDo you have any suggestions on what I should do differently or if there is any significant problem? I used 2 methods to undistort my images, but with no proper results.\n\n\n\nHere is the Python code that I used:\n\n\\`\\`\\`\n\n`import cv2`\n\n`import numpy as np`\n\n`import os`\n\n\n\nCHESSBOARD\\_SIZE = (5, 5)\n\nCALIBRATION\\_FOLDER = 'CalibImages'\n\n\n\nobject\\_points = \\[\\]\n\nimage\\_points = \\[\\]\n\n\n\nfor filename in os.listdir(CALIBRATION\\_FOLDER):\n\nimage\\_path = os.path.join(CALIBRATION\\_FOLDER, filename)\n\nimage = cv2.imread(image\\_path, cv2.IMREAD\\_GRAYSCALE)\n\n\n\nif image is None or image.size == 0 or image.shape\\[0\\] == 0 or image.shape\\[1\\] == 0:\n\nprint(f\"Error: Unable to read or invalid image file {image\\_path}\")\n\ncontinue\n\n\n\nret, corners = cv2.findChessboardCorners(image, CHESSBOARD\\_SIZE)\n\n\n\nif ret:\n\nprint(f\"Corners found in image {image\\_path}\")\n\nobject\\_points.append(np.zeros((CHESSBOARD\\_SIZE\\[0\\] \\* CHESSBOARD\\_SIZE\\[1\\], 3), np.float32))\n\nobject\\_points\\[-1\\]\\[:, :2\\] = np.mgrid\\[0:CHESSBOARD\\_SIZE\\[0\\], 0:CHESSBOARD\\_SIZE\\[1\\]\\].T.reshape(-1, 2)\n\nimage\\_points.append(corners.reshape(-1, 2))\n\n\n\nelse:\n\nprint(f\"No corners found in image {image\\_path}\")\n\n\n\nif len(image\\_points) == 0:\n\nprint(\"No images with corners found. Calibration cannot proceed.\")\n\nelse:\n\nretval, camera\\_matrix, dist\\_coeffs, rvecs, tvecs = \n\ncv2.calibrateCamera(object\\_points, image\\_points, image.shape\\[::-1\\], None, None)\n\n\n\nnp.save('camera\\_matrix.npy', camera\\_matrix)\n\nnp.save('dist\\_coeffs.npy', dist\\_coeffs)\n\n\n\nimg = cv2.imread('test12.bmp')\n\ndistorted\\_image = cv2.undistort(img, camera\\_matrix, dist\\_coeffs)\n\ncv2.imwrite('undistorted\\_image.jpg', distorted\\_image)\n\n\n\nheight, width = img.shape\\[:2\\]\n\nmapx, mapy = cv2.initUndistortRectifyMap(camera\\_matrix, dist\\_coeffs, None, camera\\_matrix, (width, height), 5)\n\nundistorted\\_image = cv2.remap(img, mapx, mapy, cv2.INTER\\_LINEAR)\n\ncv2.imwrite('undistorted\\_imageMAP.jpg', undistorted\\_image)\n\n\\`\\`\\`",
    "created_utc": "2024-09-24T02:49:51",
    "num_comments": 7,
    "comments": [
        "Increase the number of calibration images, using varied angles. Ensure chessboard corners are detected accurately. Try a larger chessboard pattern. Verify camera model settings. Consider using OpenCV's undistort function with obtained coefficients for better results.",
        "Hey Op! I see that you are using a symmetric grid. Sometimes detect corners switched the order of corners detected. It is best to use an Asymmetric grid or pass ‘cv2.CALIB_CB_SYMMETRIC_GRID’ to find checkerboard corners. You should also the check error being returned from calibrateCamera.",
        "Your images do not look distorted. Distortion is about making lines straight. What do you mean by perspective distortion ? Do you want to have your board appear like facing the camera ? In which case you need to estimate the pose or the homography of the board. It is not about distortion.",
        "Lens distortion? Your camera lens is probably rectilinear or near to it . If you know it’s not, coefficients are a better way of doing this and then storing a model. Your checkerboard isn’t the best and you have a diffraction issue. Try a bigger board further away, well lit, sharp as the res will allow. Good photography can help to eliminate variables. A nice clean lens is a bonus - and could help with the diffraction issue.",
        "Yes, I would like to transform the picture so it looks like it faces the camera directly.\n\nCan you elaborate your suggestion?  \nThank you!",
        "It is the demo n°2 : perspective correction in this opencv tutorial:https://docs.opencv.org/4.x/d9/dab/tutorial_homography.html",
        "Thank you very much!"
    ]
},
{
    "submission_id": "1fo7bzv",
    "title": "Object detection model for big dataset",
    "selftext": "Hello this is my first post and i sorry for my english.\n\nI wanna train ai model for my school project. i use the bdd100k dataset and yolov8s. actually i was train for many models for another school project but i used the small dataset and more bigger model then yolov8s and i got acceptable result. But i used to bbdk100k dt and yolov8s model i didnt get good result actually this train not finish yet but it takes a long time so i thought i'd ask on forum.\n\nbdd100k has 100k images and yolov8s has 11,2m params and 28.6 glops.\n\n[MY current model res](https://preview.redd.it/5yot4vlqspqd1.png?width=1288&format=png&auto=webp&s=9c0e23e9401c9cfcf1c4bfcd25905c3d0ae10d5c)\n\nSoo my questions i cannot increase model size because my project's hardware i will use is not large enough to handle it so should i reduce the dataset for better map result? or do you have any suggestions on what i should do  \n",
    "created_utc": "2024-09-24T01:06:16",
    "num_comments": 8,
    "comments": [
        "The original YOLOv8 pretrained models including the s variant were trained on the COCO dataset which has over 118k images, so that's not an issue.\n\nMake sure you're starting from a pretrained checkpoint. [This paper](https://www.sciencedirect.com/science/article/pii/S0921889023001975#tbl2) says they reached 65.54 mAP@0.5 with 300 epochs on YOLOv8m on BDD100K.",
        "Hybrid co-detr",
        "You can try add some data augmentation to improve your mAP",
        "Thanks i looking at this paper :D",
        "soryy, i dont understand this answers",
        "hmm that's should works, i will try. thx",
        "Hybrid Co-detr is a Transformer based object detector already trained on coco and lvis(1200 classes). It is also THE current SOTA. But it is very big and computational heavy.",
        "hmm i will look"
    ]
},
{
    "submission_id": "1fo2egr",
    "title": "Label Studio Annotation software in AWS. ",
    "selftext": "Anyone tried Label Studio annotation software for video, image over Aws marketplace Ami. \n\nI heard it’s easier to use, any comments before I decide to use which way around? ",
    "created_utc": "2024-09-23T19:42:50",
    "num_comments": 3,
    "comments": [
        "I'm using CVAT cloud version. Has all necessary functionality I need, easy to set up, very cheap and no upkeep costs.",
        "I was using roboflow it is quite good in the features they got it. But Label studio is handy for small and mid level projects. When you’re trying in aws ami, makesure that you use appropriate instance types to resize your monthly bills. \n\nAdditionally I’m looking for more suggestions too.",
        "Tried them all and found DarkMark to be the best and friendly license"
    ]
},
{
    "submission_id": "1fnx3xu",
    "title": "SFM for Google StreetView 360",
    "selftext": "I'm trying to get the image poses using ns-process in NeRFStuido on street view images (for personal project) of an indoor place. Currently Im getting the poses for only 1-2 frames.\n\n  \nI am currently using 12 360 images, of a small store, cropped the bottom etc. I want to build a gsplat of it. But without the poses i cannot proceed.",
    "created_utc": "2024-09-23T15:20:05",
    "num_comments": 2,
    "comments": [
        "Look into colmap to obtain the camera poses",
        "Thanks, Nerfstudio uses Colmap by default. I want to know if i need to change the default parameters."
    ]
},
{
    "submission_id": "1fnvoue",
    "title": "Deform 3d model",
    "selftext": "Hi all, \nI have a design intent 3d model. I also have an image of a deformed in service part. \nI’m aiming to deform the design internet as per the photo. \nI have trains yolo to recognise the areas of wear I’m interested in but struggling to find research on the next steps \n1. I need to estimate the angle image was taken from\n2. To deform the original part to match the deformity observed in the\n\nIf you know of any research into this area would be greatly appreciated\nTim",
    "created_utc": "2024-09-23T14:17:16",
    "num_comments": 0,
    "comments": []
},
{
    "submission_id": "1fnuhr9",
    "title": "USB camera bandwidth question",
    "selftext": "I am working on a project involving 2 sets of stereo camera pairs and I am trying to sync them all together. I have developed the sync board that generates PWM at the desired frequency to trigger each camera hardware trigger pins. Due to some limitation of the project, I have to use USB cameras and can't use other protocol but I have ensured that all camera are connected through USB3.0 port (my setup only have one USB3.0 root controller as well) through a USB3.0 [hub](https://www.amazon.com/StarTech-com-ST7300USBME-Port-Industrial-Protection/dp/B00V6ADRQC/ref=asc_df_B00V6ADRQC/?hvadid=692875362841&hvpos=&hvnetw=g&hvrand=128390319847609950&hvpone=&hvptwo=&hvqmt=&hvdev=c&hvdvcmdl=&hvlocint=&hvlocphy=1013139&hvtargid=pla-2281435181658&psc=1&mcid=545836481d1437bbb7fec6aeb25e5531&hvocijid=128390319847609950-B00V6ADRQC-&hvexpln=73) \n\nThe desired operating specs are one stereo pair (camera A)with 1600x1300 resolution with 8-bit greyscale image at 60fps and the other stereo pair (camera B) with 640x512 resolution with 16-bit images at 60fps. While the calculated expected bandwidth is around 2.6 Gbit/s which is below the rated 5 Gbit/s of USB3.0 but I assume that there are some overheads so I digress there. A combination of resolution and rates I found that works is for camera A 1280x720 res with 8-bit greyscale at 60fps and for camera B 640x512 res with 16-bit greyscale at 60fps which is expected to use 1.5Gbit/s. I would like to retain the max resolution of the camera A at 1600x1300 so reducing the framerate to around 30 fps is sufficient. However, due to the nature of camera B, reducing the rate to 30fps degrades the quality so camera B needs to be triggered at 60fps.\n\nMy next approach now is to have the camera pairs triggered but PWM signal with different frequencies but still synced through their rising edges and being dividers of each others (60fps for camera B and 20fps for camera A). This becomes camera A at 1600x1300 at 8-bit at 20fps and camera B at 640x512 at 16-bit at 60fps which I can choose to save a captures from camera B every 3 images (I plan on using OpenCV videostream with 1 image buffer to only grab the latest image). However, testing this configuration still results in USB bandwidth issue although it is expected to only need 1.3Gbit/s. I would like to ask if my assumption of it being a bandwidth issue correct and if so, why does the configuration with 60fps for both which uses 1.5Gbit/s works while the 60fps+20fps configuration which expected to use 1.3Gbit/s not work? Thanks in advance  ",
    "created_utc": "2024-09-23T13:26:49",
    "num_comments": 3,
    "comments": [
        "This is a Fakespot Reviews Analysis bot. Fakespot detects fake reviews, fake products and unreliable sellers using AI.\n\nHere is the analysis for the Amazon product reviews:\n\n>**Name**: 7 Port Industrial USB 3.0 Hub - with ESD Protection - Mountable - USB 3 Hub - USB Extender - Powered USB 3.0 Hub - USB Splitter \n\n>**Company**: StarTech\n\n>**Amazon Product Rating**: 4.5 \n\n>**Fakespot Reviews Grade**: B\n\n>**Adjusted Fakespot Rating**: 4.5\n\n>**Analysis Performed at**: 08-14-2023 \n\n[Link to Fakespot Analysis](https://fakespot.com/product/7-port-industrial-usb-3-0-hub-with-esd-protection-mountable-usb-3-hub-usb-extender-powered-usb-3-0-hub-usb-splitter) | [Check out the Fakespot Chrome Extension!](https://chrome.google.com/webstore/detail/fakespot-analyze-fake-ama/nakplnnackehceedgkgkokbgbmfghain)\n\n*Fakespot analyzes the reviews authenticity and not the product quality using AI. We look for real reviews that mention product issues such as counterfeits, defects, and bad return policies that fake reviews try to hide from consumers.*\n\n*We give an A-F letter for trustworthiness of reviews. A = very trustworthy reviews, F = highly untrustworthy reviews. We also provide seller ratings to warn you if the seller can be trusted or not.*",
        "lots of things going on here. usb 3.0 is closer to 3-4Gbit with all the overhead, not a rtos, shared root controller, what is the bandwidth allocation for each camera look like is it reserving more priority/bandwidth to the 60 than the 30. triggers fully pipelined ? good quality root controller? when 60/60 is used its easier to manage priority/allocation.\n\npointgrey/flir used to list usb 3 root hubs like the texas instruments which were better for camera streaming than others as all usb root/hubs are definitely not equal i dont have the specific model they recommended but i have them and they are good and should still be listed at flir",
        "Thanks for the recommendation to check the FLIR site.\n\nFor every combination, it seems like the streams from the 640x512 camera is not affected, only the cameras with the 1600x1300 resolution is affected with drop out and corrupted frames. I accepted that I won't be getting the full bandwidth but am just confused about why a configuration which estimated to use 1.3Gbit/s does not work while one with estimated bandwidth usage of 1.5Gbit/s works.\n\nI think the USB3 rooth hub are good as well. I checked the components using USB Device tree view. The host PC uses the Intel USB 3.0 eXtensible host controller while the hub uses GenesysLogic USB 3.2 Gen1 USB hub which are indeed recommended on the FLIR site."
    ]
},
{
    "submission_id": "1fnpdyk",
    "title": "Storing data on emotions detected in real-time and giving comments (complete beginner in CV and any advanced ML btw)",
    "selftext": "Hi so I am a complete beginner in computer vision and advanced machine learning. I have taken on a project which require the program to detect the emotion of a user from his/her camera for a period of time and then give comments on the emotions detected afterwards.\n\nSo currently I have been following tutorials (e.g. https://sefiks.com/2018/01/10/real-time-facial-expression-recognition-on-streaming-data/) on the first part of detecting emotions real-time mainly through this tutorial using Haar Cascade frontal face model and it is able to give a bounding box on the face and state the emotion detected -- pretty basic stuff.\n\nHowever, I do want the emotions detected to be stored somewhere throughout the time the camera is on and then after the video camera is disabled (by the user pressing something or whatnot), the program will find the most prominent emotion(s) detected and give comments. I have tried to search for other tutorials but most only cover the aforementioned first part. Is there anything I can read up on to help me build or modify to get this second part out?",
    "created_utc": "2024-09-23T09:57:51",
    "num_comments": 0,
    "comments": []
},
{
    "submission_id": "1fnnpus",
    "title": "What is the best model for 2D Human Pose Estimation ?",
    "selftext": "Hey everyone,\n\nDo you have any experience about benchmarking 2D human keypoint estimation ? What do you suggest for this task ?\n\nI am currently trying to find SOA, I think now, RTMPose is the best.\n\nThank you.",
    "created_utc": "2024-09-23T08:48:56",
    "num_comments": 13,
    "comments": [
        "RTMPose is probably a good choice if you need real time pose estimation. It is relatively easy to deploy and has good performance out of the box.\n\nBut as always, it depends on your use case.\n\nAre you doing pose estimation, or also tracking? Is it offline, online, or real time? Do you need to train the model on your own data? Is it a commercial application? Do you need something easy to deploy? Does it need to run on low spec devices? Are COCO skeletons ok, or do you need more key points?\n\nSapiens is new for example, and seems very promising. But it's non-commercial usage only. (Aside: has anyone used it?)",
        "ViTPose showed some positive results as well. It’s a bit heavy compared to the RTMPose but is definitely better when it has seen a lot of data",
        "https://paperswithcode.com/task/2d-human-pose-estimation",
        "I would say sapiens",
        "Did you try Detectron2 densepose. It get good results. And easy for production.",
        "I have to track also yes. It should be offline. (Is there any online solution for that?) I may need to train the model with my data and I need something easy to deploy. I will run this model on Jetson Orin Nano 8gb. COCO skeletons are totaly fine. This project is non commercial at all. I have tried to use sapiens a few times but could not achieve it.",
        "Thanks for your suggestion, which library did you use for running ViTPose ?",
        "Thanks, I have already checked this. I just want to hear some experiences from you guys.",
        "How did you run it ?",
        "In that case RTMPose sounds like a good fit. The MMLab ecosystem can also do detection (e.g. with RTMDet) and tracking. \n\nAn alternative is YOLOv8. It can also do pose estimation, but I have personally not used it for that purpose yet. Personally I do think that the Ultralytics codebase is easier to work with than MMLab. \n\nWith online/offline tracking I referred to whether you process a video/camera stream frame-by-frame or whether you can process an entire video at once (e.g. for annotation of a dataset). When doing the latter you can get better results.",
        "I am no expert in this, was just trying different models for Pose estimation. \n\nI used the MMPose framework to train. You can use the backbone and head implementation from this framework to create your own model. Since MMPose is built in PyTorch, you can do the hard work and implement it for your application. \nBut life would be easier if you can use the framework.",
        "I am not sure what you are looking for. The SOTAs are listed according to their performance on benchmarks. If you want something for production, then you perhaps should not use the SOTA but rather something that is adequately good but easy to deploy.",
        "Isn't it open source ?"
    ]
},
{
    "submission_id": "1fnim9i",
    "title": "OCR Advice",
    "selftext": "Hello!\n\nI am currently working on an video classification task which involves detecting credentials in videos, such as phone numbers, emails, and URLs. I decided to solve this problem using OCR models like PaddleOCR. Basically, I check every frame with regex for any credentials. My main problem now is that some videos contain multiple languages. For example, a video may have both English and Kazakh text, which messes up the text I get from OCR models because it tries to read Kazakh words in English, giving me random text. I want to check only English words and not pick up any other languages. How can i achive that or can you suggest any other methods which will help me to solve my main task.\n\nAny recommendations would be appreciated.",
    "created_utc": "2024-09-23T04:57:23",
    "num_comments": 1,
    "comments": [
        "check this : [https://api.scandocflow.com/?version=latest](https://api.scandocflow.com/?version=latest)"
    ]
},
{
    "submission_id": "1fnhxhi",
    "title": "Help needed with extracting information from an OCR output",
    "selftext": "Hello everyone,\n\nI'm trying to extract specific information from the attached image, including:\n\n* Contract Number\n* Start Date\n* End Date\n* Agency Code\n* License Plate\n* Brand\n* Vehicle Type\n* VIN\n\nHere’s the link to the image: [Image link](https://ibb.co/nCdBg2H).\n\nThe issue is that when I use PaddleOCR, I get output like this:\n\n>\n\nI understand that a common solution is to use regular expressions or named entity recognition to extract the relevant information. However, I’m curious if there are any additional tips or best practices I might be missing.\n\nThank you for your time!",
    "created_utc": "2024-09-23T04:16:36",
    "num_comments": 5,
    "comments": [
        "The trick for this kind of structured data extraction is to use a homography to align your input onto a template image then use the coordinates of each region to parse your output. DM me if you need help ",
        "First time hearing about this, can you please explain more or give me some ressources, thanks!",
        "We can jump on a quick call and go through it. Dm me ",
        "Thank you so mcuh ! I really appreciate your help!",
        "hi author,, I am also going through this problem in my semester project ,, Can you tell me how you resolved this problem"
    ]
},
{
    "submission_id": "1fnh9c4",
    "title": "What are some of the well accepted evaluation metrics for 3D reconstruction? Also how do you evaluate a scene reconstructed from methods such as V-SLAM or Visual Odometry?",
    "selftext": "I am new to the domain of computer vision and 3D reconstruction, and I have seen some very fancy results showing 3D reconstruction results from a moving camera/ single view, but I am still not sure how is the reconstruction output quantitatively evaluated? Qualitatively they look great, but research needs quantitative analysis too…",
    "created_utc": "2024-09-23T03:32:18",
    "num_comments": 8,
    "comments": [
        "There are some older videos where the trajectory is precisely known, typically because it's around a robotics lab :) and you can evaluate the end point error. When those have lidar measurements, you can use it as a (pseudo) ground truth. I wanted to find those old ones, but this is much better and from this year: [https://arxiv.org/html/2403.11496v1](https://arxiv.org/html/2403.11496v1)",
        "It's actually quite a difficult problem because you need to know the geometry ahead of time. In my lab we've been doing some testing on different reconstruction technologies and the way we've set it up is to 3d print a few different artifacts, scan them and then look at surface deviation between the printed model and the scan. At least that's the cheap version - we have a CMM so we actually compare against the results of that which means there's no errors from the printing process itself. Other than that, we mostly use qualitative results or quantitative metrics on downstream tasks which are mostly dependent on the dimensional accuracy of the mesh we've recovered",
        "PSNR, SSIM, LPIPS are used quite often for 3D reconstruction",
        "SSIM, LPIPS and PSNR for image comparison isn’t it?",
        "Yep! But you're comparing the rendered view from your reconstruction to the ground truth/input images",
        "I see. Thanks for clarifying. But aren’t there any direct evaluation metric that can tell us that we missed the ground truth by this (a quantitative value)?",
        "Maybe I misunderstand you, but those metrics are quantitative - SSIM goes from -1 to 1, where 1 means identical image. PSNR is measured in decibels, so its value approaches infinity, but if you get e.g. 100 (or whatever the cap is set to), then that's considered identical as well. For LPIPS, 0 means the images are identical\n\nYou can have a look at NeRF or 3DGS papers, they all use these metrics to evaluate the performance",
        "Thanks!! This has been very insightful."
    ]
},
{
    "submission_id": "1fnfyav",
    "title": "Running YOLOv8 15x faster on mobile phones",
    "selftext": "I just came across this really cool work that makes YOLOv8 run 15x faster on mobile using on-device smartphone NPUs instead of CPUs!\n\n🎥 vid: [https://www.youtube.com/watch?v=LkP3JDTcVN8](https://www.youtube.com/watch?v=LkP3JDTcVN8)\n\n📚 blog: [https://zetic.ai/blog/implementing-yolov8-on-device-ai-with-zetic-mlange](https://zetic.ai/blog/implementing-yolov8-on-device-ai-with-zetic-mlange)\n\n💻 repo: [https://github.com/zetic-ai/ZETIC\\_MLange\\_apps/](https://github.com/zetic-ai/ZETIC_MLange_apps/)",
    "created_utc": "2024-09-23T01:55:39",
    "num_comments": 1,
    "comments": [
        "Pretty common knowledge to run on a NPU and not a CPU.   That Apple Iphone 15's performance is impressive, didn't know their NPU was rated at 35 TOPS!   \n\nI have a [demo running three YOLOv5 models](https://youtu.be/M6mvHTNQZqM) on 720p streams using the RK3588 processor where the NPU is rated at 6 TOPS."
    ]
},
{
    "submission_id": "1fnfxe1",
    "title": "Do not show label name on Datumaro Visualizer",
    "selftext": "Hello ,   \nI am using Datumaro CLI and I want to visualize some annotation , but I do not want the label name because there is a lot of label \n\nhttps://preview.redd.it/0c1nkzuawiqd1.png?width=639&format=png&auto=webp&s=1edcacf4ff1a664c47d45332a6762896d519831a\n\n",
    "created_utc": "2024-09-23T01:53:36",
    "num_comments": 0,
    "comments": []
},
{
    "submission_id": "1fnfhec",
    "title": "Deep learning developers, what are you doing?",
    "selftext": "Hello all,  \nI've been a software developer on computer vision application for the last 5-6 years (my entire carreer work). I've never used deep learning algorithms for any applications, but now that I've started a new company, I'm seeing potential uses in my area, so I've readed some books, learned the basics of teory and developed my first application with deep learning for object detection.\n\nAs an enterpreneur, I'm looking back on what I've done for that application in a technical point of view and onestly I'm a little disappointed. All I did was choose a model, trained it and use it in my application; that's all. It was pretty easy, I don't need any crazy ideas for the application, it was a little time consuming for the training part, but, in general, the work was pretty simple.\n\nI really want to know more about this world and I'm so excited and I see opportunity everywhere, but then I have only one question: what a deep learning developer do at work? What the hundreads of company/startup are doing when they are developing applications with deep learning?\n\nI don't think many company develop their own model (that I understand is way more complex and time consuming compared to what i've done), so what else are they doing?\n\nI'm pretty sure I'm missing something very important, but i can't really understand what! Please help me to understand!",
    "created_utc": "2024-09-23T01:18:08",
    "num_comments": 39,
    "comments": [
        "I'm a CV engineer, so not only deep learning, the entire list of things I do would be very large but mainly:\n- understand what the clients want\n- design computer vision systems\n- If needed, design and develop the data acquisition system, otherwise I go to the place and personally acquire images for algorithms or models\n- develop CV algorithms \n- training models\n- sometimes also design a custom model but it's rare\n- optimize models\n- compile and test models for special hardware\n- develop the software of the CV system\n- test the software, like hundreds of times during development and around ten times post release for each iteration\n- meeting with clients\n- test new sensors and hardware \n- develop internal libraries\n- optimize software/libraries",
        "3 years of exp as a computer vision developer in a german company   \nwhat i do is train model get insights and make apis on cloud for internal users and thats it  \nAI part is short term like when i had to train models it does take time but after that we only touch it when we have to optimize it XD",
        "I do a lot of automated inspections so checking if something is produced / assembled as it should be \n\nsometimes this is easy e.g. looking for a scratch / dent and sometimes it's hard e.g. checking if something was assembled correctly when there are lots of different types of ways of screwing up an assembly and you need to use tricks like aligning the input onto a correct assembly and then do semantic comparisons of the parts",
        "All below is solely my personal opinion:\n\nWe can divide computer vision into two categories, the first one is the areas/problems that are partially solved, like face recognition and face detection, single object detection etc. And the other category is unsolved problems e.g generall object detection, aliveness detection, anti spoofing etc.\nAt my job, We have a funded project by an entity and what we do is try to fit the requirements into solved problems area and use some already existing methods, techniques,  everything available to achieve what we want. In this phase is very unlikely that we do any development because training a deep learning model is very, and I can't emphasize enough, hard. You have to worry about data, about hyper parameters tuning, about encoding labels, about creating valid loss function, optimizer, preprocessing, post processing and also time, a lot of time which is way more valuable than money and hardware resources. Developing(training) mostly requires time and computation power. If we fail in achieving what we want given the available tools then we go to fine tuning them and if it also fails then we think about creating something new. ( and trust me researchers, including myself as a MSc student, don't know what we are doing and why something work). After this, phase 2 begins. Developing an actual working product. This phase requires so many field of expertise such as hardware knowledge, model compression, c++ programming, web apis, workload management etc. So even though I'm not anything near an expert I suggest you follow the same path and play by the odds. If one day you had enough resources you can do some R&D which as the current state of research suggest, only big companies have.\n\nSo in summary, what im trying to say is unless you are trying to make a something that doesn't have a functional prototype anywhere, you better stick with what is available, everyone else are doing so. I'm not denying the importance of R&D but let's be realistic, openai spent hundreds of millions of dollars to achieve something like chat gpt4 and that was like 7 years after the original paper (attention is all you need) came out. If we want to keep up with the market we must be able to produce valid usable products and thats all customers want. And one more thing, I'm not saying you don't need any deep learning knowledge, you do, a lot of it actually, and not only deep learning, so many more areas such as optimization, just to be able to identify what is suitable and what is not.",
        "A lot of work for custom architecture, optimizers, loss functions, data generation, data collection and handling, specialized CV algorithms, embedded code etc etc.\n\nThere are a ton of things to do at highly specialized positions where licenses don't allow usage of pretrained weights or architectures and where use cases require several different features in a single optimized model and such. Then use the different outputs in different ways depending on product.\n\nCustom optimizers are needed for more robust generalization in some cases, custom losses can improve iou from 0.3 to 0.75 for example, custom architecture and training methodology in multitask settings to further improve metrics, different ways are needed to reduce overconfidence and improve model calibration for large scale production settings.\n\nIt's been a long time since the days where I could just easily pull down a model and quickly train for a smaller task. The moment one goes into bigger industry where a lot of requirements need to be matched with cost effective solutions its completely different.",
        "Check Ingoampt as an example too , we develop apps with deep learning , but in future more apps with deep leaning is coming [www.ingoampt.com](https://www.ingoampt.com)",
        "The whole point of machine learning is to minimize manual labor and let the models learn from data. There's still a lot of low hanging fruit and you can use off the shelve models like you did for many applications.\n\nProper validation of your model can require some work, keeping track of experiments, cleaning up data.  \nWhen compute is limited, doing some pareto experiments for accuracy vs time. Optimizing hyperparameters. Development in the cloud or on the edge.  \nIn my experience, custom work is most relevant when specific domain knowledge is relevant for the task. e.g. handle scale properly (object detecters are optimized for a broad range of sizes and shapes, you might have prior information that narrows it down). Or any other kind of prior knowledge you can leverage, e.g. rotation equivariant models.",
        "I'm PhD student, i m working on deep learning on embedded systems",
        "i completed the android app for detection project. now working on drone-based geo data collection",
        "I think that makes sense. The third more difficult category you mentioned is more for an R&D process. If your company has work processes or invests in R&D, then I would say that you would want your business to have that as an offering. But still expecting that most problems for clients will be tackled by the easier routes.\nSome clients will come and want you to help them stand up their own custom architectures, and a company ideally should be able to do that, but after some in depth consulting you will find that it is usually not necessary for their problem. \nI develop custom deep learning CV models and most of my learning has been through experiments in an R&D environment, no available resources other than normal guidelines for deep learning.",
        "We're learning, deeply.\n\nWe're also being excited constantly about opportunities we're seeing everywhere, and every Friday we meet with VCs, we make them write an NDA then pitch them adaptive database management, learned data structures, sparse matrix based user engagement decision making systems, turn-key crowd management solutions, highly resilient nano-uav coordinated SLAM drone swarm military paradigms.",
        "Yeah, I developed my own CV system from scratch and there is obviously a lot of work outside deep learning part.\n\nBut when you say design a custom model or optimize it, what do you mean specifically?",
        "you design CV algorithms from scratch?\n\nDon't you use already develop CV models?\n\nWhat's the distinction between those two?",
        "And before train the model? Are you developed your model or take an existing model and only train?",
        "The project where I used deep learning is very complex and the vision system do a lot of things (literally, A LOT). I used object detection only for a small part to have some partial result so I understand what you mean.\n\nWhen you use deep learning, are you use existing models or are you develop it from scratch?",
        "Thanks for sharing. \n\nI think you partialy confirmed what I thought: for vast majority of cases it's \"just\" a model training and develop a new architecture is too expensive for almost every company. In my projects, usually, there isn't a ready to use solution and we need to develop new solutions every time, but, in many cases, if not always, we can make our projects work only with classical algorithms.\n\nHowever I think deep learning could be a \"new\" powerful tool to use. For example in my first application it's resulted more robust on illumination changes and help me a lot to achieve what i want.\n\nI just want to learn how to use it in the right way.",
        "That's very interesting and probably answer my question. I don't face yet any of these problems, however i think this could be an opportunity to learn and, possibly, apply those information in my area. \n\nDo you have any resource to study these problems and how to achieve those results? Thank you!",
        "\"The whole point of machine learning is to minimize manual labor and let the models learn from data\". That's probably what i'm missing. \n\nHowever model training, validation and test is very time consuming (and expensive) for some application. I think, at least in my area, there is better and cheaper solution in many cases.",
        "How is your project going? I am a hobbyist and learning both cv and embedded. What is your topic if you don’t mind sharing here? I am looking for a direction and want to know what’s possible and what’s out there.",
        "For design I mean deciding the model layers, what inputs and outputs shapes, loss functions, regulators, developing the dataset loader and the training and validation processes.\n\nFor the optimization part is mainly the model compression and quantization depending on the accuracy and the hardware which the model will run.\n\nFor example there are applications where VPUs are used and they need to consume less power as possible and so the model must be ideally quantized in int8.",
        "1: Yes but for project specific algorithms, never for general purpose algorithms. In some rare occasions I have to accelerate some general CV algorithm in GPU.\n\n2: Yes, actually most of the times I use classic sota models for industry like yolo and I fine-tune it.\n\n3: well mathematically speaking, both are CV algorithms but today I associate \"CV algorithms\" as classic old school CV algorithms without using deep learning like edge and contour detection, projections, thresholding, etc...",
        "At least in my case, I develop custom algorithm for almost every single application. Of course I use existent algorithm (for example, ICP), but usually it's not enough to meet all customer requirements.",
        "before training we get the use case like the actual requirments   \nthen analyze which exact model to use   \nwe try to use AI as less as we can and extract solution on the basis of only Programming   \nwe use pre trained model and then trained that onto our own dataset",
        "sometimes pre-trained models, sometimes i build an architecture from scratch - depends on the situation. often i build stuff around pretrained models e.g. using a pretrained model to extract segmentation masks then use points on the masks with a homography to align an input onto a reference template kind of thing",
        "I think you misunderstood. There is almost no problem that isn't partially solvable by old methods. Deep learning is only another method to solve existing problems and it's pretty good at it. You can attack almost any problem by defining a loss function and optimizing it based on an optimizer algorithm, which is exactly what deep learning (and any other data-driven algorithm) does. It just adds some transformation steps in between (and a whole lot just by this simple approach). Also, deep learning is not just a \"could\", it is a \"is\". The rest of your statements stand true IMO.\n\nAnd about learning how to use it, ngl, it's pretty complicated. You require a lot of knowledge, some of it is just theoretical, the rest is pretty hard, and for the start, you need to have deep knowledge about how hardware even works to be able to connect the dots. Don't be fooled by some tutorials that only type some codes and declare a forward or fit method. There is so much going on underneath which is essential to know to develop a product. For example, convolution is implemented by computing the coefficients of a FFT function.",
        "For optimizers: Start with looking at newer optimizers after Adam / AdamW that aim to increase validation metrics, like AdaBelief, Gradient Centralization etc. Then you can look into methods regarding Wide / Flat Minima search such as Positive-Negative Momentum, LookAhead, Explore-Exploit Scheduler and much more. Also NormLoss and Stable Weight Decay which go into forcing more smooth rather than spiky functions in the network for better generalization and feature transferability towards related domains.\n\nFor losses: A good start is understanding Label Smoothing and why it helps in training, Neural Collapse is a good point to dive into for even more in-depth information. How to modify Crossentropy losses in different ways depending on tasks such as Exponential-Logarithm on logits to balance learning, weighting positive - negative parts of the loss based on class size, calculating class weights based on the dataset, handling noisy / pseudo labels by for example removing x % worst predictions. Understanding that some loss functions will lead to worse transferable features in the backbone for other tasks but improves the current task, important to think about in multi-task settings.\n\nFor architecture: Go through how different operations are affected by the target hardware, don't look blindly on theoretical flops or MACs as they can be very misleading depending on hardware and optimization methods. For example: Depthwise are often told to be very performance friendly but can also often be the biggest bottlenecks in an architecture for real-time systems on embedded, especially when using Depthwise Strips. Architecture also plays a role in how well you can handle objects of different shapes like thin lines, very small vs big objects, irregular shaped objects. There are meta-analysis for some of these parts and papers going into other parts that build on previous works.\n\nWould say to go through areas on Paperswithcode and googling on some keywords here. Hopefully my late night ramble was coherent enough and to some help for you!",
        "Daily activity ,fall detection elderly people, the main problem diverse dataset are not public.some good are not shared like Chinese one ...so I am facing this issue, augmentation is mandatory solution but stil when we test models on unseen videos I got worst results",
        "I'm right now facing a new application where I'm going to use an external TPU (hailo) and after a quick look on documentation I can see your point on model compression and quantization. \n\nAbout model design, what is your decision process for layers etc.? I've searched it when i start to study DN on my own, but anything exaustive. Do you have any books/resource to suggest?",
        "Gotcha!\n\nMakes sense.",
        "That's very similar to my approach!",
        "so you usually use deep learning only for a part of the project and then using traditional programming for the other, as in my case, right?",
        "Hard work and studying aren't a real issue, time is :)\n\nMy question is really coming from those tutorials, they are just too simple. From this post i learned that a lot more is involved to achieve high performance. \n\nHowever we already have a stable computer vision software for most case scenarios and I think (and hope) the time and money invested in learn will return many times in the future.",
        "That's gold.\n\nAs I understand probably I can split a deep learning application in 3 big groups:\n\n1 -  Simple (as in my case) where I need only to choose the model and train it on my dataset.\n\n2 - Medium where there are some optimization involved like custom optimizer and loss function (in this case, I can still use transfer learning, right?).\n\n3 - Hard where a new architecture model are developed from scratch.\n\nAm i right? However, master just the second scenario will require lot of study and try and error.",
        "I used halo too, I still didn't have an application running in production with it but it seems promising.\n\nBecause I don't have too much time to spend on the optimal design I usually find a good sota or at least near sota quality  but with the nearest domain application I need. Then I remove and change what I need based on my domain and problem.\n\nUnfortunately I had the same problem of finding good DL resources in the past and the problem is that this is a field where the research is running at light speed so a lot of books are already old. For example when I studied DL, transformers didn't exist and after a year everybody started using them. So at the end I just studied from my professor's slides where they are an aggregation of recent papers (and they are private unfortunately). So at the end I never ended up studying from books. I suggest doing a lot of practice, maybe starting from an online course. I heard that Deep Learning w/ Andrew Ng is a good starting point.",
        "yep - almost always :)",
        "You are absolutely right. It's all about time. And yes those tutorials are complete rip offs.",
        "Have you ever used smart cameras for this kind of inspection? For example, cognex has some cameras with integrated AI. I have always been very skeptical about that kind of product, and more I learn, the more I'm!\n\nI think they could be usefull only for very easy applications.",
        "I've tried but they're badddd - esp. cognex in my experience. Generally built-in camera AI sucks e.g. people detection on surveillance cameras is never reliable which is understandable, the models aren't trained for that camera and scene. Custom models FTW",
        "No surprise at all."
    ]
},
{
    "submission_id": "1fnf9ch",
    "title": "[Help with project] Crack segmentation on concrete",
    "selftext": "I'm looking for some advice about the procedure that I'm going through to develop a model to detect cracks. After the meeting with a professor, my group was left disorientated.\n\n1. At first, we went through some popular architecture like U-net, Hr-net, and even Yolov8 Instance segmentation to familiarize ourselves with the problems.\n\n2. After the meeting, the professor told us to define what exactly is a \"crack\". So far we thought that by using CNN, eventually, it would pick up the pattern of a crack like the shapes, the way that the blackish color of the crack is distributed. But now we are stuck trying to find a mathematical way to model the crack.  If I were to find a way to model the crack, would I stray too far from the path of computer vision?\n\n3.  The professor also told us to find a way to predict the development of a crack, I'm not sure if image segmentation can even handle this.",
    "created_utc": "2024-09-23T01:00:34",
    "num_comments": 4,
    "comments": [
        "Your main focus should be the 2nd point. Your prof is right - you need to have a consistent definition of what is a crack: size, depth, form, maybe other characteristics. If you want to use DL for this, you'll need annotated data. Your annotations will be based on that definition.",
        "Speaking as someone who regularly filters resumes for CV R&D, 2 and 3 are the valuable components that a fresh grad resume almost always lacks.\n\nIt’s easy to churn through a few model implementations, generate some Dice/IOU metrics and sound impressive to a layman. AKA basic “kaggle mode”. \n\n2 is valuable in demonstrating an ability to properly define a framework to collect clean and consistent data + annotations, to explore methods across the spectrum of compute power/data requirements and more.\n\n3 is valuable in demonstrating how the problem statement can be reworked and defined in ways that bring more customer value (if your prof has some industry insights).",
        "Thank you, both u/pm_me_your_smth  and you gave me really useful insight. But I do wonder about \"modeling a crack,\" is this done by creating a physical formula based on real-world crack properties, or is based on the patterns found in the available images?",
        "Not sure how one would even create a physical formula for cracks. Theoretically if you manage to do that, you don't need CV anymore. \n\nRealistically speaking, it's better to just go over different data samples, decide which things you will consider as cracks and which not (and annotate accordingly). Next, find or create a suitable model implementation. Third, train the model on your annotated data, see how it performs, decide if inference is fast enough for you, or some other technical requirement you have. From that point you have multiple directions: annotate more data, try a different model architecture, try to improve current architecture, or completely rethink the whole approach."
    ]
},
{
    "submission_id": "1fneeps",
    "title": "Recommendations for High Aspect Ratio OBB Problem?",
    "selftext": "Hi everyone,\n\nI'm working on an oriented bounding box (OBB) task, and I'm facing an issue related to high aspect ratio objects. Here's the dataset I'm using: [Artificial OBB Dataset](https://github.com/QuantumForgeEngineer/artificial-obb-dataset).\n\nI tried using YOLOv8 with its OBB option, but I encountered a problem: it struggles with objects that have a high aspect ratio, cropping them along the longer dimension. You can see some examples here: [YOLOv8 OBB Results](https://www.reddit.com/r/computervision/comments/1dcmyn3/yolov8obb_nano_on_artificial_dataset_model).\n\nIt seems like a straightforward problem, but I haven’t found any off-the-shelf solution that handles this well.\n\nDo you have any recommendations for dealing with this? I’m looking for a flexible approach that could generalize to other shapes beyond the ones in the artificial OBB dataset (i.e., capable of training on datasets with high aspect ratio objects).\n\nThanks in advance!",
    "created_utc": "2024-09-22T23:55:32",
    "num_comments": 0,
    "comments": []
},
{
    "submission_id": "1fnbmgh",
    "title": "Hands-on Learning for AI and Computer Vision!",
    "selftext": "Hello Computer Vision enthusiasts! 👋 I’ve been developing a learning platform that simplifies AI and ML concepts with interactive visual tools, tutorials, and Python code. You can find it on Google using the keyword **101ai**. It's perfect for anyone looking to explore topics like image recognition, convolutional layers, and more in a hands-on way. Whether you’re just starting or looking to sharpen your skills, the platform provides an engaging way to dive into the world of AI. Check it out and let me know what you think! Your feedback is much appreciated.",
    "created_utc": "2024-09-22T20:46:54",
    "num_comments": 0,
    "comments": []
},
{
    "submission_id": "1fn9j3a",
    "title": "Deep learning and machine learning techniques for head pose estimation: a survey",
    "selftext": "Our new papers 2024 with their codes:\n\n1. Deep learning and machine learning techniques for head pose estimation: a survey\n\nPublished in the Journal of Expert Systems with Applications: [https://link.springer.com/article/10.1007/s10462-024-10936-7](https://link.springer.com/article/10.1007/s10462-024-10936-7)\n\nIts code:\n\n[https://github.com/Redhwan-A/SurveyPHE2](https://github.com/Redhwan-A/SurveyPHE2)  \n\n\n\n2. Head Pose Estimation Based on 5D Rotation Representation\n\nPublished in IEEE Symposium on Wireless Technology and Applications (ISWTA):\n\n[https://ieeexplore.ieee.org/abstract/document/10651821](https://ieeexplore.ieee.org/abstract/document/10651821)\n\nIts code:https:\n\n[//github.com/Redhwan-A/HPE\\_5D3](//github.com/Redhwan-A/HPE_5D3)\n\n3. Real-time 6DoF full-range markerless head pose estimation.\n\nPublished in the Journal of Artificial Intelligence Review:\n\n[https://www.sciencedirect.com/science/article/pii/S0957417423027951](https://www.sciencedirect.com/science/article/pii/S0957417423027951)\n\nIts code:https:\n\n[//github.com/Redhwan-A/6DoFHPE](//github.com/Redhwan-A/6DoFHPE)\n\nMore video here\n\n[https://www.youtube.com/watch?v=WWmBZ\\_2eiaE](https://www.youtube.com/watch?v=WWmBZ_2eiaE)",
    "created_utc": "2024-09-22T18:51:04",
    "num_comments": 0,
    "comments": []
},
{
    "submission_id": "1fn8jnh",
    "title": "CAM is a method used to interpret CNN models. If a loss function is added to align the predicted bounding boxes with the CAM heatmap, is it possible to achieve this, and will the loss possibly converge?",
    "selftext": "～",
    "created_utc": "2024-09-22T17:59:38",
    "num_comments": 4,
    "comments": [
        "[https://openaccess.thecvf.com/content/ICCV2023/html/Rao\\_Studying\\_How\\_to\\_Efficiently\\_and\\_Effectively\\_Guide\\_Models\\_with\\_Explanations\\_ICCV\\_2023\\_paper.html](https://openaccess.thecvf.com/content/ICCV2023/html/Rao_Studying_How_to_Efficiently_and_Effectively_Guide_Models_with_Explanations_ICCV_2023_paper.html)\n\nMaybe look into this. Basically exactly what you propose.\n\n  \nBut RRR loss is not very reliable imho. These models learn fake explanations that satisfy the bounding box loss. Only halfway interesting approach is to generate segmentation maps, e.g. [https://2024.ieeeigarss.org/tempdev/view\\_paper.php?PaperNum=551](https://2024.ieeeigarss.org/tempdev/view_paper.php?PaperNum=551)",
        "Why would you want to do that? Feature heatmaps are nice because it creates a visualization interpretable by humans. But neural networks don't interpret images like humans. Forcing neural networks to learn to interpret images like humans would probably hinder the performance.",
        "the cam heatmap is taken from a trained model, so what you're suggesting is like distillation: you train a first model, compute the heatmaps, and train a second model with it. That will converge, but there is no guarantee that the heatmaps are a better supervision than the one used for the first model. The interpretability has little to do with this.",
        "the second link is broken, could you please tell me the title of the paper?"
    ]
},
{
    "submission_id": "1fn3frj",
    "title": "I built an AI file organizer with vision language model that reads and sorts your files, running 100% on your device",
    "selftext": "Hey r/computervision!\n\n**GitHub:** ([https://github.com/QiuYannnn/Local-File-Organizer](https://github.com/QiuYannnn/Local-File-Organizer))\n\nI used Nexa SDK ([https://github.com/NexaAI/nexa-sdk](https://github.com/NexaAI/nexa-sdk)) for running the model locally on different systems.  \n\n\nI am still at school and have a bunch of side projects going. So you can imagine how messy my document and download folders are: course PDFs, code files, screenshots ... I wanted a file management tool that actually understands what my files are about, so that I don't need to go over all the files when I am freeing up space…\n\nPrevious projects like LlamaFS ([https://github.com/iyaja/llama-fs](https://github.com/iyaja/llama-fs)) aren't local-first and have too many things like Groq API and AgentOps going on in the codebase. So, I created a Python script that leverages AI to organize local files, running entirely on your device for complete privacy. It uses **Google Gemma 2B** and **llava-v1.6-vicuna-7b** models for processing.\n\n**What it does:** \n\n* Scans a specified input directory for files\n* Understands the content of your files (text, images, and more) to generate relevant descriptions, folder names, and filenames\n* Organizes the files into a new directory structure based on the generated metadata\n\n**Supported file types:**\n\n* **Images:** .png, .jpg, .jpeg, .gif, .bmp\n* **Text Files:** .txt, .docx\n* **PDFs:** .pdf\n\n**Supported systems:** macOS, Linux, Windows\n\nIt's fully open source!\n\nFor demo & installation guides, here is the project link again: ([https://github.com/QiuYannnn/Local-File-Organizer](https://github.com/QiuYannnn/Local-File-Organizer))\n\nWhat do you think about this project? Is there anything you would like to see in the future version?\n\nThank you!",
    "created_utc": "2024-09-22T13:56:09",
    "num_comments": 3,
    "comments": [
        "Sounds amazing, I'll give it a try later. \n\nI'd love to use such a tool to store photo description and people names (using face recognition) inside the photo metadata so later it can be indexed by file management apps or photo gallery apps.",
        "You just mentioned u would be sharing a walk through video 2 days back😔",
        "As a data hoarder myself, this is exactly what i needed and was looking for. Thanks will def check it out!"
    ]
},
{
    "submission_id": "1fn2j13",
    "title": "detect multiple aruco id's from an image",
    "selftext": "Post deleted ,\n\nas I was able to complete it on my own",
    "created_utc": "2024-09-22T13:16:49",
    "num_comments": 2,
    "comments": [
        "I think you will need to share your code with us.",
        "yep , shared it"
    ]
},
{
    "submission_id": "1fn1nbb",
    "title": "The code is failing to capture image.",
    "selftext": "My camera is functioning properly, python has the permissions to access camera, I tried changing indexing in videocapture(). I don't know what else to do. Someone help!!!!\n\nThis is the error:\n\n    create_user(1, \"lll\")\n      File \"c:\\Users\\chait\\OneDrive\\Documents\\C Language File\\tempCodeRunnerFile.python\", line 24, in create_user\n        gray=cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n    cv2.error: OpenCV(4.10.0) D:\\a\\opencv-python\\opencv-python\\opencv\\modules\\imgproc\\src\\color.cpp:196: error: (-215:Assertion failed) !_src.empty() in function 'cv::cvtColor'\n    \n\n  \nThis is the code:\n\n    import cv2\n    import numpy as np\n    import os\n    import PIL as image\n    \n    def create_user(f_id, name):\n        web=cv2.VideoCapture(0)\n    \n        faces=cv2.CascadeClassifier(\"C:/Users/chait/AppData/Local/Programs/Python/Python312/Python VS Code/Open Cv codes/haarcascade_frontalface_default.xml\")\n    \n        f_dir='dataset'\n        if not os.path.isdir(f_dir):\n            os.mkdir(f_dir)\n        \n        f_name= name\n        path=os.path.join(f_dir, f_name)\n        if not os.path.isdir(path):\n            os.mkdir(path)\n    \n        counter=0\n        while (True):\n            ret, img=web.read()\n            img=cv2.flip(img, 1)\n            gray=cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)\n            multi_face=faces.detectMultiScale(gray, 1.3, 5)\n            for (x,y,w,h) in multi_face:\n                cv2.rectangle(img, (x,y), (x+w, y+h), (0,0,225), 2) \n                counter+=1\n    \n                cv2.imwrite(\"{}/{}.{}.{}{}\".format(path, name, f_id, counter, \".jpg\"), gray[y:y+h, x:x+h])\n                cv2.imshow(\"Image\", img)\n            k=cv2.waitKey(100) & 0xff\n            if k==27:\n                break\n            elif counter>=40:\n                break\n            web.release()\n            cv2.destroyAllWindows() \n    \n    create_user(1, \"lll\")",
    "created_utc": "2024-09-22T12:39:14",
    "num_comments": 5,
    "comments": [
        "It seems to be a temporary file you are trying to run (tempCodeRunnerFile.python). It is probably running an older snippet of the code. Update and run the correct script and you should be good to go",
        "You shouldn’t ignore the “ret” from web.read(). There’s a reason it’s provisioned. Only process frames if ret is True. It’s possible then when img is empty, ret is Flase so try investigating that. But then, I’m not sure why it didn’t error for cv2.flip() and i don’t have access to my system at the moment either to check that for you. \nKindly update here if and when you figure out the issue. Curious.",
        "[deleted]",
        "brooo GPTT on reddit ?? FR ? its just a simple question",
        "Tbh OP should've asked chatgpt in the first place, it's great for such basic things",
        "You can't debug your code.\n\nYou can't use LLM to debug your code.\n\nI wasn't trying to help you, I was exposing you publicly."
    ]
},
{
    "submission_id": "1fmzfvb",
    "title": "Calculate FID score and R-Precision for 3d images.",
    "selftext": "Hello, I have to evaluate some model generated 3d images using FID and R-Precision scores. My approach is to convert these 3d images to 2d images and then do the calculation. Is there any direct method, library or tool that can help doing these on 3d images? If not I would like to know about 2d rendering methods as i have little experience in handing obj files. Thank you!\n\n",
    "created_utc": "2024-09-22T11:04:14",
    "num_comments": 0,
    "comments": []
},
{
    "submission_id": "1fmxkmp",
    "title": "Help with CV problem modeling",
    "selftext": "Hello everyone, I am working on a project where I have to define a specific line on images by detecting two specific objects (one at the end and the other at the starting point). I modelled this as an object detection problem and it works well. But partial occlusion makes things difficult. Sometimes my algorithm only detects one of the points I need. \n\nI am wondering, since my images have kind of a symmetry (not absolute though), would it help to model the problem as pose estimation instead, and also perhaps add auxilliary keypoints that help indicate the symmetry of the image to my model? I thought of this because when looking at an image where an object is occluded I estimate its position by noticing the symmetrical features of the image. So perhaps an algorithm would be helped by this also. \n\nI am using YOLOv8 btw. Any insights would be really helpful, since reannotating with keypoints would consume a lot of time. Thank you!",
    "created_utc": "2024-09-22T09:41:35",
    "num_comments": 1,
    "comments": [
        "i might be able to help with this if you could post some sample images"
    ]
},
{
    "submission_id": "1fmwsmx",
    "title": "Training samples labling proccess",
    "selftext": "guys, have question. I need to label cars for vehicle detection. I downloaded video with traffic moving and there're some parked cars, do I need label them each frame too?(there're 600 samples where these cars are staying in the same place). I just worried that this fact will affect my model's performance, like it will learn that exactly this place has class, or something like this.(example picture with circled in red parked cars)\n\nhttps://preview.redd.it/wl29984qwdqd1.png?width=1593&format=png&auto=webp&s=defddd33120fe3b59ea4856acc9f01839d304c41\n\n",
    "created_utc": "2024-09-22T09:07:11",
    "num_comments": 11,
    "comments": [
        "yep - label all the things you want to the model to learn.",
        "How different are data samples? If it's a sequence of frames extracted from a short video, then I'd advise not to label every image because neighboring video frames are very similar and you should be avoiding having identical images in the dataset.",
        "Well, if you want to teach a model to ignore vehicles in certain area like parking, you can just ignore them in labeling part as well. However, if you need to detect parked vehicles as well, than you need to include them. \n\nIf you are using CVAT, you have propagate option which is great for vehicles that are in same spot for long time.",
        "No, label only significantly different frames. Download another (can be unrelated) video and label it the same way to prevent overfitting to the position. Use already existing datasets. Use image augmentations. Finally, model for car detection already exists (not sure if this is relevant for your use case)",
        "Alright, thanks",
        "I have 333 pictures from this video. I want to make a dataset with around 1000 samples, so I'll find another traffic cam videos and take samples out of it. I cut the video every 3 seconds, so I get the amount of data I need. Is it okay? I'm training yolov8",
        "Okay, I'll download several videos for this purpose. My goal is count traffic on the roads in russia, so there're not good datasets for this purpose, therefore I make my own. I use rotation augmentations, 30, -30, 45, -45 degrees on each sample",
        "Do you need a general model which detects cars on different streets and different angles or are you going to use the model just from this angle only for this street? If the latter, using external data might improve the model, but not significantly, so I'd just wait and accumulate more data from your camera.",
        "I’m pretty sure you can find a decent labeled car dataset for this purpose online, then add the data from your video.\n\nI’d use more augmentations and don’t keep them fixed to +-30,45 degrees, rotate it all the way around. Use color, brightness, crop, scale and other augmentations, the more, the better (as long as the results on the validation dataset are good).",
        "I need to detect cars on the road from different cameras, hence from different angles",
        "Okay, I'll add more augmentation types."
    ]
},
{
    "submission_id": "1fmw0ti",
    "title": "How to Generate a Synthetic 3D Head Dataset",
    "selftext": "Hi everyone,\n\n\nI'm currently working on a project that involves training models for face analysis tasks, and I need to create a synthetic 3D dataset of human heads. Ideally, I'm aiming to replicate the approach taken in the paper \"Fake It Till You Make It - Face Analysis in the Wild Using Synthetic Data Alone\".\n\n\nFor those unfamiliar, the paper discusses generating a large-scale synthetic dataset of 3D human faces. They managed to achieve impressive results using entirely synthetic data, which bypassed the need for real-world annotations or privacy concerns related to using actual images. \n\n\nI want to create something similar, but I'm facing challenges in automatically generating parametrized and rigged 3D heads to simulate different identities, expressions, lighting conditions, etc.\n\n\nI'm hoping to get advice on techniques or tools that can help generate a large variety of parametric 3D human heads. And methods to rig the heads for facial animation, so I can control expressions and potentially simulate motion.",
    "created_utc": "2024-09-22T08:33:07",
    "num_comments": 4,
    "comments": [
        "The authors explained their techniques in more detail in one of their previous [papers](https://arxiv.org/abs/2007.08364). If I understood correctly, they trained a model to morph a base face mesh to different identities and expressions by manipulating vertex positions, weights and a basic skeleton (nothing you haven't mentioned). \n\nIf you have the technical knowledge to figure out how to train a similar model all you need to do is to understand how rigs, blend shapes, weight painting etc. works. There are lots of tutorials on YT. \n\nAnother method could be finding a character creator (ex: [Human Generator Ultimate](https://blendermarket.com/products/humgen3d)) and generate lots of different characters with different blend shapes (age, weight etc.) offline and swap between them and expressions while rendering. \n\nFor expressions, you can use facial mocap programs and transfer captured expressions to your generated faces. Again there are tutorials on how to do it on YT.\n\nWhat I suggest is, if you don't have experience in 3D characters, just research keywords I mentioned in this comment. After that you'll be able to formulate a practical workflow. Also, if you have a budget to hire someone, find a 3D artist to speed up the process, it'll speed up your project.\n\nAlso, when the paper came out, I messaged one of the authors (don't remember which one), and he seemed like a very nice person, maybe he could share some info.",
        "Use blender, it has a python environment built-in and you can write scripts in python to manipulate blender's params.",
        "Very helpful, thank you very much!",
        "Do blender has built-in \"head engine\"?"
    ]
},
{
    "submission_id": "1fmv379",
    "title": "tracking problem",
    "selftext": "hello guys, as you can see in the photos, ball tracking (green triangle) track also penalty spot what i should do to avoid that? thank you\n\nhttps://preview.redd.it/22ahf9khjdqd1.jpg?width=1280&format=pjpg&auto=webp&s=aa797f74a7afa5c86ef00d51139c415880d604a3\n\nhttps://preview.redd.it/kjg40akhjdqd1.jpg?width=1280&format=pjpg&auto=webp&s=f5890cb442d95a053132e9903813ef87a1a1398b\n\n",
    "created_utc": "2024-09-22T07:52:04",
    "num_comments": 4,
    "comments": [
        "What is the tracking method you are using?",
        "It's not a tracking problem. It's a problem with your detection.",
        "your model needs to be more refined add images with both (penalty spot and ball) in frame and annotate the ball only except that   \nso the model learn the ball features and not detect only a circle as a football",
        "* The ball is detected in each frame using the **YOLO model** (`self.model.predict()`), which performs object detection.\n* Then, **ByteTrack** (from the `supervision` library) is used to maintain object identities across frames by associating detections between frames. This is done using the `self.tracker.update_with_detections()` method"
    ]
},
{
    "submission_id": "1fmrgy0",
    "title": "Measuring the width",
    "selftext": "Hi! What is the best computer vision for measuring the width of a filament? We wanted to have a filament of 1.75 mm, and we’re thinking of using Mask R-CNN. Is Mask R-CNN alright in measuring distances? If not, what do you suggest? Thank you so much for your time!",
    "created_utc": "2024-09-22T04:52:49",
    "num_comments": 18,
    "comments": [
        "as far as i know,  \nthe main issue with measuring in real-world requires some sort of reference, that means an object with known dimensions in the image, or a controlled environment (fixed camera distance + no object height variability).\n\nbasically, you need to have a real world measure of the pixel's size. \n\none more thing, am not sure about how much could this affect your measures, but given that it's in the mm, making sure the camera is properly calibrated might be important too.\n\ni hope other more knowledgeable peeps could help you more.",
        "A couple useful links I found\n\nhttps://www.cam2vision.com/technology-and-capability/comparison-of-speed-metering-technologies/mono-cam-image-processing/\n\nhttps://pyimagesearch.com/2016/03/28/measuring-size-of-objects-in-an-image-with-opencv/\n\nhttps://stackoverflow.com/questions/41102478/measure-distance-to-object-with-a-single-camera-in-a-static-scene\n\nhttps://profil-software.com/blog/development/measuring-the-size-of-objects-in-an-image-with-opencv-python-to-assess-the-damage\n\nThis one contains useful info about the image sensor and how it relates to 3D coordinates.\n\nhttps://www.ni.com/es/support/documentation/supplemental/18/calculating-camera-sensor-resolution-and-lens-focal-length.html?srsltid=AfmBOor8rAmziKWzG6Gan4XtWG-TZ2C04xmEPwv-y9x_0CxBN6CYp5jn",
        "2D Cameras natively do quite bad in measuring distances (because they don’t perceive 3D). However you can get this indirectly by fixing unknowns in your system.\n\nFirst, you can probably fix the distance to the camera , so your calibration equations have one less variable.\n\nThen, you can use a fixed camera, which means constant calibration parameters.\n\nThird, as some other Redditor pointed out, you can use a reference. Once you know what good looks like, and then how bad scales with width (via calibration parameters), you can figure out distances.\n\nFourth, in practical systems, cameras always have some small variations due to movements over time. Figure out a way to periodically correct for this via calibration using reference or any other way.\n\n^^ all of the above are if you’re using a more scientific approach to solve the problem. I have tried similar things with depth estimation and the likes but I wouldn’t trust it with extremely fine measurements.",
        "if there use case is pretty narrow, i.e. most of the images are like this you can probably just use a simple threshold.",
        "Personally, I'd first line fit both edges.  Then I'd pick a central point on one of the lines and compute the distance from this point to the other line.   This gives you the filament width in pixels.  To convert to real units, you can generally calibrate a single scale factor of, for example, mm/pixel.  You multiply this scale factor times the width in pixels to get the width in mm.\n\nThe above assumes your distance to the object doesn't change.  If it does, you'd need a different approach like a telecentric lens or a second camera.  A second camera would make the calibration much more complicated, so I wouldn't recommend it unless it's your only choice.",
        "The main issue you'll have with standard lens is how to find distance to the object since your object is not flat but a cylinder and you specifically need single plane of this cylinder. Therefore, the only valid approach I can think about is telecentric lens where the scale between pixels and metric units is not dependent on distance to the object. Maybe there are some good approaches with microscopes but I'm not familiar with that",
        "Calibrate your acquisition system and perform measurements at a known distance, or use one target as reference in each picture. Then use some line detectors, like Steger algorithm (now implemented in HALCON as “detect line” algorithm, that basically consists in a sophisticated ridge detector: in this way you can compute, for each pixel, center line position and real width (perpendicular distance to the edges).",
        "What does it have to do with MaskRCNN . . . ?",
        "I super appreciated this! I hope you will have a good lunch forever. Thank you so much!",
        "Thank you so much for this!!!!!! I hope you live until 90. Thank you again!!",
        "To add to that already good set of suggestions, if you can use multiple cameras you can take the average measurement from both.\n\nCan you project structured light onto the filament? ",
        "Thank you thank you so much!!!",
        "To add to that already good set of suggestions, if you can use multiple cameras you can take the average measurement from both.\n\nCan you project structured light onto the filament? ",
        "To add to that already good set of suggestions, if you can use multiple cameras you can take the average measurement from both.\n\nCan you project structured light onto the filament? ",
        "Yes, so the filament will be watched by camera real time, so it has constant distance from the camera to the filament. I will be researching simple threshold. Thank you so much!",
        "This is noted. Thank you so much!!!!!",
        "We’re trying if Mask R-CNN can measure the width hehehehe",
        "Plot twist. They’re already 92",
        "no good lunch for me? xD",
        "No! You will always have a good dinner for the rest of your life!"
    ]
},
{
    "submission_id": "1fmr6vg",
    "title": "3D CV Research groups in US ",
    "selftext": "Hello Everyone,\n\nDoes anyone here know any research group in any US university working on topics like 3D computer vision,  geometry problems, and visual mapping and localization. \n\nAny leads would be appreciated. ",
    "created_utc": "2024-09-22T04:36:08",
    "num_comments": 0,
    "comments": []
},
{
    "submission_id": "1fmqaqq",
    "title": "Image measurement and dimension analysis ",
    "selftext": "Hi guys , i developed a code using opencv to identify the dimensions of a image based on the left most object in the image's dimension. Now i need to develop it into a android app kindly assit me in converting it into a android app i tried various methods but none worked .",
    "created_utc": "2024-09-22T03:37:34",
    "num_comments": 0,
    "comments": []
},
{
    "submission_id": "1fmjwmx",
    "title": "How to pixelate an image,find its gray value,and mask it by a circle with cpp?",
    "selftext": "I am beginner in CPP I don't have any idea about it.\nI have assignment on it,please help me!!!",
    "created_utc": "2024-09-21T20:16:13",
    "num_comments": 6,
    "comments": [
        "what have you tried so far, and does it include reading the course material?",
        "Look into OpenCV. If you are new to C++ look into vcpkg as well. It will help with dependencies. \n\nWhen you say pixelate what do you mean? Blur it? Shrink it? Expand it? So many things you could me. \n\nTurning something gray just means reducing it’s 3-color components (RGB) into a single channel with zero being black and 255 or 1.0 being white. Some people with say you need to weight the color channels, others will just add them together and divide by 3. As long as it’s one channel it’s gonna end up gray.",
        "Be way is to ask my good old friend chatgpt.com.",
        "Assignment was on how to get the gray value of a pixel in an image and I need to mask the picture to make it into a circular shape just like instagram pfp",
        "Thanks 😂",
        "For circular masking, just write a function that takes in the x and y coordinates of a pixel to verify if it lies in the circle e.g (x - a)^2 + (y - b)^2 <= d, where (a, b) is the center pixel coords and d is the diameter of your circle/width/height of your image. If it fullfills the inequality, just keep the pixel value, if not, just zero it."
    ]
},
{
    "submission_id": "1fmd5r6",
    "title": "How to create GUI for my MVP (Computer Vision) with none experience with UI?",
    "selftext": "Basically I'm a Data Scientist and I'm developing an MVP for my AI-Startup, it's related to Computer Vision but how can I create the UI, I have been stuck on this part. Streamlit? just too simple. HTML, CSS , tried chatgpt but it can't properly, it gets stuck att least somewhere though I guess. Anvil? umm..not really? \n\nWhere do I really go with this one? It's just the GUI that I need to get done with. This is what I'm looking for : [https://www.canva.com/design/DAGRNEup3yk/DgTOiWtJrTTLayFPWd-R3w/edit?utm\\_content=DAGRNEup3yk&utm\\_campaign=designshare&utm\\_medium=link2&utm\\_source=sharebutton](https://www.canva.com/design/DAGRNEup3yk/DgTOiWtJrTTLayFPWd-R3w/edit?utm_content=DAGRNEup3yk&utm_campaign=designshare&utm_medium=link2&utm_source=sharebutton)",
    "created_utc": "2024-09-21T14:16:45",
    "num_comments": 13,
    "comments": [
        "Yeah ill say that requires a front end with a python backend. Have you tried [NiceGui](https://nicegui.io/)?",
        "What languages are you familiar with?  If Python I’d suggest you learn some dash",
        "PySimpleGUI is really easy to LLM some scaffolding. It will suck but it’s adequate. \n\nMore complex is to monorepo in fastapi + next.",
        "Cursor will make your ui with tkinter (python) in 2 seconds. Had it make on for my cv app.",
        "pysimplegui, kivy, tk will all get the job done. Prompt [Claude.ai](http://Claude.ai) to get you started.",
        "Streamlit is good n easy n strong balance between gradio n another thing(forgot name).\nStrongly recommended...",
        "Just do it in napari you can write buttons from functions by just decorating it with @magicgui and if your data is in numpy arrays already it’s cheese",
        "I work with cool developers from Turkey. They charge 20$/hour and do a nice work. I can connect you with them if you want",
        "Hey I can help you to build frontend If you open up for any help please DM",
        "My team has used Gradio and it works all right. A little limited but works well.",
        "Yeah probably nicegui is his best option",
        "Python only, Dash is mainly for data-driven applications so that's an obstacle",
        "What about flask?\n\nhttps://blog.miguelgrinberg.com/post/the-flask-mega-tutorial-part-i-hello-world"
    ]
},
{
    "submission_id": "1fmcyha",
    "title": "Text to Video Diffusion Models: A video survey",
    "selftext": "Sharing a YT video I made on the recent architectures and algorithms used to train Text to Video Diffusion Models… going through the seminal papers/approaches from the last few years, like  VDM, Make A Video, Imagen, Video LDM, CogVideo, DiffusionTransformers, SORA, etc. Hope yall enjoy! Leaving a like on the video helps out the channel, appreciate it.\n ",
    "created_utc": "2024-09-21T14:07:06",
    "num_comments": 0,
    "comments": []
},
{
    "submission_id": "1fmas3u",
    "title": "Can anyone de blurr the license plate on this jeep?",
    "selftext": "Can anyone de blurr the license plate on this jeep. They stole 50k in cash from me. I’m a dealership",
    "created_utc": "2024-09-21T12:26:04",
    "num_comments": 12,
    "comments": [
        "Reminder not to just take strangers on the internet at their word, especially when it comes to identifying people. Sure, maybe the owner of that jeep did steal 50k in cash from OP. But maybe it's a partner fleeing an abusive relationship and OP is trying to find them. \n  \nIf a crime has been committed it should be reported to the police.",
        "[deleted]",
        "ENHANCE!",
        "If you’re keeping 50k in cash laying around you need to reevaluate some business practices",
        "The video would be better than one still frame",
        "Enhance!",
        "Export the video as high quality as possible and post it here.  That should be no problem if this is your dealership.",
        "The police can track the two cellphones and use that to identify the two individuals, who might have some information.\n\nNext time buy sharper security cameras and more of them. ",
        "the better quality would be is to extract frames, not screenshots, if you could do that maybe it would easier to deblur",
        "Any help would greatly be appreciated. I’m pretty devastated right now and have no hope in the police helping",
        "I mean I can show you the police report if that makes you feel better.",
        "Didn’t work",
        "[deleted]",
        "Yeah that didn’t work"
    ]
},
{
    "submission_id": "1fmarpc",
    "title": "Apples and oranges",
    "selftext": "I was wondering the following:\n\nIf I have two classes, apples and oranges, each with a dataset of precise crop images, and I train the best SOTA classifier to predict whether an image contains an apple or an orange (binary classification) using this dataset, what happens if the classifier does not perform well?\n\nSpecifically, does this mean that the best object detector will also be unable to successfully distinguish between apples and oranges? The object detector might correctly predict the bounding box, but it will definitely missclassify the object.\n\nLet me know what you think!\n\n",
    "created_utc": "2024-09-21T12:25:34",
    "num_comments": 8,
    "comments": [
        "> what happens if the classifier does not perform well?\n\nThen your work is poor and you need to re-do it. It doesn't say anything about anything else. You just didn't do a good job.",
        "Sometimes apples do look like oranges....\nIf each class has too much variation, the problem might be too difficult. You should see evidence if that during training.",
        "Why do you think there would be a correlation? An object detection network might have learned features that the classifier missed because object detection demands a more detailed understanding of an image than classification.",
        "I gave the example of apples and oranges to keep things simple.\nI am not talking about the work here. What I want to focus on is the relation between classification and object detection.\nYou just missed the entire point. Unbelievable.",
        "It is actually what I am seeing - validation is terrible (all its metrics).\nBut if a classifier does not work (say I use the best one on imagenet1k), this leaves no chance for a detector, or I am wrong ?",
        "Typically object detectors have a backbone similar (somehow) to classifiers.\nIf a strong classifier cannot even separate crops of apples vs. oranges, how can an object detector learn more ? I am talking about the best classifier we can use.\nOr you mean that the object detector classification head can be better than a fully fledged classifier ?",
        "Learning depends on the objective/loss function, not simply what the backbone is composed of. Backbone would determine the capacity of learning, but that doesn't mean any classifier with that particular backbone has reached the extent of its capacity, especially when classification models are trained with very simple loss functions.\n\nObject detectors, on the other hand,  are being trained with multiple objectives, so they will be forced to extract more informative features which could in turn help them generalize better.\n\nA classifier can more easily get away with cheating or memorizing specific features that don't generalize well outside the training data.",
        "Solid point.\nWill keep it in mind!\nThank you!"
    ]
},
{
    "submission_id": "1fmaiwt",
    "title": "Computer vision is 'AI' the same way Apollo 11 is an 'aircraft carrier' --- pls use the right terminology, people! Also to me, as a non-expert in CV who's worked vocationally on a face detector with blink analysis, this is full of technical errors and non-truths.",
    "selftext": "",
    "created_utc": "2024-09-21T12:13:58",
    "num_comments": 19,
    "comments": [
        "I don't feel like this video fits this sub and should be marked as spam \nThe people in the video are not even computer vision experts and this is a nonsense non technical video.",
        "I've trained face recognition on massive datasets with different skin tones.\nThe issue is not the skin tones.\nIt's people using shitty images from cctv or really really low resolution images , if your target face picture is crap, your results will be crap.",
        "Traditional image processing and CV techniques still rely on gradients. Dark skin is hard to get good gradients on.",
        "So I watched further into the video, and it seems like this dude does not know _Shit_ about the subject matter at hand!\n\nHe recites the good ol' \"Face recognition software works worse on black faces\" crap, an excuse that CV juniors use when their software does not work well!\n\nAgain, I am not academically trained in CV, I just vocationally worked on [several](https://github.com/Chubek) CV software. I don't think it's true that 'dark-skinned' faces get less recognition by classical tools. Maybe modern, DNN-based tools that are trained on more light-skinned faces than dark-skinned faces _could_ fumble the bag, but in most these software what's important is __topology__ of the face, not its color!\n\nCorrect me if I am wrong here, but most facial-reco tools first get a point cloud of the facial features. And before you say \"well, white skin reflects more light, so the sensor sees more light!\" --- that's not how light, or human skin works. Actually, I'd argue that darker skin exudes _more_ light than lighter skin!\n\nPeople who say shit like this pretend 'dark skin' is like, this black whole that scrapes all the light from its surroundings and swallows it!\n\nThere's dozens of papers on this sujbect. Just go on Google Scholar and find one to hand to your employer when it fails to detect a black or darker face, but deep within your heart, you know it's bullshit, and these papers admit that too.\n\nThere's the problem of white-balance with darker skins, that I won't argue. My skin looks very, very dark when the sensor's WB is set wrongly. Skin colors look different under different Kelvins of light temperature. I have 2 LED and one flourscent light in my room. My skin looks darker, even to my own eyes, when only the flourscent light is on. So blame this on bad sensors, not bad software!",
        "What is a 'gradient'? Don't methods like ORB and SURF rely on point clouds --- and these both are 'traditional'?",
        "It probably took longer for you to write that text than it would to see that the face recognition performance is actually worse for people of colour (or the other way around: it is best for north American and central European males). The results of the \"gold standard\" test for face recognition are here: https://pages.nist.gov/frvt/html/frvt11.html",
        "There is less contrast in images of black faces assuming low lighting or a dark background. Hard to define edges etc. HP and Xbox Kinect had facial recognition issues due to low contrast in dim lighting. This isn’t just a DL issue, HOG would struggle in this scenario too. If we are talking about monocular facial detection, approaches aren’t making use of 3D topography, they are inferencing from 2D features from the RGB image.",
        "absorption coefficients for dark skin are ∼ 6 % to 74% greater than for light skin in the 400 to 1000 nm spectrum.",
        "Forget it, just read your other comment. You're just trolling people to get information, acting like we're some puppets. I'm not going to give you any more of my precious time, you disrespectful troll. Deleted my other comment. You don't deserve my time.\n\nYou'll be the first one I blocked. I hope nobody answers you until you learn how to ask questions nicely than to trigger people online. Learn some manners.",
        "I love how you are pretending to sound knowledgeable and nobody here is giving u anytime because a lot of the things you are saying actually make no sense at all lmao XD \n\nPoint clouds have nothing to do with SURF or SIFT or any other traditional methods. They rely on key point detection and at some point you have to find the determinant of the Hessian. That's where you need gradients. Gradients = derivatives. Derivatives are in laymen terms a differentiable rate of change. The less contrastive a piece of information is whether it be an image or audio or anything else the harder it is to obtain good gradients to learn features that represent the pdf of the data.",
        "This does not negate my point that it's a sensor issue and not a software issue. The software does its job. The sensor fails to capture a good image. As I said I am not academically trained in this field, and I am not going to argue with an expert. But you're the expert and you just agreed with me --- no matter how partially; so I feel like there's no need to argue here at all. Sensor-makers need to sort their shit out not CV people.",
        "I'll just ask ChatGPT then. [I did](https://chatgpt.com/share/66ef8085-6844-800d-846a-97ccdde6d901). This is better than any response by y'all NPCs. I don't understand why I waste my time on the fora when there's ten Library of Alexanderia's embedded within a million Library of Nishapur's at my fingertip. Neither Arabs, nor Mongols can destroy this library. It's got a kind librarian who answers to my every query --- in full details. And if I _feel_ like he's being untruthful or 'hallucinating', I can just check a fucking book, which I can download from the _other_ library of Alexanderia (you know the one!).",
        "If it's a sensor issue, it's a physics issue and not that sensor producers need to do a better job. You just seem like you're talking out of your ass tbh.",
        "You know, sensors just perceive the color and transmit that information. And some colors are hard to distinguish. I think you're confused, you want to make black people white or what? Because you can't perceive a photon that is non-existent.",
        "We also don't know why you're here.",
        "We could plausibly make binocular cameras the default, no? I think even the width of a phone would be sufficient for good depth information in many cases. No way consumers would go for it though.",
        "So I feel like I have the best skin color because it's neither too light or too dark. I just looked at the flimsy side of my upper-arm (I __don't__ lift, yes, only a brainlet would go to the gym! And I am _very_ smart), part of skin which would be untouched by sunlight (unless you wore wife-beaters all your life) --- it would be `#00ff6b` under all my 5 lights (3 LED, 2 flourscent) --- but, as dark as this color is, I have never felt any need to adjust censors during face-detection. I don't wanna _make black people white_, I just wished if people realized that, computers are not their enemy, and nobody's out to get them.",
        "Man, it's just... Serious question, are you high or something?\n\nEDIT: He admitted in one of his comments that he was just trolling people to make them mad and get actual answers. Ignore him."
    ]
},
{
    "submission_id": "1fm9e07",
    "title": "Dataset of Person in WiFi / DensePose from WiFi",
    "selftext": "Does anyone know from where can i get the dataset of Person in WiFi or DensePose from WiFi papers? ",
    "created_utc": "2024-09-21T11:22:07",
    "num_comments": 8,
    "comments": [
        "Huh?\n\nAre you talking about using wifi signals to find people? ",
        "Both are papers which work on Pose estimation using WiFi Signals. Instead of using a cam to estimate poses",
        "In that case I’d suggest contacting the authors of those two papers.",
        "Yeah, I'm planning to do that. Will mail on Monday, tmrw",
        "Hi,, I hope you are in sound health ,, I also want to go through some project related to it ,, Can you tell me about it if you have found out some solution",
        "Nope.  No solution yet :(",
        "Sad :-/ Wish you Good Luck , If i found out some solution, i would ping you here",
        "Likewise :)"
    ]
},
{
    "submission_id": "1fm92r4",
    "title": "PointNet and pointcloud classification",
    "selftext": "I have a question on the architecture used by the PointNet model.\n\nIf you look inside it you will find one of the first block to be a T-Net that based on the combination of the points it estimate an optimal transformation matrix to align the cloud to a canonical space. That's nice, is uses the information from all the points combined.\n\nNext it needs to start extracting features from each point, so it apply each point to a MLP that remap the point to a new space of dimension 64.\n\nWell here I start loosing track, while the T-Net uses the combination of all points, the MLP layer takes as input one point at time, so it have to extract feature and meaning from just the position of that point.\n\nI think that for giving meaning to a point one should look at the point surrounding it.\n\nAt first I thought that the T-Net also was performing a mapping in a space where each point have coordinates that carry some aggregated info, but everyone says that's just aligning to a canonical space.\n\nSo where the combined info of the cloud is used to extract the features?",
    "created_utc": "2024-09-21T11:07:43",
    "num_comments": 0,
    "comments": []
},
{
    "submission_id": "1fm7itz",
    "title": "Help in blending objects into a background template using AI",
    "selftext": "I have a PNG image of a product (a perfume bottle with no background) and an example of an e-commerce friendly background. I’m looking for a way to seamlessly blend the product into this background using AI, similar to what the website claid.ai does with \"template generation.\"\n\nAny suggestions on models or techniques (GANs, diffusion models, etc.) in Python to automate this?\n\nThanks for your help!",
    "created_utc": "2024-09-21T09:57:48",
    "num_comments": 3,
    "comments": [
        "Check out deep image blending\nhttps://github.com/owenzlz/DeepImageBlending",
        "These objects are clear and you want realistic lighting and shadows and stuff?"
    ]
},
{
    "submission_id": "1fm6zm0",
    "title": "Distance Estimation ",
    "selftext": "I'm currently working on an object detection project and am trying to incorporate distance estimation for the detected objects using a single camera. So far, I've experimented with using motion parallax, as well as the height and width of objects in the frame to estimate their distance from the camera. However, these methods have proven to be quite inaccurate and unreliable. I'm looking for more precise techniques or approaches for distance estimation with a single camera. If anyone has experience or suggestions on how to achieve more accurate results, I would really appreciate it!",
    "created_utc": "2024-09-21T09:33:43",
    "num_comments": 11,
    "comments": [
        "Yeah stereo depth estimation has been around for a while.\n\nThere are depth estimation AIs that are cool giving relative depth.\n\nAnd of course Lidar based depth estimation.\n\nI'd start with stereo depth estimation. Can buy a cheap stereo camera for $80 on Amazon and look into OpenCV for there libs on it.",
        "What's the application? Getting accurate depth from a single camera alone in general isn't easy (isn't possible?). But there might be ways to cheat depending on the application. For example if you're only interested in a specific object with known dimensions, or if the environment is always the same, etc.\n\nUsing motion parallax can be done accurately, it's done in robotics a lot but usually with an IMU to help accurately determine the motion of the camera between frames. \n\nIn other applications if the object or camera moves in a known motion, that information can be used to accurately determine depth.",
        "Take a look at this: https://arxiv.org/abs/1809.10548",
        "Knowing the camera intones And extrinsics, I would use object detection as a look up table  to identify items and get their size to calculate. Also you can check out ai models that estimate depth",
        "My mistake, I should've mentioned I was trying to achieve this with a singular camera. Stereo depth estimation would be easy to implement and I don't have a problem with that. I was just wondering if it was possible to get distance estimation somewhat accurately with a single camera.",
        "The application I'm developing will run on a single PC or laptop and has the ability to switch between multiple cameras connected to the device. It's primarily designed for security purposes, where estimating the distance of detected objects could be very useful.\n\nSince object detection relies heavily on the model being trained to recognize specific objects, my focus is on estimating the distance for those particular detected objects.\n\nI’ve already experimented with motion parallax, but I struggled with it. The distance estimates I got were wildly inaccurate, often in the range of thousands of meters, which clearly isn't feasible for my use case. Additionally, the computational demands of motion parallax caused significant slowdowns in the system. While optimization could help reduce this overhead, I'm hesitant to spend too much time on it, especially since I'm not fully confident in my ability to implement it effectively.\n\nAlthough I may revisit motion parallax in the future, right now I’m exploring other potential methods to estimate distance before diving back into that approach.",
        "I’m actually experimenting with that approach right now. It does provide accurate results, but I’m encountering a major issue when the detected objects are partially offscreen. To address this, I’m working on using the known width and height of the objects to calculate their aspect ratio. By comparing this ratio with the detected object, I should be able to determine which part of the object is offscreen—whether it's the width, the height, or both.\n\nOnce I’ve identified the portion that’s missing, I plan to apply some post-processing techniques to correct the distance estimation and improve the overall accuracy.",
        "Yeah unfortunately it's just kind of a physics problem. Cannot really judge how far away something is without two points of reference.",
        "Unfortunately, not reliably. Even with models trained for monocular depth estimation, you'll still need some postprocessing to minimize reprojection errors.",
        "you can try depth anything"
    ]
},
{
    "submission_id": "1fm6bl0",
    "title": "Does it know the exact position of everything in terms of coordinates?",
    "selftext": "https://preview.redd.it/y0x99pczp6qd1.png?width=1572&format=png&auto=webp&s=0fea43b10bb2e1692f3380129ad2cb41e082d84a\n\nLets assume the image as a canvas does Chat GPT know where exactly on the canvas the glasses are located for example? Like if we take the top left coordinates of the image as (0,0) would it know what coordinates the glasses are located on exactly? I'm planning on using that information to draw an in-paint mask automatically. What computer vision model would I use specifically for obtaining that information? ",
    "created_utc": "2024-09-21T09:03:03",
    "num_comments": 8,
    "comments": [
        "No, you could look at Segment Anything (SAM) to try to get a mask of the glasses. And maybe do some prompt engineering to use them together to get what you want",
        "Not likely. There are models that can fine tune to know it, like shikra\n\nAs others have said though, you're looking for either semantic segmentation or instance segmentation.",
        "I think getting pixel coordinates is relatively easier -as mentioned by others- compared with the 3D space projection of the window in the room and getting world coordinates for that. Point cloud and stereo camera is what I've been testing for the past couple of weeks but when you need hundredth decimal precision, everything is just an estimation. Segmentation and supervised methods (if you are dealing with various orientations) should be useful for your need.",
        "I reckon you'd need to segment the glasses, use pose estimation to best-guess at where the glasses are sitting in 3D space, then best-guess at its dimensions from classifying other objects. Or more reasonably you could just assume glasses are always the same size",
        "There should be a an LLM Computer vision subreddit so we have less of these posts in here",
        "Thank you for the suggestion! I just looked into it and it seems close enough. I still wish to use an inpainting technique that could only replace the glasses. I've defined the segments but now I can't figure out how to only edit a particular segment. Is this possible?",
        "Disagree, I think it’s an interesting topic and definitely much better for discussion than the “deblur this plate” or “tell me what project I should do for my college class”",
        "LLMs are becoming an integral part of computer vision. Not everything is a tightly constrained domain. "
    ]
},
{
    "submission_id": "1fm212u",
    "title": "CNN maths",
    "selftext": "",
    "created_utc": "2024-09-21T05:36:34",
    "num_comments": 0,
    "comments": []
},
{
    "submission_id": "1fm0vrp",
    "title": "Why is no one using local",
    "selftext": "Hey,\n\nI saw all the youtube tutorials are using either jupyter or something online instead of local python code editor like VSCode for example. \n\nWhy?",
    "created_utc": "2024-09-21T04:28:50",
    "num_comments": 15,
    "comments": [
        "Mainly because python's notebooks are suitable for tutorials. For real cases and projects, vscode is always used, with the exception of data visualization or for data preprocessing where notebooks are still useful.",
        "most people don’t have a local gpu workstation. you can easily sink hours and days into configuring and dealing with dependency collisions.  colab just works.",
        "[deleted]",
        ">online instead of local\n\nBecause sometime you don't want to setup python (or can't; due to some reason).",
        "People also use jupyter locally.  Don’t compare it to writing code in an editor, its meant to be an improved experience over the python REPL interface.  If you don’t know what that is, just run python on the command line and start writing code.",
        "I am super local... I have a GPU box in my apartment that runs kubernetes with juypter notebook, dask, Spark, and a few other wiz bangers, all running on a zerotier network and I even gave myself DNS records so I can just connect to the juypter server with the dns name in vscode from my other computer.\n\nBut in special.... Pretty much the other reason I do it is because I have the equipment and the knowledge to do so.\n\nI wouldn't recommend it. I've done the calculations about how much it would take to pay off my little setup versus just renting from Vast or something. Wouldn't break even running 24/7 computation until 4 years later",
        "Easy access, beginner frendly, no hardware restrictions.",
        "You can get Jupyter working in vscode too.",
        "As mentioned by others, jupyter can be as local as vscode. I personally start the code and fine-tune it in a small scale at vscode (because it feels more natural and straightforward I guess) and once I'm sure about the parameters, syntax, and all the other bits and pieces, I take it to jupyter in Jetson to scale up and exploit full potential GPU. So, everything is quite local I would say.",
        "Easier to explain visually what is going on step-by-step.\n\nEasier to share a link to your Colab notebook.\n\nEasier to plot and show images.",
        "Once you get to the point where you can afford a $5000+ gpu workstation you’re most likely a professional who doesn’t need tutorials. That’s the obnoxious response lol.\n\nThe other is that having a “standard” environment like colab takes out some room for mistakes so the tutorial developer and users can focus on what they’re trying to teach/learn rather than on setting up the environment. ",
        "Probably most people get turned off trying to install the GB's of python spaghetti code, dependencies, and versions, which can be a drama in itself to get a working environment.",
        "Because jupyter collab is good enough for the tasks in those videos",
        ">vscode is always used\n\nPycharm gang rise up!",
        "Hours and days? Weeks, months, the rest of my life maybe",
        "Yup.\n\nIn my domain it takes an hour to load my dataset and I like to do some EDA on it before configuring a training run. If I continue training in the same notebook I can avoid another hour long initialization. "
    ]
},
{
    "submission_id": "1fm0627",
    "title": "Lightweight machine vision",
    "selftext": "Hi I really need some help. Does anyone have any experience in using mobilenetV2 and train it using COCO2017 to detect people. I am stuck and processing the dataset to change it to a Tensorflow dataset. and when i managed to change, it is not parsed? correctly which results in error when i do model.fit(). I would take any help i can get.\n\nUpdate:  \nSry this is my code\n\n    # Load the COCO dataset\n    (ds_train, ds_val), ds_info = tfds.load(\n        'coco/2017',\n        split=['train[:80000]', 'validation'],\n        with_info=True\n    )\n    IMG_SIZE = 224\n    NUM_CLASSES = 80  # Number of valid classes, plus one extra for invalid samples\n    \n    def preprocess(sample):\n        image = sample['image']\n        \n        # Check if 'objects' and 'label' exist and are valid\n        if 'objects' in sample and 'label' in sample['objects'] and len(sample['objects']['label']) > 0:\n            label = tf.cast(sample['objects']['label'][0], tf.int64)\n        else:\n            # Assign a default class for invalid labels (e.g., the extra class NUM_CLASSES)\n            label = tf.cast(NUM_CLASSES, tf.int64)\n        \n        # Resize and preprocess the image\n        image = tf.image.resize(image, (IMG_SIZE, IMG_SIZE))\n        image = tf.keras.applications.mobilenet_v2.preprocess_input(image)\n        \n        return image, label\n    \n    # Apply preprocessing\n    ds_train = ds_train.map(lambda sample: preprocess(sample)).batch(32).prefetch(tf.data.AUTOTUNE)\n    ds_val = ds_val.map(lambda sample: preprocess(sample)).batch(32).prefetch(tf.data.AUTOTUNE)\n    \n    base_model = MobileNetV2(input_shape=(IMG_SIZE, IMG_SIZE, 3), include_top=False, weights='imagenet')\n    base_model.trainable = False\n    x = base_model.output\n    x = GlobalAveragePooling2D()(x)\n    x = Dense(1024, activation='relu')(x)\n    predictions = Dense(ds_info.features['objects']['label'].num_classes, activation='softmax')(x)\n    model = Model(inputs=base_model.input, outputs=predictions)\n    \n    model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n    history = model.fit(ds_train, epochs=10, validation_data=ds_val)\n    \n\nTypeError                                 Traceback (most recent call last) Cell In\\[48\\], line 20 17     return image, label 19 # Apply preprocessing ---> 20 ds\\_train = ds\\_train.map(lambda sample: preprocess(sample)).batch(32).prefetch(tf.data.AUTOTUNE) 21 ds\\_val = ds\\_val.map(lambda sample: preprocess(sample)).batch(32).prefetch(tf.data.AUTOTUNE) TypeError: in user code:\n\nTypeError: outer\\_factory.<locals>.inner\\_factory.<locals>.<lambda>() takes 1 positional argument but 2 were given\n\nI didnt managed to save the error when i was at the [mode.fit](http://mode.fit) stage but it has something to do with the shape of the sample",
    "created_utc": "2024-09-21T03:40:54",
    "num_comments": 2,
    "comments": [
        "> it is not parsed?\n\nWhat exactly does this mean? Do you get an error? What's the error message?\n\n> results in error\n\nWhat's the error? What's your code?",
        "Hi thanks for replying i updated the post. Sorry"
    ]
},
{
    "submission_id": "1flyqmt",
    "title": "Question about Scale AI (scale.com)",
    "selftext": "I recently came across Scale AI (Scale.com). They provide data and a data platform for companies to train AI algorithms for applications like for example self driving cars. \n\nMy question is that with google and Tesla having tons of data available and also companies like facebook open sourcing their algorithms each passing day, is it still viable idea to have a startup that provides video and image data like Scale AI? \n\nI can at least think of examples like data from developing countries whose road and driving conditions might still be new to many well trained algorithms. But is there anything else what platforms like Scale AI are not addressing?\n\nLooking for some insights from people in this regard.\n\nThanks",
    "created_utc": "2024-09-21T01:55:43",
    "num_comments": 3,
    "comments": [
        "I don’t know if actual mainstream car companies are using services like that. At least I hope not lol.\n\nOne would think that the Teslas and Toyotas have their own research and development departments and if they’re using the cloud, it’s on EC2 instances or something like that which is closer to bare metal versus a service oriented specifically for Cv.",
        "They provide data annotation services rather than raw data.",
        "Not to be nitpick, but Scale provides data annotation / labelling services as well.\n\nJust wanted to add that note to be sure you fully understand what they’re providing, and the other comments on this post reflect that, too."
    ]
},
{
    "submission_id": "1flw4v3",
    "title": "A computer vision project for my final year",
    "selftext": "Hey Guys,\n\nWould really appreciate if someone could give me a sort of good and novel idea (which is achievable at an undergraduate level) for my final year project. So far, I am clueless on ideas. ",
    "created_utc": "2024-09-20T22:44:02",
    "num_comments": 11,
    "comments": [
        "This is why we do literature reviews my guy. Find an area you're interested in, search Google scholar for core research, start looking through the works that cited the core research. Eventually you will get to what's called the \"knowledge frontier\" (i.e. the most recent papers on the topic). At this point you will be able to identify the gaps where you can do some work.\n\nFor a final year project you don't have to do anything too ambitious",
        "Just completed my final year project, Intelligent Road Quantification System.  \nWe developed a system for detecting and predicting road defects, such as potholes and cracks, using a combination of machine learning models. A U-Net model was trained for precise segmentation, while YOLO was used for initial defect detection. Detectron was incorporated to further refine object detection, ensuring accurate classification of road defects. Additionally, a Gaussian Hidden Markov Model (GMMHMM) predicts future positions of these defects by analyzing temporal data from video frames. The integration of these methods resulted in an efficient solution.\n\nyou may take an idea from this.",
        "I found that there is surprisingly little literature on image regression tasks, i.e. deriving the value of a continuous dependent variable from image inputs. There’s a few toy examples, like guessing the age of a person (face), but nothing serious.",
        "One shot camera calibration. More like optics project but it's pretty useful in computer vision.",
        "[https://github.com/farukalamai/top-100-computer-vision-projects-idea-for-2024](https://github.com/farukalamai/top-100-computer-vision-projects-idea-for-2024)",
        "Why not make a game based on computer vision? I once made break the brick game using pure OpenCV and where I controlled everything by gesture. I also did, Kill A Fly game too. I could share the link to those in dm. I think I used Mediapipe in some of it.",
        "thank you so much for the feedback.",
        "Wow, congrats! Do you have the project on github? I'm curious how you achieved the future positions of the defects. Also, which yolo version did you use?",
        "[https://github.com/ashishpatel26/500-AI-Machine-learning-Deep-learning-Computer-vision-NLP-Projects-with-code?tab=readme-ov-file](https://github.com/ashishpatel26/500-AI-Machine-learning-Deep-learning-Computer-vision-NLP-Projects-with-code?tab=readme-ov-file)",
        "We used v8  \nSure, here is the link for the predictions  \n[https://github.com/uzairfkhan/Pothole-prediction-using-HMM](https://github.com/uzairfkhan/Pothole-prediction-using-HMM)",
        "Thanks"
    ]
},
{
    "submission_id": "1flqecl",
    "title": "Working with Hailo-8L on Raspberry Pi based project ",
    "selftext": "Hi everyone!\n\nI have an open source LLM-based project and wanted to integrate Hailo with it.\n\nI noticed setup is not as trivial as pip install, so despite going through examples, I can’t just import as the lib doesn’t exist.\n\nAny suggestions on how to manage this?  \nJust take the example repo, copy and edit?\n\nRepo, if anyone is interested:  \nhttps://github.com/OriNachum/autonomous-intelligence\n\nAlthough, its vision was cut until Hailo is operable (I did it with Nvidia Jetson Nano and websocket communication).  \nNow the model just speaks and hears.\n",
    "created_utc": "2024-09-20T17:08:10",
    "num_comments": 1,
    "comments": [
        "the hailo chips look great, their sw is awful at the minute. I got the demos working but trying to convert your own model is not trivial and there's zero useful documentation."
    ]
},
{
    "submission_id": "1fllean",
    "title": "I have keypoint data. Now what?",
    "selftext": "There are lots of projects that use Human Pose models to detect keypoints. But how can you use this data? Is there any project our there I can get ideas from? \n\nI'm thinking about working with exercise data, where I would like to see when the technique changes, thanks to keypoints data. \n\nAnything to get me started would really help. ",
    "created_utc": "2024-09-20T13:13:34",
    "num_comments": 2,
    "comments": [
        "You program.\n\nYou make use of the data in the programmed algorithm. Check angles, distances, motion, movement direction. It all depends on what you want to do and the requirements of your algorithm.\n\n> I'm thinking about working with exercise data, where I would like to see when the technique changes, thanks to keypoints data. \n\nThere are probably plenty if you search GitHub.",
        "There are many pre built training code available on GitHub. Just search for one. \n\nLittle search: Openmmlab"
    ]
},
{
    "submission_id": "1fljv36",
    "title": "detection of fractured/seperated instruments in obturated canals using periapical x-rays",
    "selftext": "Is there any open-source datasets for me to do object detection of fractured or separated instruments of periapical x-ray images?",
    "created_utc": "2024-09-20T12:06:37",
    "num_comments": 0,
    "comments": []
},
{
    "submission_id": "1flijb1",
    "title": "Calculting gradient orientation",
    "selftext": "https://preview.redd.it/ygatj56m80qd1.png?width=1140&format=png&auto=webp&s=54779aaf6d7700cc24baa56aa020ec21e86e5cd5\n\nI am going through this blog post.  \n[https://pyimagesearch.com/2021/05/12/image-gradients-with-opencv-sobel-and-scharr/](https://pyimagesearch.com/2021/05/12/image-gradients-with-opencv-sobel-and-scharr/)  \n\n\nAccording to the exmaple and calculation, shouldnt the vector be pointing towards top left, instead of bottom left?",
    "created_utc": "2024-09-20T11:09:13",
    "num_comments": 5,
    "comments": [
        "The +y axis is downward in this case, so bottom-left is indeed +135 degrees with respect to that.",
        "Looks correct to me. Generally from dark to bright.",
        "Gradient points to maxima, negative gradient points to minima",
        "Could you tell me why +y is downward?\n\nEdit: nvm figured it out. thanks for this",
        "I understand it by looking at the diagram. But I don’t get it based on calculations. Negative x and positive y should lead the vector to top left quadrant.\n\nSomeone commented that +y is downward. Why is that?\n\nEdit: nvm figured it out. Thanks"
    ]
},
{
    "submission_id": "1flhkbf",
    "title": "What are open problems in 3D reconstruction, SfM, Visual SLAM?",
    "selftext": "Hi! I've seen kinda the same post here 2y ago where author tells he will be working on master thesis on 3D CV. I am in the same situation. I want to do my thesis on 3D CV topic.\n\nTo begin, I need to identify problems that the current algorithms have, so in addition to reading papers, I would want to ask about this here.\n\nWhat specific problems are scientists trying to address that limit the performance of the current algorithms? What are the obstacles? What are the potential areas of research? \n\n",
    "created_utc": "2024-09-20T10:28:10",
    "num_comments": 7,
    "comments": [
        "Small camera baselines. Very low image counts",
        "Look into large scale localization. The intuition is that localization becomes more and more computationally expensive as the mapped-scene becomes larger and larger.",
        "Current PhD student working in this exact area. For me, my focus has been on dynamic scenes (things moving, particularly outdoors). Additionally, the quality vs performance cost/time tradeoff",
        "Do you mean wide baselines, as in the photos are taken from much different locations?",
        "I’d welcome more research into this area! ",
        "Any recommendations on how to get started in 3D computer vision through self taught for a person who had a background in 2D. Like NLP could be a Deep learning course by Deep learning.AI then NLP course by Deep Learning.AI and then read paper. Something like that in 3D cv ?",
        "I am also learning visual SLAM.  I found this useful and also recommended by many: [https://github.com/gaoxiang12/slambook-en](https://github.com/gaoxiang12/slambook-en)"
    ]
},
{
    "submission_id": "1flh5dh",
    "title": "Aligning Astronomical Images",
    "selftext": "Hey everyone!\n\nI've been working on a project for some time now and I recently got it running. However, I'm looking for some advice on how to improve upon it as I'm fairly new to CV.\n\nThe project involves aligning astronomical images nonlinearly. There is a package out there called astroalign, which uses triangulations to detect pattern matches between two images and that is a great technique. I wanted to take into account global distortions, which made me decide to do local invariant descriptors.\n\nHere is my routine:  \n1. Find the brightest stars (100 stars for now) in the image  \n2. Do feature extractions on these bright stars using SIFT  \n3. Brute-force feature match using SIFT  \n4. RANSAC to filter out features  \n5. Use Thin Plate Spline (TPS) Interpolation for nonlinear aspect  \n  \nI don't know if SIFT is the best method for doing feature extraction, but it's one I have implemented. Also, I read that I could use Delaunay triangulation to help filter out features that are incorrect but don't know if it's worth implementing.\n\nAny advice is appreciated. Thanks!",
    "created_utc": "2024-09-20T10:09:54",
    "num_comments": 2,
    "comments": [
        "Give LightGlue a try for feature matching : [https://github.com/cvg/LightGlue](https://github.com/cvg/LightGlue)",
        "Oh that just opened up a whole new world for me. Thank you so much!"
    ]
},
{
    "submission_id": "1flgobr",
    "title": "How to get key value pairs from images with icons?",
    "selftext": "Beginner here. I've been exploring options to extract key and value pairs (LOT, Manufactured Date, Use by Date) from an image like this. \n\nTried Tesseract OCR. But couldn't figure out how to identify if a date is MFG DT or USE BY date due to the symbols.\nIn some cases, there will be only MFG DT on the label. Sometimes only EXP DT on the same.\n\nCan someone please let me know on how to approach this?",
    "created_utc": "2024-09-20T09:49:32",
    "num_comments": 19,
    "comments": [
        "You can use LayoutLM to detect layouts. Here's an example:\n\nhttps://huggingface.co/spaces/gaunernst/layoutlm-docvqa-paddleocr",
        "Here’s the output based on the blue circles numbers \n\n`\n {\n  “1”: {\n    “Length”: “110 cm”\n  },\n  “2”: {\n    “Barcode”: “(01)08712345000738(17)301234(10)123456”\n  },\n  “3”: {\n    “Address”: {\n      “Company Name”: “Acme Corporation”,\n      “Street”: “123 Main Street, Suite 45”,\n      “City”: “Anytown”,\n      “State”: “Colorado”,\n      “Zip Code”: “01234”\n    }\n  },\n  “4”: {\n    “Part Number (P/N)”: “123456-123 Rev. F”\n  },\n  “5”: {\n    “Reference Number (REF)”: “123456”\n  },\n  “6”: {\n    “Lot Number (LOT)”: “123456”\n  },\n  “7”: {\n    “Expiration Date”: “2025-02-15”\n  },\n  “8”: {\n    “Sterilization Method”: “Ethylene Oxide”\n  }\n}\n`",
        "Ok, hear me out. Layout LM might work, but it seems like a pretty straightforward computer vision issue to me. If it were me, id make a simple model looking specifically for the symbol you need, and the text of the date. Hell, you could use roboflow to make and train the model.\n\nThen, simply download the model and run it. If you make the date annotations nice and tight, they should always show up within the y1 and y2 of this symbol, and you can ignore the rest.",
        "some vision llama should be able to do it..?",
        "https://github.com/emcf/thepipe",
        "Well, maybe some may consider it \"cheating\", but you could try just using the best multimodal large models you can via an API. Like Claude 3.5 Sonnet or gpt-4o. If you really can't use an API then try to find the best local model possible. Maybe a version of LLaVA or CogVLM or something.\n\nYou could also look at PaddleOCR in layout mode combined with a very strong LLM in text mode. For local maybe phi-3.5",
        "Thanks! Will explore this.",
        "And the full output \n\n`\n{\n  “Product Name”: “PRODUCT NAME”,\n  “Product Description”: “Product description”,\n  “Quantity”: “1”,\n  “Dimensions”: {\n    “Length”: “110 cm”,\n    “Size”: “8F”,\n    “Curve”: “Large Curve”,\n    “Diameter”: “2.67 mm”\n  },\n  “Reference Number (REF)”: “123456”,\n  “Lot Number (LOT)”: “123456”,\n  “Manufacturing Date”: “2022-02-15”,\n  “Expiration Date”: “2025-02-15”,\n  “Barcode”: “(01)08712345000738(17)301234(10)123456”,\n  “Address”: {\n    “Company Name”: “Acme Corporation”,\n    “Street”: “123 Main Street, Suite 45”,\n    “City”: “Anytown”,\n    “State”: “Colorado”,\n    “Zip Code”: “01234”\n  },\n  “Part Number (P/N)”: “123456-123 Rev. F”,\n  “Symbols”: {\n    “Rx Only”: “Yes”,\n    “Sterile”: “Sterile EO”,\n    “Keep Dry”: “Yes”,\n    “Fragile”: “Yes”,\n    “Do not reuse”: “Yes”,\n    “Single Use”: “Yes”,\n    “Keep away from sunlight”: “Yes”,\n    “Sterilization Method”: “Ethylene Oxide”\n  }\n}\n`",
        "Thanks! Love your thought process.\n\nMy bad, I should have mentioned that those blue numbers won't be there in the actual use case. Also, I can't use an API.\n\nThe challenge is.. 'how to capture the text after the hourglass as the Expiration Date?'.",
        "Thanks a lot. Will explore Roboflow.",
        "No idea. Will explore. Thanks!",
        "Oh my lord!! Can't believe what I just saw. ThePipe is just magic!!! Thank you very much!! I'm still shocked the way it got the info.",
        "Thanks! Will explore.",
        "Thanks for the response! \nCan't use an API. Will check the PaddleOCR option.\nI'm wondering if a tool like Label Studio can help in this case. It has relations which I thought might be useful. But, I'm still unable to understand how to use it for this use case.",
        "This is using multimodal LLM, gpt 4o",
        "If your label always has that fixed position then maybe you can use python library ocrs to extract that position of expiration date. But easiest way is api to an LLM",
        "Cool! have similar problem i am working on, so was researching it.\n\nsaw it here: https://www.reddit.com/r/datascience/s/zn0huB2P4w",
        "The label position keeps changing with product. There is no standard layout.   \nOne option I'm trying now is the 'template matching' with openCV. Find the icons and replace them with corresponding text (eg. EXP). Then do OCR on the same. Looks doable.. but not idea if its the efficient way.",
        "Awesome! Have you decided which path to take? 🙂\n\nMy requirement is to run the entire setup on-premise though. \n\nAnd I've never researched LLMs yet. \n\nIs it possible to run some kind of light weight LLM in a local PC without GPU? And then use thepi.pe selfhosted to access it for text extraction?"
    ]
},
{
    "submission_id": "1fldzen",
    "title": "AI motion detection, only detect moving objects",
    "selftext": "",
    "created_utc": "2024-09-20T07:55:24",
    "num_comments": 37,
    "comments": [
        "I'm not sure why this needs AI. This is a pretty easily solved problem using some existing optical flow based video stabilization and background subtraction methods",
        "I can recommend using a gaussian mixture model for motion detection - i found it to be more stable than optical flow :-)",
        "I was trying to count rats in subways videos and YOLO kept mistaking garbage for rats (to be fair they do look similar) so I made this motion detection filter.\n\nOpensource and hosted demo as always:  \n[https://simpleai.darefail.com/movement/](https://simpleai.darefail.com/movement/)\n\n[https://github.com/DareFail/AI-Video-Boilerplate-Simple](https://github.com/DareFail/AI-Video-Boilerplate-Simple)",
        "Did you run this on an android phone? Because your video doesn't look like it's real time on an android device.\nI'm only making this observation because you said it needs to be streaming on Android Device.\nI agree with others, this could be done probably faster using traditional opencv methods.\nThe opencv library is amazingly fast on Android as it's built in native c.\n\nYour application of this problem and how you solved it usually neural networks is cool though.",
        "Why wasn't the man detected at 100%?",
        "hi guy，do you have a github library to share with me?",
        "Very nice. A similar pipeline is used in smart security cameras. A common way to do it will use motion detection on the whole frame and either segment the frame for AI detection or do full frame AI detection. \n\nThe reason this is becoming big in security is because only doing motion detection is you get a lot of false detections with trees, animals, and bugs. And a good reason to layer motion detection on top is so that you don't get false detections on a parking lot when the cars are sitting idle, most the time you only want it triggering on moving vehicles.\n\nEdit: Something you should/could do to improve your model is to add a exit delay timer on the object it detected. So if that object stops moving for x number of seconds or leaves the frame and then starts moving again it will still detect it. That will cut down on the choppiness of your detections.",
        "It's only easily solved if the background is easily identified.\n\nThere's definitely some cases traditional background subtraction fails pretty miserably.",
        "Do you have a link to an example somewhere? I'll check it out and compare",
        "Did you try doing optical flow?",
        "iphone, someone else mentioned android not me\nit's running on tensorflowjs / inferencejs",
        "Very few models are going to hit 100% especially on moving objects(and if it is hitting 100% I would suspect overfitting or leakage into your test set) 94 isn't bad, just depends on the training set and how good your data and even camera is",
        "https://www.reddit.com/r/computervision/s/yYQmUKTpYV",
        "Very cool that’s actually why I started looking at this. Someone who runs security camera software was asking me how possible this is and has lots of false positives.\n\nFeel free to send more thoughts you have on this, I would love to learn more",
        "It wouldn't take all that much to help it along. Personally I would use sparse optical flow and a fast overseg method. Use the 90% (random arbitrary threshold) or so of the flow with the lowest variance to calculate the video stabilization, and check the superpixels of the last 10% or so for movement that's different from the rest. If it's over a threshold, mask out those areas and get the bounding box",
        "No. I haven't seen this implemented in the way you'd need. But motion detection is pretty easy to implement. OpenCV has some good algorithms already, and bgslibrary has more. And OpenCV also has built in methods for optional flow and video stabilization.\n\nHonestly you could probably do this with just optical flow by highlighting objects that aren't moving with the rest of the frame",
        "Looking up what that is. I looked for groups of pixel that change over a certain threshold and create a bounding box that contains all of them.\n\nIt looks like optical flow would detect an object first and see if it moves within a video? I could see that being useful for say counting a car but not what I was trying to do. For instance, I wanted to be able to detect a person even if they are completely still and just waving their hand\n\nLooks like I did \"Frame Differencing\", this article I found shows 4 methods and their drawbacks\n\n\nhttps://medium.com/@abbessafa1998/motion-detection-techniques-with-code-on-opencv-18ed2c1acfaf\n\n\nI am thinking the last 2 seem more useful for my use case than optical flow",
        "Thank you. Do we get to 100% for non-moving humans?",
        "Yeah I added an edit to my previous message, but in addition to that you can also add an entry delay as well so that once motion is detected and then the ai detects it the same object must be tracked for x number of seconds before it counts as a detection.\n\nI used to work for a company doing this stuff so can't go into specifics but if you can get rid of the choppiness and dial in some numbers it would be pretty good.",
        "its not always possible like that, especially if there's always some movements happening in the scene, like rain, wind, etc.",
        "I'm aware of those methods. I actively work on this exact topic.\n\nThey work great in a lot of common applications. The problem is in various edge cases they can still fail.  And those edge cases are often important.\n\nEspecially in the case where the thing you're trying to identify is trying real hard not to be identified.  I can't quite talk in details, but I definitely have ran into situations where they fail pretty hard.",
        "Ah okay yes I looked at those libraries before. I think they're great for a more complicated setup that would stream to a server. \n\nI had to make something that would run on a phone with live video\n\nThis could potentially work I'll get back to you: https://docs.opencv.org/4.5.1/db/d7f/tutorial_js_lucas_kanade.html",
        "Optical flow is useful for tracking. You can check OmniTrack",
        "In my experience you will see a really good model getting around high 90s with moving people detection, but more realistically it will have a consistent detection around 70-80s for novel video/images. With still objects I could see it getting 100, especially if you are using a similar test set to your training set, but if you use a training set of general data from lots of different places (or only one type, eg security cameras, Tictok, sport matches, etc) and then use a phone camera at eye level or are only using security camera it will be way less accurate than using curated data/tests. You don't always want/need 100% either. For security cameras especially you want it to still sound an alarm if it thinks there is a person you just have to play with that value to what is considered acceptable for false positives and false negatives.",
        "Great thanks. Related to this - another thing I am working on is identifying unique people / animal features even if they repeatedly leave the frame and come back a day later",
        "Gaussian filter, or a guided filter if you're feeling fancy, will reduce a lot of that. A lot of optical flow and bgseg methods are pretty resilient to things like that, too\n\nAlso a sparse optical flow method will generally ignore weaker corners and edges in favor of stronger ones, which are less likely to be affected by rain/wind",
        "Hey I work on this topic, too, albeit with thermal camera data rather than RGB data. And in my experience the conditions that would make a method of motion detection like I described fail would also make any ML trained method (that can be run on a phone or a tablet at a comparable speed) fail.",
        "I don't think they would be more complicated or resource intensive than a neural net, though I can't speak for android. Sparse optical flow is very fast, and there are dense optical flow methods that can be near real time even on subpar hardware",
        "Those methods have lower precision. They really hard to tune for real life cases. I would stick with neural network, if no constraints.",
        "That is another thing that companies are doing nowadays as well. Definitely more cutting edge and resource intensive since your model has to remember things for longer times (especially if you are in a busy parking lot or shopping mall). The models I worked with had the ability to track that that but didn't get into it much before I left so afraid I'm not much help on that one. It's a cool issue to work on, though has some potential nefarious implications depending on how it's used in commercial settings.",
        "you are right, but the amount of effort you have to put to come up with something that works pretty decently is really high + you have to know your way around these stuff which means you have good prior experience in this regard something you can not expect from everyone who starts this.   \n  \nalso I'd like to point out that, I enjoy talking and discussing things like this with people like you, as it allows me to learn more myself, so I'm in no way trying to sound smart/or just challenge your points for the sake of apposing your takes, I just wanted to make that clear and thank you also for sharing your points, I appreciate it.",
        "I do it for IR too.  Largely mid-wave and similar bands.  I wish I could explain in detail, but I probably don't want to spill the beans since it deals with proprietary stuff.   But fair enough we can agree to disagree on that.",
        "I don't think you even need optical flow for this. I have done the same thing simply by applying gaussian blur and taking the absolute difference. You can resize the image to a smaller size before doing that for speedup.",
        "For sure - worth looking at everything",
        "I don't think it's really that much work, honestly. Maybe a few days to get a rough prototype\n\nAnd I like talking about it",
        "Sup fellow OGI bro",
        "OpenCV has some more robust ways of doing that, check out their background segmentation module, but the issue is that the camera is moving in some of the example videos. To account for a changing background like that you really need optical flow to judge how objects in the background are moving compared to the foreground\n\nBut yea downsizing the image can definitely help"
    ]
},
{
    "submission_id": "1fl9ieh",
    "title": "How does multi-scale training work?",
    "selftext": "I am trying to understand how multi-scale training in YOLOv8 works. If the model does not have fully-connected layers, it can process an image of arbitrary size. But the output grid size will be different. The calculation of the loss is dependent on the grid cells.\n\nHow is multi-scale training done in YOLO?",
    "created_utc": "2024-09-20T04:17:12",
    "num_comments": 1,
    "comments": [
        "The anchor points are automatically generated based on image size and the predictions are simply offsets with respect to those anchors.\n\nWhat particularly would cause issues during multi-scale training?"
    ]
},
{
    "submission_id": "1fl93ap",
    "title": "YOLOv1 loss",
    "selftext": "Recently, I have been trying to implement YOLOv1 just using tensorflow and training it myself. I have been training it on datasets containing only people (mostly crowdhuman and a subclass of PASCAL VOC which only contains images with people) however i have noticed that the loss always plateaus relatively quickly (sometimes within 3-4 epochs) and changing the learning rate after this period of time will only prevent the loss from plateauing for another few epochs. I cannot get the loss to get below 10 and im aware that i need it atleast below 1 to get accurate results on test data, has anyone got any ideas to reduce the loss? I've tried using dropout and L2 Regularisation but that results in the loss being significantly higher",
    "created_utc": "2024-09-20T03:51:27",
    "num_comments": 2,
    "comments": [
        "How big is ur training dataset",
        "~20k images"
    ]
},
{
    "submission_id": "1fl8sn2",
    "title": "CogVideoX : Open-source text-video model ",
    "selftext": "",
    "created_utc": "2024-09-20T03:31:24",
    "num_comments": 0,
    "comments": []
},
{
    "submission_id": "1fl7ppt",
    "title": "Line segmentation for hand/text written document",
    "selftext": "hello guys , is their any guide or model i can use or fine-tune them on hand_text written document to do line segmentation taking into consideration that handwritten can be curved or overlaps. ",
    "created_utc": "2024-09-20T02:11:34",
    "num_comments": 3,
    "comments": [
        "Have you tried this [A* Path approach](https://github.com/muthuspark/line-segmentation-handwritten-doc/tree/master)?",
        "and do fine-tuning yolo also results good ? \nand A* work only on hand written or can it be applied on both hand and text(computer) written ?",
        "It works on both handwritting and typed text, but i have only used it on images similaire to that in the github example, in other words an image of a page with single column of a text block, i dont know if it will work if there is multiple columns of text blocks or other exceptions.\n\nA fine-tuned YOLO should work too, you just need a good currated dataset."
    ]
},
{
    "submission_id": "1fl3h9f",
    "title": "Need Help Capturing YouTube Live Streams for a Project",
    "selftext": "Hi everyone,\n\nI’m working on a project where I want to detect animals (specifically foxes, birds, badgers, and otters) in live YouTube streams. However, I’m running into challenges accessing the live stream video feed for analysis.\n\nI've tried using libraries like `pafy` and `youtube-dl`, but I keep encountering errors related to changes in YouTube's API. It seems like accessing live streams has become increasingly difficult.\n\nHere are a few specifics:\n\n* I want to capture the live video stream and analyze it in real-time for animal detection.\n* I'm open to using alternative methods or libraries, but I'm not sure where to start or what would be the best approach.\n\nIf anyone has experience with capturing YouTube live streams, or knows of any workarounds, I would greatly appreciate your guidance. Any tips, code snippets, or recommendations for libraries that could help would be awesome!\n\n  \nCheers!",
    "created_utc": "2024-09-19T21:06:13",
    "num_comments": 4,
    "comments": [
        "feed the streams into a hardware capture card then capture and analyse that, then you aren't at the mercy of api changes or such",
        "Sounds interesting! Could you recommend how to proceed with that? Would a hardware capture card work for both physical setups or cloud-based solutions as well?",
        "it wouldn't really work for a cloud based solution, unless your cloud provider has hardware capture cards. but locally you can just set them as an output, play whatever you want on them and then process it as you would a normal opencv videosource.\n\n i use datapath visions, but those are likely overkill for you , maybe something like a razer ripsaw would do. it basically acts like a second monitor and you can stream the razer into opencv as a video source\n\nmost software capture solutions have to bypass drm's or they know not to capture things like youtube and its a cat and mouse game.  thinks like nvidia shadowplay did capture YT but i believe it no longer does. if you're looking for a. longer term solution then a hardware card will capture just about anything, just make sure there isn't DRM on the video that it'll refuse to play or capture at that point you would likely be looking things that are out of the realms of this sub to talk about i'd imagine.",
        "well you could use something like OBS, capture the youtube stream using the HW capture device, then steam OBS output to your own RTMP server in the cloud to get the video fames for analysis."
    ]
},
{
    "submission_id": "1fl1lxg",
    "title": "Call for interviewees for User study",
    "selftext": "",
    "created_utc": "2024-09-19T19:22:09",
    "num_comments": 0,
    "comments": []
},
{
    "submission_id": "1fkzp63",
    "title": "Human Action Recognition using 2D CNN with PyTorch",
    "selftext": "Human Action Recognition using 2D CNN with PyTorch\n\n[https://debuggercafe.com/human-action-recognition-using-2d-cnn/](https://debuggercafe.com/human-action-recognition-using-2d-cnn/)\n\nHuman action recognition is an important task in computer vision. Starting from real time CCTV surveillance, and sports, to even monitoring drivers in cars, it has a lot of use cases. There are a lot of pretrained models for action recognition. These models are primarily trained on the Kinetics dataset spanning over 100s of classes. But let’s try something different. In this tutorial, we will train a custom action recognition model. We will use a **2D CNN model built using PyTorch and train it for Human Action Recognition**.\n\nhttps://preview.redd.it/ds3f4imt1vpd1.png?width=1000&format=png&auto=webp&s=730d7a28897e55fe8d4f128c25aeec680620515b\n\n",
    "created_utc": "2024-09-19T17:41:35",
    "num_comments": 0,
    "comments": []
},
{
    "submission_id": "1fkv7bq",
    "title": "Jazzhands, the first Computer Vision game on Steam!",
    "selftext": "",
    "created_utc": "2024-09-19T14:08:36",
    "num_comments": 0,
    "comments": []
},
{
    "submission_id": "1fku9tw",
    "title": "How can I prepare for my interview?",
    "selftext": "I have a technical interview in one week for a Computer Vision internship, focusing on Object Tracking. I have worked on projects such as face detection, recognition, cell detection and image classification. The interviewer stated that the focus of the interview will be on my technical ability and experience with AI, mainly object tracking.\n\nWhat types of questions I might be asked? Also, how can prepare best for this interview?",
    "created_utc": "2024-09-19T13:29:53",
    "num_comments": 10,
    "comments": [
        "[removed]",
        "Spend a little time (maybe a day or two) brushing up on basics. For example, review Szeliski's \"Computer Vision: Algorithms and Applications\" and maybe Hartley & Zisserman's \"Multiple View Geometry.\"  Depending on how strong your standard algorithm background is, maybe Cormen et al's \"Introduction to Algorithms.\"  \n\nI'd also try to come up with various scenarios you might encounter during this internship and see if you can find a paper/github repo that addresses the problem. It doesn't have to be the latest and greatest, but it should be something you understand the strengths and limitations of. Read the associated papers, if possible.  This way, if the interviewer asks \"How would you approach [some specific problem]\" you will have an answer like \"I would try Foo  & Bar's AwesomeNet first, because it does [X and Y]. If this didn't work I would try...\" etc.  Demonstrate that you have a broad understanding of the field, and are ready with ideas.  Internships are brief, and they'll want you to hit the ground running so demonstrating that you are prepared is important.",
        "Sounds like its time to brush up on Kalman filter.",
        "How did you handle occlusion?\n\nHow did you handle multiple objects?\n\nWhich algorithm did you use? Can you use any other algorithm to achieve similar results?",
        "Thank you for your suggestions. I work on it.  \nI also want to know if it is better to work on an object-tracking project to show in the interview.   \nIs it better to stick with the projects I have and prepare for the theoretical aspects and scenarios you said about?",
        "I think it depends a lot on what kind of internship this is.  I'm a researcher, and all our interns are late-stage PhD students.  When we interview them, we expect them to have a project idea -- not necessarily code and results, but at least a firm, one or two sentence description of a project that we can collaborate on and write a paper about by the end of the internship.\n\nIf you're an undergrad or Master's student, they may be looking more for capability, i.e. do you know enough to have a productive internship.  My advice was aimed more at this type of internship.",
        "The internship is with a research organization, and the interviewer mentioned that it will be part of their startup's research project.   \nI am a Master's student, I will focus on theoretical aspects like you mentioned. I will try to work on an object-tracking project but I am unsure that will I be able to finish in this short period.",
        "Remember I'm just one guy, who doesn't know anything about your particular situation. Take me with a grain of salt!\n\nDepending on the organization you'd be interning with, or your relationship with the person interviewing you (do you have their email?), you could trying asking what they're looking for.  Do they want to do a deep dive into a specific project, or are will the interview be broad and about the field in general.",
        "Yeah, I contacted the person with whom I will be interviewing. He said the interview would be about my technical expertise and experience with AI, especially tracking objects."
    ]
},
{
    "submission_id": "1fkoadn",
    "title": "Is there any pose tracking model that can get the depth of the video?",
    "selftext": "I am new to computer vision and would appreciate some help on this matter :) \nI want to capture properly the joint angles for different excercising videos and I'm trying to avoid the problem of the perspective used for recording the video to get consistently the angles. So far I'm using mediapipe but I don't feel Im getting good results.",
    "created_utc": "2024-09-19T08:56:30",
    "num_comments": 2,
    "comments": [
        "Mediapipe does well. But if you are having issues maybe you can try openmmlab, contains multiple approaches for pose detection (both 2D and 3D).",
        "Thank you! I will definitely be trying it"
    ]
},
{
    "submission_id": "1fknihu",
    "title": "Label Studio footage limitations",
    "selftext": "I'm doing object classification on some drone footage. Most of the footage loads into label studio just fine, but I've got a subgroup that will import but when I try to view and label the video, I get a message \"Unable to Play\".\n\nAll the videos are MP4 and below the 250mb limit.  Are there other limitations on what Label Studio can handle? (For example, the ones that are \"Unable to Play\" are at a higher frame rate.)",
    "created_utc": "2024-09-19T08:23:38",
    "num_comments": 1,
    "comments": []
},
{
    "submission_id": "1fki6v8",
    "title": "Trained yolo model free to use commercially?",
    "selftext": "Hey everyone,\n\nI'm currently working on a startup while in school, and we're using Ultralytics YOLOv8 for object detection. We have a ridiculous quota ($5000) to work with for a team of 2!  I've been considering switching to yolov7 or any other ones that has good performance and easy to beginners in 2024.\n\nI've been researching different versions of YOLOv7, but honestly, I'm feeling a bit overwhelmed by the different variants, licenses, and implementations out there. The legal aspects and restrictions around licenses are especially confusing. We're planning to distribute our software to testers soon, so I need a trained YOLOv7 model that doesn't require too much tweaking.\n\nOur primary platform is ios, so we need yolov7 in coreml format, or easy to convert to coreml. I’m looking for a version of YOLOv7 that:\n\n1. Is free to use commercially without open source our code.\n2. Works well with coreml on iOS.\n3. Is relatively easy to implement without needing deep machine learning expertise (no one in the team has enough deep learning experience).\n\nDoes anyone have any experience with a YOLOv7 version that fits these criteria or can point me in the right direction? Any help would be greatly appreciated! Thanks in advance!",
    "created_utc": "2024-09-19T04:10:14",
    "num_comments": 24,
    "comments": [
        "This implementation uses the mit license which is fairly permissive\n\nhttps://github.com/WongKinYiu/YOLO",
        "Dude - first google search - https://github.com/WongKinYiu/yolov7/blob/main/LICENSE.md",
        "It also depends on what data it's pretrained on. Imagenet pretrained for example is very difficult to get through any legal department at companies for commercial release due to dataset licences.",
        "I have worked on object detection for several years and used some of yolo series models. I even wrote a paper for this. Every year a new model is released and for the last year, we learn this ultralytics sh*t. They made an open source ecosystem, closed, even though there were open source contributers.\n\nAnyways, why don't you use yolov5? It might look old but i think yolo series models just burnt out themselves. Our experiments showed that v5 was still competitive, and even better in some features, compared to other models like yolox, yolov6 and yolov7 (but we used nano and tiny versions of the model).\n\nI checked and v5 can be used commercially.",
        "Pretty sure that it's only the code that's licenced, the trained model weights (assuming you use your own training data) can be used anywhere as long as you don't use ultralytics for inference.",
        "Also just noticed that you want it to work with iOS. Since you want a generic yolo model and did not do any customization on it you can look at \"create ML\" from apple. [https://developer.apple.com/videos/play/wwdc2019/424/](https://developer.apple.com/videos/play/wwdc2019/424/)\n\n  \nIf you want to have more control over the models and code you can look at MLX",
        "What’s your time worth?\n\nLet’s say you work 50 hours a week and pay yourself $50/hr (including benefits). Are you able to research and implement an alternative in two weeks or less? \n\nAlso, what is the cost to your startup in loosing those two weeks? ",
        "thats too much of a reading   \ndownload it and attatch it to a LLM for answers XD",
        "Isn't GPL 3.0 still requiring you to make the application you made with this model open source?  or at least open source upon request?",
        "Might I know which version of v5 can be commercially used free without open source our own code?  Like obviously I cannot just download v5 from ultralytics.  Is there a v5 in coreml ready to use?",
        "We are still students at school.  $5000 can't just come out of nowhere.  The startup is in self-funded stage.  There is no such thing as paying myself.",
        "Yes - guess my question should have been with those requirements why did they use software with a gpl license.\n\nThese are things you check before you start ",
        "No. Program outputs (the models) from GPL 3 are not GPL. Write your own code to perform the inference. \n\nProgram output from AGPL are also not AGPL (according to the authors of FSF themselves). Ultralytics is trying to stretch the scope of the license beyond what it covers. That would be left to lawyers and courts though.\n\nYou can use Yolo v5 or v9 safely.",
        "pretty sure he means v4",
        "Valid points. ",
        "You dont have to use a single stage detection network. You can look at retinanet or faster-RCNN.",
        "I mean we are trying to find a model without gpl license",
        "This is where I became braindead.  Everyone says something different.  \n\nI am trying to get Yolov9 from [https://github.com/WongKinYiu/YOLO](https://github.com/WongKinYiu/YOLO) to work.  Thanks",
        "The argument is that the weights include the model's structure which is a copyrightable work & so anything derived from that needs to follow the upstream license. Certainly there is code included in a pt file, not just weights.\n\nYou could also argue that the model architecture is more like an API spec than code though & the Google/Oracle case could provide precedent that reimplementing that from scratch is fair use (though again would take a long and protracted legal battle to actually get certainty).\n\nIt's a gray area that hasn't been decided in court yet so for any serious business it's probably best to just avoid that risk/uncertainty.\n\nObligatory disclaimer: I'm not a lawyer; this is just my personal opinion/speculation. I'm the co-founder of Roboflow and we pay for a license with the ability to sub-license this stuff to our customers from Ultralytics & that's the judgement-call we made risk-wise even though we've re-implemented a bunch of the parts on our own also.",
        "Technically what matters is how the courts interpret the license, not what the authors intend. \n\nI can have you sign a license that gives me rights to your firstborn child, but you can probably just ignore it. ",
        "So, if I train yolov10 using their code and export it to onnx using my custom code. Then deploy it some where with custom inference. Should I need license to sell it ?",
        "Second google search led me here https://www.reddit.com/r/MachineLearning/comments/1amtndm/d_models_similar_to_yolo_mit_or_apache_license/",
        "Yes, I have this page bookmarked.  I have been bookmarking a bunch of this kinda of pages.  I got overwhelmed by 4 hours research on licensing.  Kinda of brain dead right now, just want an easy answer!",
        "The answer is in there"
    ]
},
{
    "submission_id": "1fkfba2",
    "title": "Hi, I am starting a computer vision project and was wondering what program to use.",
    "selftext": "I don't know specifically what I want to archive,\nWhat's sure so far that I am using a delta robot fitted with an industrial camera, and I have to detect and move objects on a fixed bed with back lighting. It is for a University project so what I do doesn't really matter, the important thing is that I should be able to move objects.\n\nThe first program I was checking is Zebra Aurora Vision Lite, because the machine I work with were using the pro version but doesn't have licence anymore. And the second one was opencv. Do you have any input on which one should I choose, or what is the best program for something like this?",
    "created_utc": "2024-09-19T00:37:28",
    "num_comments": 1,
    "comments": [
        "I don't know anything about Zebra Aurora Vision, but I can tell you that this is very doable using open CV. The benefit of opencv is that there are a lot of tutorials and blog posts that use it so it shouldn't be too hard to get it working and behaving itself."
    ]
},
{
    "submission_id": "1fk4ndf",
    "title": "Worth creating 3D Meshes of objects to generate 2D image training data?",
    "selftext": "If I have a model where I want to do object detection on normal 2D images (e.g. chess pieces), could it be beneficial to build these objects in blender as 3D meshes and then take 2D \"photos\" of them to build an augmented/generative training set? \n\nWhile these 3D-model images may give extra information to the model, is this information even valuable since the images are not from the same distribution of the test set that I actually want to infer on?",
    "created_utc": "2024-09-18T14:49:32",
    "num_comments": 12,
    "comments": [
        "Yes, using synthetic training data can be quite beneficial if you don't have enough real data or the annotation of real data is too expensive.",
        "Actually very much possible and a promising technique. I used Blender to create synthetic images to also train an object detector.",
        "The only problem I can see is that you get a lot of variation in chess pieces geometry and to capture that you'd need a lot of different models for each of the pieces. To combat that, assuming you don't want to make hundreds of models, I'd grab a bunch of real images and annotate those manually and combine the synthetic and real data. Maybe even weight the real data slightly heavier. \n\nIn terms of actual implementation, I'm not sure how good your blender skills are but cheap ways to get variation is to have a bunch of different materials set up for the pieces, pull positions in from free chess databases and definitely use hdris for lighting the scene. You get much better lighting and reflections using hdris. \n\nHappy to help more if you need it, synthetic data generation in blender is what I did for my masters thesis",
        "In most cases, no. Synthetic data barely helps. Don’t listen to these noobs.",
        "But as you mention, there will be a domain gap, so make sure to make the pictures as realistic as possible. Blender should work well for that.",
        "Thanks! Do you mind me asking your use case?",
        "I was planning on using real images as well, but yeah my blender skills are beginner, that's why I wanted to see if It was worth learning. I will definitely try for 1 piece and look into your advice with hdris lighting and different materials. Thanks for your willingness to help, I'll let you know if I have questions!",
        "Interesting, any use cases you've had where synthetic data didn't help?",
        "thank you!",
        "Sure. I wrote my Bachelor's Thesis on synthetic data generation for the use case of beverage crate classification. In Germany we have a circular system where the beverage crates and bottles are returned back to the supplier to restock them. A key part of this circular system is the identification of crate types when they are returned to the supplier.",
        "For textures and hdris polyhaven is a good place to start. For blender, I'd recommend going with blender gurus doughnut tutorial.",
        "Very cool, thanks"
    ]
},
{
    "submission_id": "1fk2dxj",
    "title": "(discussion&some help wanted) In the next few yrs, how you imagine the direction of vision llm towards AGI?",
    "selftext": "\nSince OpenAI announced the O1 series with its exceptional coding, data analysis, and mathematical abilities, I’ve been curious about the next step: creating an autonomous, proactive AI—capable of real-time “talking,” warnings about potential mistakes, and anticipating time-consuming steps. Think along the lines of a small-scale ‘Jarvis AGI’ with advanced perception capabilities, like sensing emotional cues, spotting dangers ahead, and even notifying me of hazards in real-time (e.g., if something is coming towards me or detecting unsafe areas).\n\nI’m working on building a personal version of this(perhaps it is not going good anyways), even at a modest scale, and would love insights on the following goals:\n\n1. Smart home control: I’d like the AI to control devices with custom functions and be proactive about possible issues (e.g., warning about malfunctioning devices or time-consuming actions).\n2. Proactive intelligence: Imagine the AI providing real-time feedback, warning me of wrong steps, anticipating challenges, and offering recommendations, like notifying me about potential dangers if I’m headed somewhere unsafe.\n3. Cybersecurity integration: I’m also considering fine-tuning it as an all-in-one cybersecurity model for automation (e.g., CTF participation, serving as an IDS), and allowing the AI to “decide” actions based on real-time data.\n\nImprovements I’m considering:\nFine-tuning with function calling and task-specific reinforcement learning.\nCreating multiple agents with different biases for refinement, leveraging Chain-of-Thought reasoning to improve accuracy in decision-making.\n\nWhat concepts, techniques or stuff would you recommend exploring to build this kind of proactive, action-taking, complex AI agent?\n\n",
    "created_utc": "2024-09-18T13:12:21",
    "num_comments": 0,
    "comments": []
},
{
    "submission_id": "1fjzrsf",
    "title": "Diffusion models in image restoration",
    "selftext": "Hello, guys!\nRecently I've read some papers on image restoration (i.e deblurring, denoising, sr) with diffusions, and they all seem to perform their experiments in 256x256 resolution, which is way too small for real images\n\nI wonder, what would a complete pipeline look like?\nSomething like this came to mind:\n\nStep 1. Encoder part of VAE of corrupted image to be restored\n\nStep 2. Reverse SDE with latent representation of corrupted image as prior/guidance\n\nStep 3. Decoder part of VAE (to different resolution in case of SR task)\n\nSo my question are:\n1) Is this a viable approach in practice?\n2) What are Sota approaches with decent inference time?\n\nThanks in advance.",
    "created_utc": "2024-09-18T11:21:55",
    "num_comments": 0,
    "comments": []
},
{
    "submission_id": "1fjxxnj",
    "title": "New Digital Radar on Chip created by an Ex-Nvidia engineer (Uhnder), Thoughts??",
    "selftext": "",
    "created_utc": "2024-09-18T10:04:09",
    "num_comments": 0,
    "comments": []
},
{
    "submission_id": "1fju1jt",
    "title": "What is the best way of performing document forgery detection?",
    "selftext": "My goal is to find out if a bank statement has had its content altered in any way. I already have a solution where my system points out if there is a discrepancy in the credit/debit/balance calculations. The other case I have to deal with is if the forger hasn't made a mistake in the balances. The bank statement will be a scanned copy of a printed PDF, so it's not a digital PDF.\n\nUnder this assumption, I looked for non-deep learning based solutions like edge detection, texture analysis, noise pattern analysis, local binary patterns, and error level analysis, but these aren't able to crack the kind of forgery I am trying to find.\n\nIf anyone could give me any pointers, it would be greatly appreciated.",
    "created_utc": "2024-09-18T07:22:33",
    "num_comments": 2,
    "comments": [
        "i've done some work on this - DM me some samples and I can take a look",
        "I am messaging you for that"
    ]
},
{
    "submission_id": "1fjtu9d",
    "title": "Pinhole camera model representation",
    "selftext": "What are the advantages of using **B** over **A** representation, except of the fact that the image plane is not flipped? What is the reason that most of the camera calibration tutorials use **B**?\n\nhttps://preview.redd.it/hzb6k6y5skpd1.png?width=436&format=png&auto=webp&s=0207c8b2c9077b6237097d7f2fb68a6fc1246b7f\n\n",
    "created_utc": "2024-09-18T07:13:51",
    "num_comments": 3,
    "comments": [
        "These two are mathematically equivalent, while **A** is physically more correct, **B** is just more convenient to visualize, especially when trying to convey things like multi-view geometry where you place multiple camera frames in a single diagram.",
        "Out of curiosity where are these diagrams from?",
        "I scraped it from google. If I remember correctly it was from a research paper."
    ]
},
{
    "submission_id": "1fjqsom",
    "title": "100 day AI challenge worth it?",
    "selftext": "Hi,\n\nI received a promotional email about OpenCV University courses. The promotion states that I can get up to $500 back if I complete all the courses. So, if I purchase the $1300 master course, it would effectively cost me $800 in the end.\n\nHas anyone taken their courses and can provide feedback? Are they worth it? I already know Python and have been studying the fundamental math needed for machine learning. \n\nMy ultimate goal isn’t to pursue a career in computer vision, but to contribute to open-source models and help the community, particularly in generative art.\n\nI want to be able to read papers like this:\n\nhttps://arxiv.org/pdf/2405.14869\nhttps://arxiv.org/pdf/2408.06072\n\nand understand what’s going on, as well as potentially make small changes to the models. Will completing this master course help me achieve that?\n\nThanks!",
    "created_utc": "2024-09-18T04:47:40",
    "num_comments": 12,
    "comments": [
        "Surprisingly, paper you mentioned does not require only high computer vision or image processing knowledge. It also requires knowledge about diffusion models, NLP, statistics, optimization, ML, Deep learning etc.   \nSo no, these courses are rip off. Start at the very basic",
        "Have you tried using Claude 3.5 Sonnet or ChatGPT o1-preview or gpt-4o as a mentor? If you get both that will cost like $40 per month.\n\nThat offer sounds kind of like a scam to me. Although a lot of normal marketing looks kind of like a scam.",
        "Another OpenCV course or pack may be better suited for your needs.",
        "No relevant code picked up just yet for \"PuzzleAvatar: Assembling 3D Avatars from Personal Albums\".\n\n[Request code](https://www.catalyzex.com/paper/arxiv:2405.14869?requestCode=true) from the authors or [ask a question](https://www.catalyzex.com/paper/arxiv:2405.14869?autofocus=question).\n\nIf you have code to share with the community, please add it [here](https://www.catalyzex.com/add_code?paper_url=https://arxiv.org/abs/2405.14869&title=PuzzleAvatar%3A+Assembling+3D+Avatars+from+Personal+Albums) 😊🙏\n\nCreate an alert for new code releases here [here](https://www.catalyzex.com/alerts/code/create?paper_url=https://arxiv.org/abs/2405.14869&paper_title=PuzzleAvatar: Assembling 3D Avatars from Personal Albums&paper_arxiv_id=2405.14869)\n\n --\n\nFound [4 relevant code implementations](https://www.catalyzex.com/paper/arxiv:2408.06072/code) for \"CogVideoX: Text-to-Video Diffusion Models with An Expert Transformer\".\n\n[Ask the author(s) a question](https://www.catalyzex.com/paper/arxiv:2408.06072?autofocus=question) about the paper or code.\n\nIf you have code to share with the community, please add it [here](https://www.catalyzex.com/add_code?paper_url=https://arxiv.org/abs/2408.06072&title=CogVideoX%3A+Text-to-Video+Diffusion+Models+with+An+Expert+Transformer) 😊🙏\n\nCreate an alert for new code releases here [here](https://www.catalyzex.com/alerts/code/create?paper_url=https://arxiv.org/abs/2408.06072&paper_title=CogVideoX: Text-to-Video Diffusion Models with An Expert Transformer&paper_arxiv_id=2408.06072)\n\n--\n\nTo opt out from receiving code links, DM me.",
        "You can actually just mail them and ask what's best for your needs.\n\nEdit: Email id:  [courses@opencv.org](mailto:courses@opencv.org)",
        "Gotcha thanks for the reply. I'll continue with the foundations and just lookup free videos on topics as I go.",
        "Thats what I do now...Im currently going through all the math courses here \n\nhttps://deeplearningcourses.com/catalog\nlinear algebra\ncalc\nprob+stats\n\nOnce I'm done with the maths my plan is to find a free course on computer vision then learn what ever I need to learn from there to understand that paper. Thats my end goal...when I can read that paper and not be intimidated by it.",
        "Sounds good! Thanks!",
        "Asking them, a company interested in selling you courses, for advise whether you need their courses?",
        "It's great to tinker with the code too. CogvideoX code is opensource. The code itself isn't actually too complex (torch deals with a lot of the complexity).\n\nUnless you have a large gpu, you'll likely need to run it on a cloud service which is it's own learning experience.",
        "yeah that was dumb"
    ]
},
{
    "submission_id": "1fjpt0m",
    "title": "Weakly Supervised Segmentation Model Training",
    "selftext": "Hi,\n\nI am trying to improve object detection under heavy rain for road scenes. I have a custom dataset with bounding boxes but would like to explore weakly supervised training a segmentation model to identify regions of interest for enhancement. No pixel level annotations are available. What models would be the friendliest for this and is it a good idea? Thanks.",
    "created_utc": "2024-09-18T03:48:01",
    "num_comments": 0,
    "comments": []
},
{
    "submission_id": "1fjogoy",
    "title": "Cheapest sensor for getting pointcloud data from 2400x1200mm flat area from distance of 1000-1200mm. Pointcloud dont need to be very accurate but it must fill the whole area ",
    "selftext": "I got intel D435-i sensor but unfortunetely its FoV is too small for this project. SIPEED MaixSense-A010 wont cover the whole area.  \n\nAny other ideas that come to mind? TOF sensor / cheap lidars with large fov? ",
    "created_utc": "2024-09-18T02:11:35",
    "num_comments": 2,
    "comments": [
        "What experiences do people have with PS4 and PS5 stereocameras?   \nWould the be worth trying out?",
        "you could try luma ai which could be the fastest, and easiest. you could take photos and use colmap if luma doesnt work.  this would require nothing more than a camera"
    ]
},
{
    "submission_id": "1fjnftf",
    "title": "双目相机和单目相机区别",
    "selftext": "是不是两个单目相机就是双目呢？",
    "created_utc": "2024-09-18T00:52:22",
    "num_comments": 2,
    "comments": [
        "Title: Differences between binocular and monocular cameras\n\nQuestion: Are two monocular cameras binocular?\n\nAnswer: If they point into the same point w/ close distance and are linked for the \"observer\" to use, then two monocular cameras become binocular. \n\nSee [https://en.wikipedia.org/wiki/Stereo\\_camera](https://en.wikipedia.org/wiki/Stereo_camera) for depth explanations and the like.",
        "2024 and the shitty reddit app doesn't have a translate button"
    ]
},
{
    "submission_id": "1fjn309",
    "title": "Hyperspectral images vs thermal images vs RGB images for predicting shelf life / freshness of fruits and vegetables",
    "selftext": "",
    "created_utc": "2024-09-18T00:24:24",
    "num_comments": 10,
    "comments": [
        "I can speak specifically to thermal images: it's complex.\n\nAnybody can buy an RGB camera. Thanks to the [https://en.wikipedia.org/wiki/Atmospheric\\_window](https://en.wikipedia.org/wiki/Atmospheric_window) there only some bands of thermal can be captured through the earth's atmosphere: NIR, MWIR and LWIR. \n\nYou can obtain a NWIR camera by simply removing the filter from an RGB camera. However, MWIR and LWIR cameras will be considerably more expensive. \n\nFinally, I'm not sure the value thermal imaging will provide w.r.t. fruit. Shelf fruit will likely have equalized with the ambient temperature. \n\nMaybe something like a gigahertz or terahertz imaging radar would be more informative?",
        "Brix can be used to measure fruit ripeness, [https://felixinstruments.com/blog/brix-as-a-metric-of-fruit-maturity/](https://felixinstruments.com/blog/brix-as-a-metric-of-fruit-maturity/)",
        "Do you or will you have access to hyperspectral cameras? Those can get pretty expensive and need calibration and controlled lighting for it to work best. Also, since you are trying to predict how long the fruit would last, how are you thinking to get the dataset? would it be something like you take images each day, and when it gets bad, you then mark how many days until it went bad? and train some regression models to predict?  is it per fruit basis or per batch of fruits in general? From my work with hyperspectral camera, I feel like it is very good to capture information from hundreds of bands and identify which bands are particularly useful, and then later use only those bands, maybe with a multispectral camera to do the inference. My suggestion would be to first try RGB, then thermal, then hyperspectral as it gets more and more complex. You will definitely get more information from hyperspectral, but its not really practical in general use cases.",
        "Yes the first approach we decided to go with was RGB images. We also started collecting data but there was not much promisng research using RGB images. Thanks for your insights about thermal imaging. We also considered the cost of hyperspectral camers is insane and we won't get funds for it. The radar approach we will look into it",
        "Could possibly use differences in emissivity in the skin of ripening or bruised fruit (for some fruits). It's also possible that there's an observable (via thermal camera) textural change of the skin of the fruit. Idk how well that will correlate to shelf life, though",
        "Yes that is what we are looking for. We thought maybe with hyperspectral imaging it would be possible to extract information about the brix",
        "We don’t have hyperspectral cameras but we can talk with university for one. We have already started working with RGB images. We are collecting images the same way you just wrote about collecting day by day to observe outer texture differences. Same was the idea for hyperspectral ones. We just wanted someone’s opinion about hyperspectral and thermal imaging if they will be effective. For RGB we were thinking we will collect data for each fruit. And if a batch of fruits is there we were thinking we can just do object recognition and identify which fruit it is and then pass this to the model.",
        "LWIR thermal cameras have gotten a *lot* cheaper in the last few years. If you can get away with low resolution and low sensitivity you can get one for a few hundred dollars.\n\nHowever I would suggest looking into millimeter wave imaging, similar to what they use in airports to see if people are carrying weapons. You may be able to get a measure of density throughout a fruit",
        "Yes the outer texture changes can be detected with thermal imaging. We were thinking we will observe the thermal patterns over the life cycle of a fruit and collect the data labelled with days. And maybe we can train a model to predict the shelf life",
        "BRIX does have some IR absorption bands, there is a lot of research publications on this."
    ]
},
{
    "submission_id": "1fjlq9y",
    "title": "A Survey of Latest VLMs and VLM Benchmarks",
    "selftext": "",
    "created_utc": "2024-09-17T22:48:15",
    "num_comments": 0,
    "comments": []
},
{
    "submission_id": "1fjli51",
    "title": "Detecting connections between objects - Any techniques or methods?",
    "selftext": "I'm working on a project where I need to detect different objects and connections between objects from an image. For example, let's say there are two squares (A and B) and I want to determine if there's a line connection between Square A and Square B. Square A and Square B are differentiated by a text annotation. I'm aware of YOLO for object detection (Square A and Square B), but determining the connection between two blocks without losing the contextual information (i.e., the connection is between Square A and Square B) - what techniques can be used?\n\nIs there a technique or method that can help me achieve this? I've tried searching online, but I'm not sure where to start.",
    "created_utc": "2024-09-17T22:33:19",
    "num_comments": 2,
    "comments": [
        "Why not just train a detector on the connections? We really need pictures to understand the problems.",
        "I found an article similar to my use case. [pid digitization](https://devblogs.microsoft.com/ise/engineering-document-pid-digitization/)"
    ]
},
{
    "submission_id": "1fjhqi6",
    "title": "Storing Depth Videos Efficiently",
    "selftext": "Hello All,\n\nI've been experimenting with monocular depth estimation on videos, where for each RGB frame I get a 16 bit image. Ideally for each video I'd like to store a corresponding depth video, but I don't know what would be ideal in terms of compression, which I need to be lossless, or very nearly lossless. At the moment I am simply storing a 16 bit .png frame per video frame, but this results in a huge number of files and uses a lot of storage.\n\nAre there alternative/better storage solutions for lossless or nearly lossless depth videos?",
    "created_utc": "2024-09-17T19:03:17",
    "num_comments": 14,
    "comments": [
        "Lossless, you can get 3-4x compression with most methods. Just put them through zstd or something.\n\nLossy is much more interesting. You can convert to color and put it through a regular video encoder configured for high quality, or you can do something really specialized like decompose the scene into a set of planes. There are a few papers on this that get amazing compression ratios.",
        "I have three main lines of thought on this.\n\nFirst, have you considered storing disparity instead of depth? Disparity, being a reciprocal of depth, will represent close ranges with more fidelity than long ranges. There's a common assumption that resolution at greater range is less important. 20cm vs. 50cm matters much more than 2000 cm vs. 2030cm, in many use cases.\n\nSecond, there are HDR image and video formats. However, while HDR video formats (like HDR10+) can efficiently store videos with 16-bit depth, they aren't particularly well supported currently.\n\nThird, you could encode your depth video into 24-bit sRGB video. But, you need to give thought to the encoding. For example, consider 24-bits `ABCDEFGHIJKLMNOPQRSTUVWX` (from MSB to LSB). It might be better to encode this as:\n\n`R = CFILORUX`  \n`G = BEHK NQTW`  \n`B = ADGJMPRV` than as\n\n`R = ABCDEFGH`  \n`G = HIJKLMNOP`  \n`B = QRSTUVWX`\n\nThe reason being common compression algorithms will treat each channel equally. So you don't want to encode all your MSBs in one channel. With some thought you can likely come up with something even more cleaver.",
        "Do you really need 16 bit precision for depth estimation?",
        "Btw check out r/ffmpeg and other video encoding subs. Someone out there is already encoding grayscale video for archival purposes where quality is paramount. ",
        "Last reply from me lol. \n\nDon’t overestimate the quality of depth maps obtained from monocular depth models! They are definitely quite inaccurate, and even fairly heavy lossy compression probably will lead to  minimal loss of actual meaningful information. Biggest risk is probably edge artifacts but you can probably find a video codec that minimizes those, and can probably “fix” most of them in post processing. The good thing is that video codecs tend to sacrifice texture in order to preserve edges, and texture is the element of a depth map that tends to be the least accurate anyways. \n\nWhatever you end up doing, be sure to A:B test it end-to-end in your application. I would not be surprised if a 10x compression ratio is perfectly tolerable. ",
        "I wonder if you can get away with a sparse depth map that only stores values for regions of interest?\n\nWord of warning though, before you go too far down this compression route lookup whether it’s actually saving money or time. Storage and compute are crazy cheap these days (even in the cloud) and unless you’re talking about tens of terabytes of depth maps I doubt it’s worth your time implementing a compression scheme. \n\nAlso remember that compression requires compute which could easily outweigh storage cost savings. ",
        "Great. Learning a lot with these answers, thanks so much. Zstd is neat that sounds like a good start. I'll probably do some preliminary experiments with lossless first. \n\nI know that in the NTU RGBD dataset they just mask out depth ranges to 0 they aren't interested in so at the very least I might be able to do something like that as well.",
        "Doesn’t png already do that?",
        "but watch out you don't save in a format that uses [https://en.wikipedia.org/wiki/Chroma\\_subsampling](https://en.wikipedia.org/wiki/Chroma_subsampling) otherwise you'll destroy information in some channels",
        "Great, will read more thanks.",
        "Not OP, but yes you (usually) do.\n\nOP you can combine them into one zip or tar.gz.  Not sure what you are doing with them, we use ros where I work so usually our stereo data is just stored in ros bags, which usually have lz4 compression, and when we don’t need them for a while we gzip them.",
        "What alternative precision did you have in mind,  quantizing to 8 bits like uint8?",
        "Yes, PNG is a difference filter + DEFLATE compression, which is probably better than zstd alone, so that was not a great suggestion for me to make as it won't compress an image better than the PNG OP was already doing. Packing the output into one big compressed file though, instead of many individual PNGs, might have some benefit both in compression and in filesystem efficiency.\n\nTo really maximize lossless depth compression ratio, the absolute best thing I have found is FLIF. It trains a model on each image you give it. Downsides are very high encoding cost and low format compatibility (the format has been superseded and very few image processing tools support it). Unfortunately the superseding format (JPEG XL) doesn't actually compress as well.\n\nIn encoding compute/time *is* a concern though, and it must be lossless, I recommend accepting PNG's compression ratio and attempting to get close to it but with a cheaper algorithm like RVL (PNG was never meant to be realtime).",
        "Well, depending on your application, the range and the resolution you need, uint8 might be a possible candidate here (if, for example, your maximum range is less than 25.5 meters and a resolution of 10cms is accurate enough, then represent your data as uint8 * 10cm)."
    ]
},
{
    "submission_id": "1fjckvu",
    "title": "Interactive Art",
    "selftext": "Hey r/computervision! I'm working on an interactive art exhibit and need help choosing the right depth camera. I've got a convoluted set of questions as I'm early in my exploration of the project.  \n\nHere's my setup/requirements:\n\n* Portable exhibit in an enclosed space (gazebo-like)\n* Mostly low light conditions (occasionally day light but not direct sunlight (inside blackened gazebo).\n* Close-range interaction (within 5m)\n* Up to 5 participants at once\n* On-site inference for object/posture detection, possibly 3D mapping\n\n**Current gear:** Raspberry Pi 5 (8GB) (for other components of the system, doesn't necessarily need to be part of this new setup), considering Jetson Orin Nano 16gb as a on site inference tool. \n\n**Questions:**\n\n1. TOF vs. Structured Light vs. Stereo - best for variable low-light? I'm assuming TOF or structured light\n2. I feel like I need to determine TOF or SL, but I've seen good reviews of OAK-D, Orbbec  intel, RealSense,  Cubeeye or and Zed.  But without real-world experience its hard to properly evaluate usability and my use-case is weird, does anyone have any applicable experience?\n3. I'll probably do one camera for now but unsure if I need multiple and sync them?\n4. Is Jetson Orin Nano sufficient or overkill? Should I just get a AI accelerator like Hailo for the RP5?\n5. Are there potential challenges in enclosed spaces?",
    "created_utc": "2024-09-17T15:13:59",
    "num_comments": 0,
    "comments": []
},
{
    "submission_id": "1fjbd33",
    "title": "Help with InsightFace Python Module - Facial Manipulation",
    "selftext": "Hello! I am trying to manipulate my face like the Snapchat  \"Tough Guy\" facial lenses does. I am using the InsightFace python library and followed this tutorial: [https://www.youtube.com/watch?v=a8vFMaH2aDw](https://www.youtube.com/watch?v=a8vFMaH2aDw) . \n\n  \nI was able to follow the tutorial well, but when it came to blending a Chad tough guy face onto the loaded image, the output was horrible. I have the code I used below, which is different than the tutorial because I figure I just need to perform facial manipulations. \n\n  \nI haven't been able to do that correctly yet, and really need some help here. I am new to CV and trying out this project to push my capabilities so that I can create some cool animations.\n\n  \nHere is the code I have below:\n\n\\`\\`\\`python\n\n  \nimport cv2  \nimport numpy as np  \nimport matplotlib.pyplot as plt\n\n    # Load Nate's image and detect face landmarks\n    nate_faces = app.get(nate)\n    nate_face = nate_faces[0]\n    nate_landmarks = nate_face.landmark_2d_106\n    \n    # Visualize landmarks on Nate's face\n    for point in nate_landmarks:\n        cv2.circle(nate, tuple(point.astype(int)), 2, (0, 255, 0), -1)\n    plt.imshow(nate[:,:,::-1])\n    plt.show()\n    \n    # Load Chad's image and detect face landmarks\n    chad_faces = app.get(chad)\n    chad_face = chad_faces[0]\n    chad_landmarks = chad_face.landmark_2d_106\n    \n    # Visualize landmarks on Chad's face\n    for point in chad_landmarks:\n        cv2.circle(chad, tuple(point.astype(int)), 2, (0, 255, 0), -1)\n    plt.imshow(chad[:,:,::-1])\n    plt.show()\n    \n    def calculate_delaunay_triangles(rect, points):\n        subdiv = cv2.Subdiv2D(rect)\n        for p in points:\n            subdiv.insert((p[0], p[1]))\n        triangle_list = subdiv.getTriangleList()\n        delaunay_triangles = []\n        pt = []\n        \n        for t in triangle_list:\n            pt.append((t[0], t[1]))\n            pt.append((t[2], t[3]))\n            pt.append((t[4], t[5]))\n            \n            pt1 = (t[0], t[1])\n            pt2 = (t[2], t[3])\n            pt3 = (t[4], t[5])\n            \n            if rect_contains(rect, pt1) and rect_contains(rect, pt2) and rect_contains(rect, pt3):\n                ind = []\n                for j in range(0, 3):\n                    for k in range(0, len(points)):\n                        if abs(pt[j][0] - points[k][0]) < 1 and abs(pt[j][1] - points[k][1]) < 1:\n                            ind.append(k)\n                if len(ind) == 3:\n                    delaunay_triangles.append((ind[0], ind[1], ind[2]))\n            pt = []\n        \n        return delaunay_triangles\n    \n    def rect_contains(rect, point):\n        if point[0] < rect[0]:\n            return False\n        if point[1] < rect[1]:\n            return False\n        if point[0] > rect[0] + rect[2]:\n            return False\n        if point[1] > rect[1] + rect[3]:\n            return False\n        return True\n    \n    def warp_triangle(img1, img2, t1, t2):\n        rect1 = cv2.boundingRect(np.float32([t1]))\n        rect2 = cv2.boundingRect(np.float32([t2]))\n        \n        t1_rect = []\n        t2_rect = []\n        t2_rect_int = []\n        \n        for i in range(0, 3):\n            t1_rect.append(((t1[i][0] - rect1[0]),(t1[i][1] - rect1[1])))\n            t2_rect.append(((t2[i][0] - rect2[0]),(t2[i][1] - rect2[1])))\n            t2_rect_int.append(((t2[i][0] - rect2[0]),(t2[i][1] - rect2[1])))\n    \n        mask = np.zeros((rect2[3], rect2[2], 3), dtype=np.float32)\n        cv2.fillConvexPoly(mask, np.int32(t2_rect_int), (1.0, 1.0, 1.0), 16, 0)\n    \n        img1_rect = img1[rect1[1]:rect1[1]+rect1[3], rect1[0]:rect1[0]+rect1[2]]\n    \n        size = (rect2[2], rect2[3])\n        img2_rect = apply_affine_transform(img1_rect, t1_rect, t2_rect, size)\n    \n        img2_rect = img2_rect * mask\n    \n        img2[rect2[1]:rect2[1]+rect2[3], rect2[0]:rect2[0]+rect2[2]] = (\n            img2[rect2[1]:rect2[1]+rect2[3], rect2[0]:rect2[0]+rect2[2]] * (1.0 - mask) + img2_rect\n        )\n    \n    def apply_affine_transform(src, src_tris, dst_tris, size):\n        warp_mat = cv2.getAffineTransform( np.float32(src_tris), np.float32(dst_tris) )\n        dst = cv2.warpAffine( src, warp_mat, (size[0], size[1]), None, flags=cv2.INTER_LINEAR, borderMode=cv2.BORDER_REFLECT_101)\n        return dst\n    \n    def correct_colors(im1, im2, landmarks1):\n        try:\n            eye_region1 = landmarks1[36:42]\n            eye_region2 = landmarks1[42:48]\n            blur_amount = 0.4 * np.linalg.norm(np.mean(eye_region1, axis=0) - np.mean(eye_region2, axis=0))\n            blur_amount = int(blur_amount)\n    \n            if blur_amount % 2 == 0:\n                blur_amount += 1\n    \n            im1_blur = cv2.GaussianBlur(im1, (blur_amount, blur_amount), 0)\n            im2_blur = cv2.GaussianBlur(im2, (blur_amount, blur_amount), 0)\n    \n            im2_blur += 128 * (im2_blur <= 1.0).astype(im2_blur.dtype)\n    \n            result = im2.astype(np.float64) * im1_blur.astype(np.float64) / im2_blur.astype(np.float64)\n            result = np.clip(result, 0, 255).astype(np.uint8)\n    \n            return result\n        except Exception as e:\n            print(f\"Color correction failed: {e}\")\n            return im1  # Return the original image if color correction fails\n    \n    def morph_faces(src_img, src_landmarks, dst_img, dst_landmarks):\n        dst_img = np.copy(dst_img)\n    \n        # Find convex hull\n        hull1 = []\n        hull2 = []\n    \n        hull_index = cv2.convexHull(np.array(dst_landmarks), returnPoints=False)\n    \n        for i in range(0, len(hull_index)):\n            hull1.append(src_landmarks[int(hull_index[i])])\n            hull2.append(dst_landmarks[int(hull_index[i])])\n    \n        hull1 = np.array(hull1)\n        hull2 = np.array(hull2)\n    \n        # Calculate Delaunay triangles\n        rect = (0, 0, dst_img.shape[1], dst_img.shape[0])\n        dt = calculate_delaunay_triangles(rect, hull2)\n    \n        if len(dt) == 0:\n            return dst_img\n    \n        # Apply affine transformation to Delaunay triangles\n        for i in range(len(dt)):\n            t1 = []\n            t2 = []\n    \n            for j in range(0, 3):\n                t1.append(hull1[dt[i][j]])\n                t2.append(hull2[dt[i][j]])\n    \n            warp_triangle(src_img, dst_img, t1, t2)\n    \n        # Clone seamlessly\n        mask = np.zeros(dst_img.shape, dtype=dst_img.dtype)\n        cv2.fillConvexPoly(mask, np.int32([hull2]), (255, 255, 255))\n        r = cv2.boundingRect(np.float32([hull2]))\n        center = (r[0] + int(r[2] / 2), r[1] + int(r[3] / 2))\n        \n        # Color correction to better match the skin tones\n        corrected_dst_img = correct_colors(dst_img, src_img, hull2)\n    \n        output = cv2.seamlessClone(corrected_dst_img, dst_img, mask, center, cv2.NORMAL_CLONE)\n    \n        return output\n    \n    # Morph Nate's face to Chad's landmarks\n    morphed_face = morph_faces(nate, nate_landmarks, chad, chad_landmarks)\n    \n    # Display the morphed face\n    plt.imshow(morphed_face[:,:,::-1])\n    plt.show()\n    \n    # Optionally, save the result\n    output_path_morph = \"./Images/morphed_chad.png\"\n    cv2.imwrite(output_path_morph, morphed_face)\n    \n    ```\n\n  \nYour help will be greatly appreciated! Thank you!\n\n  \nI am also open to using other packages.\n\n  \n",
    "created_utc": "2024-09-17T14:25:16",
    "num_comments": 0,
    "comments": []
},
{
    "submission_id": "1fj8bfp",
    "title": "Object-aware image enhancement",
    "selftext": "I am trying to improve object detection performance on my dataset with heavy rain images. I will be using YOLOv8 to benchmark the enhancement pipeline. What are some effective ways to boost regions of interest under such adverse conditions to obtain higher mAP scores?",
    "created_utc": "2024-09-17T12:26:48",
    "num_comments": 9,
    "comments": [
        "My initial thought would be to train a de-rain model or algorithm using something like the NAVER LABS' Virtual KITTI Dataset. Basically, render pairs of images with rain on and off, then train some kind of image translation model to map from the rain domain to the sunny day domain. \n\nMy second thought is - just train your detection model end-to-end with additional rain data. Maybe train something like Y = F(G(X)) with augmented data for rain, fog, dust, etc. Y is the detector predictions, X is the image, G an image translation model, and F is YOLOv8.",
        "Depends, does speed matter? If not you can use generative models and look at rain drops like noises and train a denoising model first",
        "Thank you for your input. My issue is that I am supposed to only work with the target domain images. I have fine tuned the yolov8 model with my annotated heavy rain data and there were clear improvements. However, the goal is still image processing methods to enhance regions of interest. And I probably have to use some kind of pre existing solution to identify them as I don’t have the data to train that segmentation (?) model…",
        "Eventually yes, but for my project the focus will be accuracy improvements",
        "I would suggest using sensor fusion, like lidar and radar. But if this is your thesis or something I wouldnt go that direction because you stand no chance to improve anything. The field is very dense and players are mostly big companies with lots of resources. It probably would take around 8 - 12 months to only catch up with the field and come up with a new idea(my experience).",
        "Haha yes I am actually part of a bigger project on sensor fusion, but I have been tasked to focus on the image part of it, so gotta make do with what I have:)",
        "You have better chance to find a flying badass scarfaced double sword wielding unicorn than improving the image only object detection in bad weather, this was the MOTIVATION FOR SENSOR FUSION.",
        "Hahahaha… well good thing it is a bachelor level project so the expectations for results aren’t too high\n\nMore exploratory I guess?",
        "Perhaps"
    ]
},
{
    "submission_id": "1fj3awg",
    "title": "Detecting smaller objects with Yolo",
    "selftext": "I have trained a yolo v8 object detection model. What are some tips and tricks to make it more efficient in detecting smaller objects?",
    "created_utc": "2024-09-17T09:13:24",
    "num_comments": 25,
    "comments": [
        "SAHI (Slicing Aided Hyper Inference) is a very effective method to improve small object detection in object detection models such as YOLOv8. SAHI enables better detection of small objects by slicing large images. By working on small sections rather than looking at the entire image, the model can detect small objects with higher accuracy.",
        "Set the image_size higher.... Default Yolov8 resolutions are 640 x 640 and if your objects to detect are smaller, when the input images are resized to the 640 x 640 resolution, it gets lost in the wilderness......And if you are trying \"s\", \"m\", try to use Yolov8l... But higher the configuration like Yolov8l or Yolov8x, higher is the compute requirements...Which will force you to lower your \"batch_size\"..... Minimal \"batch_size\" of 4 will surely give you better training performance and in-turn reflecting in higher_metrics and detection performance on unseen images....",
        "It improves the \"generalisation\" capability of the model.... A model's world is the \"training_data\"... And no \"training_data\", which is \" finite\" can represent the \"infinite\" diversity of the real world....\"Eval\" data, should be again a \"diverse\" subset of the total \"data\".... Model, while training, is not exposed to the \"ground_truth_labels\" of \" Eval\"data... But a \"diverse\"  eval_data, can push the mAP down by a few points, but if the \" generalisation\" is improved because of that, it is the \" optimized_model\" for a given use case.... Remember, adjustment of the weights during back propagation and gradient decent will be definitely better, if using a higher batch size... And how much you can fill up the GPU's memory/utilization, again depends on the Image_size, size of the dataset, and batch size... Try a higher architecture\"l\", increase the \" image_size\", if you want to perfectly detect \"small objects\" and do not want to use SAHi during \"inference\"....",
        "There's a P2 variant of YOLOv8 which you can load by using `YOLO(\"yolov8n-p2.yaml\")`. It has an extra scale to extract smaller features. But that also means it's slower. If your dataset only has small images, you could reduce the extra overhead by removing some of the larger scales from the `yaml` such as P5 and P4 since they won't be needed.",
        "try to provide more small objects to training data. Even some kind of copy -> decrease size -> paste augmentation should helps",
        "But, it's way slow. Basically, it slides over the image, so one image inference takes time of many image inference.",
        "But how to integrate it into yolo?!",
        "This is probably the way of you can’t do tricks like SAHI or other tiling approaches.\n\nYou might also have to bump up the model size. Assuming you’re doing this professionally that should not be an issue…",
        "I use Nano. And batch size of 64. Should I decrease the batch to get better metrics?",
        "Interesting. My dataset actually includes a lot of large objects too. But YOLO has a scale data augmentation too...not sure how useful it is",
        "Yolo has a scale augmentation parameter. Would that help? But a scale of 0.5 would be 0.5x - 1.5x",
        "https://docs.ultralytics.com/guides/sahi-tiled-inference/#installation-and-preparation",
        "Generally you use the biggest batch size you can given the amount of GPU memory on your system. I think uktralytics can actually figure out the optimal size at the start of trainings ",
        "I am not sure if it is the same . The goal is to provide smaller objects samples to model during trainig",
        "But it looks like an inference approach right? I need at training. Inference I can't touch",
        "Yeah larger batch definitely makes training faster. But not sure how it affects metrics",
        "Btw can you explain your suggestion more?\n\" copy -> decrease size -> paste augmentation\". Do you mean I resize the images? But YOLO reaizes them again to my input size.",
        "Then you probably have to integrate it in the yolo model itself.",
        "You’ll have to figure that out. I do believe I’ve seen scripts related to training yolov8 with sahi. I think it involves slicing up your training images and labels or something like that.",
        "Slicing an image is an identity-convolution operation like at the beginning of a ViT - really straightforward.",
        "If you're slicing during training, but not during inference, then it will lead to worse accuracy because you're supposed to keep the training data similar to what you encounter during inference.",
        "It can help or hurt final metrics but usually doesn’t make a huge difference. Sometimes an “unstable” network depends on a bigger batch size to keep it from falling into local minima. ",
        "Copy and paste augmentation",
        "https://github.com/obss/sahi/discussions/1059",
        "How about resizing?"
    ]
},
{
    "submission_id": "1fj2m0w",
    "title": "Simple method for visualizing XYZ hand movement?",
    "selftext": "Hello all,\n\nAs part of my research, I have been using a VR headset where people experience repeated encounters and need to move their left or right hand (the entire hand, not the fingers) to render a response. What I am hoping to do is visualize the XYZ path of the hand either as a static beginning-to-end path image, and/or dynamically over time. \n\n  \nMy data are an X, Y, Z coordinate for each hand at each time-point. The encounters themselves run anywhere from a few hundred ms to 1s, depending on how long it takes the user to render their response.\n\n  \nThanks for any suggestions! ",
    "created_utc": "2024-09-17T08:45:58",
    "num_comments": 1,
    "comments": [
        "anyone?"
    ]
},
{
    "submission_id": "1fj033x",
    "title": "[Help] Streaming USB Camera Feed to Multiple Clients for Face Recognition within different machine",
    "selftext": "Hey r/computervision! I'm working on a distributed face recognition project and could use some input.\n\n# Current Setup:\n\n* \\*\\*Server\\*\\*: One machine(A) with a USB camera another machine B where the algo (socket.io) is running\n* \\*\\*Goal\\*\\*: Stream live camera feed from machine A to B and then after doing the algos (face recognition, emotion , bpm,etc) then sends the data through sockets to the UI (Angular APP)\n\n# Current Implementation:\n\nI'm using Python with OpenCV for camera capture and ZeroMQ for network communication. Here's the basic flow:\n\n1. Machine A captures frames from the USB camera\n2. Machine A encodes frames and broadcasts them using ZeroMQ's PUB socket\n3. Machine B connects to the server using ZeroMQ's SUB sockets\n4. Machine B receive frames and run face recognition algorithms\n\n# Questions:\n\n1. Is this a good approach for multiple clients?\n2. How can I improve performance or scalability?\n3. Are there better alternatives to ZeroMQ for this use case?\n\nI tried with mjpg streamer to stream the data through HTTP or RTSP and receive it on machine B, but it shows segmentation core dumped error after running for 1 minute, but it works smoothly when I run everything in one machine, I mean the camera is connected to the same machine where the camera is.\n\nI'd really appreciate any advice, best practices, or alternative approaches. Thanks in advance!",
    "created_utc": "2024-09-17T07:03:12",
    "num_comments": 9,
    "comments": [
        "0mq is fine for the network messaging, but might as well use one of the other standards like rtsp or other streamers for the video data, there are a lot of them using that so the crash you are having is likely just a local error. \n\nyou can setup a rtsp streamer to ingest and rebroadcast as well, ffmpeg, gstreamer, live555/666\n\nbut since you haven't really described the problem or scale its hard to recommend. are you making twitch or security camera viewing and so on.. but either way its similar, encode, send to rtsp or such based on scale of use and load balance it\n\npython isn't exactly the strongest solution here but you haven't really said much about the system. is it multiple cameras per server etc , can you use gpus , how much latency is allowed etc",
        "No,it is just a a simple webapp for one short face recognition with emotion detection and bpm calculation\nI have a socket server which sends the data to the UI .\nIt works fine when I use the camera index to run the App.\nSince my backend is hosted on a GPU and I can't directly connect the camera to that machine. \nThe USB camera is connected to an Intel nuc,(doesn'thave GPU) from where I wanted to send the frame to the GPU machine where my algo is running \n\nIt is a single camera and 4,5 seconds lag is fine.\nI will check with ffmpeg on how to stream rtsp.\nI didn't wanted to make it so complex that's why thought of going with Zeromq",
        "that sounds very basic so any of the solutions should work just fine. i have done a similar setup in the past with 30+ usb cameras split up on nuc's streaming to a central service at 120fps each so that sounds perfectly doable.\n\n0mq to do the control aspect, and rtsp or such to stream  ffmpeg is very simple to stream and if you need one to many you can do a distributed rtsp easily enough",
        "Thank you so much ✨️",
        "I actually tried a pretty basic one, but it kind of lagging and glitching, idk am doing it correctly  \nThis is what I did  \nI installed rtsp-simple-server and  \n  \n\\`ffmpeg -f v4l2 -i /dev/video0 -c:v libx264 -preset ultrafast -tune zerolatency -f rtsp rtsp://0.0.0.0:8554/stream\\`\n\nlater I tried giving framerate and all those things\n\n\\`nohup ffmpeg -f v4l2 -input\\_format mjpeg -framerate 30 -video\\_size 640x480 -i /dev/video0 \\\\  \n       -c:v libx264 -preset ultrafast -tune zerolatency \\\\  \n       -b:v 1M -maxrate 1M -bufsize 2M \\\\  \n       -g 30 -keyint\\_min 30 \\\\  \n       -r 20 \\\\  \n       -f rtsp -rtsp\\_transport tcp rtsp://0.0.0.0:8554/stream > \"$LOG\\_DIR/ffmpeg\\_stream.log\" 2>&1 \\`\n\nwhat am I doing wrong?",
        "are you meaning to change the framerate from 30 to 20 and can you use udp instead of tcp? then tune the buffer size/gop\n\nbut you'll have to do some investigation and see where the bottlenecks are, is it cpu/io/network bound etc.",
        "I changed bitrate and then frame rate,Didn't workout as I expected tho,i'll try changing from tcp to udp and change and GoP aswell",
        "yeah you might want to look at cpu/network usage see what is a bottleneck",
        "Yep will do that"
    ]
},
{
    "submission_id": "1fiwls9",
    "title": "How to open this file type?",
    "selftext": "https://preview.redd.it/tqwawfzrtcpd1.jpg?width=436&format=pjpg&auto=webp&s=5ffadf09cc3b6c331a6a90df07865c0909e9d86e\n\nHow can we open this file type to view its contents? its generated via record3d.",
    "created_utc": "2024-09-17T04:24:58",
    "num_comments": 7,
    "comments": [
        "Try open3d",
        "Where did the file come from?",
        "Can you open it in a text reader like notepad? ",
        "do you know the exact command to do so? i was not able to find it. can you post the link?",
        "record3d",
        "it is not in human readable format. i was able to open it using **depth\\_map = np.fromfile(depth\\_file\\_path, dtype=np.float32)**",
        "[https://www.open3d.org/docs/latest/tutorial/Basic/rgbd\\_image.html](https://www.open3d.org/docs/latest/tutorial/Basic/rgbd_image.html)"
    ]
},
{
    "submission_id": "1fiwdlb",
    "title": "How should I go about detecting face pose & position in a constrained embedded device?",
    "selftext": "Hello!\n\nI‘m trying to build a device that requires accurate position and pose tracking of a subjects face.\nEssentially, I want to track the position (and direction) of the users eyes to move the device to always face the user.\n\nMy hardware is already fixed, it consists of a Quad ARM Cortex-A53 CPU @ 2.0 GHz with a Mali-G52 GPU and also a 1 TOPS NPU. I have two 1280x800 global shutter cameras in a stereo setup with a 20 cm baseline (But I don‘t have to use both of them).\n\nMy initial idea was to run a SBGM implementation across the two images to generate a depth map, and merge this with the face detection result to get the position of the observer.\nHowever, I‘m not sure if this is the best way to go about this: It’s seems computationally complex, and also I‘m not sure how I should go about merging the face detection results with the depth map. I‘ve found a few monocular head pose estimation algorithms, but many of them only focus on the pose, not the position, and the latter seems more important to me.\n\nA simple face detection is sadly not good enough for me, as i need the depth information for the mechanical movement of the device. But if a ML model is good enough to give me somewhat accurate estimates (preferably with an error <10 cm) I‘d also be open to implementing that.\n\nI‘m looking forward to any suggestions!",
    "created_utc": "2024-09-17T04:12:40",
    "num_comments": 4,
    "comments": [
        "What kind of NPU device are you using? Retina face does a decent job with 5 key points (allowing for gaze estimation) using the rockchip (20ms). Works fine with openvino too but not as fast.",
        "> Essentially, I want to track the position (and direction) of the users eyes to move the device to always face the user.\n\nMove it in front of their face or in front of their eyes?",
        "I hadn’t heard of that before, but it seems like a decent option, since I’m using a RK3568. Thank you for the suggestion!",
        "In front of their eyes, the functionality depends on the device adjusting to the viewing angle of the user"
    ]
},
{
    "submission_id": "1fivujv",
    "title": "Feedback on a synthetic data project idea - any feedback would be appreciated!",
    "selftext": "As a project, I am working on building a self-service synthetic image API that uses Stable Diffusion XL, Flux etc. for computer vision engineers to do quick modifications and augmentations to their training data. The goal is to help me (and hopefully others) reduce data drift, acquire new images quickly and cheaply and increase the speed of iteration while increasing model performance. The images that are generated will keep the existing labels in place. \n\nTo start off with, I am thinking of allowing a couple initial modifications:\n\n* Controlled lighting changes\n* Weather changes (rain, snow, sun etc.)\n* Time of day changes (daw, day, evening, night etc.)\n* Addition of occlusions and lighting flares\n* Object additions\n* And more\n\nI have a bunch of ideas on how to expand this further, but while I am building this initial prototype, I was curious on feedback. Do you think people might pay for this, why or why not? Do you think this would be useful?\n\nThanks in advance for the feedback!",
    "created_utc": "2024-09-17T03:43:19",
    "num_comments": 4,
    "comments": [
        "My company works on synthetic data and we have been tackling all these problems both using augmentations using control nets and SD models and 3D graphics based pipelines. Would be happy to chat about it.",
        "I think the company I work on struggles a lot because we lack very little labelled anomaly image data. It is not possible either to make them on realtime. I tried custom augmentation techniques but all seems fake. I also would love to work on project like this one and recently something similar. I will start with a scene and gradually add objects (layers) on it, transform and translate it over the time and I can get the various instances of an event on the image. This is just an idea though. I have also built a synthetic smoke generator with pygame. One thing I struggled on was making it reproducible, but assiging randomstate everywhere made it possible.\n\nIf you are looking for a collab, please ping me. I would love to work on this too.",
        "That would be awesome, and I'd love to learn more. I'll reach out over PM",
        "Thanks for reaching back and the detail! We've been working on custom modification/augmentations and have been able to get a good level of photorealism to our images. Would love to collab, will ping you directly"
    ]
},
{
    "submission_id": "1fiurqf",
    "title": "extracting images from depth maps",
    "selftext": "Hi,\n\nI am using iphone 12 pro. I clicked an image in 'potrait mode'. The object is in focus and background a bit blurry as seen in potrait mode. I then try to extract the depth information using this image by using python - \n\n\n\nim = Image.open(\"a.HEIC\")\n\nif im.info\\[\"depth\\_images\"\\]:\n\ndepth\\_image = im.info\\[\"depth\\_images\"\\]\\[0\\]\n\npil\\_image = depth\\_image.to\\_pillow()\n\nnp\\_image = np.asarray(depth\\_image)\n\n   I noticed that the original image resolution and depth image resolutions are different. The original image is large like 3kx4k in this range and the depth image is considerably smaller like 576x768. I also thought that the depth image would contain the actual real world values. For example -  if a point in the scene is 5 meters away from the camera, the corresponding pixel value will be 5.0. But i notice it is ranged between 0-255. Cna someone explain?\n\n\n\n",
    "created_utc": "2024-09-17T02:36:58",
    "num_comments": 2,
    "comments": [
        "The iPhone 12 has a time of flight camera, a sort of lidar. It's a different sensor than the one giving you the image. It's much lower resolution in reality, and I expect its output to be float32.\n\nI found this random github that somewhat confirms my intuition: [https://github.com/BernieZhu/RGBD-Video-iPhone12Pro](https://github.com/BernieZhu/RGBD-Video-iPhone12Pro)",
        "Thanks a lot, that explains the size. I had expected the output depth images to be similar like how a realsense camera works, in meters. but yeah the values are scaled from 0-255."
    ]
},
{
    "submission_id": "1fiu6m8",
    "title": "Which camera should I use for an agricultural drone?",
    "selftext": "I'm building a weed detection model which detects weeds in crop fields at a height of 2-3m.\n\nThe camera needs to be connected to Jetson Nano.\n\nI have a raspberry pi camera v1.3 but it gives overexposed images(too bright, nothing can be seen). \n\nAlso, I'm not sure about which resolution and specifications of the camera should be enough.\n\nShould I use an auto-focus or fixed focus camera? Should I an auto exposure camera or not? What resolution should I use? How many MP?\n\nI would highly appreciate any suggestions.",
    "created_utc": "2024-09-17T01:57:57",
    "num_comments": 11,
    "comments": [
        "There's a bunch of settings at the back of the picam pdf.  \nHave you tried changing the exposure time? e.g. from [https://github.com/raspberrypi/picamera2/discussions/449](https://github.com/raspberrypi/picamera2/discussions/449)\n\n    def do_capture(exp_time):\n        picam2.set_controls({\"ExposureTime\": exp_time, \"AnalogueGain\": 1.0})\n        picam2.start()\n        array = picam2.capture_array()\n        picam2.stop()\n        return array\n    \n    short = do_capture(1000)\n    medium = do_capture(5000)\n    long = do_capture(9000)",
        "Based on your questions, maybe you are not ready to tackle such a project.\n\nYou need to experiment more with the camera you have, and see how it works and how different settings affect image quality for you situation.\n\nThere is no \"ideal camera for crop photography from drone\".\nIt depends on your specific conditions, and especially how fast the drone is moving.",
        "Are you trying to attach the computer and jetson to the drone? As someone who uses a drone for computer vision professionally, I’ve never seen the need to do inference on the UAS itself. There is nothing I couldn’t record first then use the image/video files after for inference applications. No matter what you are doing adding the extra weight of a computer and its power supply to a drone will impact drone performance rather heavily vs the benefit you will get out of it",
        "You could do with a bit more research here, you need an near-infrared camera to detect the higher blue content in weeds",
        "To build an effective weed detection model using the Jetson Nano, you'll need a camera that can handle the specific conditions of crop fields at a height of 2-3 meters. Here are some recommendations, including e-con Systems cameras:\n\n# Recommended Camera Specifications\n\n1. **Resolution**:\n   * **Minimum 2MP**: This resolution provides a good balance between detail and processing load. Higher resolutions (4MP or more) can be beneficial for capturing finer details but will require more processing power and storage.\n2. **Dynamic Range**:\n   * Look for a camera with high dynamic range (HDR) capabilities to avoid overexposed images in bright sunlight. This will help in capturing details in both bright and shaded areas of the field.\n3. **Auto-Focus vs. Fixed Focus**:\n   * **Auto-Focus**: Recommended for varying distances and ensuring that the weeds are clearly in focus, especially since you'll be detecting them from different heights and angles.\n   * **Fixed Focus**: If your application requires consistent distance, this can also work, but may not provide the flexibility needed in a dynamic environment.\n4. **Auto Exposure**:\n   * **Auto Exposure**: Recommended to automatically adjust the exposure based on lighting conditions, preventing overexposed images in bright sunlight.\n5. **Connectivity**:\n   * Ensure that the camera is compatible with the Jetson Nano. Look for USB or MIPI CSI interfaces that are easy to integrate.\n\n# Suggested e-con Systems Cameras\n\n# [https://www.e-consystems.com/camera-category/hdr-usb-mipi-cameras.asp](https://www.e-consystems.com/camera-category/hdr-usb-mipi-cameras.asp)",
        "Oh great info, thanks!! \n\nI came across this post, [https://www.imakewebsites.ca/posts/2015-07-28-fixing-incorrect-raspberry-pi-cam-auto-exposure/](https://www.imakewebsites.ca/posts/2015-07-28-fixing-incorrect-raspberry-pi-cam-auto-exposure/) however it still didn't solve my problem. When I take images indoors or in the shade, the images come fine. But when I take them in the sunlight, they are overexposed and the exposure time is around 1/600 instead of the usual 1/100.",
        "Hmm, well I don't really have a choice at this point, this is my final year project at uni and its too late to change.\n\nWould a 1080p webcam work? I've searched for some, but they seem to be pretty grainy.",
        "Basically, its a drone that identifies weeds in real time and sprays them with herbicide. And the Jetson Nano is pretty light, like a little more heavier than a pi. \n\nFor now though, I just need to get the detection model working, with or without the drone. \n\nAnd I can't seem to find a good camera. I thought about webcams but they seem too grainy.",
        "1080 could be enough. A 720 webcam would probably also do. Grainy doesn't mean low resolution (probably just cheap camera)\nIt depends on the type of crops, flying speed, stability, and other factors.\nAs someone mentioned before - I suggest you just google the topic a bit more as you really sound like you need it.",
        "Ahhh the spraying them with weeds makes much more sense now. What sort of payload is the drone in question rated to?",
        "herbicide spraying drone using cv sounds cool. i could also see it taking pictures and coupling them with location data , then detecting weed and if found , resulting in locations with weed growing. it will still have an impact in agricultural purposes - provide the exact locations with weed growing to the farmer, maybe even with the image , so the farmer makes sure its weed and not an invalid detection. good luck."
    ]
},
{
    "submission_id": "1fit6rc",
    "title": "Usage of “Test Chart for Industrial Vision” from Stemmer Imaging",
    "selftext": "I have an image of the “Test Chart for Industrial Vision” from Stemmer Imaging, but I can’t find the tutorial on how to use it. Do you have any idea how each test works? Thank you in advance.\n\nhttps://preview.redd.it/8dkihoefrbpd1.jpg?width=4284&format=pjpg&auto=webp&s=1e06cfecf65bbe4998722b2cbc1f5d6a85ac5546\n\n",
    "created_utc": "2024-09-17T00:50:42",
    "num_comments": 0,
    "comments": []
},
{
    "submission_id": "1fisrq3",
    "title": "Best open source face recognition models? Is there something better than AdaFace or QMagFace? Maybe new open datasets?",
    "selftext": "Best open source face recognition models? Is there something better than AdaFace or QMagFace? Maybe new open datasets?",
    "created_utc": "2024-09-17T00:21:51",
    "num_comments": 1,
    "comments": [
        "As of my knowledge facenet is still the best in practice. Don't trust what papers claim. Look at benchmarks in github. And also don't trust those too. Generate one for your own ant test the models"
    ]
},
{
    "submission_id": "1fin42y",
    "title": "Package (language) to work with Sensor, ultrasonic, thermal and lidar",
    "selftext": "I am working on struct, can and flex ray(python as main) and in progress of enhancing connection between engineering team and computer vision team, please let me know any kind of package or other language to accelerate the task. Thanks in advance.",
    "created_utc": "2024-09-16T19:11:50",
    "num_comments": 0,
    "comments": []
},
{
    "submission_id": "1fijs5x",
    "title": "Are FPGA still relevant in Computer Vision? ",
    "selftext": "I'm about to graduate from a degree in electronic engineering(I live in Europe) and I've been contacted from a quite small company to work for them and they are specialized in Computer Vision applications running on Xilinx FPGAs.\nI have actually never thought of combining the twos together and I was wondering if this could be a good career path and if what I would learn in this job could be useful to land a different job in the future.",
    "created_utc": "2024-09-16T16:41:33",
    "num_comments": 19,
    "comments": [
        "Sort of. I have seen a lot of FPGAs used in lidar signal processing. There are still FPGAs used in some industrial machine vision cameras. But as many people have pointed out that dedicated ISP's and GPUs now dominate the computer vision processing space.",
        "Yes. Satellites and drones. GPUs will absolutely kill a power budget. Learning how to convert CNN inference architectures to FPGAs will serve you well in these fields",
        "As far as I'm aware there is a neural inference chip on these Xilinx FPGAs which they call a DPU (Deep Learning Processing Unit). Using a tool called Vitis-AI, Tensorflow, Pytorch and ONNX models can be compiled for this chip.\n\nNow the documentation for Vitis AI is kind of iffy, but once you have it installed, quantizing, compiling and quantized training of models is relatively straight forward. Most importantly it is an alternative for NVIDIAs near monopoly on neural network acceleration, and I know of at least one large company whose only mode of AI deployment for the foreseeable future are Xilinx FPGAs.",
        "Just because Jensen's gpus are the most popular solutions, it does not mean that they are the only solution. I think the gpus we use currently are overkill. They need a lot of power, but look, Jensen says do not learn programming, just use my boards :).\n\nAnyway, I think FPGAs might be future of model inferencing. However, there is not much resources yet so the learning curve might be slow.",
        "Do as little as possible with an FPGA. Once you’re at video frame rates do it all in a GPU.\n\nFPGAs were the only way to do CV at video frame rates in the past, so there’s a lot of people that still do that out of inertia, especially in low SWAP environments.  \n\n But you haven’t seen much recently because the people doing it are still working on it because it’s so dang slow to develop vs GPUs.\n\nJust put a Jetson on it.",
        "I've seen a couple papers using them for hyperspectral real time stuff 5 or 6 years ago, but it was a very small number and I've seen none since. Maybe check scholar for any reviews of FPGA applications\n\nMy (very) uneducated takeaway was that they seemed good for very simple tasks and algorithms, but if you wanted to use more complex algos like deep learning (which is where everything is going) you're better off just getting an edge device like a NVIDIA Jetson or something. Given that hardware continues to get smaller anyway, FPGA seemed limited in the benefits it could provide in most practical settings",
        "They are pretty useful for on device processing, you seen them on lidar and stereo cameras (the fancier ones like carnegie robotics multisense, not the basic ones that just give you two video streams obviously).  So it depends what part of cv you work on.",
        "Yes they are relevant but not in the way you expect. Typically there are a few types of operation that we do a lot in computer vision and almost all of them are some basic matrix operation like multiplication but in the large scale. Even for complicated operations such as logarithm there are some simple algorithms.\nThat being said, there are already tons of specially designed hardwares for matrix operations such as numerous vector extentions on microprocessors, Nvidia chips, some other special hws etc.\nSo yes the use of fpga in computer vision is usually not straight forward or for creating pipelines for existing algorithms but  for accelerating new algorithms that can't be optimized using current hws or for some preprocessing steps in sensor fusion such as radar, lidar or any kind of sensor (but I think we have dsp module for some of it)",
        "Lots of thermal cameras use then",
        "Well, there's nothing really unique or interesting about combining the two. You use FPGA's because you can get high performance (throughput or latency) for your task - better than just running something in PyTorch or OpenCV but slower than if you designed ASIC (very expensive).\n\nYou should spend some time thinking about what you want to work on, then decide if it's the right job. I don't think anyone here will be able to tell you whether it's a great idea or not.",
        "Yes, absolutely. I once worked at a university where a team developed speed sign detection on an FPGA. It worked very well and only consumed 3 W of power processing 60 frames per second at 1080p.",
        "when you suggested FPGA tutor responded, no first use gpu",
        "depends on the environment  - a LOT of cameras still use FPGAs on the sensor to convert to USB/GigE but some are moving to ACIS, and there are FPGA signaling for use on image processing on camera / embedded, but many of those are moving to Jetsons. \n\noutside of that a GPU will be far easier and powerful to process images. \n\non a camera, FPGAs are still a good option, but its a niche of a niche",
        "Also worth noting that you can readily get radhard FPGAs for space.",
        "Do I need hardware knowledge if I want to build my own computer vision software?",
        "I think you are mistaking. I really doubt if they re-implemented the Conv operation or matrix multiplication using something like VHDL or Verilog. They probably used the neural network accelerator available on some FPGAs and did not use the Programmable Array part.",
        "Only if you want to build the camera, too",
        "Here is the title of a PhD thesis that came out of the project. You can find a pdf on Google scholar. Happy reading. \n\nReal-time optical character recognition for advanced driver assistance systems using neural networks on FPGA",
        "Isn't the neural network accelerator part also implemented in the programmable array part?"
    ]
},
{
    "submission_id": "1fiagoe",
    "title": "Synthetic Image Data Generation with Unity Course (UPDATE)",
    "selftext": "https://reddit.com/link/1fiagoe/video/hat1ta8ug7pd1/player\n\nA while back, I shared that I was creating a course on synthetic image data generation to help computer vision engineers and the technical artists on their teams **improve the accuracy of their computer vision models**.\n\nIt got a lot of upvotes and interest (21K views), so thank you to everyone here.\n\nI am about 60% done with the course, and here is a 1-minute excerpt from one of the lessons.\n\nIf you're interested in following the course creation process, you can sign up for my newsletter using this [LINK HERE](https://eli-nartey-27162697.hubspotpagebuilder.eu/synthetic-image-generation-notif-page). This also means you’ll be among the first to take the course.\n\nThanks!",
    "created_utc": "2024-09-16T10:24:09",
    "num_comments": 0,
    "comments": []
},
{
    "submission_id": "1fi6gan",
    "title": "How to add multiple directional lights in open3D? ",
    "selftext": "I want to add more than light source to improve visualisation of my output. Is it possible to add a another directional light (or any other kind of light) in a scene which already has one? Any help would be really appreciated. I chose open3D because it was very easy to play around with looked beginner friendly. (I would like to stick to open3d only.)",
    "created_utc": "2024-09-16T07:45:11",
    "num_comments": 3,
    "comments": [
        "Theres some lights in here of various kinds\n\nhttps://www.open3d.org/docs/0.12.0/python_api/open3d.visualization.rendering.Scene.html",
        "Yeah I used this documentation. But I am unable to add two directional lights at different angles at the same time in my scene. I wanted to ask how can I do that?",
        "Im not too familiar sorry. I use pyopengl these days, in this case you just need an appropriate fragment shader and some light locations and a bit of math"
    ]
},
{
    "submission_id": "1fi5kqt",
    "title": "Any suggestions on Deep Learning Project without high GPU performance",
    "selftext": "Hi, I'm interested in learning and training a model related to structure from motion / perception-based computer vision such as work done by Paul-Edouard Sarlin. My problem is that I only have GPU with 8GB of vram, which limited me to work on very interesting github projects. Do you have any ideas for projects/deep learning model I can learn from that related to geometry-based computer vision which doesn't consume much GPU? Thank you so much.\n\nPS. I have tried NeRF and it fits my GPU but the model is too simple. ",
    "created_utc": "2024-09-16T07:08:34",
    "num_comments": 15,
    "comments": [
        "rent a you from a cloud provider",
        "Look into tinyML or anything for with resource constrained systems such as embedded devices.",
        "You can try Gaussian Splatting instead of NeRF - it's lighter on the GPU",
        "you can use google colab and you wont have to use your own gpu at all.",
        "I don't get what the issue is with the model being too simple in Nerf. Not sure what the latest and greatest use but if all you need is a project the NN in Nerf isn't your bottleneck it's the volumetric ray marching step.",
        "Kaggle gives u 30h of gpu per week if u verify ur account.",
        "Thanks. That might be a solution. I'm currently paying for google colab - not ideal but it's affordable.",
        "Thanks for the idea! I'll look into this.",
        "But it isn't deep learning tho, unless there are newer papers that use NNs?",
        "Are you talking about inference?\n\nI have 8GB VRAM gpu and can't use it to train gaussian splat, but it's fine for training a nerf model.",
        "Yes, I have colab pro. I think it's good for training a model but sometimes I want to debug the model similar to when we do it on laptop using vs code. I don't think you can do that on colab?",
        "Sorry for any confusion. I want to learn DL models and I am interested in a model which involves computer vision geometry like the ones done by the researcher I cited. His networks requires huge gpu memory which I can't afford. NeRFs is a mixed of DL and CV geometry which matches my study plan and did learn a lot from NeRF, but I want to also learn other models within my GPU capacity.",
        "Never know that. Thanks.",
        "8gb is a big constraint for any 3d work. Advice to use cloud compute. You would just have to pay when you spin up a machine. So do your debugging on your local machine and then launch a job using some workload manager like slurm of whatever on an instance.",
        "Thanks so much for your reply! I didn't know you could do that! I don't have any cloud machine anyway but that's a useful advice.\n\nI only have a window laptop with nvidia gpu. Do you have any suggestion on any cv model  I should try? Thanks!"
    ]
},
{
    "submission_id": "1fi551g",
    "title": "Which microcontroller fits the best? LPR",
    "selftext": "Hello, I want to create a license plate recognition system with a camera and possibly a small screen or LED light to inform the driver about the recognized plates. I also want to include a database or something similar so I can update it regularly, or maybe just make it so the driver only needs to plug it into a PC and type one command to update the license plates, to avoid the Wi-Fi connection. Which microcontroller fits the best for a low price? I'm considering ESP32, Orange Pi Zero, Arduino, or maybe there are other alternatives that are low-cost and can 100% do this job? Thank you.",
    "created_utc": "2024-09-16T06:50:07",
    "num_comments": 5,
    "comments": [
        "For license plate recognition, which Model are you planning to use?\n\nAm Assuming , for LPR you are going to use some sort of low profile object detector.\n\nYou can start with Raspberry Pi which comes with IOs and interface for LED, Camera and display screen.\n\nYou may consider chipsets which has NPU/TPU for running Neural Networks like Object detector. Many Options are available like VIM board, Ambarella, NXP.",
        "I have an extra Android phone that I haven't been using. Is it possible to make it perform the same functions?",
        "Sorry not an expert in that area.",
        "How old is it? Modern phones have loads of processing power that should easily be up to the task.",
        "its a Xiaomi Redmi Note 8 Pro  \nreleased in 2019"
    ]
},
{
    "submission_id": "1fi2iub",
    "title": "What's your strategy for hyperparameter tuning ",
    "selftext": "I'm a junior computer vision engineer, and I'm wondering about how you approach the issue of hyperparameter tunning.\nI believe we all face hardware limitations, so it's not feasible to grid search over hundreds of different combinations.\nMy question is how do you set the first combination of hyperparameters, specifficaly the main ones (eg. lr, epochs, batch size) and how do you improve from there. ",
    "created_utc": "2024-09-16T04:48:03",
    "num_comments": 6,
    "comments": [
        "https://github.com/google-research/tuning_playbook",
        "Randomized search is my go-to:\nhttps://scikit-learn.org/stable/modules/generated/sklearn.model_selection.RandomizedSearchCV.html",
        "I use bayesian optimization (either SMAC3 or Optuna). SMAC3 is better but more cumbersome to use; whereas Optuna is very simple and works fine for most problems",
        "Set it pretty low and let an optimizer do the work. Sometimes I have to change it a bit, but usually it'll get there.",
        "Thank you, I'll give it a read",
        "I'd recommend to spend a little more time to learn how to use optuna. Much more efficient than grid/random search"
    ]
},
{
    "submission_id": "1fi1wza",
    "title": "Most studies on image matching on 2023 are still relying on Colmap and SIFT",
    "selftext": "Interesting observation from CVPR 2023 Image Matching workshop - most studies on image matching are still relying on Colmap and SIFT. \n\nVideo can be found here: https://www.youtube.com/watch?v=9JpGjpITiDM. (time mark for this slide 5:25)\n\nhttps://preview.redd.it/s4lkqtgnm5pd1.png?width=912&format=png&auto=webp&s=7653664b342723075a769a1503345c6fd772d17e\n\n",
    "created_utc": "2024-09-16T04:14:32",
    "num_comments": 7,
    "comments": [
        "You can extract your features and do the matching with an algorithm other than SIFT, then import them into pycolmap to use their SfM, and it works perfectly.",
        "Yeah…more than a decade ago I wrote report showing how everything is better than SIFT, and yet still, if people aren’t using SIFT they’re at least comparing against it. Doesn’t make sense for something that old and oft-replaced to be considered a standard.",
        "There is new version of Colmap released few months back. Checkout Colmap organisation on Gh.",
        "I hate SIFT, so slow and sometimes looks at junk as features",
        "Yes, for example demonstrated in https://github.com/cvg/Hierarchical-Localization. \n\nThis does increase accuracy, but the time limiting step is still the reconstruction/optimization in colmap.",
        "It does when you need a reason to publish your shitty little algorithm when you know damn well it would lose a benchmark test against serious alternatives.",
        "Thank you for news. I was just thinking about trying Colmap in action"
    ]
},
{
    "submission_id": "1fi02yz",
    "title": "How to Landing First Computer Vision Job?",
    "selftext": "I recently graduated with a degree in Computer Science, and I am seeking advice on how to land my first job in the field. My final year project was entirely focused on computer vision, and I am confident in my ability to pursue a career in this area, given my hands-on experience. However, there are limited computer vision-based software houses and job opportunities in my country. Should I continue to pursue a career in computer vision despite these constraints? \n\nI am considering pursuing a master’s degree in Artificial Intelligence/Machine Learning. Would this be a good move for advancing my career in computer vision? I am also interested in finding a remote internship, even if it is unpaid, as I am eager to learn and gain experience. What are the best ways to secure a remote internship in computer vision? Thank you for your guidance!",
    "created_utc": "2024-09-16T02:11:15",
    "num_comments": 2,
    "comments": [
        "[removed]"
    ]
},
{
    "submission_id": "1fhxhcj",
    "title": "Scene change detection",
    "selftext": "I’m working on a project focused on scene change detection. My goal is to track objects and trigger an alarm if an object disappears or changes position. However, I want to avoid false alarms, such as when an object is temporarily occluded, for instance by people. The challenge is that objects can vary greatly, and direct object detection is not feasible. What would be the best approach to handle this? ",
    "created_utc": "2024-09-15T22:53:12",
    "num_comments": 17,
    "comments": [
        "My two cents:\n- Detect objects\n- Keep time frame window\n- If objects occluded, and does not detect after specified time, raise the alarm!",
        "Anomaly detection..? Why can’t you train an object detector? Because you have too few samples or you don’t know what they will be in the future? Care to share an example?",
        "This seems quite challenging! This seems like a job for a general object tracking task. Usually they work starting from a bounding box given by a user and then it tracks the object frame by frame.",
        "> I’m working on a project focused on scene change detection.\n\nTook me a few re-reads of the comments to understand you're not after \"Scene change detection\" like https://www.scenedetect.com/ \"the camera cuts to a different area\" and instead \"recognised object movement and/or removal\".\n\nYou can't use SIFT because you need a reference image for every object in the scene. You can't use YOLO for the same reasons. \n\nIf you used a median filter approach to remove temporary obstructions and then re-focus the next frames processing only the detected occluded areas you could probably come up with a kind of \"metric of difference\" like a simple XOR of current:prior pixels and then count-the-ones giving you a number of changed pixels. But it'd be crude and so if someone rotated a product for example, or put it back on the shelf back-to-front it'd trigger.",
        "Do you want it to trigger if someone moves an object at all?",
        "I think SAM is able to segment all objects in an image when you pass the whole image as input. Once you get those segments, you could apply some heuristics to rule out non-objects.\n\nYou can compare the segments between certain time intervals and check the difference. Or compare them when you detect movements in the scene. That way, you don't need to run it every frame which would be very slow.",
        "Do you know what a clean unobstructed frame looks like can you do a time differential",
        "Thanks for your reply. Object detection is not feasible for me. Isn’t it possible to run a key point detection algorithm on each frame and trigger an alarm if a point changes in another frame?",
        "It’s primarily for theft prevention. As you mentioned, we don’t know what objects will be present in the future.",
        "But here my problem is a bit different. I was thinking to run an algorithm like SIFT on each frame and if a key point is missing or moved in the next frame, I trigger an alarm. What do you think?",
        "I think your idea is very similar to motion detection. I also thought about it but how can we disregard short term changes?",
        "Yes",
        "I would try a TLD tracker, where you have to mark the objects in the first frame by hand and the tracker locks onto it.",
        "With SIFT you can easily track points that are not interesting to you, like people. Also I'm not sure how reliable the points are, I think it's common that for just a light change a SIFT point disappears in the next frames. \n\nI think you need to at least have a way to define an object in some way and then track it with some algorithm. If you don't define what is an object you can risk that a light change in the scene be detected as an anomaly",
        "Thanks a lot. I check it out. Please share any other idea you might have. I appreciate your help.",
        "Exactly. I have used object tracking like ByteTrack in my current job but I do not want follow the same path. Because here the tracking does not matter. I am more interested in change tracking",
        "Then maybe a custom anomaly detection model inferencing videos that is able to classify between good changes (like whether or night) or bad changes, like a thief. But it seems more of a research project because you need to design the model architecture and the training process. \n\nMaybe with a classic movement detection algorithm and then a classification model/anomaly detection you can do a similar job."
    ]
},
{
    "submission_id": "1fhrbxy",
    "title": "How to change classes in a yolo dataset",
    "selftext": "Hello everyone I'm a AI and ML student. I'm working on a project were I have to fine tune a yolo model.\n\nFir that I have been preparing a dataset. Due the the objects being farely common I found many of them on roboflow. \n\nNow the issue is that due to these datasets have been made separately they all have the class 0 in their labels.\n\nI have around 10 datasets of objects and each have class 0\n\nAny solutions for how can I tackle this problem.",
    "created_utc": "2024-09-15T17:14:25",
    "num_comments": 12,
    "comments": [
        "When I have to do same changes like that I just write a Python script and do what I need to do. Open the file and change the label automatically with a for loop. No tricks here",
        "The labels are in text files format",
        "You should be able to export the datasets from roboflow and map the classes to a different value.  \n\nIf they are organized you could just iterate over the dataset and change the first value like the other guy said",
        "I just remap in roboflow makes it easy",
        "You can merge them together in Roboflow & remap the classes to combine them if they're named differently before export.",
        "I have a script for it [here](https://gist.github.com/Y-T-G/6ad550e03822cbd9b0722c83e3dfe2cb). You can adapt it.",
        "I don't get your question, as far I know the 0s that you talk about are probabilities of whether that particular object is there or not. If you can elaborate on this, I can help you on it. \n\nDM if you want to know more!",
        "The first 0 indicate classes. Make it into the number you want. Say 1. Edit: yiu also have to adjust the data.yaml file to include the class with the name and NC=Amount of classes. Don't think you can have a class nr 3 with nr 2 and 1 ect. So each line in the dataset is {ID X Y Xlen ylen} when predicting u can get the output {ID X Y Xlen ylen probability} as someone mentioned.",
        "Can you tell me the steps for it.",
        "They are the class labels of the bounding box. At least in the case of yolov8",
        "upload data, add to dataset, generate version, add pre processing step modify classes. With the modify classes step you can override, combine or drop.  If you drop classes and also don’t want those images as null in your dataset add the filter null step too. Then add augmentations if you want, if you do don’t go crazy. Then generate and then you can export or just train and deploy with roboflow inference package ",
        "Correct. The first 0 can be replaced by any number as it represents the class. For instance if I had multiple objects in a dataset the other objects can be represented by 1,2,3 respectively."
    ]
},
{
    "submission_id": "1fhkpnn",
    "title": "Estimate the average blob of the part of the image. ",
    "selftext": "Does something like this exist?  Without specifying the number of regions? I can't find a good way to estimate a blob size for the image. \n\nLet's say something like this as a generic example https://imgur.com/a/Pcy6JHA. I divide the image into regions. I detect edges using the Scharr operator to find brightness changes and combine them to get the edge strength.\n\nThen, I highlight the strongest edges by applying a threshold and filter out the weaker ones. Then, inverting the image to make edges the background, I label the regions between them. \n\nI want to get the average blob size for each region I am getting, but I struggle with methods. ",
    "created_utc": "2024-09-15T12:19:55",
    "num_comments": 10,
    "comments": [
        "Make solid white blobs on black background. Binary image. Hit it with that distance transform. Find max index. Hit it with that flood fill. It’ll return the number of pixels you changed. Put that shit in an array with the pixel count, and center coordinates (the target point on your flood fill). Keep finding max index and flood filling until nothing remains. Wiggity wiggity what?!? That’s a modified watershed bro.",
        "You can have a try at morphological granulometry. It will give you a size distribution that you may use for computing the average.",
        "I would try the [watershed algorithm](https://docs.opencv.org/4.x/d3/db4/tutorial_py_watershed.html) for segmentation, then try to get the average size of the segmented contours.",
        "What does it look like in the frequency domain? Do you know how to do that? This might just be one of those rare times where the FFT is actually useful in a vision task. \n\nBut I will say that I'm not sure what counts as a single blob... the question seems ill posed. What exactly is a blob in this context?",
        "Thanks for the suggestion! I should mention that my image is grayscale as in the link example, not binary. I’m working on detecting regions based on brightness and edges, rather than just white blobs. Watershed on its own over segments to high heaven.",
        "Thank you very much, I'll try this one.",
        "That's what I'm doing now, was thinking if there's a better way around it. Problem is that the watershed is over segmenting quite a bit",
        "I was looking at it using a wavelet transform and then trying getting contours with sobel (mainly because it's appropriate for my dataset) but it doesn't give consistent results.",
        "Approximate size of the segments. Not precise",
        "Unfortunately,  that's the best I got. Maybe do an intensity flood fill. Get the marker after the distance transform and get the points within all of the markers. Get the mode of the intensity values, then flood fill only the nearby intensity."
    ]
},
{
    "submission_id": "1fhfdwd",
    "title": "Application Preference Advice Needed",
    "selftext": "Hi, I'm looking to start trying to use computer vision to automate stats for a UK based Football (🏈) team. \n\nI'm fairly new to the skill so any advice on software would be really helpful (even if it's just \"get better equipment for starters\"). I have added an example of the quality of video that would need to be analyzed for the model. This is something I have proposed I will work on over the next 3 years for test and training until a beta can be rolled out.. so yeah, any advice would be awesome! ",
    "created_utc": "2024-09-15T08:37:38",
    "num_comments": 1,
    "comments": [
        "check out cvpr soccer net\nhttps://www.soccer-net.org/challenges/2024"
    ]
},
{
    "submission_id": "1fh9maq",
    "title": "Image Composition",
    "selftext": "can anyone pl guide me how to do server deployment  hugging face-Gradio pretrained deep learning model (CNN,encoder,attention) with tensorflow python (GPU)for my image blending tasks. thanks",
    "created_utc": "2024-09-15T03:39:15",
    "num_comments": 0,
    "comments": []
},
{
    "submission_id": "1fh8yye",
    "title": "OCR for reading hardcoded Japanese subtitles",
    "selftext": "I'm pretty new to computer vision and I am doing a hobby python project to read hardcoded Japanese subtitles off videos. I have tested Tesseract but it wasn't good enough - so I switched to Google Cloud Vision API which was pretty good. Currently, I am on a free trial but it seems that it will be very expensive to use. Is there any other alternatives that I can use which are free/cheaper to use and would be good to recognize Japanese text? ",
    "created_utc": "2024-09-15T02:53:21",
    "num_comments": 2,
    "comments": [
        "My suggestions are: EasyOCR, manga-ocr or PaddleOCR",
        "Thank you - will try those"
    ]
},
{
    "submission_id": "1fh8wq9",
    "title": "Display center point of object as well as bounding boxes.",
    "selftext": "I'm working on a weed detection model in which I want the YOLO model to find the root(center) of the weed so the drone can target it with herbicide spray or laser. Targeting the root of the weed would be the most effective I think. \n\nSo is there a way I can find the center point of the object with YOLO? Along with its coordinates?",
    "created_utc": "2024-09-15T02:48:51",
    "num_comments": 7,
    "comments": [
        "You could train a YOLOv8-Pose model with a keypoint label marking the root of the weed. It would perform both object detection and keypoint estimation.\n\nCheck out the [docs](https://docs.ultralytics.com/datasets/pose/) for YOLOv8-Pose.",
        "you should label a subset for the soil point and try some simple ways of getting the soil point using your existing model detections.  if you cant find an acceptable method that way, you can try detecting the object you actually want.  if you want to target the soil, your labels should be of the soil.  so a second model for targeting.  Is there always line of sight to the target point or will there be occlusion?",
        "If I'm understanding correctly, you can train an object detection yolo model to detect weeds, then you can easily find the center point of each bounding box with cv2 or your package of choice",
        "But yolov8-pose is slower, it takes almost twice the time for inference. I need the realtime detection of the weeds.",
        "Also do you have any suggestions about which camera I can use? I have a raspberry pi camera v1.3 (5MP), but it gives overexposed images. I looked around for webcams, but they have grainy quality.",
        "I don't think it's slower. How did you test it? It consumes less than 0.3 GFLOPs more compute than the detection model for the same variant. And only 0.5ms slower in my testing of `yolov8n-pose` vs `yolov8n`",
        "You should consider higher end cameras, especially if you're flying at high altitudes. The project I'm working on uses a zenmuse p1, which is pricy, but I'm sure you can find something cheaper that gives reasonable results if it's a side project. I can't help you with recommendations as I don't know much about camera models."
    ]
},
{
    "submission_id": "1fh4ckq",
    "title": "Classifying HTML form parts",
    "selftext": "This might be a very stupid and beginner question but Here I go.\n\nI have zero xp with computer vision, some xp with basic ML algos and tons of xp with python and Data Engineering.\n\nIn my personal project I'm looking for a way to send screenshots of individual parts of a webform (e.g.: A group of radio buttons, tabs, a group of checkboxes, different buttons) to a sort of ML/AI classifier and get the type of of that specific part as a result. In addition I would like to train the classifier for some \"edge cases\" like multi part tabbed forms, etc.\n\nWhat's the best way (something that balances the learning curve for me and efficacy) to implement such classifier?\n\nYour help and ideas is greatly appreciated.\n\nThanks!\n\nEDIT: I've already coded a classifier by analysing the html code, but I need an image classifier as a complementary method as layouts and UX codes can become vastly different for similar-looking forms.",
    "created_utc": "2024-09-14T21:28:02",
    "num_comments": 2,
    "comments": [
        "I'm not sure if I fully understand what you want to achieve, it all depends on what your input is and what the classes are. Is it just classification or detection problem? What are the classes (anything besides the plain html tags like li/ul/p/etc)? How isolated are the patches?\n\nIn a general case, I’d probably go for a detection model to find atomic elements with some post-processing for grouping. The true hierarchical classification would be a huge pain to get right.",
        "Thanks for the comment. The problem is that I'm not even familiar with the jargons of the field. \n\n\nSuppose that we have a webform with multiple parts. \nOne part asks your name and address. \nThe other part consists of questions each with multiple choice answers. \n\n\nNow, I can take screenshots from each individual part (my algorithm does this successful because of HTML hierarchy), all I want from the model is it could rule what picture consists of text fields (so that must be filled) and what picture consists is multiple choices (so that must be clicked)."
    ]
},
{
    "submission_id": "1fh0889",
    "title": "How to train model locally, and use in web app.",
    "selftext": "Basically I want to run a simple image classification model that will work in real time on a web app I'm making. I can't train this on the website itself for compute reasons, so I want to train it locally in Python and then export the model to be loaded and used on the website. \n\n  \nMy approach rn is to load and train a mobilenet or mobilevit-small using Transformers and then upload the model to huggingface and getting the most updated model from my webapp. Right now the problem is many of these models can't be loaded in JS because they're missing ONXX. I found a way to convert but it's a grueling process and I'm thinking there ought to be a better way people go about doing this..\n\n  \ncame here basically to ask how this sort of thing is usually done.",
    "created_utc": "2024-09-14T17:33:00",
    "num_comments": 20,
    "comments": [
        "Deploy it as an api and hit that from your website",
        "Why you have need to train your model? What are your target class that you wish to detect or classify ? Are they part of pretrained class?",
        "That's how models are usually trained. They aren't trained on websites (not sure what that means).\n\nIt should be pretty straightforward and you don't need to use transformers for it. You can just use PyTorch, but if you want something even simpler, you could probably use PyTorch Lightning.\n\nAnd converting an image classification model from PyTorch to ONNX is really simple and just takes a few lines of code. Even ChatGPT could provide you the code for it.",
        "This is meant to give predictions in real time as someone is drawing on a canvas. Sending requests at that rate wouldn’t work for my use case",
        "I’m just doing the old classify numbers 0-9. I’m doing this for learning, not for any real reason. So I’d like to not use zero shot",
        "I have a feeling he’s trying to jam it all into some website hosting service as opposed to more traditional cloud provider services",
        "> It should be pretty straightforward and you don't need to use transformers for it\n\nNot clear here, why are you advising against transformers? It's an easy to use framework, many pretrained models, nice docs/tutorials. Can fit OP's case just fine",
        "You could potentially hit the api between drawing events - like if the user draws by dragging just hit the API when the drag stops and ignore the response if they start dragging again.",
        "No I can train it locally or use a dedicated VPS. I’m just having a hard time doing so🥴",
        "OP said simple image classification model that would run real-time as a web app. I'm not sure how transformers isn't overkill for that.\n\nI can't even find a Hugging Face training tutorial for MobileNet, ResNet or some small image classification model that would run on a web browser while googling for it in the first few results.\n\nWhile I got the PyTorch Lightning training tutorial [to train a ResNet](https://lightning.ai/lightning-ai/studios/image-classification-with-pytorch-lightning) just in the second result of the search.\n\nI don't see why OP should overcomplicate his task by using transformers.",
        "would be cooler with real time tho. :D",
        "If you’ve got a dedicated server this shouldn’t be hard to do.  It sounds like you’re missing some of the SE/MLE skills to put it all together",
        "That’s just what I know from an NLP class I took. I’ll try it out with PyTorch. \n\nAlso I wanted to use hugging face hub so I could dynamically update the model as I trained it. \n\nThe project is meant to be a model trained by crowdsourced data. Each day a cron will run a python script that trains the model with the data people voted on and uploads it somewhere.",
        "I’m not convinced it would make much practical difference, what are you expecting it to detect from a half drawn image? \nThe other option is build a mobile app and do real time (I did this with an iPad app and Apple Pencil)",
        "I just think I'm taking the wrong approach, doing shit with Transformers library when that's prolly not the best solution.",
        "You should still be able to upload it even if you train a model without transformers.",
        "Whether or not it’s realtime, I wouldn’t want an API. Images are a lot to send over a network when they don’t have to be",
        "Yeah that’s fair enough but i think the constraints you’re putting on it mean it’s not possible as a website",
        "I will prove you wrong :D \n\nI've also seen this done in the wild, so I know it's possible.",
        "I’m looking forward to seeing what you come up with :)"
    ]
},
{
    "submission_id": "1fgtisx",
    "title": "Source of image in metadata?",
    "selftext": "I am downloading a bunch of free-use images online and I want to use OpenCV/Python to get the source of the image (a URL, preferably). Is this even possible? I have been doing research, but cannot find it. ",
    "created_utc": "2024-09-14T12:11:50",
    "num_comments": 5,
    "comments": [
        "The \"source url\" of an image is not a standard metadata. What do you mean by source? The URL that you downloaded it from?",
        "You mean like doing a reverse image search to find out where the image came from?",
        "Something in which I can give the original author/source credit is what I mean, e.g. \"ESPN\" or \"www.ESPN.com\" or \"AuthorLastName, AuthorFirstName\" - something like that.",
        "Not sure if this is a r/computervision thing.\n\nIt's just metadata.\n\nAs I mentioned, such metadata isn't standard. You can read whatever is in the EXIF metadata, but there are no guarantees that there is anything related to attribution there unless the site explicitly provides it.\n\nHave you tried using something like ExifTool to look at the metadata?",
        "It’s not even guaranteed there would be a meaningful EXIF in there, lol"
    ]
},
{
    "submission_id": "1fgik9s",
    "title": "Set up this Tiny AI Camera is Super Easy! Pre-build On-device Node-RED Workflow and Live-check Streams from Any Browsers!",
    "selftext": "",
    "created_utc": "2024-09-14T02:48:01",
    "num_comments": 4,
    "comments": [
        "reCamera is the open-source, programmable AI camera that's super easy to set up and customize—all directly on the device!💡 With pre-built Node-RED for on-device workflow configuration, you can quickly build and modify your applications. Plus, you can live-check streams directly from any browser. GitHub repo at: [https://github.com/Seeed-Studio/OSHW-reCamera-Series](https://github.com/Seeed-Studio/OSHW-reCamera-Series)",
        "Powered by a RISC-V SoC, reCamera delivers 1 TOPS of AI performance, making it perfect for object detection and other computer vision tasks. It’s an ideal solution to use as a programmable IP camera for your next project.See more about reCamera: [https://www.seeedstudio.com/recamera](https://www.seeedstudio.com/recamera)",
        "Interesting, I was looking at the LicheeRV Nano for some portable low power object detection, but the support and documentation wasn't that good. Do you guys have more information/set-up guides etc? Also how much will the a basic set up be? (Board + camera)"
    ]
},
{
    "submission_id": "1fgicra",
    "title": "Update with yolov5 yolov8",
    "selftext": "",
    "created_utc": "2024-09-14T02:31:38",
    "num_comments": 0,
    "comments": []
},
{
    "submission_id": "1fgi4qa",
    "title": "Nee CV Framework",
    "selftext": "Hi,\nSince the mm Echosystem ist nit really supported / improved anymore, which are the best alternatives for the Future? I think in 2-5 years mm will be dead. Does anyone See that different?",
    "created_utc": "2024-09-14T02:14:22",
    "num_comments": 2,
    "comments": [
        "Jesus, I'd hope so. All the code bases based using mm are atrocious. Things like DAFormer, Semi-DETR are virtually impossible to modify or debug. I never understood why you wouldn't just use pure pytorch.",
        "Learn pytorch and write your scripts. Frameworks like MMDetection are used just for learning CV at the beginning + there are a lot of limitations.  In any research problem / industry, you cannot deliver the CV product without really knowing pytorch, how to manipulate data and optimization methods (tensorrt, vulkan, camera pipelines, preprocessing methods etc)."
    ]
},
{
    "submission_id": "1fght0n",
    "title": "Initiating  TensorRT engine for inference without ultralytics",
    "selftext": "\\* Hello Everyone,\n\nI am trying to make an inference pipeline for my engine for the YOLOv8 ,I have found out a way to convert my trained model from .pt file to a .engine() file .I tried to initiate the engine myself however the bounding boxes were all over the place ...(I cannot use the ultralytics library ) ...I searched a lot on how I can initiate it but none seem to work ..can anyone help by pointing me to the right direction.",
    "created_utc": "2024-09-14T01:49:55",
    "num_comments": 2,
    "comments": [
        "You can just follow the [code](https://github.com/ultralytics/ultralytics/blob/02e995383de4ee3754ceb96a993f91b070c71e77/ultralytics/nn/autobackend.py#L222) in Ultralytics.",
        "Make sure your channel order is correct: RGB vs BGR."
    ]
},
{
    "submission_id": "1fgd4jb",
    "title": "Looking for a video dataset",
    "selftext": "Hello! I am looking for a vast, big video dataset. The videos should, if possible, not be too long or of high quality (for faster processing). I am trying to build a VIdeo Frame Interpolation network from scratch. Any advice is also welcome, I am new to the VFI journey and don't know where to start.",
    "created_utc": "2024-09-13T20:27:04",
    "num_comments": 2,
    "comments": [
        "Ooof... could you possibly use public access tv streams and break it into chunks?",
        "Might work. Thank you."
    ]
},
{
    "submission_id": "1fgag1k",
    "title": "Simple Object Detection for Special Needs Son Bus Arrival",
    "selftext": "My son has special needs and gets picked up and dropped off at my house. The bus timing isn't exact and there have been times I've been engrossed in work and not noticed the bus arriving to drop him off and they sit there until I come up. I'd really love a fairly simple solution to notify me when the bus arrives so they aren't waiting for me to come up, but there are a ton of options out there and I'm sure it would be easy to go down the rabbit hole. \n\nI have pretty extensive C# experience and I've modified some Python code, but I don't want to spend all my free time putting something together. I have a Raspberry Pi 3b and a crappy old webcam, but I can add more hardware as necessary as long as it isn't too expensive.\n\nDoes anyone have some suggestions on how to accomplish this? ",
    "created_utc": "2024-09-13T17:59:27",
    "num_comments": 8,
    "comments": [
        "Wouldn't it be easier to attach a GPS tracker to your son's backpack and set up notifications based on location and time from that?",
        "you should setup a camera and record footage of that bus. You can DM me and I can help with the implementation once you have raw bideos of the bus where you expect to see it",
        "If you have a computer running all the time you can setup a Python script that continuously analyzes the last N frames of a video feed. I guess a raspberry pi counts…although I only have experience with a “full size” computer so I can’t help you there. \n\nThis script would feed each frame through a neural network model. One popular Python package for this purpose is called Ultralytics (this one is hotly debated due to licensing, but for your case it’s completely free and legitimate). \n\nYou could keep things simple and use a pretrained model with a region of interest. Most object detection models, Including the ones from Ultralytics, are pre trained on a dataset called COCO which includes 80 categories (I think) a few of which are vehicles. These will give you a box around the bus (the object detection version), or even its exact outline (the segmentation version). You would then check if the box/outline is located within a predetermined region where the bus typically parks to pickup your son. This of course assumes the camera is securely mounted/static.  You can also enable the tracking feature and use it to see if the position of the box/outline is static, suggesting that the vehicle is parked and not simply driving by. Wait until it’s been static for N seconds before sounding the alarm. \n\nIt’ll false positive if you have a package delivery or something…but maybe that’s ok? It might not work reliably enough if the position and timing of the bus isn’t at least somewhat predictable, or if it has a unique appearance that’s tricky for the pre trained model to recognize. \n\nThe nice thing about this approach is they you can start using it before you have a ton of historic footage. Then, once you do have footage, spanning several months or more ideally, you can use that to fine tune the model specifically for your son’s which will make it more accurate so it can reject things like package deliveries.  \n\nSounds like a fun project and I wish you and your son the best!",
        "Btw I came across another sub that might be useful. \n\n https://www.reddit.com/r/homeassistant/comments/f94waz/are_there_any_relatively_easy_ways_to_implement/",
        "This is a great idea no matter how I proceed, but I'd also like to recieve a notification in the morning when it arrives to pick him up. Sometimes it shows up 5 minutes before the 10 minute pikcup window and surprises us, so a heads up would be helpful too. \n\nThanks for the tracker idea!",
        "I need to figure out my setup to get some video, but I'll try to get something together in the next week. Thanks for the offer!",
        "This actually got me on the right path for a simple start. I was able to use my Raspberry Pi to take a pic every 15 seconds and send it to an instance of DOODS2 I got running on my PC. The model offers \"bus\" but since the special needs bus is shorter, it returns as \"truck\", so I just look for that response. Since truck is too common, I limited the window to the \\~15 minutes before/after the bus is supposed to arrive and only alert in that window. For a simple alert I hooked up my pi board and have it buzz a few times. It's very quick and dirty, but it's working for now while I figure out what I want to do next and when I have the time to do it. Thanks for the help!",
        "Yeah getting a good amount of video is step 0! \nJust start dumping it someplace and then you can come back later and harvest what you need. Keep the quality high if possible. "
    ]
},
{
    "submission_id": "1fg8s0f",
    "title": "OCR 4 Tb of PDFs.",
    "selftext": "Hello, I am looking for a manager/search engine that has the OCR capacity to import 4 TB of PDFs that I have. I am looking for a solution that can handle this amount efficiently and that of course, can use a graphic that I have to do the OCR if possible. I know of some solutions but I have heard that when they handle a certain number of documents, performance drops considerably.",
    "created_utc": "2024-09-13T16:35:29",
    "num_comments": 6,
    "comments": [
        "What are you doing with the pdfs",
        "Do you need to ocr the PDFs or can you pull the text from them?\n\nI would use pymupdf and pytesseract (if you must). Convert to text files, the. Load the text files into whatever database you’re planning to use.",
        "I just started looking deeper in this area myself, and have been looking at ocrmypdf. A post from a year ago discussed it too: https://old.reddit.com/r/selfhosted/comments/132bffb/ocrmypdfonweb_web_ui_for_ocrmypdf/   This handles PDFs that are scanned images of text, does OCR on them, and then adds back a hidden text layer matching the text in the scan. The PDF appears unaltered, but now the text can be retrieved.",
        "[https://www.acodis.io](https://www.acodis.io)",
        "http://www.tobias-elze.de/pdfsandwich/"
    ]
},
{
    "submission_id": "1fg8fih",
    "title": "Free alternatives to Yolo v8 object detection?",
    "selftext": "I'm using Yolov8 (Nano) object detection model which so far, has been good both in speed and accuracy. Only problem is that it is not free. Is there a free alternative (preferrably newer models) with same or better accuracy?",
    "created_utc": "2024-09-13T16:18:35",
    "num_comments": 25,
    "comments": [
        "There are so many sketchy things about Ultralytics, from their shoddy code, to how they’ve done releases, to the ChatGPT generated responses to issues, and of course the license. Hopefully an alternative emerges. MMDetection was at least promising, but it’s seemingly dead.",
        "I think there would be faster rcnn, resnet, yolo-nas and maybe yolov7 ?",
        "YOLOv9 is actually being re-written to be licensed under MIT license and not rely on any GPL licensed software (like some previous YOLO versions). An early version is already [out](https://github.com/WongKinYiu/YOLO).\n\nWhat this means is it will be free to use, train, deploy, customize the YOLOv9-MIT model.\n\nSuggestion: I am not a lawyer, use it with caution.",
        "Can you check yolo-v4. Compare once for performance and accuracy.",
        "Have u tried RT-DERT ?",
        "Check PP-YoloE",
        "It is Affero GPLed code. As long as you don’t ship any product with yolov8 or put it on the internet and provide a service, it is free. If you’re internally using it inside your organization, you don’t need to pay any licensing fees. But not a lawyer, so consult with one anyways before making a decision.",
        "Yolo-NAS?",
        "I use yolox - Apache 2.0",
        "Maybe try a classifier + selection search?",
        "Yolov10. Basically the same issues through, still Ultralytics based.\nI share your frustration. Object detection is in a really bad spot.",
        "[deleted]",
        "[deleted]",
        "This projects says the code has memory leak issues. Shall not use it now",
        "Good suggestion. But i need something as fast as Nano.",
        "Not a lawyer either, but as far as I'm aware, commercial use covers not just client facing applications, but also internal use. You will need a license too.",
        "Tried. Wasn't as good",
        "Can you explain more? Any references to point?",
        "Remaking r-cnn ?",
        "Yeah I liked yolov10 but same license issues",
        "I don’t believe it is for commercial use.",
        "you're confusing _freely available for download_ with _licensed for commercial use_. \n\nit's [AGPL-3.0](https://www.ultralytics.com/license) with a separately available commercial (and quite costly) license.",
        "Using yolo v7 and after testing all other v_ see no reason to migrate. Saying “older” is misguided",
        "You can train it and then prune it using pythorch built-in method and see if you can get better latency performance with less weight.",
        "I find it *very* bizarre, how they think their licensing terms apply to freshly trained models.\n\nI mean, sure that is what they write. But the GPL (and adjacent licenses) absolutely and explicitly do not cover output (the model) of a program.\n\nI would be happy to just ignore this restriction, unless either i) using their python code in a product, ii) using a pretrained model (but this has more to do with the licensing of the underlying datasets).",
        "Yes I think if it was ever challenged in court, it would not hold up. They just need to change the license. They’re not open source in any meaningful way.",
        "The other sketchy thing: their enterprise license explicitly covers pre trained models. Pre trained models include classification trained on Imagenet. The Imagenet terms only allow non-commercial and educational use.\n\nSo, whoever came up with their licensing terms needs to make up their mind, and beyond that appears to be fucking clueless. Which would make me even more inclined to ignore whatever legalese they dream up."
    ]
},
{
    "submission_id": "1fg44la",
    "title": "Why would you want to have a CRA (Chief ray angle) of anything but a 0 degrees on a CMOS sensor?",
    "selftext": "\nThe lens is directly in front of the sensor, the chief ray angle never exceeds a couple degrees, why do embedded sensors have 28 degrees? what is the point?",
    "created_utc": "2024-09-13T13:06:15",
    "num_comments": 1,
    "comments": [
        "There's several chief ray angles: the one of interest here is probably the sensor's, as in \"the chief rays that we expect from the lens, for each pixel\". Even with a lens perfectly parallel to a sensor, rays do fall on pixels at various angles for different pixels: the center ones are close to 0°, the corner ones are much higher.\n\nThis is something you can take into account: the sensor's microlenses can be slightly offset to adapt to angles. But if you use a lens with a sensor whose chief ray angles' don't match, you get issues: luma and chroma shading, crosstalk, loss of sharpness, etc..."
    ]
},
{
    "submission_id": "1fg02uu",
    "title": "Update with yolov5 yolov8",
    "selftext": "Hello friends, I am working on a project. I want to do the work done with yolo5 on this site with yolov8. If you have any information about this subject, I would be very happy if you could help me.https://codeocean.com/capsule/7713588/tree/v1  \nIf you want, help me with this code and we can examine it together.\n\n",
    "created_utc": "2024-09-13T10:14:52",
    "num_comments": 0,
    "comments": []
},
{
    "submission_id": "1ffz4qk",
    "title": "Best OCR model for text extraction from images of products",
    "selftext": "I currently tried Tesseract but it does not have that good performance. Can anyone tell me what other alternatives do I have for the same. Also if possible do tell me some which does not use API calls in their model.",
    "created_utc": "2024-09-13T09:34:53",
    "num_comments": 33,
    "comments": [
        "Lmao bro is cheating for amazon ML challenge .. don't help him.",
        "PaddleOCR... Their off-the-shelf models does most of the OCR tasks in real world... But, if your use-case is different, a custom model can be trained....I had a use case of detecting and recognition of \"etched\" characters on a rough and rugged casting surface... Keyence rule-based algorithm failed miserably....The guys from Keyence were asking the client to provide them a smooth surface like a tile, while in the casting industry it is next to impossible... Tried all commonly available OCR tools, and all of it failed too.... Stumbled on PaddleOCR and it took some time to understand the way they have structured their GitHub repository....",
        "Not sure about costs but phi vision 3.5 is what we're using.",
        "Competing for Amazon ml challenge?",
        "Do we have dataset for such cases?\nAny benchmark?",
        "I once tried Parseq OCR it is fast and gave good results for my application",
        "try with TrOCR",
        "Lmao... Is chatgpt code allowed?",
        "Yes, many are recommending paddleOCR but I'll compare it with OCR 2.0 which has been recently launched and shows promising results. Also do you know about some lvms that can do ocr tasks on par with these. One I know is florence and I will compare the performance for each of them",
        "Have you got any insight on comparison between phi3 in terms of accuracy",
        "I've personally tried and tested TrOCR,\n\nHandwritten and machine printed texts, both models are prety good I'd say, though I had to retrain it for my use case and also made it multi line text recognition",
        "Tf ?? If they can't catch gpt code , how will they punish it ?",
        "I have tried them in the use-case, where I got into PaddleOCR.....Detections/Recognitions of OCR's were good when the image\\_capture/lighting were good......My use\\_case had erratic \"image\\_capturing\" and manu of the images captured, were from \"very\\_bad\" to \"worst\", which Llm's were not unable to correctly detect/recognize.....That is when I started to search for the best framework and I stumbled upon PaddleOCR......And my \"use\\_case\" was for a production environment and I had to give my client the best to perform in all kinds of scenarios.....I haven't tried OCR 2.0, and I will have a look at it....Thanks for the info...",
        "Only anecdotal. We're reading labels on industrial equipment, with a variety of different formats etc. The key thing is that it is able to understand context. Things like what's the brand serial number, type of chemical etc. It understands the context and spits out a json for us. Much easier post processing",
        "I'm also retraining TrOCR for cheque amounts in French. I haven't achieved perfect results yet because my dataset is small (150 images for training)",
        "How do they punish?",
        "What a coincidence! I did it for the cheque (English) as well but not just amount, I did for almost everything from the cheque.",
        "idk banning you or something",
        "That's awesome! Personally, I'm fine-tuning TrOCR just for the amount in numbers and using another model for the amount in words. Then, I'll use YOLO to extract the areas where the amount is written in numbers and words. I came across a big dataset of cheques (English) on Kaggle, let me know if you need it.",
        "Good luck with writing all the code by yourself",
        "My process pipeline is:\n\n1. Cheque classification - handwritten/ machine printed ( custom model with resnet-18 as feature extractor)\n2. ROI detection ( YOLO, tbh it works really well for detecting text regions)\n3. Text recognition ( TrOCR)\n \nI think you can try TrOCR for bothamount in numbers and words, it does perform well on both.\n\nAlso I'd really appreciate if you could share Kaggle dataset.\n\nThanks in advance🍻",
        "Bhai tu baat hi ni smjha ...",
        "Personally, my client just said she wants the amount in numbers to match the one in words to confirm the detection was correct. So I came up with this pipeline:\n\n1. Using YOLO to extract the areas where the amount is written in numbers and in words.\n\n2. With the fine-tuned TrOCR, I'll process the amount in numbers separately and the amount in words separately as well. I thought this approach would minimize errors, like avoiding detection errors such as 'One' being detected as '0ne'. The amounts in words can be in French or Arabic.\n\n3. Then, I thought of using NLP to correct any errors, like if 'six' is detected as 'sin', I would use NLP to fix it. For now, I’m still figuring out how to do it.\n\nHere’s the dataset of cheques in English:\n\n- [https://www.kaggle.com/datasets/victordibia/signverod](https://www.kaggle.com/datasets/victordibia/signverod)\n\n- [https://www.kaggle.com/datasets/saifkhichi96/bank-checks-signatures-segmentation-dataset](https://www.kaggle.com/datasets/saifkhichi96/bank-checks-signatures-segmentation-dataset)\n\nThese are from India (not sure if they’ll help you):\n\n- [https://www.kaggle.com/datasets/victordibia/signverod](https://www.kaggle.com/datasets/victordibia/signverod)\n\n- [https://www.kaggle.com/datasets/pranav10000/chequedetection](https://www.kaggle.com/datasets/pranav10000/chequedetection)\n\n- [https://www.kaggle.com/datasets/jdranpariya/cheque-data](https://www.kaggle.com/datasets/jdranpariya/cheque-data)\n\nThanks for sharing your pipeline, and good luck with your project!",
        "Ohh.../s tha ?",
        "Thanks for the dataset.\n\nJust FYI, \n\n3. I had implemented simple autocorrect mechanism based on text similarity about mispelled words and it worked amazingly accurate. It might save you lots of time. Let me know if you want some help with it.\n\nGood luck to you too!🍻",
        "Arey Mai yeh keh rha tha ki wo pakad hi nhi paayenge ki code gpt sey likhwaaya hai ya nhi",
        "Oh really cool! I wouldn’t say no to your help with that. Is it an NLP model?",
        "Oooooooh... Noted",
        "No, it's simple text matching algorithm based on [levenshtein distance](https://www.google.co.in/url?sa=t&source=web&rct=j&opi=89978449&url=https://en.wikipedia.org/wiki/Levenshtein_distance&ved=2ahUKEwjny_is_cSIAxU6yDgGHYkBBOEQFnoECDMQAQ&usg=AOvVaw3ueH2atTwXc-qxAhJ7c7PH)",
        "Living up to your username,  huh?",
        "That's interesting! Can you share it with me?",
        "😂😂😂",
        "No prob! I don't have it handy, but I can share with you tomorrow. Just dm me your mail"
    ]
},
{
    "submission_id": "1ffyqcw",
    "title": "Blog post: Use cases of CV implementation in the power utility industry and smart cities",
    "selftext": "This [blog post](https://www.opencv.ai/blog/ai-in-utilities-and-smart-cities) includes the most frequently used cases of CV implementation in power utilities and smart cities. In this blog post, you can find some good examples of projects which work in this sphere. It is not a technical post, but it can be useful to start a thread. If you have more interesting Use cases or projects, please add them to the thread. It would be very useful to me. Thank you. ",
    "created_utc": "2024-09-13T09:17:29",
    "num_comments": 0,
    "comments": []
},
{
    "submission_id": "1ffwkfq",
    "title": "How good is Computer Vision course by Andreas Geiger?",
    "selftext": "I recently finished CS231n from stanford. Is this course worth it? How good is it? Also, are there any homework problems? I found a link to a drive on their website but the drive contained the lecture notes and slides, Is there no HW problems for this course online?\n\n[Videos  ](https://www.youtube.com/playlist?app=desktop&list=PL05umP7R6ij35L2MHGzis8AEHz7mg381_)\n\n[Website](https://uni-tuebingen.de/fakultaeten/mathematisch-naturwissenschaftliche-fakultaet/fachbereiche/informatik/lehrstuehle/autonomous-vision/lectures/computer-vision/)",
    "created_utc": "2024-09-13T07:47:45",
    "num_comments": 25,
    "comments": [
        "The guy wrote a seminal paper in 3D - KITTI360, which has become a standard in setting up Lidars and cameras on autonomous driving systems.  Not sure about the course, but he's a big deal in those circles.",
        "I followed the YouTube videos with the lectures notes, it’s a good course, he explains thing very well, with maths equations, you learn fundamentals and more “classical” deep learning stuff like RNNs, CNNs, GAN. The course is designed to be a lecture so you don’t get much access to practical stuff. \n\nOn a side note, I think that there are also (in addition) other great courses from the Tübingen University on [YouTube](https://www.youtube.com/c/T%C3%BCbingenML), like the one from Phillip Hennig on Probabilistic Machine Learning to dive even deeper into maths.",
        "Would've never expected to read his name here. I wrote my master thesis in his team and went to a bunch of his lectures at university, he's super passionate and knowledgeable, can only recommend!",
        "Speaking as a computational photography nerd, the course according to the website looks like it frickin’ rocks. It depends on what you’re looking for, as this seems more focused on fundamentals than the “latest and greatest”\n\nPardon my French but screw homework anyways, build projects.",
        "Following",
        "Im currently following The Ancient Secrets of Computer Vision course and its been very enjoyable ",
        "Does this institution offer other free courses",
        "Have you taken the Deep learning course or Computer Vision course?",
        "I'm actually worried about there being no HW. I learn better when there is a mix of theory and application of that theory. I think I'm gonna just take the lectures as it is. For reference, I have taken Andrew Ng's ML specialization and CS231n by stanford.",
        "I want to learn the fundamentals as I'm a beginner. Thanks, I will just take the course as just lectures then.",
        "Are you planning to do this course?",
        "The DL",
        "Did you start the course? How’s it going so far? I’m also a complete beginner and would like to start learning the word of computer vision.\n\nDid you learn the maths before starting this? I just want to learn as a hobby not get a job in the industry.",
        "Yes. I have already started, but I want to see if it's worth continuing as it's a very long one.",
        "Not yet, but soon. Try cs231n by Stanford before this.\nDidn't need to learn anything new math wise. If you know linear algebra, stats, then you are good to go.",
        "How much have you done?\nDid u find HW problems?\nWhat courses have u done before this?\n\n\nIf you don't mind, can you answer these Qs and help a fellow learner out?\n\nThanks",
        "I don’t know any maths, haven’t done math since high school. I know how to code already so that’s a plus I guess.",
        "Coding in CV is not that difficult. Especially with the frameworks in place. Even then, you will mostly use numpy, matplot to implement models from stratch and then pytorch/tensorflow to implement models from a higher level of abstraction. \n\n\nI would suggest you revise your math concepts and then do CV. Do you know ML basics?",
        "Negative I’m a complete beginner. I’ve used models created by other people but never looked into the black box.  Now I want to start peaking into it and learn some of this stuff. I mostly want to be able to follow and understand papers and make small modifications to existing open source models or troubleshoot PyTorch code if needed. I don’t ever plan on trying to create models from scratch, that requires PHD level knowledge and a team.",
        "I would suggest you start with Andrew ng's ML specialization on Coursera. It will teach you the basics of ML. If you can't solve the assignments, then search on google \"Andrew Ng coursera Solution Github.\" Apply for financial aid. You can easily get the course for free. That should give you enough knowledge to mess with models. \n\nThe only downside is that it uses TensorFlow, but you shouldn't have any difficulty with it as it's quite similar to Pytorch.",
        "Thanks for the reply and help! I think I’m going to start with the maths and get it out of the way. Then start those courses. I learn best when following a structured path/roadmap.",
        "Welcome and good luck!",
        "This is the curriculum the new chatgpt model created for me. Any thoughts?\n\nhttps://pastebin.com/RiVBK112",
        "Yea, it's fine. Maybe watch some videos on youtube about roadmaps. They are BS, but they will give an idea about what to do and resources."
    ]
},
{
    "submission_id": "1ffuers",
    "title": "Is it feasible to produce quality training data with digital rendering?",
    "selftext": "I'm curious, can automatically generated images of different angles, camera effects, for example hand modelling a 3d scene then rendering a bunch of different camera angles, effectively supplement(not replace) authentic training data, or is it total waste of time?",
    "created_utc": "2024-09-13T06:12:59",
    "num_comments": 10,
    "comments": [
        "Absolutely feasible, does it make sense?  Depends on the situation.",
        "Depends on your task and what you're really simulating, really.\n\nIt's also much harder than people think to have many detailed 3D scenes: it takes a lot of work and is very time consuming to work with.",
        "In a 2D space, I created this simulation that produces new scenes for instance segmentation tasks. I've noticed an increase in my mAP after generating synthetic data from it. Although I really need to quantify it and produce a graph showing the improvement. https://github.com/ainascan/phy_cut_paste",
        "Is it feasible? Yes. Is it trivial? Depends. It is incredibly hard to do with non-traditional image problems like SAR/ISAR, but I’ve seen Sims-level simulated data be sufficient for training object detection algorithms on household items.",
        "It depends on how much real data you have and what do you want to get out of using synthetic data.\n\nIf you have limited or no real data it mostly works. It is not a one to one replacement to real data, but like u/LucasThePatator mentioned there are systems solely trained on SD. \n\nThe type of generation you need and how hard it is depends on your use case. If you have a static inference environment (ex: detecting parts on an assembly line), you can model a 3d scene and randomize appearance of parts on that scene. On the other hand, if you are trying to create an autonomous fruit picking robot you need to create lots of variations in your environment, which is infeasible to hand craft. You’ll end up creating your environments procedurally.\n\nThere’s an upfront cost (time & money) in creating SD. You have to create/procure assets (3D models, textures, shaders etc.) and design randomizations. However, after the initial phase it is very cheap to generate data. \n\nThe whole workflow (from scratch) requires the following tools/sites: 3D modeling (free model sites or tools like Blender), texture authoring (free texture sites or tools like Material Maker), a rendering engine (again Blender or a game engine). Dig into my older comments for more detail.\n\nSend me a message if you have more questions.",
        "A lot of systems are trained almost exclusively on synthetic data. There's an entire science to it however.",
        "Physics-based? Aren't you just using image processing?",
        "Yeah it's an interesting approach I tried. Slap a bunch  of contours into a 2D physics simulation, convert them all into convex hulls for faster collision detection, give them a bunch of random force vectors, let the collision take place for some amount of time, then take a snapshot of the contours in there new positions.\n\nIt's like the CUT-AND-PASTE strategy of data augmentation, but with rotational and translational vectors and allowing multiple annotations per image to behave and not overlap.",
        "Nice! Now go spend 6 months writing a 15-page paper lol\n\nDo you find this works better in practice than simple random copy-paste?",
        "Haha 😄 thanks. Maybe I should write a paper on it. 6 months full time with no pay sounds about right.\n\nBut, I have seen an improvement on my dataset using MaskRCNN I'm working with. But it's just subjective at this point. I would need to quantify it and rent some bigger GPUs."
    ]
},
{
    "submission_id": "1fft0yi",
    "title": "3d reconstruction with open3d",
    "selftext": "I am trying to do 3d reconstruction with open3d - [System overview - Open3D 0.18.0 documentation](https://www.open3d.org/docs/release/tutorial/reconstruction_system/system_overview.html)\n\nmy dataset is clicked via my iphone. depth maps generated via monodepth model. when i run the line with my own config file, i get a bad memory allocation error, can anyone explain me why?\n\n    python run_system.py --make --register --refine --integrate",
    "created_utc": "2024-09-13T05:04:09",
    "num_comments": 3,
    "comments": [
        "Kinda need more information than your giving us…\n\nWhat’s the code in run_system.py? \n\nWhat’s the actual error message? \n\nVersions?\n\nWhat have you tried already? Did it work before?",
        "Oh; and how big is your imagery and how much RAM do you have!",
        "i fixed the issue. the actual depth images contains data in mtrs or millimtrs. But monodepth model has pixels mapping to how close or far an object is, but does not give the actual data in mtrs or millimeters. That was the problem"
    ]
},
{
    "submission_id": "1ffsnqn",
    "title": "Exploring RNNs/LSTMs for final pose estimation using positional data.",
    "selftext": "I started a project for solving visual captchas, which requires the tracking of 8 identical, rotating and overlapping 2D sprites. All default trackers from OpenCV fail at this task due to the rotation and the objects being fully identical. I created my own tracker using feature detection with SIFT, hungarian etc., which correctly tracks the object with the diamond 96% of times. The tracking method itself is fully sufficient for the application, however I want to try and use a recurrent neural network to handle the actual tracking.\n\nMy Idea: Use SIFT for feature based detection of the objects in each frame, which gives me the rotation and position of each detected object. Instead of using the Hungarian Method and e.g. the Kalman Filter for keeping track of the identification, I want to apply a RNN (or LSTM) on the data provided by the detection. I do not want to use the neural network for detection of the objects nor feature matching.\n\nCurrently I am collecting training data, but am still unsure whether my approach is at all worth pursuing. Unfortunately I couldnt find much literature about this exact approach to tracking, which usually suggests its a bad idea. Most NN based trackers are used for feature matching and detection, not the actual tracking itself as far as I know. Though my knowledge about this topic is fairly limited. Perhaps you can give me some feedback on my idea or an alternative solution. \n\nTraining data for each frame in a sequence would look something like this:  \n\\[angle1, posX1, posY1, angle2, posX2, posY2, ..., angle8, posX8, posY8\\]\n\n[Example Captcha \\(ignore the fact that the object of interest is rotating faster than the others\\)](https://reddit.com/link/1ffsnqn/video/kwod015makod1/player)\n\n",
    "created_utc": "2024-09-13T04:45:09",
    "num_comments": 0,
    "comments": []
},
{
    "submission_id": "1ffrduz",
    "title": "Template Matching OPENCV",
    "selftext": "Hi guys. I’m trying to perform template matching with opencv. I have a problem: when the template is not present in the image the algorithm draws a meaningless bounding box: how can I detect by code that the bounding box is wrong? I mean that I didn’t find a working confidence value: very very similar values whether the bounding box is correct or not..\nThanks in advance!",
    "created_utc": "2024-09-13T03:27:51",
    "num_comments": 6,
    "comments": [
        "Template Match goes over the image, and computes similarity between the template and the roi and storing the results in the given pixel. When you call get Min/Max Index, you are finding the pixel with either the lowest or highest similarity score based on your given metric. How do you get rid of the meaningless box? Simply check the value. If value at index is outside of tolerance, you don’t have a match. It will never be an exact match it’s just telling you how close it is.",
        "Usually you can count the matches after VoteForUniqueness and VoteForSizeAndOrientation. If the nonZeroCount is above 4 that usually indicates that a match has been found.  \n  \nint nonZeroCount = CvInvoke.CountNonZero(mask);\n\nif (nonZeroCount >= 4)\n\n  \nIf this is already implemented you can check the validity of the Bounding-poly after the PerspectiveTransform using the PointPolygonTest function on the observed keypoints. If the keypoints lie outside the poly you can assume the match is invalid.",
        "It would be easier to help if you could share your code and failed results.\n\nOnly simple mistake that I can think of is probably using a non-normalized correlation function, which is why you can't seem to find a good threshold.",
        "Thanks, but what do you mean by “mask”?",
        "The mask is filled by the VoteForUniqueness(matches, uniquenessThreshold, mask) function  \nIt filters out the relevant matches.\n\nEdit: Sorry i just realized youre talking about template matching, not feature matching. Ignore my previous answers then.\n\nI use the CcoeffNormed for template matching and check if the maxValue\\[0\\] from function MinMax is > 0.9 for exact matching. If not, i assume that no match has been found and do not draw a bounding box."
    ]
},
{
    "submission_id": "1ffpsgc",
    "title": "How to Segment Skin Melanoma using Res-Unet [project]",
    "selftext": "https://preview.redd.it/f6ka62eefjod1.png?width=1280&format=png&auto=webp&s=815c030fa352a82812c2cf725227fc20bb2c9ddf\n\nThis tutorial provides a step-by-step guide on how to implement and train a **Res-UNet** model for **skin Melanoma** detection and **segmentation** using TensorFlow and Keras.\n\nWhat You'll Learn :\n\n- **Building Res-Unet model** : Learn how to construct the model using TensorFlow and Keras.\n\n- **Model Training**: We'll guide you through the training process, optimizing your model to distinguish Melanoma from non-Melanoma skin lesions.\n\n- **Testing and Evaluation**: Run the pre-trained model on a new fresh images .\n\nExplore how to generate masks that highlight Melanoma regions within the images.\n\nVisualizing Results: See the results in real-time as we compare predicted masks with actual ground truth masks.\n\nYou can find more tutorials, and join my newsletter here : [https://eranfeit.net/](https://eranfeit.net/)\n\n \n\nCheck out our tutorial here : [https://youtu.be/5inxPSZz7no&list=UULFTiWJJhaH6BviSWKLJUM9sg](https://youtu.be/5inxPSZz7no&list=UULFTiWJJhaH6BviSWKLJUM9sg)\n\n \n\nEnjoy\n\nEran",
    "created_utc": "2024-09-13T01:32:31",
    "num_comments": 0,
    "comments": []
},
{
    "submission_id": "1ffp0at",
    "title": "Vehicle orientation (front,left,right,back) detection ",
    "selftext": "I want to build a model to detect the vehicle orientation wether it’s (front, left, right, back) what do you think how can I do it ? Do I should use yolo ? Or is there something better give me your input please ",
    "created_utc": "2024-09-13T00:32:39",
    "num_comments": 8,
    "comments": [
        "yeah i'd train different classes for each view with yolo or similar",
        "I think right and left is pretty hard. Because it's symmetrical. You only need to now the front or back. And if the car is with his nose to the right or left and then you know which side the car is.",
        "What do you plan to do with top, bottom, tilted, side view that cover both the view like Front and Left?",
        "What if it's at an angle like [this](https://images.unsplash.com/photo-1618843479313-40f8afb4b4d8?fm=jpg&q=60&w=3000&ixlib=rb-4.0.3&ixid=M3wxMjA3fDB8MHxwaG90by1yZWxhdGVkfDEzfHx8ZW58MHx8fHx8)? What orientation is it?",
        "The main thing to look out for is in your preprocessing. Popular pipelines automatically mirror images to increase the amount of training data. Just make sure to turn off that feature",
        "But I think it’s just about left,right flipping so this is the only difference between it",
        "I doubt they actually need to tell the difference in the left vs the right mirrored image.  They just want to tell which direction the vehicle is facing relative to the camera.  Is the front bumper on the right or left is all",
        "This is very hard I’m thinking about adding new classes like(back-side, front-side) to detect the angel but I’m not sure if it will work or not"
    ]
},
{
    "submission_id": "1ffnyza",
    "title": "Defect Detection of a Mechanism from X-Ray images (Non-Destructive-Testing)",
    "selftext": "Hi, I'm trying to work out if it's at all plausible to be able to train a NN from a relatively small dataset of X-ray images that I produce myself, in order to detect various types of defect in a mechanism, perhaps using YOLO/darknet/darkmark. (For my FYP)\n\nI'd probably be able to produce a quantity of faults in the tens, working with the same mechanism - my own patience and time available is probably the only limitation on this and the number of images I can take - and then augment. I'd likely need to produce images from different angles and probably train for each angle?\n\nI've seen the [following](https://www.reddit.com/r/computervision/comments/18b301g/defect_detection_using_computer_vision/) but it doesn't quite apply to my specific scenario - I'm concerned because I've seen very little information with regards to X-ray images specifically, and with mechanisms as opposed to a basic part - and am wondering if I should follow an alternative project idea.\n\nI was hoping to produce a proof of concept to give myself confidence before starting but I don't even have Linux installed yet and am running short on time to decide between this and another project idea (I prefer this one). Hence why I've instead turned to the superior knowledge and experience of the Reddit community.",
    "created_utc": "2024-09-12T23:17:25",
    "num_comments": 4,
    "comments": [
        "Check [https://github.com/openvinotoolkit/anomalib](https://github.com/openvinotoolkit/anomalib)",
        "Thanks! This has opened up a whole new rabbit hole of possibilities. Is there some way I can combine anomaly detection with defect identification so that it can name the defect type?",
        "for that something like yolo can help. For example run over your images anomalib, find the bounding box for the anomalies, and then you need to tag them and train a yolo-like model. That's could be an option"
    ]
},
{
    "submission_id": "1ffln9j",
    "title": "Practical uses of dated object detection algorithms",
    "selftext": "What are some ways that outdated 2D object classifiers, like Viola+Jones Haar Cascade classifier, are still used today despite the existence of YOLO and CNN?",
    "created_utc": "2024-09-12T20:53:04",
    "num_comments": 3,
    "comments": [
        "I'm not sure where they are being used but I see following pros\n- Faster\n- Cheaper/less resources hungry\n- Does the work they are supposed to be doing",
        "Face detection is the only one, I think. \n\nIt was the first one to be widely deployed on all digital cameras in the 2000s, and I suspect many of them haven't changed the algorithm with newer versions.",
        "I do a lot of image segmentation, often times on embedded system. I find myself using ”traditional” methods (thresholding, binary operations, simple edge detectors) more often than not for a couple of reasons: \n - No need for training data. This is the biggest one. I do niche problems where there’s rarely any open training data available and annotating takes A LOT of time. \n - Better explainability. Image segmentation is done in (human readable) steps making it easier to debug. \n - Hardware limitation. Although this is rarely the sole reason why I go with traditional methods."
    ]
},
{
    "submission_id": "1ffl3to",
    "title": "Segmentation of Different Classes Problem",
    "selftext": "Hello!\n\nI'm currently trying to solve a medical segmentation problem where I'm trying to segment a main class A. However, there is also a second class B, and there is \\*much\\* more instances of class B than class A. One part that is for sure is that there are no instances of class A wherever there is class B, as in, class A will never be surrounded by class B.\n\nWhat I'm thinking is, could I have two binary segmentation models, one for class B and one for class A. I could then use the results from the class B model to mask out the raw input image for class A, so my class A model will have an easier time to segment out the image?\n\nIs this in general, a useful approach?\n\nThanks!",
    "created_utc": "2024-09-12T20:22:06",
    "num_comments": 2,
    "comments": [
        "I don't think you should be masking the raw input images. The model should be allowed to learn to distinguish A from B. You can do the masking or verification in the post-processing step instead",
        "In an imbalanced dataset such as this just modify your training data to include images with better class balance , only select images that meet some threshold percentage for class an and b , or explore some normalization methods. Look up “class imbalance in semantic segmentation” and you should find some good results"
    ]
},
{
    "submission_id": "1ffhv6h",
    "title": "Using Custom Backbone for PyTorch SSD for Object Detection",
    "selftext": "Using Custom Backbone for PyTorch SSD for Object Detection\n\n[https://debuggercafe.com/custom-backbone-for-pytorch-ssd/](https://debuggercafe.com/custom-backbone-for-pytorch-ssd/)\n\nA lot of times, the pretrained models out there may not serve our purpose for the problem that we have at hand. In deep learning, one may face this issue with pretrained object detection models quite a lot. For instance, Torchvision has two SSD models pretrained on the COCO dataset. One with VGG16 backbone, and a lite version with MobileNetV3 backbone. *But what if we want to change the backbone with a more efficient one?* Like ResNet or maybe ShuffleNet. In this tutorial, we will learn how to use a Torchvision ImageNet pretrained ***custom backbone for PyTorch SSD***.\n\nhttps://preview.redd.it/3lnaj9g32hod1.png?width=1000&format=png&auto=webp&s=f6220b086e0f49c548bcc0a27cab5328392c53b5\n\n",
    "created_utc": "2024-09-12T17:34:03",
    "num_comments": 0,
    "comments": []
},
{
    "submission_id": "1ffh60h",
    "title": "Mothbox: AI Powered Open Source Insect Monitor For Conservation",
    "selftext": "Hi! I wanted to share an Open Science Hardware tool we just released publicly. It's a low-cost, high performance insect monitor that you can build yourself with off-the-shelf parts! We have dozens of deployments here in Panama, and so it can withstand really harsh environments.\n\n  \nAfter it collects all your data, we also made custom open AI programs to detect all the insects (modified YOLO) and try to identify what they are (modified BioCLIP). It's an open project, and our computervision parts are really just a bare minimum we need to process stuff, and if you want to improve things totally fork it and make improvements! :)\n\n  \nAll the info and documentation for making your own is right here: [https://digital-naturalism-laboratories.github.io/Mothbox/](https://digital-naturalism-laboratories.github.io/Mothbox/)",
    "created_utc": "2024-09-12T16:58:38",
    "num_comments": 0,
    "comments": []
},
{
    "submission_id": "1ffg2fy",
    "title": "Fine-tune model vs train from scratch?",
    "selftext": "I'm training a 2 class Yolov8 Small detection model, and iterated through the model re-training over the previous best model a few times. Now, I added more data and the size of dataset is 2x (200,000 images now). Generally, should I again fine-tune the previous model or better to train from scratch?\n\nFrom scratch:\n\n`yolo detect train model =yolov8s.pt ...`\n\nFinetune:\n\n`yolo detect train pretrained =best.pt ...`",
    "created_utc": "2024-09-12T16:05:06",
    "num_comments": 11,
    "comments": [
        "It should be better to start from the original pretrained model; the first one (training with `model=yolov8s.pt` isn't \"scratch\" training, it's starting from a pretrained COCO model). This is due to a problem with the [warm-starting neural networks](https://arxiv.org/abs/1910.08475).",
        "Scratch.",
        "to extend upon this and to answer your other comments OP,\n\nif you didn't have a lot of data, this would be a perfect fit for a transfer learning setup. this is when you can even freeze most of the backbone and retrain the high levels parts. but because you have a lot of data, this is just too much for a few \"parts\" of the model to train.\n\nin my opinion, try both starting from scratch and with a warm start. i don't see a lot of harm in either of them. \n\n  \nfor your specific question and context: if your previous model trained on the \"same\" type of data (objects, classes etc), try fine tuning it, it sounds like a good start else probably just go from scratch or start with a COCO dataset trained checkpoint.",
        "Interesting. Some models use coarse vs fine tuned training in sequence, (stable diffusion for example). Why does that differ so you think?",
        "Why do you think so?",
        "The behavior could probably be different for larger models I guess. The paper says transfer learning shows the same problem when you have enough data:\n\n> As previously discussed, the warm-start problem is very similar to the idea of\runsupervised and supervised pre-training [47, 11, 10, 48]. Under that paradigm, learning where\r\nlimited labeled data are available is aided by first training on related data. The warm start problem,\rhowever, is not about limited labeled data in the second round of training. Instead, the goal of warm\rstarting is to hasten the time required to fit a neural network by initializing using a similar supervised\rproblem without damaging generalization. Our results suggest that while warm-starting is beneficial\rwhen labeled data are limited, it actually damages generalization to warm-start in data-rich situations.\n\nBut there are other works that also challenged the findings in this papers. For example, [this paper](https://proceedings.mlr.press/v232/galashov23a.html) from Deepmind:\n\n> In the literature (Ash & Adams,\r2020), this strategy is called warmstarting. Unlike learning from scratch, warmstarting allows to leverage pre-trained\rmodel to speed up finetuning on new data. It was shown (Ash & Adams, 2020) that warmstarting leads to sub-optimal\rperformance due to loss of plasticity. We, however, do not observe this phenomenon in our experiments and find that\rwarmstarting generally leads to better than from scratch performance.",
        "Experience, mostly",
        "Is it bc the dataset suddenly grows? If the added data was limited, still scratch?",
        "Unless you're adapting the same architecture to a completely different objective or on completely different data (i.e. there's zero contextual overlap between the pre-trained model and your application), fine tuning is almost always better",
        "Regardless of scratch or fine tuning. If you train with your own data and finetune the most significant help is training times due to quicker convergence. If you do the same data from scratch then it will take longer",
        "But this is opposite of what he says"
    ]
},
{
    "submission_id": "1ffaamt",
    "title": "Geometry + computer vision, and which is actually useful in real world??",
    "selftext": "I don’t have anything specific field in mind. But I just have a simple question. \n\nIs there anything which combines (/uses) geometry in CV? Any subfield or any specific use case?\n\nI have worked on Pose detection models before (uplifting 2D to 3D). Although that didn’t turn out well, I found it actually interesting so trying to find out something new.m :) ",
    "created_utc": "2024-09-12T11:53:59",
    "num_comments": 9,
    "comments": [
        "I use geometry in almost every CV project. A classic example is running a contour detection in a thresholded image, get the contour with the biggest area, compute convex hull, compute subscribed circle in the convex hull and voilá, now you can measure with good precision the smaller side of any convex object that you are able to threshold. And if you know the shape and size of the object, you can estimate its distance from the camera.",
        "Yes, absolutely. \n\nMany products employ visual inertial navigation systems (VIO/VINS), for example. This technology involves considerable amounts of geometry-based 3D vision, and is applied in AR/VR headsets, mobile phones, and autonomous drones and vehicles. \n\nBundle adjustment (BA) is an optimization problem that features plenty of geometry. It makes up a core piece of 3D reconstruction pipelines.",
        "pick up hartley and zisserman\nhttps://www.abebooks.com/servlet/BookDetailsPL?bi=31824513831&dest=usa&ref_=ps_ggl_11147913055&cm_mmc=ggl-_-US_Shopp_Textbook-_-product_id=COM9780521540513USED-_-keyword=&gbraid=0AAAAAD3Y6gtsb8lFy8CiTW7FR9lhVj3uA&gclid=CjwKCAjwooq3BhB3EiwAYqYoErlCkFjOs7Isv6-hM4Ijf0tfNoDP36H3LcH7DUu5sHJbb7FHrmVD4RoCCTYQAvD_BwE",
        "I feel like almost everything in CV used geometry if you’re trying to make use of “real space” from your target image. Ex) I made a program to locate species and position of waterfowl from drone imagery. That data can now be used in GIS software.",
        "Any CV which works in 3d uses geometry. Basic geometry example is projections to and from camera",
        "Lidsr",
        "equivariant networks and all around it",
        "Any resources to learn more about geometry based vision?",
        "Lol I remember reading the ORBSLAM 2 paper and it referenced a BA paper that has my favorite introduction to a paper ever. Mentioned something along the lines of \"the CV community needs to stop reinventing photogrammetry techniques discovered years ago\"\n\nPaper was titled, \"Bundle Adjustment - A Modern Synthesis\""
    ]
},
{
    "submission_id": "1ff9z4c",
    "title": "Can I segment separate fingers, hands and forearms using SAM2 only? Or would I need another model as well?",
    "selftext": "I have some POV video data from a cooking video, I want to classify the forearm, hand and separate fingers for a project. I only could prompt the hand as a whole with SAM2 using points, but haven’t been able to separate the arm into different portions.",
    "created_utc": "2024-09-12T11:40:38",
    "num_comments": 7,
    "comments": [
        "Hi,\n\nI have used groudning dino with different bersions of sam. If i gave dino a prompt like \"thumb\" or \"pink\" it could in some good enough pictures outut a bounding box area around a single finger. Using that boxes as text prompts to sam helped me get segmentation masks on fingers. \n\nNote that its not perfect in anycase but for 70% of my images i could use it.",
        "I would check out sapiens\nhttps://about.meta.com/realitylabs/codecavatars/sapiens/",
        "You should be able to get decent results if you fine tune a model with some training data of the segmentations you’re looking for\n\nWhat’s the use case btw",
        "That’s nice thanks for that will try doing it with video and see how good it is.",
        "This was a good one, thank you, ended up going with this",
        "Will give this a go, thanks. Never done fine tuning, would you say it needs a lot of training data? \n\nThe use case would be similar to an EGO4D training data for AR / AI experiences, specifically for cooking",
        "there is a lot of research specifically on hands of you need more accuracy down the line"
    ]
},
{
    "submission_id": "1ff4h4j",
    "title": "Expert Straight Line inference",
    "selftext": "Hello everyone,\n\nI am working on a project where I have to collaborate with experts. They have found our problem can be solved by using the statistics existing on a specific straight line they can define on the images. It is asked of me to automate the process of the line creation. They offered to create the lines on the dataset for me to perform segmentation. But won't the 1D line be semantically non-existent? (Even a bounding box around it won't work since the image is mostly saturated at that part).  The experts draw lines by seeing the starting and ending points, which have semantic information around them. So I thought perhaps bounding boxes around them, detection and then just a straight line between the central points of the detected objects.\n\n* This modelling still seems to have many weak points. Are there better ways to model this straight line inference problem?\n* (This is secondary) Since the statistics of the points of a single line can be used to solve the original problem doesn't this mean there must be an easier way to extract meaningful features? (Too few samples for NNs but still..)\n\nThanks in advance!\n\n  \nEdit: this is a toy example of two probable images in another domain. The annotation would be the red line (assume it's 1D). Could have various lengths and orientations. The main semantic property the expert uses to draw it is the eyes. \n\nhttps://preview.redd.it/5bebrp6bknod1.png?width=1248&format=png&auto=webp&s=8458872716dbe739abd02ef5af826b49b9b32828\n\n",
    "created_utc": "2024-09-12T07:51:58",
    "num_comments": 5,
    "comments": [
        "I'd start with sharing an example image to at least get an idea of what are you talking about. Hard to solve computer vision problems without any visual",
        "Thank you very much for your answer. Unfortunately I cannot share an image since this is a research problem and it would give away the expert's solution. In a more general manner my question is this: I want to infer a line on an image in a manner an expert would see it. I have examples of the images with drawn lines on them (annotated). Regardless of the image specifics, what would be some good ideas to solve this problem (segmentation, detection of the line, detection of starting or ending points etc., generative approaches)? I understand it might be difficult without any visuals. If you have any insights with this setup I would appreciate it. Thanks either way for the interest :)",
        "Without a visual it's not just difficult, it's nearly impossible to determine any strategy, let alone a good one. How do the lines look like? Are they attached to some objects? What is their orientation? What are the rules for inferring them? There's a multitude of possible options - from classical image processing, to whatever your imagination allows you to come up with. \n\nYou can try making/drawing an abstract visual of your data. Or showing something similar but from another context.",
        "I've added two examples with an analogous problem with dog images. The lines are 1D, they may have various lenghts or orientations. They are not attached to anything but eyes are a feature the expert uses to define them. Unfortunately this is the definition provided and many examples of lines drawn. I thought somehow, detecting the eyes (which have semantic information could help me define the line). The main issue here is that the algorithm might effectively find the eyes but this doesn't mean it will locate exactly the points of interest. (Slight offsets might change the orientation or length a lot). Would I have a reason to believe another method might be more accurate?",
        "Yeah, my first idea is to do semantic segmentation on eyes and then draw a line between the two masks.\n\n> but this doesn't mean it will locate exactly the points of interest. (Slight offsets might change the orientation or length a lot)\n\nHow are experts doing such adjustments? Do they have a list of criteria which they follow to change the orientation/length/offset?"
    ]
},
{
    "submission_id": "1ff2g90",
    "title": "Need suggestions for an interesting project",
    "selftext": "Hey guys I'm a uni student currently a little new to the field.\n\nI was thinking if there's any way I can use a screenshot of a flowchart or something like that to create PowerPoint copy paste-able elements?\n\nis that a thing that's possible? How can I go about doing it?",
    "created_utc": "2024-09-12T06:22:19",
    "num_comments": 0,
    "comments": []
},
{
    "submission_id": "1feyati",
    "title": "Img transform for dot matrix OCR",
    "selftext": "It’s my second project - first one get decent results, but this one gives me trouble. \n\nWhat are the transforms to get best results for easy-OCR\n\nI currently do grayscale, Gaussian blur, adaptive tresh - I attached images\n\nNow I’m getting 'Blaha płaxa |1i7713/9 29'",
    "created_utc": "2024-09-12T02:21:53",
    "num_comments": 8,
    "comments": [
        "You could try thinckening the lines with dilate",
        "Maybe try qwen vl 7b?",
        "Whatever you do with your preprocessing, it’s likely not gonna be enough, just because your data is likely very different from what they’ve trained on. For best results consider fine-tuning with your own data",
        "I tried dilate - cloudnt get the right setting to get it work - maybe some other way to get there or am I on right track",
        "https://huggingface.co/spaces/Qwen/Qwen2-VL\n\nVery close I got this first try\n\n\"Blacha płaska 1103670819 29\"",
        "Thanks - meaning custom font from easy-OCR or neural network?",
        "I'm fact with your current preprocessed version it gets it correct as it's a bit easier \n\nBlacha płaska 1103870319 29",
        "I mean you’d likely need to train a pre trained easyocr or other model on your data.\n \nIf you have a similar font at hand, you can try generating synthetic data with something like synth tiger.  \nAnother option is to try and find similar datasets online. \nOtherwise, you’d need to collect a couple thousands of examples of such text and annotate it manually.\n\nI mean, don’t get me wrong, you still can try and squeeze something out of the stock model with preprocessing alone, but it just wouldn’t take you far. Especially if you need decent results, that is"
    ]
},
{
    "submission_id": "1fey1q1",
    "title": "Multi-target classification loss with highly unbalanced targets",
    "selftext": "Hi everyone,\n\nI have a model (resnet50) to classify some images. I have 6 classes with respectively (2, 4, 8, 20, 30, 200) labels, so a total number of 264 labels. However, these labels can be heavily unbalanced intra class (for example, inside the class with the 200 labels, I have some classes with a frequency of 10^(-5).)\n\nI have read that in most multi-target classification cases, a BCEWithLogits loss is used, with pos\\_weights set as num\\_negative\\_examples / num\\_positive\\_examples. However, it makes my loss skyrocket with all of these small frequency labels, and these take a way higher value than in the classes with a small number of labels.\n\nI thought to normalize per class by setting a \"class importance\": if we name $C\\_{i}$ the class, we define `W_i` its weight such as `sum W_i = |C|` and `sum P_{i, j} = W_i` with  `P_{i, j}` the positive weight of the label `j` of the class `i`, and then use a scheduler to increase the weight of the hard-to-predict classes over time.\n\nTo that, I would add a capping parameter `c_i` which caps the `P_{i, j}` value to avoid too high positive weight values which would make the frequent labels insignificant. So our scheduler is a hook defined by the epoch when it takes effect and the list of tuples `(W_i, c_i).`\n\nDo you think it would work ? What would be your solution ?",
    "created_utc": "2024-09-12T02:03:16",
    "num_comments": 3,
    "comments": [
        "Collect more data so things aren't unbalanced. The usual answer... architecture changes won't solve the underlying problem in a robust way.",
        "Balance classes in loss function, but you will still need to have big enough batches and somewhat balanced classes so most classes are represented in each batch. This may not work for you",
        "regularization is only going to get you so far.  my experience is balance is more important by a lot than volume.  so you are better off throwing away a lot of data than providing a lot of data thats all the same on key dimensions.  so yes, you can collect more data or you can force balance what you have and maybe you already have enough data"
    ]
},
{
    "submission_id": "1few0l8",
    "title": "Help me !! ",
    "selftext": "I'm getting 502 bad gateway error in kaggle.Com , I tried out clearing cache in the browser and tried in incognito mode too, still it didn't work, is there any way to clear this error?? ",
    "created_utc": "2024-09-11T23:31:25",
    "num_comments": 3,
    "comments": [
        "This sub needs to be moderated. How is this relevant to CV??",
        "have you tried repenting for your sins",
        "A 502 Bad Gateway error typically indicates a server-side issue, meaning the server received an invalid response from another server it was accessing. Since you've already tried clearing the cache and using incognito mode, which rules out local browser issues, here are a few steps you can try:\n\n1. Check Kaggle Status: Visit Kaggle’s status page or their Twitter account to see if they are experiencing any outages or server issues. If they are, you might need to wait until they resolve it.\n\n\n2. Try a Different Network: Sometimes network-specific issues cause 502 errors. Try accessing Kaggle from a different network (e.g., switch from Wi-Fi to mobile data).\n\n\n3. Restart Your Router: If you're using a home network, restarting your router can sometimes clear up network-related issues.\n\n\n4. Disable Browser Extensions: Some browser extensions can interfere with website connections. Try disabling your extensions or using a different browser.\n\n\n5. Contact Kaggle Support: If none of the above works and the issue persists, consider contacting Kaggle support through their Help Center for more specific guidance.\n\n\n\nIf the problem is on Kaggle’s end, there might not be much you can do except wait for them to fix it."
    ]
},
{
    "submission_id": "1fevqsx",
    "title": "Diameter of a filament",
    "selftext": "Can I use YoloV8 to measure the diameter of a Filament of a 3D Printer? Note that the filament is transparent cause we are using PET Filament",
    "created_utc": "2024-09-11T23:12:07",
    "num_comments": 6,
    "comments": [
        "Could you post pictures of what you're trying to accomplish? Computer Vision has the word Vision in it, it's not called Computer Describe this in Words.\n\nYolo in general is great for identifying and locating stuff in a 2D image, not intended for measuring stuff.",
        "Generally, if you can see something in an image so can CV/AI. If it's so transparent that you struggle to see it clearly then so will a model.",
        "while mostly true, I use a detection model to measure the amount of a substance in a clear silo. Simply the model is trained on the substance and the height of bounding box simply maps to % full.",
        "We’re also going to do a comparative analysis on YoloV8, Mask R-CNN, and Douglas-Peucker Algorithm on what MLA-CV can measure the diameter more accurate",
        "Still helps if we could see your problem. Some sort of semantic segmentation like with R-CNNs would make more sense, but then if you're trying to just count the pixels between two parallel lines, you could come up with something simpler. If it's for a commercial product, you probably are much better off speed, accuracy, and cost using a sensor approach instead of camera-based. Think \"2-Axis Laser Diameter Measuring\" that Prusa brags about with their filaments.",
        "Hi! I DMed you. Thanks!"
    ]
},
{
    "submission_id": "1fer6hz",
    "title": "Image size for training Yolov8",
    "selftext": "As I understand it, the default image size for YoloV8 is 640x640.  \nBut the images I have to put in for training are usually more wide than high (license plates, to be precise).   \nThey would tend to be of width around 200 and height 50, for example.  \n  \nWith that in mind, should I still train the model with 640x640 (automatically resizing and padding the images), or should I select an input size closer to the original size of the images? What would tend to deliver the best performances?",
    "created_utc": "2024-09-11T18:46:59",
    "num_comments": 5,
    "comments": [
        "How does your dataset look like? is it just license plates or vehicles with license plates.\n\n- With padding you will retain aspect ratio of the actual object within image, This can be better approach but this will be waste some compute as in your case close to 50% will be padding.\n\n- Resizing your original image to 640x640 will distort object in the images further.\n\n- Another approach is you can train yolo on a  resolution closer to your requirements like 512/256 x 64 here you need to train a model from scratch.",
        "640x640. try sahi with a tile overlap that mitigates split characters https://github.com/obss/sahi",
        "> How does your dataset look like?\n\nImages of license plates, already cropped tightly around the plate. The model I train is to detect and classify the text characters inside the plate, not the plate itself. So the labels are the bounding boxes of the characters and their corresponding class.\n\n> Another approach is you can train yolo on a resolution closer to your requirements like 512/256 x 64 here you need to train a model from scratch.\n\nSo you are saying that in that case, we can't just fine-tune the existing model, we have to train it all over again, right?  \nSeems better for me to keep the default ratio (640x640) with the padding in that case.   \nI was just thinking of a scenario where the training mostly works the same way except for just changing the input size hyperparam, and I was wondering if the default size was better than the 256x64 ratio or whatever. \n\n  \nI guess it's not so viable to change the ratio then. Thanks for the info!",
        "Have you considered using any OCR model for this?",
        "I'm dealing with plates of non-US country. The plates have non-latin characters. Also, the plates come in different formats (sometimes all the characters are on one line, sometimes they take two lines, sometimes a word in written vertically instead of horizontally while the rest of the plate is written horizontally, etc).\n\nFor those reasons, I need to have the precise location of the bounding boxes so I can put the characters back in order. I don't think an OCR could work for all those edge cases (but I'm not familiar with OCR, so feel free to tell if if you think I'm wrong)."
    ]
},
{
    "submission_id": "1fengay",
    "title": "Understanding need of orientation in ML based inertial odometry",
    "selftext": "I was going through following research papers that apply different machine learing approaches to IMU data to predict pose of the target device (mobile or micro aerial vehicle):\n\n[\\[1\\] GPS-Denied Navigation Using Low-Cost Inertial Sensors and Recurrent Neural Networks](https://arxiv.org/pdf/2109.04861)  \n[\\[2\\] RIOT: Recursive Inertial Odometry Transformer for Localisation from Low-Cost IMU Measurements](https://arxiv.org/abs/2303.01641)  \n[\\[3\\] End-to-End Learning Framework for IMU-Based 6-DOF Odometry](https://www.mdpi.com/1424-8220/19/17/3777)  \n[\\[4\\] AbolDeepIO: A Novel Deep Inertial Odometry Network for Autonomous Vehicles](https://ieeexplore.ieee.org/document/8693766)  \n[\\[5\\] IONet: Learning to Cure the Curse of Drift in Inertial Odometry](https://arxiv.org/abs/1802.02209)  \n[\\[6\\] RoNIN: Robust Neural Inertial Navigation in the Wild: Benchmark, Evaluations, and New Methods](https://arxiv.org/abs/1905.12853)\n\nI am confused about need for using heading information (from magnetometer) as an input to the model during training.\n\nMy understanding is that we need heading information in addition to accelerometer and gyroscope readings as a model input during training for it to learn predicting delta position (displacement) in world frame. For example consider model M takes only accel and gyro reading as input and learns to predict delta position or displacement as (Δn,Δe,Δd) (NED being world frame):\n\nhttps://preview.redd.it/xu9bjjqhd9od1.png?width=1805&format=png&auto=webp&s=f728c6c45573b4fbbb0096c7fd02b25d93a43fc6\n\nIn this case, the model will predict same (Δn,Δe,Δd), whenever it encounters same accel and gyro input even when we have IMU at same position P0, but with different orientation h0’. However, in reality, same accel and gyro input might take it to different position P1’ due to different heading h0’:\n\nhttps://preview.redd.it/8o4mc1hid9od1.png?width=1628&format=png&auto=webp&s=b746f994fa99ef09195c06295da965e7498c55c4\n\nThis makes me it difficult to understand how some of above papers use of only accel and gyro input for model training.\n\nFirst two papers use magnetometer as input to the model for training. Second paper explicitly says:\n\n>By incorporating magnetometer measurements we enable the system to determine its initial attitude.\n\nRest of the papers does not use as heading information in any form as an input for model during training.\n\nThird paper says:\n\n>The proposed solution for the 6-DOF odometry with an IMU takes a sequence of gyroscope and accelerometer readings as input, and outputs a relative pose between two sequential moments. By successively performing this operation over time, a 3D trajectory can be estimated. Given an initial position and orientation, the computed pose changes are incrementally composed to finally obtain the pose in the reference coordinate system.\n\nFourth paper says:\n\n>Modeling learning procedure in this manner is because IMU measurements are relative to the base node and not the world coordinate, and so it is not possible to design the network to output and learn the exact x, y and z measurements directly.\n\nHowever both paper uses [EuRoC MAV dataset](https://projects.asl.ethz.ch/datasets/doku.php?id=kmavvisualinertialdatasets) which specifies pose captured by Vicon motion capture system. That means for learning to estimate delta position in the frame of motion capture system, we must need heading information along with accel and gyro data as model input, correct?\n\nFifth paper derives equation to help get rid of need of orientation (heading) information. It says:\n\n>We can compute the absolute change in distance over a window as the L-2 norm i.e. Δ*l* = ||ΔL||₂, effectively decoupling the distance traveled from the orientation (e.g. heading angle) traveled, leading to \n\nhttps://preview.redd.it/s04azedjd9od1.png?width=363&format=png&auto=webp&s=11212cf6ec6073d8d68e50d79d5f825a533a1874\n\nhttps://preview.redd.it/k7yj5iqkd9od1.png?width=406&format=png&auto=webp&s=a1f1d3bee25c8caa22d48d4ab788ca448a9ba380\n\nEven though I can somewhat get the above equation, I am not fully able to build real world / logical intuition about how this gets rid of the requirement of heading information in the model input during training.\n\nLast paper does not seem to discuss the model architecture and input in clear words, but [from their source code](https://github.com/Sachini/ronin/blob/805b7f0f28bb164ce89ada9ac05a9470dbe3d715/source/ronin_lstm_tcn.py#L31), it seem that they indeed use only gyro and accelerometer data. The paper however say following (section 4.1):\n\n>RoNIN uses a heading-agnostic coordinate frame (HACF), that is, any coordinate frame whose Z axis is aligned with gravity. In other words, we can pick any such coordinate frame as long as we keep it consistent through out the sequence. The coordinate transformation into HACF does not suffer from singularities or discontinuities with proper rotation representation, e.g. with quaternion.  \nDuring training, we use a random HACF at each step, which is defined by randomly rotating ground-truth trajectories on the horizontal plane. IMU data is transformed into the same HACF by the device orientation and the same horizontal rotation. The use of device orientations effectively incorporates sensor fusion into our data-driven system. At test time, we use the coordinate frame defined by system device orientations from Android or iOS, whose Z axis is aligned with gravity.\n\nIn section 5.2, it says:\n\n>For RoNIN, we use the estimated device orientations if the end-sequence alignment error is below 20 degree, otherwise choose the ground-truth to minimize noise during training.\n\nMy guess is that the authors of papers 3 and 4 are mistaken to not consider magnetometer input for model training. Third paper does not give any visualization of ground truth and predicted trajectories. Fourth paper indeed give trajectories with some very poor predictions, which I believe are due to change in the IMU orientation as discussed in the diagram. Second paper gives more consistent trajectories given have used magnetometer input. Fifth paper gives okayish trajectories\n\nI will love to know following:\n\n1. Is magnetometer information / heading information essential for training model to predict delta position?\n2. If answer to above is NO, then how a model can learn to predict delta position merely from gyro and accel data?\n3. What is the intuition behind fifth paper equations for getting rid of heading / orientation information requirement for model training?",
    "created_utc": "2024-09-11T15:46:24",
    "num_comments": 3,
    "comments": [
        "Your lucky! I did a thesis like 2 months ago about learned inerial odometry, so it's fresh in my memory.\n\n1. It is essential for the system, but there are trick you can use so you don't need them as input of NN model (but you will need it in inference). \nYou are correct in thay you need an absolute orientation wrt to world at all timestamps to convert from acc measuremenrs to displacement measurements in world, if not it happens as you say, you got the magnitude but no way to orient it. \nThen, gyroscope, while it gives us angular velocity, doesn't give us the initial orientation. You can estimate the gravity from your acceleracion measurements (in the end one should be close to 9.8) but that leaves us with 1 DoF unkown (yaw). So just with gyro and acc it is imposible to do a 6DoF estimator. \nBut these papers assume you got an initial orientation from somewhere else, or you just set you initial position to yaw=0\n\nFor training a NN that predict some type of displacement you got 2 options:\n- Provide of initial orientation to the model somehow, then the model could learn how to convert that initial orientation + imu measurments into absolute displacement (in RNN you can for example make sure that all batches of data are aligned wrt to initial yaw)\n- Or, assume NN is going to give displacement in a relative frame of reference. Ouput of NN needs to be aligned with yaw at timestamp of prediction before being used as a displacement in world (so GT will be changed to have initial yaw of 0 when used in training) \n\nFirst 2 papers contain magnetometer, no issue there directly provide orientation. 3rd and next ones I think do second approach:\n- Input of NN is only acc, gyro, no orientation. Gt of that output is displacement but in a yaw substracted frame of reference (assume yaw=0 always at the beggining of IMU data). So our model learns to predict a 0 yaw displacement. Then in inference we can take the output and convert to displacement in world by knowing the estimated yaw\n\nSo in training:\nLoss is L = MSE(R_yaw^T * Δd_gt, Δd_pred)\nIn inference:\nΔd_inWorld = R_yaw * Δd_pred \n\n3. Fifth paper just explains again (in my opinion not super clearly) that from a window of imu data (assuming no bias/noise) you should get a displacement relative to the orientation of the initial of the IMU data. If you look at their update version, they are updating the x and y as\nxn = x0 + Δl cos (phi + Δphi)\nWhich is equivalent to doing in 3D:\nt_inWorld = t_0_inWorld + R_yaw * Δd_pred\n\nOther notes:\nAFAIK 3rd paper also mentiond they are updating with relative orientation (which you can get from gyroscope called delta phi), not only with displacement. Same with 5th, not sure about the rest\n\n4th paper is the only one I never read, but I assume does the same as 3rd or it is incorrect\n\nThere are more recent papers that use EKF systems that use a ML model only to update a delta position that is yaw substracted, gravity aligned with the world of reference. See TLIO from meta (MIT licence) which compares against RoNIN, or others lile AirIMU (pose graph optimization + NN). These improve upon each other, so they are nice read.\n\nTLIO for example nails down much better how you need to be super careful by defining what a NN predicts from acceleration and gyroscope as a displacement in a yaw substracted frame of reference",
        "No relevant code picked up just yet for \"GPS-Denied Navigation Using Low-Cost Inertial Sensors and Recurrent Neural Networks\".\n\n[Request code](https://www.catalyzex.com/paper/arxiv:2109.04861?requestCode=true) from the authors or [ask a question](https://www.catalyzex.com/paper/arxiv:2109.04861?autofocus=question).\n\nIf you have code to share with the community, please add it [here](https://www.catalyzex.com/add_code?paper_url=https://arxiv.org/abs/2109.04861&title=GPS-Denied+Navigation+Using+Low-Cost+Inertial+Sensors+and+Recurrent+Neural+Networks) 😊🙏\n\nCreate an alert for new code releases here [here](https://www.catalyzex.com/alerts/code/create?paper_url=https://arxiv.org/abs/2109.04861&paper_title=GPS-Denied Navigation Using Low-Cost Inertial Sensors and Recurrent Neural Networks&paper_arxiv_id=2109.04861)\n\n --\n\nNo relevant code picked up just yet for \"RIOT: Recursive Inertial Odometry Transformer for Localisation from Low-Cost IMU Measurements\".\n\n[Request code](https://www.catalyzex.com/paper/arxiv:2303.01641?requestCode=true) from the authors or [ask a question](https://www.catalyzex.com/paper/arxiv:2303.01641?autofocus=question).\n\nIf you have code to share with the community, please add it [here](https://www.catalyzex.com/add_code?paper_url=https://arxiv.org/abs/2303.01641&title=RIOT%3A+Recursive+Inertial+Odometry+Transformer+for+Localisation+from+Low-Cost+IMU+Measurements) 😊🙏\n\nCreate an alert for new code releases here [here](https://www.catalyzex.com/alerts/code/create?paper_url=https://arxiv.org/abs/2303.01641&paper_title=RIOT: Recursive Inertial Odometry Transformer for Localisation from Low-Cost IMU Measurements&paper_arxiv_id=2303.01641)\n\n --\n\nNo relevant code picked up just yet for \"IONet: Learning to Cure the Curse of Drift in Inertial Odometry\".\n\n[Request code](https://www.catalyzex.com/paper/arxiv:1802.02209?requestCode=true) from the authors or [ask a question](https://www.catalyzex.com/paper/arxiv:1802.02209?autofocus=question).\n\nIf you have code to share with the community, please add it [here](https://www.catalyzex.com/add_code?paper_url=https://arxiv.org/abs/1802.02209&title=IONet%3A+Learning+to+Cure+the+Curse+of+Drift+in+Inertial+Odometry) 😊🙏\n\nCreate an alert for new code releases here [here](https://www.catalyzex.com/alerts/code/create?paper_url=https://arxiv.org/abs/1802.02209&paper_title=IONet: Learning to Cure the Curse of Drift in Inertial Odometry&paper_arxiv_id=1802.02209)\n\n --\n\nFound [2 relevant code implementations](https://www.catalyzex.com/paper/arxiv:1905.12853/code) for \"RoNIN: Robust Neural Inertial Navigation in the Wild: Benchmark, Evaluations, and New Methods\".\n\n[Ask the author(s) a question](https://www.catalyzex.com/paper/arxiv:1905.12853?autofocus=question) about the paper or code.\n\nIf you have code to share with the community, please add it [here](https://www.catalyzex.com/add_code?paper_url=https://arxiv.org/abs/1905.12853&title=RoNIN%3A+Robust+Neural+Inertial+Navigation+in+the+Wild%3A+Benchmark%2C+Evaluations%2C+and+New+Methods) 😊🙏\n\nCreate an alert for new code releases here [here](https://www.catalyzex.com/alerts/code/create?paper_url=https://arxiv.org/abs/1905.12853&paper_title=RoNIN: Robust Neural Inertial Navigation in the Wild: Benchmark, Evaluations, and New Methods&paper_arxiv_id=1905.12853)\n\n--\n\nTo opt out from receiving code links, DM me.",
        "Also, feel free to DM me if you need more specific help with some equation/piece of code related to this\nAnd great questions! It's one of the most exhaustive ones I've seen in this subreddit and you gave me a lot to read through to understand where your comming from! Nice!"
    ]
},
{
    "submission_id": "1fehaa7",
    "title": "Problem in text extraction in paddleOCR ",
    "selftext": "Hi I need some help in paddleOCR if somebody worked in it please assist me 🙏",
    "created_utc": "2024-09-11T11:24:54",
    "num_comments": 6,
    "comments": [
        "What is your question?",
        "I have a image of product and I want to extract expiry date , manufacturing date ,EAN and brand name from it but the code I wrote is not giving me desired output.",
        "Can you put the code and image somewhere so we can see it.    Maybe add to github.",
        "Comfortable with dm?",
        "Shared everything on dm",
        "hey, did you get any solution. similar problem I am also facing"
    ]
},
{
    "submission_id": "1fee1d8",
    "title": "Why HOG doesn't work in my code?",
    "selftext": "I want to detect and track people on video, but HOG doesn't work, neither with nor without my pre-processing (background subtraction and noise filtering). What's wrong?\n\n    #include <opencv2/opencv.hpp>\n    #include <iostream>\n    const std::string videoPath = \"/home/kuver/Documents/CPP/test.mp4\";\n    \n    void preprocessFrame(cv::Mat& frame) {\n        cv::resize(frame, frame, cv::Size(128, 64));\n    }\n    \n    void applyBackgroundSubtraction(cv::Mat& frame, cv::Ptr<cv::BackgroundSubtractor>& bgSubtractor, cv::Mat& fgMask) {\n        bgSubtractor->apply(frame, fgMask);\n    }\n    \n    void applyNoiseReduction(cv::Mat& fgMask) {\n        cv::GaussianBlur(fgMask, fgMask, cv::Size(15, 15), 0);\n    \n        // Apply morphological operations to remove noise and refine the mask\n        cv::erode(fgMask, fgMask, cv::Mat(), cv::Point(-1, -1), 2);   // Remove small noise (erode)\n        cv::dilate(fgMask, fgMask, cv::Mat(), cv::Point(-1, -1), 2);  // Fill gaps (dilate)\n    }\n    \n    void applyHOGAndDraw(cv::Mat& frame) {\n        cv::HOGDescriptor hog;\n        hog.setSVMDetector(cv::HOGDescriptor::\n    getDefaultPeopleDetector\n    ());\n    \n        std::vector<cv::Rect> detections;\n        std::vector<double> weights;\n    \n        // Perform HOG detection\n        hog.detectMultiScale(frame, detections, weights, 0, cv::Size(8, 8), cv::Size(32, 32), 1.05, 2);\n    \n        std::cout << detections.size() << std::endl;\n    \n        // Draw rectangles around detected humans\n        for (size_t i = 0; i < detections.size(); i++) {\n            cv::rectangle(frame, detections[i], cv::Scalar(0, 255, 0), 2); // Draw green rectangle\n        }\n    \n        // Display the frame with HOG-detected humans\n        cv::imshow(\"Processed Frame with HOG\", frame);\n    }\n    \n    int main() {\n        cv::VideoCapture cap(videoPath);\n        if (!cap.isOpened()) {\n            std::cerr << \"Error: Could not open the video file!\" << std::endl;\n            return -1;\n        }\n    \n        // Create background subtractor\n        cv::Ptr<cv::BackgroundSubtractor> bgSubtractor = cv::createBackgroundSubtractorKNN(100, 2000, false);\n    \n        cv::Mat frame, fgMask;\n    \n        while (cap.read(frame)) {\n            if (frame.empty()) break;\n    \n            preprocessFrame(frame);\n    \n            applyBackgroundSubtraction(frame, bgSubtractor, fgMask);\n    \n            applyNoiseReduction(fgMask);\n    \n    \n            //cv::imshow(\"Foreground Mask (Noise Reduced)\", fgMask);\n            applyHOGAndDraw(fgMask);\n    \n            if (cv::waitKey(30) == 27) break;\n        }\n    \n        cap.release();\n        cv::destroyAllWindows();\n    \n        return 0;\n    }\n    #include <opencv2/opencv.hpp>\n    #include <iostream>\n    \n    const std::string videoPath = \"/home/kuver/Documents/CPP/test.mp4\";\n    \n    void preprocessFrame(cv::Mat& frame) {\n        cv::resize(frame, frame, cv::Size(128, 64));\n    }\n    \n    void applyBackgroundSubtraction(cv::Mat& frame, cv::Ptr<cv::BackgroundSubtractor>& bgSubtractor, cv::Mat& fgMask) {\n        bgSubtractor->apply(frame, fgMask);\n    }\n    \n    void applyNoiseReduction(cv::Mat& fgMask) {\n        cv::GaussianBlur(fgMask, fgMask, cv::Size(15, 15), 0);\n    \n        // Apply morphological operations to remove noise and refine the mask\n        cv::erode(fgMask, fgMask, cv::Mat(), cv::Point(-1, -1), 2);   // Remove small noise (erode)\n        cv::dilate(fgMask, fgMask, cv::Mat(), cv::Point(-1, -1), 2);  // Fill gaps (dilate)\n    }\n    \n    void applyHOGAndDraw(cv::Mat& frame) {\n        cv::HOGDescriptor hog;\n        hog.setSVMDetector(cv::HOGDescriptor::getDefaultPeopleDetector());\n    \n        std::vector<cv::Rect> detections;\n        std::vector<double> weights;\n    \n        // Perform HOG detection\n        hog.detectMultiScale(frame, detections, weights, 0, cv::Size(8, 8), cv::Size(32, 32), 1.05, 2);\n    \n        std::cout << detections.size() << std::endl;\n    \n        // Draw rectangles around detected humans\n        for (size_t i = 0; i < detections.size(); i++) {\n            cv::rectangle(frame, detections[i], cv::Scalar(0, 255, 0), 2); // Draw green rectangle\n        }\n    \n        // Display the frame with HOG-detected humans\n        cv::imshow(\"Processed Frame with HOG\", frame);\n    }\n    \n    int main() {\n        cv::VideoCapture cap(videoPath);\n        if (!cap.isOpened()) {\n            std::cerr << \"Error: Could not open the video file!\" << std::endl;\n            return -1;\n        }\n    \n        // Create background subtractor\n        cv::Ptr<cv::BackgroundSubtractor> bgSubtractor = cv::createBackgroundSubtractorKNN(100, 2000, false);\n    \n        cv::Mat frame, fgMask;\n    \n        while (cap.read(frame)) {\n            if (frame.empty()) break;\n    \n            preprocessFrame(frame);\n    \n            applyBackgroundSubtraction(frame, bgSubtractor, fgMask);\n    \n            applyNoiseReduction(fgMask);\n    \n    \n            //cv::imshow(\"Foreground Mask (Noise Reduced)\", fgMask);\n    \n            applyHOGAndDraw(fgMask);\n    \n            if (cv::waitKey(30) == 27) break;\n        }\n    \n        cap.release();\n        cv::destroyAllWindows();\n    \n        return 0;\n    }",
    "created_utc": "2024-09-11T09:12:29",
    "num_comments": 13,
    "comments": [
        "Looks like you know what you are doing.. Why ask in reddit? Fix it yourself sucka",
        "What's your exact problem? What do you mean it doesn't work? It can be anything. What did you try? What pre/post process? Did you ask gpt? Why the fuck are you rude with people trying to help you? Well at least you put your full code not like many other posts",
        "Codestral Ai - The issue with your code is that you're applying the HOG descriptor on the foreground mask (fgMask) instead of the original frame. The HOG descriptor is designed to work on the original image, not on a binary mask.\n\nHere's the corrected part of your code:\n\n    while (cap.read(frame)) {\n        if (frame.empty()) break;\n    \n        preprocessFrame(frame);\n    \n        applyBackgroundSubtraction(frame, bgSubtractor, fgMask);\n    \n        applyNoiseReduction(fgMask);\n    \n        //cv::imshow(\"Foreground Mask (Noise Reduced)\", fgMask);\n    \n        // Apply HOG on the original frame, not on the foreground mask\n        applyHOGAndDraw(frame);\n    \n        if (cv::waitKey(30) == 27) break;\n    }\n\nBy applying the HOG descriptor on the original frame, you should be able to detect and track people in the video.",
        "Why do you have a 15px blur on a 64 px wide image, have you tried before resizing?",
        "try cranking that HOG",
        ">neither with nor without my pre-processing\n\nI'm not an idiot, I've already tried, applying to original frame doesn't change anything",
        "Here is a screenshots. I'm clearly visible on them: [https://imgur.com/a/IJpX0bt](https://imgur.com/a/IJpX0bt)  \nBut I agree, 15px blur is probably too much.\n\n>have you tried before resizing?\n\nTried what?",
        "If that is your standard response to a stranger who knows nothing about you or your capabilities trying to help you over the internet, I wish you the best of luck.",
        "> Tried what?\n\nBlur before resize, rather than resize then blur is what I think he meant.",
        "If your standard \"help\" is to copypaste prompt from another clone of ChatGPT without reading my post, then I don't need such \"help\" from such strangers.",
        "Haven't tried that, probably should. Good suggestion, but the problem isn't with blur :D",
        "Okay, good luck with your project and have a good day.",
        "The problem is that the detect person algorithm in opencv is built for full color images"
    ]
},
{
    "submission_id": "1fe6cf3",
    "title": "Selecting a Camera Sensor for Wide-baseline Stereo Vision Setup",
    "selftext": "Selecting a Camera for Automotive Stereo Camera\n\nI have a question regarding selecting a camera model for building a stereo setup for automotive application. \n\nI noticed that I have one of two options, monochrome with HDR sensor and global shutter, RGB global shutter with 70 db Dynamic range and without LFM.\n\nMy question, which sensor should or recommended to use?",
    "created_utc": "2024-09-11T02:55:47",
    "num_comments": 2,
    "comments": [
        "I'm trying to build wide-baseline stereo setup for estimating long range objects away from a moving car ~ 150m, and the environment is basically outdoor. The question is do I need the HDR (100db) feature and global feature that is exist in monochrome sensor, or use RGB with 70 db dynamic range?",
        "HDR vs Monochrome choice depends on whether colour features are important for your application\n\n  \nCan you post some pics of what you're trying to work with and give some detail on what you're trying to achieve e.g. inspection?"
    ]
},
{
    "submission_id": "1fe6940",
    "title": "Would You Try a PyTorch Version of RTM-Det? Let’s Talk About Performance, Usability, and Real-World Benefits!",
    "selftext": "\n\nI'm considering creating a PyTorch implementation of RTM-Det, incorporating both a detection head and an instance segmentation head.\n\nFor reference:\n\n* **Official code**: [mmdetection RTM-Det](https://github.com/open-mmlab/mmdetection/tree/3.x/configs/rtmdet)\n* **Research paper**: [RTM-Det Paper](https://arxiv.org/pdf/2212.07784)\n\nThe reasoning behind this: While the MMDetection framework is powerful, it can be quite challenging for beginners, and installation issues are common. However, once you get past that, it becomes much easier to work with.\n\nNow, I'd love to hear your thoughts—would you be interested in using a PyTorch implementation of RTM-Det? Or do you feel it's not worth the effort? I’ve seen some discussions suggesting that its latency might not be as low as the paper claims (though I haven’t come across a verified source for this).",
    "created_utc": "2024-09-11T02:49:23",
    "num_comments": 8,
    "comments": [
        "Found [1 relevant code implementation](https://www.catalyzex.com/paper/arxiv:2212.07784/code) for \"RTMDet: An Empirical Study of Designing Real-Time Object Detectors\".\n\n[Ask the author(s) a question](https://www.catalyzex.com/paper/arxiv:2212.07784?autofocus=question) about the paper or code.\n\nIf you have code to share with the community, please add it [here](https://www.catalyzex.com/add_code?paper_url=https://arxiv.org/abs/2212.07784&title=RTMDet%3A+An+Empirical+Study+of+Designing+Real-Time+Object+Detectors) 😊🙏\n\nCreate an alert for new code releases here [here](https://www.catalyzex.com/alerts/code/create?paper_url=https://arxiv.org/abs/2212.07784&paper_title=RTMDet: An Empirical Study of Designing Real-Time Object Detectors&paper_arxiv_id=2212.07784)\n\n--\n\nTo opt out from receiving code links, DM me.",
        "Interested in this implementation",
        "I think the MM ecosystem is also on life support as the team is focusing on LLMs. I would advise against using it",
        "I would play around with it if it existed.",
        "I would definitely use it. MMDetection is a pain in the ass to get up and running. When you want to prototype quickly, PyTorch models offer faster ways to build pipelines.",
        "It's very frustrating to get your MM projects working, it will always give some or other errors related to versions.",
        "I view it as a way of trying lots of different models quickly and then you should try and get out of the ecosystem with your findings",
        "Thats been my issue too, just getting ANY model to actually work without errors has been my goal as a newbie. \n\nThen finding one that's actually open source with a completely open license has been the problem"
    ]
},
{
    "submission_id": "1fe4tu4",
    "title": "YouTube Videos for Research",
    "selftext": "Can I use YouTube videos (streets walk) for my research purpose? For example, if I crop persons from it and train the model to detect person's specific features? Can I make this dataset available strictly for research purpose? Share your thoughts.",
    "created_utc": "2024-09-11T01:01:27",
    "num_comments": 3,
    "comments": [
        "The main issue with that is if people want to replicate your research and some of the videos become unavailable, it makes reproducibility difficult.",
        "yup you can do that, if that data is publicly available, there's no issue.",
        "Videos are available on YouTube, I am planning to download them using Python Script 👍🏻"
    ]
},
{
    "submission_id": "1fe1hja",
    "title": "Scaling Megapixel resolution for miniature model?",
    "selftext": "Hi everyone, I'm trying to make a multi-view object triangulator.\n\nThe real scale is about a soccer field size, and will be using iPhones with 48 MP cameras.\n\nI'm making a 1:272 miniature model, and I'm wondering how should I scale the megapixels down to emulate it using a 48MP in the real world?\n\nDo I use an inverse square law to figure what MP camera I should use?\n\n(The reason why I need to know is because I need a higher FPS, but that would be mean a lower resolution needed).",
    "created_utc": "2024-09-10T21:12:09",
    "num_comments": 4,
    "comments": [
        "You don't scale down the number of pixels. With a small model, you take a picture very close to the model. For a real one, you are way further. The distance to the camera is varying, not the number of Pixels.",
        "I know that.\n\nI’m trying to emulate the amount of information a 48MP camera can capture at maximum 150m at a scaled down version of maximum 56cm with a smaller MP camera",
        "Calculate the instantaneous field of view (ifov) per pixel in the real world scenario and then estimate the number of pixels on target. You can then use the same formula to calculate the right distance to put your model to get the same number of pixels. ",
        "Thanks! I think this is what I need to do here."
    ]
},
{
    "submission_id": "1fdyezp",
    "title": "Fisheye stereo cameras - tracking accuracy? ",
    "selftext": "I'm hoping to do head tracking with a wide FoV setup: ~180degH, ~100degV. Range ~0m to <10m. Indoors (residential). \n\nVertical FoV is not that crucial, but around 100degV is desired. \n\nI'm happy with the accuracy of the Xbox Kinect V2 (there's no noticeable jitter when tracked), so if they achieve that accuracy that's good. \n\nWill 180deg fisheye lenses do stereoscopy well enough for my application?\n\nHas anyone done a wide angle stereo setup, and ran into issues? \n",
    "created_utc": "2024-09-10T18:25:37",
    "num_comments": 9,
    "comments": [
        "It can work, but:\n\n- When triangulating, your effective baseline is reduced as you look to the side. If you have a wide lens, you can look to the side quite FAR, which will reduce your baseline by a LOT. Lower baseline means \"more sensitive to errors\"\n\n- If doing any kind of rectification (like for dense stereo), the usual rectification to a pinhole model will be a problem: a pinhole projection is highly distorted when trying to apply it to wide views. As you approach 180deg FOV, the pinhole distortion approaches infinity. If you need to rectify, non-pinhole rectification will be essential\n\n- As noted above, you'll be more sensitive to calibration errors as you approach the edges. As a direct consequence, the low-fidelity models employed by all the usual tools will break at the edges.\n\nYou want to use the mrcal tools, since they're the only ones that will solve any of these problems. I've tried to do wide-angle, long-range stereo using the tranditional techniques, discovered that they do not work at all, and had to build mrcal to solve all the issues.",
        "* if my cameras are instead mounted vertically, will that address the baseline issue? tracking the viewer horizontally is more important, and the baseline will be >100mm i think (i can space them more if necessary). the viewer should never be directly below the cameras, although they might be completely to the side of them (it will be mounted to a wall)\n* i'll look into mrcal then! \n\nwas your result successful? did the wide-angle long-range stereo work? how accurate was the tracking/depth (i don't know what the correct term would be)?\n\nthanks for your help! wait if you're Dima didn't you create mrcal? i'm just now reading the tour... boy maybe i'm over my head",
        "It works, but the performance depends on the situation. You should try it.",
        "Yeah I'll give it a go \n\n\nWhat sort of resolution do you think is sufficient/I'll need? I see you used a mirrorless \n\n\nIs there a benefit to doing wide angle over using 4 cameras (2 for 90deg left, and 2 for 2deg right. Total 180deg coverage)? Just thought I'd ask",
        "I don't know what you need. Try it (for real, or in a simulation). The mrcal-stereo tool will tell you the error effects in each triangulation, and you can then judge.\n\nUsing multiple narrow cameras in place of one wide camera certainly works. You get a much different aspect ratio in the two cases; I don't know what you need. The narrower lenses will fit simpler models better. That might be good for your case.\n\nUltimately, wide stereo isn't something that anybody does, so you'll have to try stuff to figure it out. Trying the traditional tools would be educational too. Good luck.",
        "What do you think of single shot-based intrinsics approaches?\n\nFor instance: [https://www.youtube.com/watch?v=1Pfr-Sk7EwU](https://www.youtube.com/watch?v=1Pfr-Sk7EwU)\n\n  \n[https://www.youtube.com/watch?v=t0-9BrlyxZg](https://www.youtube.com/watch?v=t0-9BrlyxZg)",
        "If you point me to a paper, I'll look at it. But as usual, it's a question of validation. The mrcal tools give you feedback about your projection uncertainty and various residual diagnostics so that you know how good your intrinsics are. A single image doesn't have enough information to get precise intrinsics. So any technique that aims to do that almost certainly makes heavy assumptions (like assuming the lense follows some low-parameter model) and probably doesn't do any validation at all. Depending on the application, it might be good-enough.",
        "I have not seen any academic literature on the method; here is a technical presentation: [https://www.image-engineering.de/content/products/equipment/measurement\\_devices/geocal/downloads/Geometric\\_calibration\\_technical\\_background.pdf](https://www.image-engineering.de/content/products/equipment/measurement_devices/geocal/downloads/Geometric_calibration_technical_background.pdf)",
        "OK. They have a custom laser projector that nobody else has. They don't say anything about validation. And they're using a very lean model to describe a very wide lens, which works only if you're willing to accept high errors. Meh."
    ]
},
{
    "submission_id": "1fdqyle",
    "title": "Need help in developing a image classification model",
    "selftext": "Hello everyone!\n\nI’m currently working with a dataset of two different bee species, trying to build a machine learning model that can differentiate between them based on their wing venation patterns (landmarks). But I’m facing a challenge: the venation patterns are very similar, and despite properly annotating the images, the model is not able to distinguish between the species. \n\nThe image data is a bit confidential, but I can share it in dm.\n\nI’m new to this image classification domain.\nI would appreciate any suggestions or assistance on how to improve the model's performance. Thank you!",
    "created_utc": "2024-09-10T12:45:07",
    "num_comments": 9,
    "comments": [
        "You can check fine-grained classification problem https://paperswithcode.com/task/fine-grained-image-classification. There are datasets with similar problem: classify birds, dog breeds",
        "How big is the dataset?",
        "Can I ask what model you're using?",
        "Thanks for the reply, the provided URL is not working.",
        "260 images for species A and 272 Images for species B",
        "Hello, \n\nThe model is yolov8.",
        "fixed",
        "Send 'em over via DM I can give them a shot though my pipeline ",
        "Thank you, this looks helpful."
    ]
},
{
    "submission_id": "1fdmlt3",
    "title": "Help with noise while tracking",
    "selftext": "I'm working on tracking people using a webcam and a Raspberry Pi. You can see my current implementation here: [PeopleTracking Project](https://github.com/Lesaje/PeopleTracking/blob/main/main.cpp).\n\nHowever, I'm facing two problems:\n\n1. When wind shakes trees or bushes in the background, it sometimes gets detected as human movement, which is not what I want.\n2. When someone leaves the frame, the camera seems to adjust something (maybe dynamic range or exposure), which creates noise, and this also gets mistakenly tracked as movement.\n\nAny ideas on how to fix these issues?",
    "created_utc": "2024-09-10T09:48:38",
    "num_comments": 4,
    "comments": [
        "For Point 1:\n\nYou are using Background Subtraction Method, \nThis method uses background modeling which identifies or defines stationary objects in a frame.\n\nFor every new frame from camera, algo will compare with background model.\n\nIn your situation background is not static due to shaking bushes and trees.\n\nMay be you can use contour properties to filter object by pixel area. \n\nOpenCV has really good documentation on this.\n\nFor Point 2:\nWhat Camera sensor are you using? Can you disable if autofocus mode is enabled",
        "point 1: would it be good idea to use some neural network (like ssd lite for example) after background subtraction instead of \"dumb\" method of selecting largest contour? Or maybe use something like optical flow instead of background subtraction? \n\npoint2: I will investigate this. Didn't knew it was possible at all!",
        "For Point 1\nSure it will be better idea to use Object Detector along with Object Tracker.\n\nSince you are using RPi, you can try with SSD-MobileNet model for object detection. It will be more robust solution for person detection in real world.\n\nFor object tracking you can consider bytetracker. C++ Implementation is available in official repo, check once.",
        "What if I'll try this: with background subtraction I detect some large moving contour, and then pass part of the image, where this contour is located, to classification network, and determine if this a person or not, and if this is a person, then track this object with some tracker? Would it be better than use object detection + tracking?\n\nWhy would i want it: classification models eating much less resources and have much better accuracy then detection models (https://pytorch.org/vision/main/models.html#object-detection-instance-segmentation-and-person-keypoint-detection)\n\n  \nEDIT: I checked - there is no \"person\" class in ImageNet? So my idea with classification model wouldn't work, since it's trained on ImageNet, right? Isn't there any pre-trained classification models for COCO dataset? I don't like 21.3 Box MAP, it's seems low..."
    ]
},
{
    "submission_id": "1fdlsgv",
    "title": "Built a chess piece detector in order to render overlay with best moves in a VR headset",
    "selftext": "",
    "created_utc": "2024-09-10T09:14:52",
    "num_comments": 58,
    "comments": [
        "Brilliant. However, I now know never to trust a chess opponent wearing a VR headset.",
        "I love how solid this is, really well done. I run the official OpenCV podcast, would you be interested in appearing on the show to talk about yourself a bit and how you made this? Send me a message",
        "This is super cool. Do you have a more technical description on how you accompilsh this?",
        "This is excellent! You'd get a much better UI if you didn't show the piece icons and just showed the move lines (or perhaps only show the icon for any piece you are proposing a move for).",
        "Very nice!\n\nWhat VR device are you using?\n\nIf it is just a head set that has a stream link to your PC, what head set are you using?",
        "Wow thats so cool imagine if your glasses had this feature then you would always see the best move",
        "This is really dope. You should be proud!!\n\nAnywhere I can put it on my quest?",
        "Definitely the most amazing project I've seen! \n\nI'm curious about the AR part you made. Can you explain how you achieved that?",
        "I guess a practical downside of this is the need for the upper camera, but I guess building something that would try to recognize the state just from the vr headset would require quite a bit more training and maybe a more sophisticated heuristic of recognizing the units.",
        "[https://www.linkedin.com/posts/w-michalski\\_chess-ai-machinelearning-ugcPost-7239220337639342080-Clhh?utm\\_source=share&utm\\_medium=member\\_desktop](https://www.linkedin.com/posts/w-michalski_chess-ai-machinelearning-ugcPost-7239220337639342080-Clhh?utm_source=share&utm_medium=member_desktop)\n\nis that you my friend ?",
        "This is siiiiiick!!! Great job 👏👏👏👏👏",
        "This was a thrilling game to watch, I wish you posted the full match",
        "This looks awesome.\n\nBut isn't this Augmented Reality (AR)",
        "Dude that's called cheating",
        "Wait, how is this simulation running a machine vision model on the Quest’s Cameras?",
        "Yes, but how would I fit the entire visor up my butt?",
        "Damm!! This is Soo cool! Is this project open sourced? I'm still learning a lot about AR/VR space and would love to implement something to similar to get my hands dirty",
        "Great idea, but this is already achieved using a butt plug",
        "This is insanely cool, the positional tracking for the HUD is so good(though I know thats in the headset not your program).  Still your implementation is super good.",
        "I love it!!",
        "Holy hell",
        "This is a great project! I think this would really kick off if it was on a phone instead (you could have it overlay the moves via a streaming camera, and have it make sounds based on the quality of the move if the player is not looking at the camera. Also enabling a one-player mode for someone who wants to play chess in a physical board (phone could both have a UI saying which move to make and also say the move)",
        "Amazing work",
        "Nice are you using OpenXR in the PC app for the headset pose? What library are you using to render the icons OpenGL?",
        "So is this using two cameras? An overhead camera +  the headset?\n\nNeeds some yugioh style attack animations on captures",
        "WOW",
        "This is excellent and I’m surprised it didn’t already exist. I’d love to be able to take a screenshot of OTB game and then import it to Lichess or Chess(dot)com. \n\nAnother suggestion I’d have is to have the VR automatically annotate moves. Maybe even have it overlay the PGN to the right hand side.",
        "Nice job dude! I posted on your linkedin post but thought I'd post it here too for redditors.\n\nThis is awesome and funnily enough I have a similar interest! I've had a small hobby of chessboard detection for the past decade through various CV/ML approaches, some questions on your project:\n\n1 - It looks like you have a separate overhead camera from the VR headset tracking the pieces and positions, and that's being piped to the headset that is tracking the board through some other mechanism, is this happening in real-time and to what degree? Very cool combination of tools.\n\n2 - Are the piece types being detected via ML or based off of the initial game position? Are there any priors in this model or system or is it independent per-frame?\n\n3 - Finally, is your dataset specific to that board and piece style (is it also specific to that camera/angle/lighting?), and/or how well may it transfer? I've tried to hand label pieces and found it obnoxious heh, congrats on getting through 150 images.\n\nIn the last month or so I've played around with synthetic datasets for piece detection that you may find relevant and interesting,\n\nYT video series: [https://www.youtube.com/playlist?list=PLvPT8yiPYHne3qB84Vb2MujuIHcCTPSaW](https://www.youtube.com/playlist?list=PLvPT8yiPYHne3qB84Vb2MujuIHcCTPSaW) \n\nGithub: [https://github.com/Elucidation/ChessboardDetect](https://github.com/Elucidation/ChessboardDetect)\n\nI'm happy to discuss/share the synthetic dataset or process to build your own if you want more data for your project.",
        "Wow this is insanely cool. If you ever do a longer write up on how you set it up, I would be very interested.",
        "I wish the quest allowed us access to the raw cameras",
        "Put this in some normal looking glasses and we have our first irl chess hacks",
        "Just to clarify: Can it discern just by looking at the pieces what kind of piece each one is? Or are you keeping track of movements of pieces from a known position instead?",
        "good stuff.\n\n  \nWhere'd you get that board?  I have almost the same one",
        "Only the worst kind of person uses an engine WHILE PLAYING A GAME. This is a terribly silly use case.",
        "Especially attached to beads",
        "Just need a contact hans/lens",
        "Thanks for kind words, I have sent a message!",
        "Damn didnt know there was a podcast",
        "Thanks for kind words! The pipeline works like this:\n\n1. Videostream from an external camera (the one on a tripod) is streamed to my PC\n2. Pieces are being detected using MMDetection library. I trained RTMDet on my custom dataset (\\~350 images)\n3. At the very beginning of the game, detections are used to calibrate the board. For example, when we have 2 white rooks, then we know that A1 square is occupied by the rook which is closer to the white queen, while H1 square is occupied by the rook which is closer to the king. By repeating this process for other pieces (and extrapolating data for squares where there is no chess piece) we can find rough estimation of coordinates of specific squares\n4. These coordinates are used during the game to map pieces to squares (during each frame I use bbox coordinates to assign each piece to the closest square)\n5. However, the process from point 4 may be problematic -- if the chessboard is occluded by a player's hand, then mapping based on bboxes is prone to errors. So in order to prevent them, square-to-piece mappings from each frame are appended to a FIFO buffer of fixed length. If at least 40% of elements in that buffer are the same, then this mapping becomes a new chessboard state\n6. Additionally I proofcheck the new chessboard state to check whether it's reachable within 2 moves from the previous chessboard state (why 2 moves? because if both players made their moves in quick succession, then it could break otherwise)\n7. The chessboard state is wrapped with `python-chess`library (so that it can keep track of whether castling or en passant is allowed) and best moves are calculated using the Stockfish 16 engine.\n8. The overlay is rendered on a PC using OpenCV, and is streamed to the headset using Virtual Desktop app.",
        "I guess it also helps to confirm that they are correctly recognized",
        "Thanks! It's a Meta Quest 3 connected to my PC via Virtual Desktop.",
        "Thanks a lot!     \nI am planning to release the code some (unspecified) time in the future, but it would most likely still require some tinkering before using it (the model used for detecting pieces was trained on my custom dataset, which contained only photos of the chess set I own)",
        "Thanks for kind words! All the processing is done on my PC, including rendering the overlay with OpenCV (which, at that point, is just a flat overlay with black background). Then I use Virtual Desktop app to stream window with that overlay onto my headset (and I enable transparency). That streamed window can be then placed above the IRL chessboard.  \n\nIt does sound a bit hacky, but at least doesn't require aruco markers around the board (and I don't have much experience developing my own VR apps) 😅",
        "Yup!",
        "Object detection is running on a PC, which is connected to an external camera (the one on a tripod). Quest 3 is used just for displaying AR overlay (which is rendered on the PC and streamed into the headset).",
        "I also came here for the board discussion :). Also very similar board,  the letter font is a little different.  Got it with my grandpa (RIP) at a local market in Ukraine, around 2006. Extremely precious (don't let my toddler play with it yet haha), but we probably paid something like $10 at the time.",
        ":(",
        "Yeah we’re over here https://youtube.com/@opencvofficial",
        "Impressive! Thanks for sharing these!\n\n\nWhat would you think could be the next improvements?",
        "Awesome, please write paper, report or make a YouTube video showing these details",
        "Any chance the code has been put up? Really looking forward to playing with this!",
        "What are the PC specs?  The detections and overlay updates look pretty speedy!",
        "I don't think you can access passthrough video from the headset anyways, so your method maybe the only viable method to achieve this overlay.",
        "ahaha thats nice conneting you on linked in <3",
        "I got mine from a little shop in Croatia for like $15.\n\nAbsolutely love it, very precious to me, but sounds like they are mass produced somewhere ",
        "i5-13600K and 4080. It could probably work even a little bit faster if it wasn't for several video encoding/decoding operations in the process (both camera and headset work wirelessly, plus I had issues with accessing camera within WSL so I wrote a script running on windows for sending and receiving videostream to/from WSL via local network) :P"
    ]
},
{
    "submission_id": "1fdkmrf",
    "title": "What approach do you use to track handheld objects?",
    "selftext": "I'm working on a project in which I would like to track handheld object(s). For example, I'm holding a mug or a notebook. What are potential approach to do this? I can think of classic computer visions such as KLT tracker but I don't think that's the only solution. Any idea? Thank you.",
    "created_utc": "2024-09-10T08:27:40",
    "num_comments": 13,
    "comments": [
        "Track by detection is currently popular.  Find all the objects using something like YOLO and then track the detections. The tracker depends on the complexity of the task, number of objects, path crossing etc. One of the easiest starting point is SORT, which might work for you, but there are a lot of others such as bytetrack. Newer trackers are starting to use transformers to predict motion, but they need training.",
        "yolo tracker ? that might help u i think  \nlike Bytrack",
        "YOLO or SSD. You could also explore more advanced methods like deep learning-based trackers, such as those using Siamese networks, which can be good for tracking objects through different frames even if they change a bit.\n\nAnother thing is to use markers or features on the objects themselves. For example, placing a small, unique marker on the mug could make it easier to track consistently.",
        "Object Detector followed by Object Tracker should be helpful here.\n\nHere you detect objects in a given frame and assign tracking IDs using tracker which will be used for tracking once object is lost again object detector should be triggered.\n\nJust make sure that object detector that you are going to use is trained on your target object.\n\nAs mentioned in another comment SORT/ ByteTrack are good to start with.",
        "If you're looking for other visual tracking algorithms, you can find some that work out of the box in the [pytracking](https://github.com/visionml/pytracking) repo",
        "Thank you. I've never heard of SORT before. I've just looked up. Thanks for introducing a new concept to learn. Are any of these can detect hands or if I need to finetune it to detect hands to be able find the object that is being held?",
        "Thank you. I'm not sure if yolo can detect hands or if I need to finetune it. In that case, I should be able to find out the object that is being held. Is my understanding correct?",
        "The latter you suggested sounds like a classic computer vision approach. I don't think I'm allowed to use markers.\n\nI will have a look. Thank you so much.",
        "YOLO tends to be trained on the COCO dataset so it doesn't detect hands, body parts etc, but the whole person. You might be able to find pre-trained weights online to do this.",
        "Yeah just train a custom model based on your requirements\nThe one thing I'll suggest to use a pre trained model and try to track objects which are already in pre trained model to check that whether it fulfills your requirements or not",
        "Thanks for your prompt reply! Appreciate it. I'll have a look.",
        "Thanks for your quick response. I don't think I know in advance which object is being held. \n\nI'll check the pre-trained model. Thanks.",
        "Ok you can get hand detection models from roboflow as well that would help you"
    ]
},
{
    "submission_id": "1fdfmhu",
    "title": "What Topics Would You Want to Read About?",
    "selftext": "I’m really into computer vision and love learning and talking about it. I’m thinking of starting a series of articles on Medium to dive deeper into different topics. Any ideas on what I should start with? Are there any topics in computer vision that aren’t talked about much but would be interesting to read about?? Also, if anyone’s got tips on how to make technical articles, I’d really appreciate it!\n\nThanks in advance for any recommendations!",
    "created_utc": "2024-09-10T04:33:56",
    "num_comments": 16,
    "comments": [
        "Transformer based vision models: Generally everbody talks about cnn based model as computer vison model.",
        "Person Reidentification in video streams is still a huge problem, why don't you make content on that also",
        "I would love to see some articles that explain multiview geometry without immediately going deep into the math. ",
        "modern (last 10 years) computational photography",
        "optimization for things that need to run real-time and on desktop. Most things you find online assume you either can run asynchronous on a server and deliver the results, or run on a dedicated piece of hardware. I always find it hard to find info when I want to run something on a user's laptop and need to really optimize inference time.",
        "If you're looking for unique topics, you might want to consider how computer vision is being used in less mainstream applications, like environmental monitoring or mental health. \n\nAnd try to include real-life examples or case studies to make technical articles more engaging. Breaking down complex concepts with simple analogies can help a lot too. And don’t forget to include some visuals or diagrams—those can make a big difference in understanding.",
        "You are asking about topics, which you will write and we have to read them, behind a paywall.... Post it in \"medium\" and ask for their wishlist....",
        "Real, i will write about it   \nThank u",
        "Noted, I will talk about it.  \nThanks",
        "Thanks for the response!   \nI did my master's thesis on 3D reconstruction of bridges using drone images. I talked about this concepts, but didn’t go super deep. It’s a good idea to make these concepts easier to understand.   \ntbh, perso I’ve forgotten all the math formulas, I only remember the coding part now.  \ni should rewatch my sensei's videos : [https://www.youtube.com/@firstprinciplesofcomputerv3258/playlists](https://www.youtube.com/@firstprinciplesofcomputerv3258/playlists)",
        "Interesting, I didn't think about it. Thank you!",
        "tbh, I’m not that advanced. I struggle with it a lot. I don't think I would be able to write an article about something that I m not good at. But thanks for the suggestion! I should definitely learn it.",
        "Thanks for the suggestions!",
        "Why do you post AI comments in this sub?",
        "I’m planning to keep my articles free because I genuinely enjoy writing and learning through the process. I’ve got a few topics in mind, but I want to make sure they’re useful and interesting for people in the field.\n\nI’ve seen a ton of questions here about a lot of subjects, so I thought about also writing some friendly articles that break down complex topics for newbies.  \n  \n+ it’s a great way for me to push my own knowledge boundaries.\n\nI didn’t ask on Medium because I wouldn’t get any responses."
    ]
},
{
    "submission_id": "1fdesot",
    "title": "Vertebra lesion classification",
    "selftext": "Hi everyone!\n\nI'm working on a binary classification task to determine whether there is a lesion in the spinal vertebrae. The data I have is quite varied (I would say wrt the quantity of organs that I’m able to view): some slices zoom in on the vertebra, while others include the entire body section, including lungs. HU transformation was applied (clipping 400-1800)\n\nAt the moment, I'm trying pre-trained backbones on ImageNet with all weights frozen (they require [0-255] range) adding only a dense layer at the end (with a sigmoid activation). Despite having very few parameters to train, the performance is not good: I have a somewhat imbalanced dataset (6k healthy and 1k sick), and I'm using class weights: however I did trials also with balanced training set but more or less I got the same performances. The train and validation losses decrease together, but the train F1 score is much higher than the validation F1 score (which suggests overfitting). Could someone help me out? Thanks a lot!",
    "created_utc": "2024-09-10T03:42:25",
    "num_comments": 9,
    "comments": [
        "Have you tried cleaning up the days to increase consistency across images? Maybe some gross feature identification followed by a reduced region of interest? \n\nWhen I was in grad school (a long time ago) working on classifying x-ray images, one of the biggest issues I had in generating good data sets was that many radiologists drew or wrote on the radiographs. So, the first step I took was to discard the heavily annotated images. Then I cropped the oversized fields of view. After that, my data sets were more consistent, and I was able to make progress.",
        "Assuming you mention HU and slices, you are working on CT images of spine. How does the input looks like? How many cases do you have of vertebra with lesions and those without lesions? Is there a lot of variability between different scans wrt scan quality? Did you consider clipping the HU between -100 to 1000 to acccount for lesions and bones and then normalize it to 0-1 before feeding it to the model?\n\nPlease edit your question and add these details so that people can have understanding of the full task and provide useful suggestions.",
        "You didn't mention which models you tried out (only that you use pretrained Imagenet backbones). Given the small dataset size, I would start with small networks like fNet or some variant of efficient net. Here is a link to similar paper which compares different architecture on imbalanced vertebral dataset for prognosis task.\n\n\nhttps://dl.acm.org/doi/10.1007/978-3-031-66958-3_4\n\n\nfull disclosure: I am an author of this paper. If you cannot access it, let me know. I will send you the paper via dm.",
        "You mean crop only the vertebra from all slices?",
        "Ok sorry, I have updated the question",
        "Right now I'm testing ResnNet50. Thanks for the paper, maybe I need to use a localizer of the vertebra in order to give it to the CNN",
        "I mean have you tried running some preprocessing to clean up your dataset? \n\nIt sounds like the images you're working with are highly variable, which is what I would expect from radiography. While we can train networks to address higher degrees of variation in our data, more consistent inputs tend to produce more repeatable and reliable outputs. \n\nI could also see a possible benefit from first separating the images into multiple groups based on general presentation, then running each group through a separate model. That way, we'd reduce the problem domain.",
        "Is your dataset publicly available? We have an inhouse localizer so I can help you out with that part",
        "Yes I converted all the slices in HU.\nBtw maybe you’re right. I have some slices that represents only the vertebra, other that represents a lot of other organs like lungs, liver and so on (i.e. less zoom). In my opinion it could have been a way to allow the model to generalize but maybe at this point, it’s not the right consideration"
    ]
},
{
    "submission_id": "1fdeki0",
    "title": "Detecting blur and sudden glare in an image",
    "selftext": "Hello. I am reading a video frame by frame, and I need to detect if the image is blurry or contains high brightness. I will try the algorithms (FFT, HWT, LAP, TEN) that were previously shared for blur detection only for blur detection part. Later, I will also detect brightness using an additional algorithm. However, I don't know if there is an algorithm that can detect both at the same time. It is very important for me that the algorithm works quickly, so I need to use traditional algorithms.   \nCan you give me any advice? It could be for detecting brightness, blur, or both. Thank you.",
    "created_utc": "2024-09-10T03:26:50",
    "num_comments": 5,
    "comments": [
        "Also a ML model can easily work fast enough. The challenge is getting good training data though. \n\nHundreds of frames per second is doable even without a GPU. ",
        "For blur detection, I've often used one of the sum of laplacian algorithms. It's pretty fast, and it's a standard approach to determine if an object is in focus and to drive autofocus calculations, assuming you have at least a few lines within your field of view. It won't work for images that only have smooth edge transitions.",
        "Online or offline?\n\nIf online, how much lag is tolerable? ",
        "thanks",
        "offline"
    ]
},
{
    "submission_id": "1fddnmq",
    "title": "Speed Camera Detection with Computer Vision",
    "selftext": "Hi everyone,  \nI want to detect speed cameras in highways with computer vision. I don't want to detect cars or their speeds, to be clear here. Is there any dataset that you have worked with for these? What could be the best long distance camera that could be integrated with embedded system like jetson nano or rpi ?\n\nThank you..",
    "created_utc": "2024-09-10T02:22:34",
    "num_comments": 5,
    "comments": [
        "For personal or commercial use? - you could maybe use street view for a personal project.  \nAt what distance are you trying to detect them?",
        "You want to detect things like these? https://kustomsignals.com/blog/speed-enforcement-cameras-the-different-types-and-uses-of-cameras-explained",
        "I want to start like project because it is interesting. Distance of 100-150 meters or more would be great.",
        "Yes exactly.",
        "You'll probably need to collect your own dataset then with a narrow fov camera."
    ]
},
{
    "submission_id": "1fdcj7n",
    "title": "Help with setting up yolo and deep reid",
    "selftext": "Hi all, I'm working on a project where I need to do live user tracking using deep reid and yolo.  \nFor this purpose and I need to capture images and do feature matching using deep reid altogether and for this I need to set up their github repos, now I'm confused as of what to do with this. Should I setup the ultralytics repo and inside that setup the deep reid repo or something else",
    "created_utc": "2024-09-10T00:56:53",
    "num_comments": 0,
    "comments": []
},
{
    "submission_id": "1fda9vu",
    "title": "Stop trying to parse your documents and use ColPali (Open Source)",
    "selftext": "",
    "created_utc": "2024-09-09T22:12:08",
    "num_comments": 0,
    "comments": []
},
{
    "submission_id": "1fd73c1",
    "title": "Custom Object Detection Model Training: CIoU Loss Decreasing, but Bounding Box MSE Increasing",
    "selftext": "I am currently working on a computer vision project where I have created a custom object detection model that roughly follows the YoloV8 architecture. I am using CIoU loss for the bounding box and have noticed a strange pattern while training. As the CIoU loss decreases, the MSE of the bounding boxes is increasing. This  pattern has persisted throughout roughly 150 steps (15 epochs with 10 batches), and I was wondering whether this is normal or is a point of concern. In addition to this, is it possible that the MSE will start decreasing after further training?",
    "created_utc": "2024-09-09T19:12:21",
    "num_comments": 3,
    "comments": [
        "Use huber loss",
        "What exactly is the advantage of using Huber?",
        "discard my previous comment sorry"
    ]
},
{
    "submission_id": "1fd50ft",
    "title": "Segmenting tiny objects",
    "selftext": "Is there any algorithm that you guys could suggest to segment tiny objects in an image? I am looking to detect numerous tiny spots on skin using CV. If there is something you guys could suggest me, I'd appreciate that.",
    "created_utc": "2024-09-09T17:28:20",
    "num_comments": 11,
    "comments": [
        "First try would be UNet. I am referring to UNet because it is relatively faster than many other algorithms. To segment tiny objects, there are different loss functions to try as well, like boundary aware loss functions like Hausdorff distance, or pixels based like Focal Loss.",
        "How many pixels are the object and what are the dimensions of the image.",
        "do you have dataset? how many images are in the dataset? are annotations available?",
        "You can try SAM or SAM2(better speed and works on videos)",
        "What are you looking to detect? Pores? Freckles? Moles? My approach would be different for each. \n\nAlso, if you have any control over the imaging hardware, skin looks different under IR, so hyperspectral imaging can add a lot of information to your images. For example, with the addition of IR, melanomas will have spindly extensions from the center of mass that aren't present in other blemishes.",
        "object has about 20-50 px whereas the image is of size of around (4000,4000)",
        "Yes, its available. About 4000 images",
        "It is moles. And I have no control on hardware. Also the images are not a face of single person but of a group so the moles are really small but people do have a lot of moles.",
        "Good to go. there are segmentation methods. I will help you with that. Drop me an email, I will give you the code: My email: [pooya\\_cim@outlook.com](mailto:pooya_cim@outlook.com)",
        "How small? If you don't have enough resolution, it'll be difficult or even impossible to disambiguate the moles from other blemishes and random noise. \n\nAdditionally, in the visible spectrum, this is going to be a more difficult problem to solve for darker skin tones due to decreased contrast. \n\nRegardless, my approach would probably be to reduce my domain to each single face, do some particle analysis to find suspect features to save processing time, then run those features through a classification model which would hopefully handle the natural variation we would expect to see here.",
        "Thanks. That is the approach I am taking as well. By classificaton, you mean pixel-wise classification?"
    ]
},
{
    "submission_id": "1fd24g2",
    "title": "Making a store like amazon go for clothes",
    "selftext": "Hey everyone, I'm making a cashier less store like amazon go(it's concept given at the end if you're not familiar with it) but for a clothing store as our final year project. We needed to clarify a few things. \n What we think we have to do is:\n1. person identification for tracking through reID classification \n2. Pose detection, identifying the persons movement to detect when he's about to pick up or leave something on shelves\n3. Object detection of the items in the store. Clothing items\n\n(We're only implementing the CV part of amazon go)\n\nWe have the dataset for each of above\nBUT\nwe don't have a dataset of cctv footages of clothing stores. I wanna ask is\n\nQ1) Do we really need the exact footage dataset of clothing stores or can we train the model on grocery stores cctv footage. \n\nQ2) is there a dataset of cctv footages of a clothing store out there if yes then where. \n\nQ3) we're also ambiguous on how we'd execute the whole project like what should be the workflow or pipeline i.e the first step doubts. \n\nIt would be really great if someone can guide us or help us in any regard. \n\nAbout amazon go : it is a cashier-less store. In which you enter, scan your  money account and the camera detects you. Then as you go along the store, you pick up items of your choice or leave them after picking up, the cameras detect everything and virtually make up a cart of all the items you picked and then when you leave it just bills you on your account. ",
    "created_utc": "2024-09-09T15:13:12",
    "num_comments": 13,
    "comments": [
        "You could check out meta's sapiens for human parsing, also the CIHP, LIP and other parsing datasets for segmentation related to clothing and such, sapiens also has keypoint estimation. As for CCTV footage you might have a hard time finding that to be honest, I'd be curious what you end up finding, but most public datasets that use CCTV footage that I've encountered are related to anomaly detection, violence detection, etc.",
        "amazon go is powered by actual people in india.",
        "> Q1) Do we really need the exact footage dataset of clothing stores or can we train the model on grocery stores cctv footage. \n\nTrain what exactly? The people detection and pose estimation model? No, you wouldn't need CCTV videos from clothing stores in particular to train. Just videos from similar angles. But you will need the videos to test your system.\n\n> Q2) is there a dataset of cctv footages of a clothing store out there if yes then where.\n\nDoubt it. CCTV video datasets are rare and often of really poor quality.\n\n> Q3) we're also ambiguous on how we'd execute the whole project like what should be the workflow or pipeline i.e the first step doubts. \n\nIt's quite a large project. Making a system like this that doesn't throw false positives every few seconds is a pretty big undertaking. Lack of data makes it even worse.\n\nHere's how it usually goes:\n\nStep 1: Object detection - 90% accurate.   \nStep 2: Pose estimation - 90% accurate.    \nStep 3: Tracking - 90% accurate.    \nStep 4: Person reidentification - 90% accurate.    \nStep 5: Your logic, typically based on some heuristics - 90% accurate.  \n\n**Overall accuracy**: 0.9 * 0.9 * 0.9 * 0.9 * 0.9 = 0.59\n\nThat's what happens when everything in the chain introduces some error.",
        "1) You don't need a clothing store dataset for pose and person detection. I would first evaluate how current openly available models cameras work against your camera. Models should generalize relatively okay across environments this is also dependent on the kind of camera you use. Fine-tuning with your data will improve accuracy  \n  \n2) I doubt you'll find real CCTV footage, but you can explore synthetic datasets or curating a set from current open source datasets (Retail context) like   \n  \n3) Workflow  \nChaining too many tasks will reduce performance, very essential to have guardrails. Also consider event based triggering.   \n  \nPrimary:  \nObject detector (Person) -> Tracker (ReID) -> In zone ( Pose estimation) -> Object detection (Product) -> Logic maintain inventory   \n  \nSecondary:  \nMonitor shelfs for better / after interaction -> Diff segmentation\n\nPreviously was part of a similar project before and happy provide insights",
        "Yea and most of them are related to that. But can I train my model based on datasets for grocery shopping etc because the actions performed seem the same but would there be the issue?",
        "Haha that might be.. but if we're talking seriously then based on my research on this project that's not the case😂 maybe I'm wrong",
        "It was insightful but I have a few other questions as well.. would you mind if I DM you discussing a few things.. don't worry I wont suck your brain alot lol",
        "The errors aren't stacking here unless they're dependent on each other.  Tracking isn't dependent on pose estimation.  Person reidentification doesn't rely on the errors of any of those.  So thanks for confirming my points in other thread.  Enjoy being unemployed in 4 years.",
        "Hmmm that seems interesting enough... Thank you for the insight ❤️",
        "Hard to say, I think if the background differs a lot between the two settings, then video models might have a bit of adjusting to do, but you might get decent results. There's always the changes from one setting to another in computer vision: image resolution, blurriness, distance of subjects from the camera, color accuracy/grayscale, lighting conditions, etc. These amplify the already mediocre results of pose estimators when it comes to occlusions, multi-person scenarios, awkward unseen poses from the training distribution etc. This sounds like a hard project. My recommendation would be to try to set small, achievable goals and build from there. If the promise is too grandiose and vague I can basically guarantee the results won't match expectations.",
        "No I'm being serious https://www.washingtontimes.com/news/2024/apr/4/amazons-just-walk-out-stores-relied-on-1000-people/",
        "Would it be good if I approach a clothing store and ask them for their footages and train and also test on the same store?",
        "Yes I've seen that. Haven't seen any evidence tho.. but it has gotten alot of coverage. Lol"
    ]
},
{
    "submission_id": "1fd0tvm",
    "title": "Novelty in shadow removal",
    "selftext": "Hey everyone! I'm currently working on my master's project focused on shadow removal. Right now, I’m in the literature review phase, reading papers and gathering ideas. Whether I can turn this into a research paper depends on whether I can create something novel. If not, it’ll still be a really cool learning experience, even if it's just a project.\n\nI do have a few ideas, mostly combining techniques from different papers. Can that be considered novel? This is my first attempt at writing a research paper, and while I have decent experience in computer vision for a new grad, I'm still figuring out how to approach this.\n\nI have around 4 to 5 months, which isn't a lot of time for something as complex as shadow removal or image restoration, but I want to give it a shot. In case anyone's wondering I do have a mentor/professor who's experienced in this field, though he’s quite busy.\n\nHere's my rough problem statement: I'm aiming to solve the problem of shadow removal without using masks. I’ve been looking into transformer-based architectures, GANs, and diffusion networks, and diffusion networks seem promising. One idea I’m considering is combining a technique from a diffusion paper with a GAN architecture, along with a few other tweaks.\n\nA couple of questions:\n\n1. How does one come up with something truly novel?\n2. Is this goal realistic within a 4 to 5 month timeline?\n3. What other areas in literature can I take a look at (maybe image restoration tasks (e.g., inpainting)), the reason I ask this is because we can always formulate our problem in a way that we can use ideas from different papers, right?\n\nThanks in advance!",
    "created_utc": "2024-09-09T14:18:42",
    "num_comments": 1,
    "comments": [
        "In terms of novelty you absolutely can adapt methods used in one field to an application for another. Novelty is just a contribution that adds something new to the field \n\nUsing your example, you'd basically do a critical literature review of shadow masking methods and highlight what's missing, and then a brief review of GANs or diffusion and how they can fit in.\n\nIf your gan can mask shadows in an unsupervised way, you then compare it to similar unsupervised methods to demonstrate it's advantages.\n\nFor a timeline of 4 to 5 months if you've done the review it might be doable, but really only you can answer this one. Remember, you've got to source a dataset (or multiple), code your model, do an ablation study, and compare it to other models. Ideally you would use existing datasets from other papers, then you can directly use their results for comparison instead of having to recode their method. Also keep in mind the review process, which can take a few months and might require revisions."
    ]
},
{
    "submission_id": "1fcxbwo",
    "title": "Severless deployment for GroundedSAM",
    "selftext": "I am building a lightweight app that needs to run GroundedSAM once a day. The GroundedSAM version on replicate is not useful for me. \n\nI need to find an optimal way where I can deploy my app but the GroundedSAM function remains serverless (preferrably) so that GPU compute is summoned only when I need to get inference from GroundedeSAM so that my costs remain minimal.",
    "created_utc": "2024-09-09T11:59:22",
    "num_comments": 2,
    "comments": [
        "If you only need to run once a day do you really need the GPU?  If not you can get pretty far with Lambda and a container.  If you do need the GPU then you could always just deploy it on EC2 and only turn on the instance when you need it.\n\n\nThere are various serverless GPU solutions coming up these days though, curious to know other people's experience with them.",
        "I'm using RunPod for this kind of jobs, where nb of request is pretty small. \nThe launch time are decent, for StableDiffusion it was like 5-10s."
    ]
},
{
    "submission_id": "1fcux1z",
    "title": "Implementing papers worth?",
    "selftext": "Hello all,\n\nI have a masters in robotics (had courses on ML, CV, DL and Mathematics) and lately i've been very interested in 3D Computer Vision so i looked into some projects. I found deepSDF https://arxiv.org/abs/1901.05103. My goal is to implement it on C++, use CUDA & SIMD and test on a real camera for online SDF building.\n\nAlso been planning to implement 3D Gaussian Splatting as well.\n\nBut my friend says don't bother, because everyone can implement those papers so i need to write my own papers instead. Is he right? Am i losing time?",
    "created_utc": "2024-09-09T10:20:13",
    "num_comments": 21,
    "comments": [
        "If you want to implement it (i.e., build an application on top of a paper), use others' code. If you're planning on researching (i.e., building a new method based on the old method), either implement it yourself or reimplement it. The goal is to familiarize yourself with the method and code, including techniques and parameters. But this is only my personal experience.",
        ">everyone can implement those papers\n\nNot true. Especially if you're doing it in C++ with CUDA and SIMD. This is a super niche field.\n\nIt's not a waste of time if you see it as an opportunity to learn something new and to demonstrate that you can implement someone else' work.",
        "Found [6 relevant code implementations](https://www.catalyzex.com/paper/arxiv:1901.05103/code) for \"DeepSDF: Learning Continuous Signed Distance Functions for Shape Representation\".\n\nIf you have code to share with the community, please add it [here](https://www.catalyzex.com/add_code?paper_url=https://arxiv.org/abs/1901.05103&title=DeepSDF%3A+Learning+Continuous+Signed+Distance+Functions+for+Shape+Representation) 😊🙏\n\nCreate an alert for new code releases here [here](https://www.catalyzex.com/alerts/code/create?paper_url=https://arxiv.org/abs/1901.05103&paper_title=DeepSDF: Learning Continuous Signed Distance Functions for Shape Representation&paper_arxiv_id=1901.05103)\n\n--\n\nTo opt out from receiving code links, DM me.",
        "It's really a question of your goals. Why do you want to implement this paper? If it's just for your own personal use, then do whatever you want of course. If you're trying to get into industry or a PhD program, then this doesn't sound very interesting. It might be worthwhile if:\n\na) This paper is important. The vast majority of papers on arxiv are total garbage because there's no peer review. Even most peer reviewed papers aren't that interesting. Why did you pick this paper? Do you have a strong reason for believing in it?\n\nb) Any existing implementations are insufficient. Maybe no one's implemented it, or maybe it's been implemented, but it's too slow, and you have a way to make it faster?\n\nIf you're trying to impress someone, then both a) and b) better be true, and even then it might not be the best use of your time.",
        "I found that for most papers trying to implement it from scratch is a waste of time. Unless you are trying to build off of it or you will use the code for your own research. \n\nUse code that’s already out there and only write the code you need.",
        "Some papers don't release the code.  Probably,  it's better to implement the code for these papers and release them.",
        "Thank you. Its not really an application. I just want to show competence in SIMD, CUDA, C++ and Computer Vision.",
        "Thank you. I feel better. \"demonstrate that you can implement someone else' work.\" is what industry is all about anyways.",
        "The paper is very important as it laid some ground and it is often cited. Its also an area which i care about. 3D perception and robotics. \n\n\nCurrent implementations are sufficient, but none of them was actually tested online with a real camera. Only datasets and offline.\n\n\nOne worthy goal is to implement the paper on a camera and run it in real time on a jetson nano for example. Maybe do pruning, quantization, knowledge distillation, pre-models, etc. ",
        "In my opinion, it doesn't seem practical to reimplement something for the sake of reimplementing, even if it's for understanding. To showcase your understanding, maybe think of a goal. For example, as I mentioned before, you can build an application with it or contribute to an open-source project? \n\nIn any case, it's honestly up to you. If you want to practice this way and it's effective, go ahead and do it. You know what's best for you.",
        "You don't develop much competence in anything by simply running other people's research code.\n\nCoding something yourself is an excellent way to build competence and understanding. One suggestion... Some things are much harder to code than they look. Start smaller, pick a simpler problem first.\n\nSDF? Sure why not. But focus on  running properly before diving into the optimization soup.\n\nGaussian splat? Sure, but why not do a 2d version first? Simpler, faster, and easier to debug. Then go 3d. And then maybe 4d? ;-)\n\nAlso, try not to be a \"dataset slave\". Make sure you can build your own synthetic dataset for testing and debugging. \n\nEnjoy,",
        "One of my early career jobs was literally take this paper, code it in Octave (an open source Matlab clone), demonstrate it working on a dataset. Others ported my code to C++ and optimized it for the hardware the company sells.",
        "By goals I meant what are you trying to accomplish career-wise. Are you trying to impress someone and get an industry job? Or a PhD position? Or something else? The answer may be completely different, depending on your answer to this question. If you aren't trying to advance your career by doing this work, then again you should do whatever you want. \n\nIf you want to take a popular algorithm and test it in a setting where it's never been tested before, that certainly could be worthwhile. But if the existing implementations are sufficient, you should use them instead of implementing it yourself (again, when I say \"should\" I mean only if you're trying to impress someone). \n\nBut if you're aiming to get farther in academia, conducting research on your own at your house is not typically the way to do it--you'd be far better off finding a professor you could work with on this. On the other hand, if you're seeking an industry position, I can't say whether this is worthwhile, as I don't have experience in that area.",
        "Ideally would be contributing to an open-source project. But i also can build a SDF in real-time.",
        "The coincidence haha.",
        "Its for industry. Thank you for your reply. ",
        "Although i might pursue a PhD, but of course i will talk to a professor ",
        "If you build for web-assembly (you should be able to build with C++) it probably would have a larger use case for open source. \n\nA lot of the gaussian splat repos for built for web are doing well. I am also bias because SDF seem super useful for webxr, but many obvious libraries right now.\n\nI'm just wishlisting. Learning is probably more important.",
        "I didn't mean the paper you mentioned. My manager gave me a pdf of some paper related to the problem he wanted solved.",
        "Okay, then I would ask people explicitly (maybe in a new post) whether conducting independent research is helpful for getting a job in industry. That's gonna be the key question. If it is, the work could be worthwhile, but again it's about testing the algorithm in a new setting, not about reimplementing an algorithm that's already been implemented.\n\nAnd even for industry, I expect you'd be better off conducting research as part of a lab, rather than independently. So if you have any existing relationships with professors in your area, you might want to consider that.",
        "Thank you for your advice. I can takk to my professors to do some research. "
    ]
},
{
    "submission_id": "1fcu7na",
    "title": "Trying to train my first model, any tips?",
    "selftext": "Hi, I'm not a big fan of learning something through getting familiar with a lot of theory first, I find it way more effective to just figure out what I'd like to build and then get there step by step. So my endgoal is to come into possession of a model that'll recognize the current situation on an online poker table from a screenshot, however for now I'll be content if it detects all the players sitting at that table.\n\n  \nThe screenshots are usually very similar (you can google \"ggpoker table\"), sometimes the table has a different color of the felt than usual or cards on the board can be different if there are any etc., but in general these screenshots are similar containing a 2d table with player avatars and info around it so I don't think I need to gather extremely big datasets for this.\n\n  \nI tried labeling 15 images and fine-tuning yolov8n with them all put in the training set, without any test/validation sets as shown in one tut on youtube but even after 80 epochs it still doesn't detect anything on a new image so I guess I need more images and I should put some of them into testing and validation sets.\n\n  \nMy main questions:\n\n1. How many labeled images do you think I need to see the first results?\n\n2. Is there any free and somewhat easy to use tool that'll annotate the images for me automatically or semi-automatically?\n\n3. Do you have other general tips for me to achieve my goal? Will this be extremely time-consuming because if so, I would re-consider doing it at all?\n\n  \nThanks in advance for your help",
    "created_utc": "2024-09-09T09:51:03",
    "num_comments": 4,
    "comments": [
        "Usually 2k is a good starting point.",
        "Here's a dataset for ggpoker: https://universe.roboflow.com/julian-barthel-ygatb/ggpoker/dataset/4",
        "I generally see models starting to learn after about 30-50 examples, depending on complexity. Then between 500-1000 for more 'production' grade.",
        "My man this is gonna be very helpful! Thanks!"
    ]
},
{
    "submission_id": "1fcr2b3",
    "title": "Good community for this field?",
    "selftext": "What's a good community where people discuss new models, bounce ideas off each other, bring up bugs and errors? \n\nI'm new to this field and just like the way I learned how to code I enjoy being active in communities, where do you guys often hangout. This subreddit is decent but it could be a bit more active",
    "created_utc": "2024-09-09T07:40:51",
    "num_comments": 1,
    "comments": []
},
{
    "submission_id": "1fcnnmw",
    "title": "Best foundation model",
    "selftext": "For the people interested in Monocular Depth Estimation, any experience with fine-tuning foundation model to some use cases? As an example, for detecting depth in texture-less areas? Elevations maps? I saw three models and would like hands-on feedback, if possible. (1. Depthanything 2. metric3D 3. Unidepth) \n\nWould like to hear some feedback!",
    "created_utc": "2024-09-09T04:59:54",
    "num_comments": 2,
    "comments": [
        "I m also interested and your list is very similar to candidate models i found during search.",
        "depth anything is quite good, and it has its second version released.. they used massive amounts of data so it pretty much works in every cases except very specific domain data (e.g. microbiological images)"
    ]
},
{
    "submission_id": "1fcna59",
    "title": "YOLOv8 resuming   training on a pre-trained model or saving the model and re-training",
    "selftext": "I have a YOLO v8 trained model model on a certain number of images ... Now can I extract the model's .pt file ,in my case the accuracy is of utmost importance (weapon detection computer vision) .So which approach should I follow  \n 1. Train on a certain amount of images --> Generate a .pt file train on rest of the images again  \n2. Train for a large number of epochs ...make checkpoints on it  then resume training   \nPlease suggest   \nThank you for your help :)",
    "created_utc": "2024-09-09T04:38:35",
    "num_comments": 2,
    "comments": [
        "YOLOv8 uses an LR scheduler that adjusts itself based on the number of epochs you're training for. So training in one go is the best. Option 2 would be the closest for that. Also the last 10 epochs run without mosaic augmentation so that it can improve without the aggressive augmentation.\n\nBut if you want to go with Option 1, [here](https://reddit.com/r/Ultralytics/comments/1eolwl8/the_correct_way_to_train_from_a_previously/) are some things to keep in mind regarding the training arguments.",
        "Thanks a  lot mate :)"
    ]
},
{
    "submission_id": "1fcmm5d",
    "title": "Krunker Aimbot with Yolov8 and Roboflow Step by Step Guide - Sly Automation",
    "selftext": "https://reddit.com/link/1fcmm5d/video/8g2uk8rxlrnd1/player\n\n[https://www.slyautomation.com/blog/krunker-aimbot-with-yolov8-and-roboflow/](https://www.slyautomation.com/blog/krunker-aimbot-with-yolov8-and-roboflow/)",
    "created_utc": "2024-09-09T03:59:09",
    "num_comments": 1,
    "comments": [
        "The world doesn't need more tutorials on how to create aimbots for multiplayer FPS."
    ]
},
{
    "submission_id": "1fcloaz",
    "title": "Best Model for fine-tuning on unconventional images.",
    "selftext": "Hello,\n\nI am working on training yolov8 on images from a Time-Of-Flight camera. Each capture consists of 3 images: \n\n* Intensity (grayscale image)\n* Depth map \n* Normal map (calculated from the depth map)\n\nCurrently I train three independent YoloV8 models on each image type and combine the results into one prediction. This works quite well, but I was wondering if there are better models to use as a training base for \"unconventional\" images. \n\nBy \"unconventional\" images I mean images that are very different from classical training images (e.g. Coco or ImageNet). \n\nI've read that vision transform based systems are better at generalization when trained on large datasets. Could it lead to better results if I fine tune a system with a ViT or are there models already trained on a wide range of image types?\n\nThank you for your help in advance!",
    "created_utc": "2024-09-09T02:55:28",
    "num_comments": 5,
    "comments": [
        "ViTs work better for most of the “unconventional” images as the data they have trained it on was huge compared to other models. \n\nHaving said that, there’s no guarantee you will have the best chance with specific model even with finetuning. I find SAM for my segmentation much better than anything else.",
        "Interesting. I wonder what would happen if you just stacked the three channels and treated them as RGB. Probably would need to override the default augmentation or do that offline, but at least this would be an easy thing to try,",
        "ViTs are designed to handle a variety of image types and might offer better generalization for unconventional data. It might be useful to experiment with them and see if they improve your results.",
        "I think it depends a lot on data your have, for example number of channels can dictate a lot how to train",
        "Thank you for your answer!\nI already use SAM in the Labeling Process, but its too big for my target system and the needed speed. But i've not yet tried faster versions like FastSam or MobileSam.\n\nI've also heard of fine-tuning sam but then i still need manual prompt-inputs, or am I wrong?"
    ]
},
{
    "submission_id": "1fclbo3",
    "title": "Selecting a Camera for Automotive Stereo Camera",
    "selftext": "I have a question regarding selecting a camera model for building a stereo setup for automotive application. \n\nI noticed that I have one of four options, monochrome sensor with global shutter, monochrome with HDR sensor and global shutter, RGB global shutter, Automotive rolling shutter with HDR and LFM. \n\nMy question, Is it MANDATORY to select a sensor with global shutter for wide-baseline stereo vision?",
    "created_utc": "2024-09-09T02:29:13",
    "num_comments": 6,
    "comments": [
        "To get accurate stereo, you need synchronized cameras.\nOnce you synchronize at the line level, it doesn't matter much if you have rolling shutters.\nSync is more important than shutter.\nHowever, rolling shutter will require that you take into account the véhicule speed, as it moves during the frame capture. If you want \"instantaneous 3d\" because you don't know or care about camera motion, then stick to progressive (global shutter).",
        "To get accurate stereo, you need synchronized cameras.\nOnce you synchronize, it doesn't matter much if you have rolling shutters.\nSync is more important than shutter.\nHowever, rolling shutter will require that you take into account the véhicule speed, as it moves during the frame capture. If you want \"instantaneous 3d\" because you don't know or care about camera motion, then stick to progressive (global shutter).",
        "To get accurate stereo, you need synchronized cameras.\nOnce you synchronize, it doesn't matter much if you have rolling shutters.\nSync is more important than shutter.\nHowever, rolling shutter will require that you take into account the véhicule speed, as it moves during the frame capture. If you want \"instantaneous 3d\" because you don't know or care about camera motion, then stick to progressive (global shutter).",
        "Global shutter is nice as you don't have to deal with rolling shutter effects - so the question is how pronounced will the rolling shutter effect be for your particular scene with the particular camera sensor that you're using (you can somewhat guesstimate this using the TRow, pixel / mm, and speed). Problem with automotive is that you're typically dealing with a fast moving scene.\n\nIf the scene is largely static or slow moving you might get away with rolling shutter.",
        "Okay, Now I'm convinced with the global shutter. \nFor an automotive highway application, do I buy global shutter RGB sensor with dynamic range of 71db or monochrome sensor with high dynamic range 100 db?",
        "I recommend the STURDeCAM34 for your application due to its impressive features tailored for automotive environments. This camera module utilizes the AR0341AT CMOS image sensor, which offers a remarkable high dynamic range of up to 140 dB through its super-exposure pixel technology. This is especially beneficial for handling challenging lighting conditions often encountered on highways, ensuring both highlights and shadows are well-represented.\n\nAdditionally, the STURDeCAM34 supports LED flicker mitigation (LFM), which minimizes artifacts from artificial lighting, a crucial feature for Advanced Driver Assistance Systems (ADAS). Its global shutter capability also enables precise capture of fast-moving scenes without motion blur, making it ideal for safety-critical applications.\n\nIn summary, the STURDeCAM34 provides the dynamic range, responsiveness, and performance necessary for accurate and reliable imaging in dynamic automotive settings. [https://www.youtube.com/watch?v=iAdlvfHqxPg](https://www.youtube.com/watch?v=iAdlvfHqxPg)",
        "Thanks for your reply, but this camera is rolling shutter not global shutter?",
        "I think they're using an LLM to push their own product, unfortunately"
    ]
},
{
    "submission_id": "1fcjs4s",
    "title": "Gaussian Filter according to ISO 16610-Part 21 - How is σ Sigma defined in relation to λ",
    "selftext": "I'm currently working on a scratched aluminum plate. (Nevermind the green/red spikes (outliers), they're accounted for). Photographed with white light interferometry.\n\nTo smoothen the surface I want to run a gaussian filter.\n\n[Resolution of 1024xpx\\*1024px \\* 6µm](https://preview.redd.it/0quva6ogjqnd1.png?width=394&format=png&auto=webp&s=4dad0ee1fd6f7cbfb398ddf89c55d5ccce87c596)\n\nAs a reference point I'm using DigitalSurfs Mountainsmap. French program that offers a lot of functions in regards to surface analysis\n\nIn MountainsMap I get four possible values as λs in µm to do gauss. (the ß in Gauß is german for ss)\n\n[DigitalSurf MountainsMap - S-Filter](https://preview.redd.it/dvynf7t4kqnd1.png?width=342&format=png&auto=webp&s=8b66db9d15884893689af077f2acabf90fdff85d)\n\nYet my program to do gaussian filter (and all other programs I could find) use only σ Sigma Values for gaussian filter.\n\nTranslating those λ MountainsMap Values to σ Sigma Values can't be that hard I thought.  \nGiven those formulas from Wikipedia:\n\nhttps://preview.redd.it/im8ycdjyjqnd1.png?width=322&format=png&auto=webp&s=3e361b508ed6329d639f38f2e9e69c59ca13ca11\n\nwith σ = Sigma\n\nhttps://preview.redd.it/fej0x591kqnd1.png?width=90&format=png&auto=webp&s=5f62e99638fa4a212a37eca342ffa524720bd89b\n\nI get the following conversion:\n\n|Wavelength (λ)|Sigma (σ)|\n|:-|:-|\n|0.8 µm|0.127 µm|\n|2.5 µm|0.398 µm|\n|8 µm|1.273 µm|\n|25 µm|3.978 µm|\n\nYet running the Digital Surf gaussian filter with λ wavelength side by side with my gaussian filter with the calculated σ Sigma Values gives slightly vastly different results.\n\nSo since I don't know the ISO mentioned here:\n\n[DigitalSurf MountainsMap](https://preview.redd.it/sheoc0hzdqnd1.png?width=336&format=png&auto=webp&s=2e666e8839bd15846710093e92523ff5102c9daa)\n\nI'm assuming a different formula to calculate σ, hidden in the ISO, is the culprit.\n\nMy questions:  \nIs there a repository with a gaussian filter that uses λ instead of σ Sigma?  \nIs there some special magic in the gaussian filter based on that ISO?  \nOr is the σ Sigma calculated in a different way than σ = λ / 2π?\n\nEdit: Images broke - reuploaded them",
    "created_utc": "2024-09-09T00:29:36",
    "num_comments": 0,
    "comments": []
},
{
    "submission_id": "1fcj5xt",
    "title": "Seeking Open Source Solutions for Real-Time Video Stabilization During Treadmill Desk Usage",
    "selftext": "Hello!\n\nI'm encountering a challenge with my treadmill desk setup during video conferences. The movement is causing a noticeable camera shake, distracting my colleagues. I'm looking for open-source software solutions to address this issue in real-time.\n\nApproaches tried so far:\n\n1. A basic internet search for \"real-time video stabilization\" and \"webcam face tracking software\"\n2. Explored built-in background replacement features in Teams, but they don't fully solve the issue\n\nIdeal solution features:\n\n1. Real-time face tracking\n2. Video stabilization or background replacement\n3. Potential for simulating a stationary position even while in motion\n\nQuestions:\n\n1. Are there any open-source projects specifically designed for this use case?\n2. What computer vision techniques or libraries would be most suitable for implementing such a solution?\n3. Has anyone tackled a similar problem and can share their experience or approach?\n\nI'm also open to suggestions for alternative strategies. I would greatly appreciate any insights or directions for further research.\n\nThank you for your time and expertise.",
    "created_utc": "2024-09-08T23:42:56",
    "num_comments": 5,
    "comments": [
        "I ran into the same issue, simply figure out why it’s shaking your desk from the movement, are you bumping it? I simply just placed the webcam on a soft pad/cloth to resolve small shaking. Ideally the can would be mounted on something not touching/interacting with the treadmill. Sounds like something is up with your setup, I have other colleagues using walking desk without this issue.",
        "The camera is on an arm that is mounted to the desk, yes. And I would think that probably the movement of my entire upper body in the frame could also be distracting during meetings, so eliminating the desk shaking could probably still no fix the issue... hmm!",
        "I have been using walking desks for 15yrs, and have a background in VFX (this includes stabilizing video footage). Stabilizing yourself in any fashion will look even more appealing unnatural, a bit like your dancing in place. \n\nHonestly the better solution is minimizing the real world (physical) camera from shaking, and slow your speed of walking to 1.5mph or less while on video. Also move your camera back a bit if possible so the perspective of you moving is not filling the frame. \n\nI have never had issues from co workers who I have had calls with, even clients, being on the treadmill any time I am at my desk…I have no chair. There are solutions with the setup, it will just take some experimenting. If they have issues after that, it’s their issue or bias, not yours.",
        "Came here to say the same. My camera isn't shaking while walking so maybe it's a problem with your standing desk? Is it wobbling a lot?",
        "Thanks, I'll experiment a bit with your tips! :)"
    ]
},
{
    "submission_id": "1fcipzi",
    "title": "Trouble transforming world coordinates to image (pixel) coordinates",
    "selftext": "Hi.\n\nI have images taken from under an airplane at an angle of 45degrees. I have 3D objects that are photographed by the plane. The camera is reported to have lens distortion less than 0.2mm.\n\nI would like to automatically find these 3D objects in the taken images. So: I want to transform world to image coordinates. I have a lot of information, and tried to make a script using online sources that does the conversion, but I keep getting wrong results. So either my processing in the script is wrong, or my parameters (or units?) are wrong, but I can't see the fault.\n\nWhat am I doing wrong, and how do I fix the issue? All the parameters that I have and processing that I perform are in the code below.\n\nMy results upon running the code are \\[5382, 7327\\]. But when I visually inspect it, the results should be roughly : \\[4115, 4666\\]\n\nHope someone can give me some help!\n\n    import numpy as np\n    \n    # Matrix K. intrinsic\n    W = 39.706 # sensor width in mm\n    H = 53.181 # sensor height in mm\n    pixel_size = 0.003760000 # mm\n    w = 10560 # image width  in px\n    h = 14144 # image height in px\n    fmm = 123.38 \n    f = (fmm * W) / w\n    cx, cy = (w/2)+((-6.68 *W)/w), (h/2)+0  # Principal point has offset of mm: (-6.68,0)\n    \n    K = np.array([\n        [f, 0, cx],\n        [0, f, cy],\n        [0, 0, 1]\n    ])\n    \n    # Extrinsic parameters\n    theta = np.radians(45) # camera was looking down at angle (45 deg)\n    R = np.array(  # Rotation matrix\n    [[np.cos(theta), 0, np.sin(theta)],\n     [ 0, 1, 0],\n     [-np.sin(theta), 0, np.cos(theta)]])\n    \n    t = np.array([  # Translation vector (camera position in world coordinates)\n        [233528],\n        [580261],\n        [1052]\n    ])\n    \n    # Transformation matrix T = [R|t]\n    T = np.concatenate((R, t), axis=1)\n    \n    # World coordinate object of interest \n    Pw = np.array(\n        [[233495.8],\n         [581729.4],\n         [24],\n         [1]]\n    )\n    \n    # Project (Xw, Yw, Zw, 0) into cameras coordinate system\n    Pc = np.matmul(T, Pw)\n    \n    # Apply camera intrinsics to map (Xc, Yc, Zc) to p=(x, y, z)\n    Pc_norml = (Pc / Pc[2][0])\n    p = np.matmul(K, Pc_norml)\n    \n    uv = p[:-1]\n    \n    print(uv)\n\n  \n",
    "created_utc": "2024-09-08T23:10:11",
    "num_comments": 4,
    "comments": [
        "I didn't read it all, but I saw that your f is in mm, it should be unitless, or in pixels. I see the same issue with cx/cy.",
        "Why do you care about camera translation when the point you project is at infinity?",
        "Hey, thanks for the comment. I transform the mm into pixels by doing (fmm \\* Sensorwidth in mm) / amount of pixels in width. Same for cx and cy!",
        "I'm assuming fmm is the focal in mm, you're multiplying by a width in mm and dividing by unitless pixels, you get mm². You should get a unitless result. If fmm is unitless, then your result is in mm, still wrong."
    ]
},
{
    "submission_id": "1fc6sb0",
    "title": "Are there cameras that do on-board hue rotation?",
    "selftext": "I have a 1 bit RGB vision system that uses only 3bits per pixel to spot identical targets. I want to further simplify the system by using 1 bit per pixel. I think I can do this by rotating the hue to match the target's color. This only works this can be done on the camera before transferring the data. Do any cameras support this?",
    "created_utc": "2024-09-08T13:00:51",
    "num_comments": 3,
    "comments": [
        "What camera are you using today? This seems pretty manufacturer specific, as they're either doing this at the sensor level (and you're hosed) or they're doing it in software somewhere, and then it'll definitely depend on that software.",
        "I'm using an OV7670 that has GRB4:2:2 and I'm taking only 3 of data lines to get RGB. If I'm looking for something basically red, I can get away with using one line.  If the camera can do some colorspace configuration or some such, I may be able to use the one-line trick to spot objects of an arbitrary color instead of just red, green and blue.",
        "Quick googling only got me to https://www.eevblog.com/forum/projects/ov7670-color-settings/ but I don't know how well that will work when clipped to most significant bit."
    ]
},
{
    "submission_id": "1fc6rnf",
    "title": "Plz help size project: detect and interpret a person's movement in a gym to give advise",
    "selftext": "Hi folks. Not from the field.\nI am considering this as an entrepreneurial project and before I'll search for co-founders and funding, I'm doing some research and asking for your help: \nwhat would it mean for you in terms of work effort as a co-founder, employee or a freelancer (in full-time and part-time) to part take in such a project. If you can put a price tag behind it that be a very sweet cherry on top!\n\nPlease help me assess the scope of the project:\n- place is a gym with good cameras and bandwidth \n- gym goers wear a smart watch with health app\n\nTask: \n- identify a person working out\n- assess movements \n- combine data with other meaningful data\nTo make a meaningful interpretation e.g. to offer training advice or notice if someone does a movement in a way that is harmful.\n\nIs there any other info you need for your assessment?\nThank you!!\n\n  ",
    "created_utc": "2024-09-08T13:00:11",
    "num_comments": 5,
    "comments": [
        "There are a number of startups doing something similar, using a camera to track workout poses. \n\nThis being: \n\n[https://www.kickstarter.com/projects/133045743/aimoov?ref=bwdfut](https://www.kickstarter.com/projects/133045743/aimoov?ref=bwdfut)  \n[https://tempo.fit/](https://tempo.fit/)\n\nThere is PoseNet\n\n[https://github.com/tensorflow/tfjs-models/tree/master/pose-detection](https://github.com/tensorflow/tfjs-models/tree/master/pose-detection)\n\nWhich can detect the key joints of people in the scene, you can then use that to detect how they move and if they are using correct form. The Xbox Kinect also had similar \"games\". This have been tried before, I would try to understand what you are doing that provides value to your potential users. \n\nI think with a good developer you could have a proof of concept in less than 3 months that can be used for market validation, and have an MVP in 6 months. It would be gluing together existing technologies.",
        "I would do market research first ",
        "Sounds like a good digital twin use case.",
        "Thank you so much. Hearing that this part is rather simple does help a lot!",
        "That part was easy and already done.\nThere is\nVicon, Qualsys, Gait Up and many more"
    ]
},
{
    "submission_id": "1fc5jse",
    "title": "Question about configuring an industrial camera for specific applications",
    "selftext": "Hello everyone,\n\nI’m tasked with configuring an industrial camera for a specific application and am looking for helpful literature or recommendations to deepen my understanding of the subject. I’m particularly interested in information on various camera settings, accessories like lenses and lighting, and how to optimally adjust these to the specific use case.\n\nDo you have any tips on useful books, articles, or online resources? Or maybe you could share some advice from your own experience on the best approach?\n\nThanks in advance for your help!\n\n",
    "created_utc": "2024-09-08T12:08:10",
    "num_comments": 2,
    "comments": [
        "What's the application?",
        "Analyse Carbon Fiber in water. Carbon has a thickness of 3 micrometer"
    ]
},
{
    "submission_id": "1fc1wvh",
    "title": "Sort Images by Similarity Using Computer Vision",
    "selftext": "Hi everyone 🙂  \nI’m new to the world of computer vision and would really appreciate some crowd wisdom.  \nIs there a way, using today's tools and libraries, to categorize a folder full of images of places and buildings? For example, if I have a folder with 2 images of the Eiffel Tower, 3 images of Pisa, and 4 images of the Colosseum (for simplicity, let's assume the images are taken from the same or very similar angles), can I write a code that will eventually sort these into 3 folders, each containing similar images? To clarify, I’m not talking about a model that recognizes specific landmarks like the *Eiffel Tower*, but rather one that organizes the images into folders based on their similarity to each other.  \nThanks to everyone who helps! 🙂",
    "created_utc": "2024-09-08T09:31:56",
    "num_comments": 18,
    "comments": [
        "Use CLIP/DINO to get embeddings for your images and cluster them.",
        "Convert them to perceptual hash and k-means cluster. Use something like the elbow technique to find appropriate number of clusters.",
        "Extract feature embeddings and use the FAISS library. It’s a similarity search library and might have exactly what you want with optimised implementation. ",
        "This is the way. In fact, you can use any network's last layer to get the embeddings and compare those embeddings to determine the similarity between two images.\n\nI used same approach few years back to detect the duplicate images. In fact, there is a complete big project on GitHub for image comparison, I don't remember it now unfortunately.\n\nBut, what approach do we use to cluster those images based on the embeddings?",
        "An example of a perceptual hash is [dhash](https://github.com/benhoyt/dhash)",
        "Are you transforming hashes into some format more friendly for clustering?",
        "I was just going to suggest this. It works really really really well.\nI use it in our hotel room DB ,",
        "I have used faiss, a vector index by facebook research, for that purpose. I extract embeddings, add them to the index and use it to search for new query images or search within itself. This is a somewhat old project, I’d appreciate any feedback on whether this approach is still reasonable these days!",
        "I guess you can just use the regular K-Means right?",
        "If you want some literature on this, there is a somewhat recent paper with code [https://github.com/ssundaram21/dreamsim](https://github.com/ssundaram21/dreamsim) that evaluates some models wrt to their perceptual similarity based on clip/dino and also has an ensemble that, in their evaluation, outperforms individual models.",
        "don’t use the overfit final layers",
        "You may have to do something to get it working - but nothing big.",
        "You don't need to cluster when you use Perceptual hashes. You just compare the source to the database.",
        "I recommend UMAP or t-SNE.",
        "They want to get groups of similar images. They don't have a database.",
        "I recommend anything except these two algorithms with meaningless global embeddings",
        "A database can just be a folder of images \nPHash each one, they group them all by 90% similarity into separate lists.\nNo need for kmeans",
        "This will work but it feels like an early optimisation to me. What have you got against k-means? Iteratively finding centroids seems a more robust approach. Otherwise you will get folders of images that are 90% similar as opposed an abstract centroid that includes potentially a more generalised solution."
    ]
},
{
    "submission_id": "1fby3w6",
    "title": "Need Help finding papers on handling pre-processing errors for OCR.",
    "selftext": "I apologise for asking something so vague, but there has been a miss communication with my guide and all the work I did is now wasted, I need to show a last minute literature review on Handling Pre-processing errors for OCR in Indic Scripts. I would appreciate if I get any  pointers as I don't have enough time to go through all the papers. ",
    "created_utc": "2024-09-08T06:43:56",
    "num_comments": 2,
    "comments": [
        "I know there is a bunch of stuff through the people working on the ocrd project on GitHub. I usually hunt them down on GH and ask them. Most are super nice.",
        "Sorry for the late response, but your method worked, I hunted down a couple of contributors, and it worked,"
    ]
},
{
    "submission_id": "1fbv1bk",
    "title": "Using torch model with YOLO ",
    "selftext": "i have torch model that do image classification , but now i want to put in production using webcam , but couldn't figure out how to do it . the model trained on images that i choose , now how to make it run on webcam and do box annotation and label with in box ?",
    "created_utc": "2024-09-08T03:51:35",
    "num_comments": 8,
    "comments": [
        "Are you asking how to crop the boxes from the image and classify it?",
        "my description seems low  , live multi-class classification , just like yolo , it do box annotation , labeling , classification. the same i want to do it with my torch model.",
        "If you mean you want to get a box from your model, then you can't get a box from an image classification model.\n\nIf you mean you want to create an overlay displaying the inference results from your model, then you can use OpenCV to add an overlay text to the image or frame.",
        "So you want yolo with your classes?",
        "yes",
        "There’s guides on how to do exactly what you’re trying to do ",
        "can you share some ?",
        "I can’t vouch for the veracity of any of them but a quick google shows plenty.  “Yolo with custom classes”."
    ]
},
{
    "submission_id": "1fbn4af",
    "title": "low resolution high fps camera system recommendation",
    "selftext": "I need to record low resolution grayscale video (**30 by 30 pixels**) at a high frame rate (**1000+ fps**) for a computer vision project. I don't need to do any real time processing with the video, I just need to be able to store it on a drive. Ideally I would be able to record for close to 10 min at a time, but I can compromise on that if needed.\n\nAfter doing some research, I found that most digital cameras are able to transmit data fast enough for my needs, but I am not sure how I would set up a micro controller to grab frames from the camera and store them at 1000fps.\n\nMy budget is about **200$** for the micro controller and camera combined. Does anyone have any suggestions/previous experience with similar systems?",
    "created_utc": "2024-09-07T19:02:43",
    "num_comments": 1,
    "comments": [
        "Many machine vision grade cameras allow you to set a ROI, this reduces the data burden and lets you capture higher FPS. "
    ]
},
{
    "submission_id": "1fbmqal",
    "title": "Advices If I  need a Master of CV",
    "selftext": "Hi,\n\nI need some advice for my career. I am a mobile developer focusing on Android. I have been working for three years and am familiar with the app development process. I can not see any challenges in this industry.  My next step may be to advance to a senior position, or I start with other areas. I can see the development of AI, and I really like the VR/AR field. Could studying CV give me the opportunity to enter this field? Or it is more important to keep my current job, especially in the current Canadian job market. I am not very confident that I can find a suitable job after I get a master's degree.",
    "created_utc": "2024-09-07T18:40:53",
    "num_comments": 3,
    "comments": [
        "Following",
        "Following",
        "Work will always become repetitive once you master the process. Suppose you learn all about computer vision, and become an expert. Won't you just get bored again and want to switch? This isn't sustainable. If you want to do research and experiment with novel things regularly, get a PhD. This has its own challenges, however..."
    ]
},
{
    "submission_id": "1fblbcn",
    "title": "real-time head tracking: ~180deg FoV, ~0.1m min distance, max distance ~5m (<10m), 30Hz+. 3D ToF or Stereoscopic camera setup?",
    "selftext": "Currently using a Kinect V2 (Xbox One Kinect), which is good, however has a narrow FoV of around 70degH and 60degV, and a min dist of around 0.3m.\n\nI'm looking for a method to achieve similar tracking performance as the Kinect, in real-time, but with a wider FoV (approaching 180deg) and smaller min distance (near zero). \n\nIt will be in-door use only, situated in a room of a house. so max distance would be around 5m, maybe up to 10m. \n\nFrom what I can see, other 3D ToF sensors seem to be of similar FoVs as the Kinect, and range is similar. Are there any 3D ToF sensors suitable?\n\nAs an alternative, would a stereoscopic camera setup be a better choice? I was looking at using Blazeface however only the short-range version is yet released. \n\nAny help would be appreciated!",
    "created_utc": "2024-09-07T17:24:31",
    "num_comments": 1,
    "comments": [
        "Just run four (or more) kinects"
    ]
},
{
    "submission_id": "1fbg0m4",
    "title": "Any good computer vision and image gen ai startup where I can get intern",
    "selftext": "I'm looking AI startup where I can do remote internship and gain some experience particularly in field of cv and image gen\n\nEdit - sorry for low effort post,\nHere are the details\n\nLooking for internship in startup and want to learn from great leader and great team\n\nI'm final year btech student from India, I started working on  my own AI based startup ideas few months back. \nTalk to many customers and validated my idea. Hired 3 interns and Build poc, We also talk with many VC but we are not able raise good seed money. \nThe problem with us is \n- ML model is not delivering good results, there's lots of errors.\n\n- For changing architecture of ML model we don't have enough tech skills to do that, that requires some industry experienced person which we don't have right now. \n\n- For integrating with brands, we need really good cloud engineer who have experience of doing that otherwise our partner brand's customer will face the problem, along with that cloud costs is also problem. \n\n- Because of bad results, customers will leave the website. \n\n- Model is not trained on the Indian dataset, very less Indian dataset is available. \n\n- We're getting grants from i-hub (government grants) but it'll take 4 months of time to come money in the bank. Grants money can't be utilised to hire experienced developer. It's only for cloud costs. Same will be tough for interns to continue without any stipend. \n\n- Without good results and we're students so it's tough to raise money from private investors. And it'll take so much time. So our college is ending in a few months. \n\n- We don't have any idea regarding privacy policy or legal issues that should be approved. Or it'll take time & money. \n\n- There are other problems that can only be solved with a high amount of money which we can't fund right now.\n\nWe have gave ourselves time upto August for raising money, because we also need to start earning after college. \nBesides this I also felt that before becoming a leader I need to follow a leader and learns how startup actually works.\nSo we stopped working on this\n\nSo I'm looking for internships in AI based startup,\nTechnical(i had knowledge of Computer vision, image gen side, but open for learning ) and also interested buisness related roles also.\nThank you \n",
    "created_utc": "2024-09-07T13:11:22",
    "num_comments": 7,
    "comments": [
        "Based on this post... No.",
        "You know the world is very big place. If you don't specify location or working type (full time, onsite, online, hybrid, ...), it's really hard for people to recommend you.",
        "Those are two different vague questions, and one of them cannot even be answered.",
        "Based on OP's profile he's been trying to create his own startup with... 0 experience??",
        "Ik it's kind off very low effort, because many things was going with me , i should have properly mention everything",
        "I'm looking for remote opportunities",
        "Yeah its reads to me as - \"I've been playing around with Stable Diffusion and such. How can I get a job in this field\""
    ]
},
{
    "submission_id": "1fbc67v",
    "title": "I tried to code my own YOLO model to detect Football players",
    "selftext": "A breakdown of the YOLO architecture, and what I learnt implementing it from scratch in PyTorch. Plus some object detection tricks for football datasets. Hope y’all enjoy (leave a like on YT if you do thanks!)",
    "created_utc": "2024-09-07T10:22:04",
    "num_comments": 0,
    "comments": []
},
{
    "submission_id": "1fbbmos",
    "title": "Starst3r: Fast 3D reconstruction framework wrapper around Mast3r.",
    "selftext": "[Starst3r in Blender.](https://i.redd.it/ce6djrvl3fnd1.gif)\n\nRecently, the Mast3r and Dust3r papers revolutionized 3D reconstruction.  \nThey replace the whole pipeline (poses, intrinsics, sparse, dense, etc.) with a single end to end vision transformer.\n\nI have created a Python library and Blender add-on that make code integration easier:  \n[https://github.com/phuang1024/Starst3r](https://github.com/phuang1024/Starst3r)\n\nFuture plans for this library are:  \nExposing and porting more of the research code.  \nIntegration with gsplat (like InstantSplat).",
    "created_utc": "2024-09-07T09:58:50",
    "num_comments": 4,
    "comments": [
        "This kicks ass. I run the official OpenCV live streams, and we cover projects like this on the show. Would you be interested in doing a presentation on the lib and plugin? Send me a message",
        "This is amazing...I'm digging into Dust3r and Mast3r as a basis for my own research for my PhD, definitely gonna give this a deep dive!",
        "Great work guys",
        "This is soo cool!"
    ]
},
{
    "submission_id": "1fbbmfs",
    "title": "How to detect when fully or slightly occluded? ",
    "selftext": "Imagine you're creating a model to detect number plates, the rest of the car isn't important. The model has been trained only to detect 1 class, number plates. What techniques would you use to train the model to make a prediction on where the number plate may be when it may be occluded. For example, the second car on the left has the number plate occluded from view. We know its there but the model doesn't. A good portion of the car is visible. How could you use the information from the image (the rest of the car) to make a detection on where the number plate is which is occluded by the car. Or would this only be possible with additional classes, like detecting the car first, then predicting where the occluded number plate is?\n\nThanks\n\nhttps://preview.redd.it/1yr5z93c4fnd1.jpg?width=1999&format=pjpg&auto=webp&s=453707ede91994b68d8948bd3ed415c566fbcf5e\n\n",
    "created_utc": "2024-09-07T09:58:31",
    "num_comments": 7,
    "comments": [
        "Curious as to why you’d want to detect a completely occluded plate and what value that could bring or if it’s primarily a learning project.",
        "does your training data have partial plates like that? And do your augmentations include occlusions?",
        "Look into kalman filters, you can use object tracking to detect objects when occluded and kalman filters is how you do it.",
        "Pose estimation. You'd train a model to output the pose of the license plate for a car. Normally these networks have a fixed output dimension so you may need to pre-process the image to crop down to each car individually.\n\nHere's one example: [Pose - Ultralytics YOLO Docs](https://docs.ultralytics.com/tasks/pose/)",
        "BTW a really good way to augment this data is copy-paste. Use a segmentation network to \"cut out\" cars or other relevant objects from a random image and then \"paste\" them into a new image. This will give you an infinite number of examples of obscured license plates on which to train your model.",
        "Fair question, tbh the scenario and image was just an example, maybe I should rephrase what I was asking\n\nIs it possible to get a model to learn features and patterns from areas of an image which isn't annotated which can then can point/predict to a specific area of importance (the bounding box) which may be occluded.\n\nSorry it's difficult to word what I'm asking in a short question, which is why I tried to use the example\n\nEDIT: I'm not an expert and I've only just begun learning computer vision in recent months\n\n\nEDIT: For example, could I use pictures of cars with an empty background, annotate the number plate, then the model understands what is around the numberplate therefore being able to detect where the plate may be if it only has a portion of the car in view?",
        "Yes my dataset has partial plates. My augmentations are like this, it's basically the default from yolonas:\n\n            'transforms': [\n                {'DetectionMosaic': {'input_dim': [800, 800], 'prob': 0.7}}, \n                {'DetectionRandomAffine': {'degrees': 3.0, 'translate': 0.1, 'scales': [0.1, 2], 'shear': 2.0, 'target_size': [800, 800], 'filter_box_candidates': True, 'wh_thr': 2, 'area_thr': 0.1, 'ar_thr': 20}}, \n                {'DetectionMixup': {'input_dim': [800, 800], 'mixup_scale': [0.5, 1.5], 'prob': 0.7, 'flip_prob': 0}}, \n                {'DetectionHSV': {'prob': 1.0, 'hgain': 5, 'sgain': 30, 'vgain': 30}},\n                {'DetectionPaddedRescale': {'input_dim': [800, 800]}}, \n                {'DetectionTargetsFormatTransform': {'input_dim': [800, 800], 'output_format': 'LABEL_CXCYWH'}}\n                ]"
    ]
},
{
    "submission_id": "1fbbbgs",
    "title": "First time robotics camera project advice",
    "selftext": "Hi all, I'm pretty new to computer vision but have some software development experience. I am creating a camera-based project that can count the number of people coming in and out of a room. The idea is that it's a camera that I can mount above a door, and it will tell me how many people have come in and out. What is the best way to prototype such a project? I was thinking of getting a Raspberry Pi5, a camera accessory, and using YoloV8, but I wanted your advice! Thanks :)",
    "created_utc": "2024-09-07T09:45:08",
    "num_comments": 3,
    "comments": [
        "Before buying hardware, it might be a good idea to take some sample videos with what you have on hand, such as your laptop webcam or similar.\nOnce you test a model, yolo or others, and you are satisfied with the result and performance, you can start selecting ng hardware...\nOtherwise, it's hard to know in advance if rpi5 is good enough, what kind of camera quality/speed you need, etc...\n\nTest first, buy later.....",
        "If youre able to get a RTSP camera or smth you can run yolov8s or above with tracker to have some really good results. In rasberry pi youll need to run a lighter model which will drop accuracy but both of these will work.",
        "Yep that configuration should work"
    ]
},
{
    "submission_id": "1fb9i8r",
    "title": "Tensorflow basics importance for CV",
    "selftext": "Background: \nI started freelancing as a fresher. Started with data analytics and simple ML projects for students. Learned about YOLO, object detection, segmentation, classification on my own, from yt videos. Did not dive deep into the basics of tensor flow and PyTorch. \nI have crossed the $1.2k mark as earnings currently . Mostly from object detection projects. \nNow for upskilling and to gain knowledge about different models like u-net, rcnn, mask rcnn and various fields like medical imaging and all; should I take up a course offered by deeplearning.ai on coursera? So that the basics are strong. Or I should just keep learning how to make things work? \nI am interested in this field of computer vision and don’t wanna get stuck on just yolo. \nI also intended to learn about stable diffusion, mid journey, dall e. \nSo my main question is, should I go deep and get certified or just build some projects with the help of yt? ",
    "created_utc": "2024-09-07T08:27:28",
    "num_comments": 20,
    "comments": [
        "Good work. Sorry my questions offtopic but what products did you sell to make 1.2k?",
        "I will suggest that If you get a project take it and learn and work at a same time but if you got any spare time then ill suggest to take some course it will help you develop strong basics.",
        "Ok so your journey pretty  much relates to mine. I also started like this back in 2019. My advice to you is to take Deep Learning specialisation course from coursera. Its good in terms of expanding your existing knowledge in CV as well as NLP. You will also get learn how to manage a machine learning project. What to do when your model is underperforming (instead of hit and trying).",
        "I would say that basics are important but you should focus on PyTorch instead of TensorFlow. Go deep. Not sure what certification means in this industry so can't advise on that.",
        "Great! Btw, on which platform you got clients?",
        "Thanks you so much.\n1. Case study/POC for PPE kit detection \n2. Trained a custom model for bugs detection. Data provided by client.\n3. Trained a custom model for fight detection. Gathered data from fight related videos. \n4. Person counter from image url. Deployed a fastapi on a web server\n5. Face matching for attendance purposes. Deployed it on a web server as well. \n6. Driveway segmentation. Differentiating between tar and concrete driveways. \n7. Traffic monitoring system \n\nSome clients provided annotated data, some provided images which I had to annotate (using CVAT) and some did not provide data. \nAll the projects utilise YOLO and one utilises facenet512",
        "Understood. Thanks",
        "Understood! 👍\nThank you!!",
        "Understood! 👍 Thank you!!",
        "Thanks! \nMost on Upwork. Apart from that, freelancer.in and Fiverr.",
        "Just to be clear, you did all this work for $1200?\n\nAs to your question on Tensorflow, if you were able to do all this without a degree I would go get a degree and make some real money. If you have a degree you are extremely underpaid (that's true without a degree too probably). Everything you listed is consultant level work, I wouldn't do it for less than $100/hour at a very bare minimum.",
        "I hope you are aware of ultralytics' annoying AGPL license where you'll either have to buy an enterprise license or make your source code and dataset available to your customers",
        "Can i dm you?",
        "Yes. \n1. As a complete fresher and zero work experience, is setting a rate of $100/hr reasonable? \nIt has just been 8-9 months in this field and with the competition around I think just getting work ex and building a solid portfolio should be a priority. \nAt this point, just getting a project in itself is a big deal and if I set higher rates I don’t think anybody would hire me. \n2. That is my question; will having certifications help me stand out from the crowd and help me in any other way or at the industry level the clients just look at your work ex, reviews and project portfolio? \nMore suggestions welcome",
        "YOLO was supposed to be open source, so there’s some implementation of it that is very permissible for commercial use.",
        "There are lots of different YOLOs. \n\nAlso some/many customers prefer to have the source code and data. I wouldn't worry about that. Once you start working with customers who can afford your services the cost of  licensing isn't a big deal anymore....it's like how nobody complains about having to license Microsoft Windows and Office...it's just part of the price of doing business.",
        "I don’t have to worry about that. The clients must be taking care of that. I just prepare the data, build the models and give it to them.",
        "Yeah sure",
        "Did the clients pay for the training cost?\n\n1. Given you're using the term \"fresher\", I'm assuming you're from India (since only Indians use it to mean fresh graduates). I don't think you can get away with setting a rate of $100/hr because the rates over there are much lower.\n2. Certifications are not useful in this field.",
        "I am sorry, I don't know anything about the market in India and I should not have offered advice without knowing your location. I agree that certifications are not really useful in this domain, but getting a degree where you understand the underlying theory of the algorithms you are using would be useful even if you stay in ML engineering/applied research."
    ]
},
{
    "submission_id": "1fb5vkj",
    "title": "Which camera do I use for 180 degree view for my computer vision project?",
    "selftext": "We’re creating an automatic basketball shooting turret that launches the ball either as a chest, bounce or overhead pass to the detected player at his detected location using yolov8n on the Raspberry Pi 5. I was thinking about either the raspi camera with a fisheye, vs the ELP usb camera. \n\nThis is my first time using computer vision so any help and advice would be appreciated. ",
    "created_utc": "2024-09-07T05:33:27",
    "num_comments": 2,
    "comments": [
        "What camera/lens setup did you train your model with? You might find accuracy drops when you move to fisheye as the distortion can really change the appearance.",
        "Go with a USB camera so you can change to a different SBC and avoid all the camera ISP problems.  \n\n\nA Rockchip RK3588 based SBC blows the Pi 5 away.  Also when a new SBC comes out you have the freedom to switch to it without running into problems with the camera sensor not being supported."
    ]
},
{
    "submission_id": "1fb1wwn",
    "title": "How do you validate/test image processing pipelines for their quality?",
    "selftext": "I am looking for advice on how to validate the quality and production fitness for (any) image processing pipelines. Of course there are a lot of different contexts so I will try to lay out an example to discuss around:\n\n**The Context:**  \nI developed an image processing algorithm for a client that takes four images of a glass surface and returns a defect map. Based on this some of the DUTs (device under test) get trashed and other get shipped to customers. There are a lot of parameters that go into this pipeline. Like the allowed area of defects in certain regions as well as technical parameters like thresholds for certain masks etc. There are also many different products with varying parameters. Sometimes new DUT types also need to get \"teached/programmed\" into the system which voids the validation of the previous DUT types.\n\n**The Problem:**  \nThis was a rather big project with many moving parts. Along the way I got really frustrated with how I validated the image processing algorithm. It went something like this:\n\n1. Create initial image processing pipeline\n2. Manually run some tests with some of the DUT types (feed images, take a look at them manually, maybe correct parameters if it improved the end results etc...)\n3. A change comes along: Add feature xyz\n4. Implement feature xyz\n5. Retest whole image processing pipeline manually again because it might have affected other features/areas too\n\nThis would go on for many, many cycles. Somewhere along the way I thought it would be nice to be able to do something like a \"unit test\" which I can just run automated, but for this kind of data. I tried out to implement some things but ultimately wasn't satisfied with it. Mostly because I wasn't able to generate some ground truth data. (for example for the defect masks)\n\n**Questions:**\n\n1. How would you do the validation of the image processing pipeline described in here?\n   * Also manually\n   * by generating ground truth data (then validate with metrics used for example in machine learning, IoU, etc...)\n   * by using any special software that would make the manual process easier\n   * difference based (by just viewing the delta of different algorithm versions)\n   * any other approach...\n2. How would you do it in general? For example for smaller projects with less moving parts.",
    "created_utc": "2024-09-07T01:01:33",
    "num_comments": 14,
    "comments": [
        "We have basically setup the tests that compare the resulting images pixel-wise with prevuous iteration. You need to dustinguish a lot of cases here. Some parts of your code that are deterministic are very easy to test, just  run your deterministic part of the  pipeline once, save results and use them as the ground truth later.  If this test fails later, you are confident that some part of your pipeline was affected by some of your changes.\n\nSome, like ML models, are seemingly non-deterministic, results depend on the inference engine used or the hardware used. Here, we try to use some statistical analysis, like the difference between images should not exceed certain threshold, but we still can't bet our ass on the correctness of this method. I can just give some advices here. First and foremost, normalize the data on which you do statistical analysis. There can't be an absolute threshold that works for all images (e.g. more bright images have natuarally larger differences). Second, test all you have, bounding boxes, segmentation masks, output images, whatever you have. The more the merrier, the better chance to actually catch something.",
        "Generally this is what non-regression tests are for.\n\nWe validate a certain version of the algorithm in production, and then we dump all the validated data and regularly check (in nightly builds) that the variation isn't larger than thresholds that we also define. In general for a single test there are multiple thresholds (i.e. I know that variations of more than 0.1 start being significant for this metric, but anything below 1.0 is not significant for this other one). Any variation above these thresholds is looked at carefully to figure out the cause.\n\nInteger-only pipelines should be bitexact, but there are a number of things that can (and have, for us) affected floating-point outputs causing non-significant variation, some being:\n\n- Switching to a different architecture (nearly everything that did floating-point calculations had small differences when we switched from x86 to x64 a while ago) or floating point type (float32 to float64)\n- Some libraries are inherently slightly noisy (e.g. Intel IPP sometimes trades speed for lower stability in the last digits of a float, or video decoding libraries trading off speed for skipping frames when under heavy load) or change behavior on upgrade (NumPy having some corner cases of np.mean in the past, for example)\n- Different ways of calculating a metric (for example there are several ways of calculating an average, and a refactor changing that might affect particularly noisy estimations)",
        "I’ve had a similar problem before of needing to ensure that the changes I have made have not impacted the process and finding that a simple, did all the images that failed last time fail this time, doesn’t cut it. For me this was mostly because images were failing for multiple reasons. I went for a bounding box approach, pixel by pixel was too picky. Chose a high overlap percentage and ensure that all initial bounding boxes are covered by a bounding box produced by the new process. Manually inspect any new bounding boxes as they are either new defects (win) or false defects. Not too familiar with computer vision but this is my take on the problem from a machine vision perspective.",
        "Did you create your own tests/metrics to test from scratch or were you able to use an existing framework for this? (Personally I use C# with OpenCVSharp and Math.Net but language isn't that important as I am just looking for some guidance implementations.",
        "If I understand it correctly the procedure looks something like this:\n\n1. Implement feature x\n2. Test feature x manually and dump output data\n3. Implement tests that validate that the output is similar enough to the initial manual tests done in step 2 (oversection high enough, measured value xyz in range, etc...)\n4. Implement feature y\n5. Test feature y manually, dump data etc...\n6. Implement tests for feature y\n7. Make sure all the tests pass. (This will now be automatic due to the integrated tests)\n\nDid I undestand this about right?",
        "Did you integrate these tests yourself or were you able to use a framework to test the intersection of your bounding boxes automatically?",
        "Honestly, no, we use python and we did not find any existing framework for that. There are too many small things you need to take care of, and its certainly very usecase specific. For example, if there are only few objects detected, small changes of mask size can lead to large differences. In some cases, we don't care if some objects are not detected (there are classes of non-identifiable objects). And so on and so forth.\n\nI suggest following workflow - first, develop a metric that describes difference between two outputs for your scenario and test it manually on a large dataset. If you are satisfied with the results, try to setup the threshold difference to some very low value. Then, whenever you catch false positive, increase this threshold. After certain pain inducing time, you will have a good threshold and you will never have to do that work again. At least that is how we did it.",
        "Yes, exactly. Rinse and repeat. Our output files are committed to Git in JSON format because they need to be updated every now and then ; the input files (images, videos) are too large and don't change that often so they're are stored on a NAS (with snapshots, which can save your bacon if you ever make a mistake) and synced on the builder nodes before each build.",
        "I wrote a separate inspection to test the intersection of the bounding boxes as the testing framework only wanted a final true/false for each test",
        "Thank you for your reply! This sounds very reasonable. First I tried to create some ground truth data manually but wasn't really satisfied with it. (Takes a long time, have to do it for many different types of DUT, etc...) \n\nI think I will give the delta test a try. The main effort here will probably be to implement the specific metrics that will track the difference between two tests. But after that it sounds like smooth sailing.",
        "Yeah, there is no framework I know of for testing large datasets of numerical/scientific computations in Python...\n\nThese \"too many small things\" are where you really feal the inadequacy of the common Python testing frameworks (e.g. pytest) after a while, they're much better suited to very small unit tests IMO... and we eventually had to create a few additional layers of code to help us run non-regression tests.",
        "No problem! Also, from our experience, some suggestions that will make your life easier. I suppose you will have several different sets of parametrisations for your algorithm that you want to test.  \n\n\n1. Have a way to easily re-create assets for particular parametrisation of the algorithm. \n\n2. Don't fail test if one difference is bigger than the threshold, let them all run. It's stupid, I know, but for some reason I initially failed tests when the first error appears and it was quite annoying. It's better to have a full picture of the difference when you change your threshold.\n\n3. Some images just not behave. We are doing face recognition and there was an image that had a watch that was randomly recognized as a face. The tests became much better when we threw this image out.\n\n4. Have a quick way to visually compare images. What I mean: if you software is complex enough, at some point these tests will take couple of hours or more to complete. You will run them in CI/CD pipeline and it might become very hard to understand what exactly fails if you don't have a readily access to the results.\n\nGood luck!",
        "For the pixelwise deterministic tests, at least for us, pytest works well. The test is just very straightforward and we use fixtures to test different parameters. For the ML pipeline, yeah, all bets are off, I agree with you."
    ]
},
{
    "submission_id": "1fb1lb6",
    "title": "Discord AI Community: From Beginner to Niche Topic Guidance & Support",
    "selftext": "We're building a [serious AI community](https://discord.gg/MF4urwHT) on helping and supporting each other through the entire AI journey, from **beginner topics** to **advanced niche areas**. this is the initiative taken by my [Mentor](https://www.linkedin.com/in/jjayasri/) who already managing two What's app communities [AI Focused ](https://www.linkedin.com/company/aifocusedtech/)& [Product Focused](https://www.linkedin.com/company/productfocused/about/).\n\n[Invite](https://discord.gg/MF4urwHT)\n\nExpect:  \n• **Collaborative Projects**  \n• **Research Paper Reading Clubs**  \n• **Mentorship & Guidance**\n\nIf you're serious about AI and looking for real growth, join us!",
    "created_utc": "2024-09-07T00:37:00",
    "num_comments": 1,
    "comments": [
        "Is there a CV community?"
    ]
},
{
    "submission_id": "1fb1if9",
    "title": "Real-Time Inpainting of People from Live Streams/Webcam Footage",
    "selftext": "Hi everyone,\n\nI'm working on a project where I need to inpaint people out of live streams and webcam footage in real-time. I can use Mediapipe's selfie segmentation to identify and mask people at runtime, replacing them with a clean plate from the scene. This approach works great, achieving nearly 30-40fps even on mobile devices. However, it has significant downsides: it requires a clean plate and a static camera.\n\nTo overcome these limitations, I explored alternatives like MI-GAN from Picsart AI, which offers an ONNX model. While promising, their method maxes out at around 7-8 fps on mobile, which isn't ideal for real-time applications.\n\nAre there better solutions or models that could achieve real-time inpainting on mobile devices at a higher frame rate? Any suggestions or insights would be greatly appreciated!\n\nFor reference, this is what I am talking about:\n\n",
    "created_utc": "2024-09-07T00:30:54",
    "num_comments": 4,
    "comments": [
        "are you limited to targeting mobile devices or can this run on a PC?",
        "I'm fine with it running on a PC, but my only requirement is that it shouldn't need a very powerful GPU and should be able to run even without a GPU if possible, while still providing decent FPS.",
        "I'm not sure what FPS this gets or if the quality is good enough, but if you want CPU optimised models try https://docs.openvino.ai/2023.3/notebooks/215-image-inpainting-with-output.html",
        "Hey thanks. I will check it out!"
    ]
},
{
    "submission_id": "1farivm",
    "title": "How can I perform multiple perspective Perspective n Point analysis?",
    "selftext": "I have two markers that are positioned simultaneously within one scene. How can I perform PnP without them erroneously interfering with each other? I tried to choose certain points, however this resulted in horrible time complexity. How can I approach this?",
    "created_utc": "2024-09-06T15:11:40",
    "num_comments": 11,
    "comments": [
        "You need to distinguish the 2 markers. Either by using charuco or similar tagged board instead of a standard chessboard, or by tracking them if you have a continuous video.\n\nThen, you need to estimate the extrinsic between the 2 boards by estimating separately the pnp of each board and multiplying the extrinsic of one board by the inverse of the extrinsic of the other board.\n\nThen, you choose one board as reference and apply the extrinsic to compute the 3d points of the other board relative to your reference board. Then, you can apply pnp on the whole set of 3d points. Note that some pnp algorithms have limitations to plannar correspondence only, you need one algorithm without this limitation.\n\nI also recommend to do a global refinement (I like to use ceres library for that) to refine both the extrinsic between your boards and the reprojection error.",
        "[deleted]",
        "Do you have 2D-3D correspondences?",
        "This doesn't allow me to perform PnP with overlapping points.",
        "Yes, I have a camera and a point map.",
        "You may be talking about something more like slam or monocular odometry",
        "I don't need odometry, I have a set of LED markers that I need to find the PnP of.",
        "If you know the 3d location of each led point and can identify each point in your image (knowing the 3d position of each led you detect in the image), you can directly apply Pnp on it, it's the standard case. Just choose a Pnp algorithm that doesn't require the points to be coplanar if they are not coplanar.\n\nIf you know the 3d location of each led point but can not identify them (all the trackers look the same), you can either :\n1)Use ransac with a Pnp algo. Complexity can be really bad if you have many candidates but you can reduce the number of tries if you have some knowledge about your scene. For example, is the video mostly vertical, can you guess that a projected point should be left/right of another one when observed,...\n2)Track the 2d points for a few frames with slam, obtain their up-to-scale relative 3d positions. Then you apply ransac on the 3d-to-3d (much faster than Pnp) and can find the matching. Then, for the following frames, you keep track of the points and apply standard pnp on them.",
        "The difficulty is that they are all IR LEDs and in a 3 dimensional grid pattern. Tracking them isn't viable since I need positional updates at 120+ fps. Heuristics and SLAM aren't viable either since they can be in any orientation. I might try some IR+UV combinations to try to differentiate every point. Thank you for your help.",
        "Do you have a picture or a schema of what you are trying to track?\nIf some of the leds are in a simple geometric structure (lines, circles, ...), you can use projective properties to reduce the number of candidates to evaluate.\n\nDoes the leds move? if yes, is it rigid or deformable motion? does the camera move?\nDid you consider adding an IMU to reduce the number of degrees of freedom to estimate?\n\nCan you blink the leds to identify them? I think oculus was using led blinking for identification at least in the first versions of their headset and controllers",
        "It's just 8 leds in a cube pattern. It doesn't move, fixed. Initially, since I was aiming for 100+ trackers, I thought about blinking each tracker only once a second. However, I need 100 millisecond accuracy over 12 hours, and my quartz timer deviates by 4ppm, so I have to handle cases where two overlap. IMU isn't needed since it's axially and radially symmetrical in all sides.",
        "For a 8 leds in a cube pattern, I think you can use vanishing point properties to fit the cube structure to your set of 2d points. Any parallel lines (parallel edges of your cube) in 3d will either be parallel in image space or cross when extended to the same vanishing point.\nVanishing point testing from a set of up to 8 points should be very fast.\n\nAbout IMU, you could put the IMU on your camera to have inertial prediction of the motion of your camera, it would give you an initial guess very close to the real pose and you would just need to refine the pose by using the leds. For high framerate tracking, it's even better because after an initial detection, it allows you to only process a small portion of the image because you have a good confidence of the region in which the leds are located.\n\nBlinking pattern does not need time synchronization with the source. You can make them turn off for 20ms (~2 frames) at different framerate or even time pattern.\nExample :\nwith 20ms per state :\nCube 1 :  ON,  ON,  ON, OFF, ON, ON, OFF, ON, OFF, ON, ON\nCube 2 :  ON,  OFF, ON,  ON, ON, ON,  ON, ON, OFF, ON, ON\nCube 3 :  ON,  ON, OFF, ON, OFF, ON, OFF, ON,  ON, ON, ON\nThe delay between each ON/OFF provides a unique recognizable pattern for each cube. You can adjust the ratio of time ON vs OFF, synchronize the clocks from the start of the pattern to predict when it will turn off (to avoid tracking errors,...)"
    ]
},
{
    "submission_id": "1faqq0y",
    "title": "intersection of AI and visual computing master",
    "selftext": "Hello, I recently graduated with a master's in artificial intelligence and have been hired as a computer vision engineer at the company where I completed my graduation project and it was a success. This was my first substantial experience in computer vision, specifically in the autonomous driving field, which I found fascinating. However, I realize I still lack in-depth knowledge, especially in 3D vision, as I only had one introductory course of vision fundamentals during my master's. Since I’m passionate about this field , I have a strong backgrounsd in Ai and plan to pursue a PhD, I’ve decided to pursue a second master's to deepen my expertise in visual computing and increase my chances for a PhD. I'm looking for an affordable one-year program in France, Germany, or Spain, as I feel a two-year master’s is too long, especially at 23, with big ambitions and a sense of urgency but if they don't exist I can go for 2 years master , I can’t help but feel upset, thinking I wasted years on a master's program in my country that felt unfulfilling, especially since I’m such a dedicated student. Unfortunately, I didn’t have the financial means to study abroad, which is why I stayed (non EU-citizen).",
    "created_utc": "2024-09-06T14:36:06",
    "num_comments": 3,
    "comments": [
        "You already have a masters. Apply to PhD programs this admission cycle. A second one won’t help you",
        "but I feel that I am not qualified enough to be accepted in phd mostly majority of supervisors search for students graduated from well-known universities unlike me :(",
        "Update your resume, highlight your work experience, and you’ll be okay\n\nNetwork if possible: contact the PIs directly"
    ]
},
{
    "submission_id": "1fao6hq",
    "title": "Guidance in creating higher accuracy face recognition and tracking system for my company",
    "selftext": "Hi everyone!\n\nI’m currently working on a project for the retail shops my company owns, where I need to create a system using multiple cameras to recognize employees, track them throughout the store, and log their work hours. I recently got hired as a Data Scientist, but my background is more focused on NLP, so this **computer vision task is new territory for me.**\n\nAfter doing some research, I realized I need a facial detection model, followed by a facial recognition model, and finally an object tracking model to make this work. Based on what I've found, the best state-of-the-art models for facial detection are RetinaFace, and for recognition, models like FaceNet512 and InsightFace seem promising. I’ve been using them through the DeepFace library, but they don’t perform well when the employee is far from the camera. They fail to recognize faces at longer distances.\n\nI also came across a post here on Reddit that mentioned these models aren’t great for distance, and someone suggested using KNN for recognition, which could work for distances between 1-20 meters. I’m not sure if that’s true and don’t have much experience with it. Additionally, I read that to create accurate face embeddings, I should take multiple images of each person (with and without glasses, from different angles, etc.), average those embeddings, and then use that as the baseline for recognition.\n\nI’m really confused right now and also tight on deadline. **I’d really appreciate any advice or guidance you guys can provide and help me to get through this!**\n\nThanks so much in advance!",
    "created_utc": "2024-09-06T12:48:34",
    "num_comments": 4,
    "comments": [
        ">  tight on deadline\n\nwhat, lol. Delivering something like this would take multiple person-years at my old job.",
        "Contact me",
        "IKR, it’s a lot to take on, but I still have to make progress and show something to my leads.   \nEdit: Right now, I’m just trying to figure out if it’s possible to recognize an employee when they’re a couple of feet away from the camera.",
        "check your inbox"
    ]
},
{
    "submission_id": "1fanypf",
    "title": "Face recognition",
    "selftext": "What is the most popular frameworks/models for face recognition?\n\nI have heard good things about retinaface? But the publication is from 2019 - so I am wondering if there are any other major advances in the field since? ",
    "created_utc": "2024-09-06T12:39:32",
    "num_comments": 13,
    "comments": [
        "Retinaface is for face detection and not for facial recognition, right? and for facial recognition facenet512 and some models from insightface like arcface and buffalo are top performing and all of them are state of the art models having accuracy greater than 99.7 on  LFW dataset.",
        "Dlib face recognition module is pretty good and you can give it a try.",
        "[Octuplet loss](https://github.com/Martlgap/octuplet-loss) is one of the latest adaptions to improve performance for variable/low resolution.",
        "I have used YOLOv8 model trained to detect faces and Facenet512 for embeddings. Works well for me for attendance marking purpose. Quite accurate \nFace detection model :\n• akanametov/yolo-face on GitHub",
        "Ahh I see! In the paper it mentions a combination of retinaface as part of the process!  Does it work well for faces in a distance? I tried deep face but the accuracy was not good at images several feet from the camera",
        "How does it work on images where the person is seen from distance?",
        "Cool, sounds like what I need! - will try this out!",
        "So facenet for embeddings and then similarity searchto find closest target image?",
        "yup in facial recognition model it is followed my facial detection model. and yeah the accuracy sucks when it comes to distance and if its the cctv live footage. I'm also trying to figure out what will be the best models for recognition from distance. heard we can use KNN for facial recognition",
        "And know that none of the models of insightface is valid for commercial use.",
        "Well its not that accurate on images where person is very far away. However you can try on your use case as the code is pretty simple.",
        "Yes correct. Cosine similarity or Euclidean or any other. Whichever gives you the best results.",
        "I saw some mentions of few shot learning in combination with face detection that could work, but haven’t figured out the correct way to do it so far"
    ]
},
{
    "submission_id": "1fan78e",
    "title": "Inspection System for Lead Parts",
    "selftext": "Looking to see if there is any specialized inspection system on the market that can determine if a lead burned/welded part can be inspected to know the burn was properly filled in. This isn't something that can be done purely on vision and must be some other type of X-Ray, RF, or some other technology. Any suggestions or thoughts are appreciated!!!",
    "created_utc": "2024-09-06T12:06:58",
    "num_comments": 0,
    "comments": []
},
{
    "submission_id": "1fafkvv",
    "title": "[P] Project Deepfake Detection",
    "selftext": "Hi everyone,\n\nI created a project on the deepfake detection challenge on kaggle. My notebook to the challenge is [here](https://www.kaggle.com/code/bhavay192/deepfake-detection/notebook?scriptVersionId=195502690). Please let me know of the suggestions on how to improve this. I only have kaggle GPU and memory.\n\nThanks",
    "created_utc": "2024-09-06T06:45:44",
    "num_comments": 0,
    "comments": []
},
{
    "submission_id": "1fa8lhy",
    "title": "Human pose stimation",
    "selftext": "Hello guys!\nI am trying to make a project on Human pose stimation. Happens that I am trying to stimate the 3D pose from a 2D picture.  But since I am quite a newbie, hope that my question is not dumb. \n\nWhat program do you recommend? I was giving a look to OpenPose but maybe there is a better one?\n\nIf you have any comments or suggestions I would be glad to read you! Thanks in advance!",
    "created_utc": "2024-09-05T23:33:27",
    "num_comments": 9,
    "comments": [
        "For estimating 3D poses from 2D images, OpenPose is a good choice and widely used, but you might also want to check out PoseNet and MediaPipe by Google. Both are user-friendly and can give good results for various pose estimation tasks.",
        "Are you trying to say estimation?",
        "MediaPipe. Reading the docs, 15-20 minutes, working implementation 10-20 minutes more. 2D to 3D with one camera, full body, face included, multiple bodies. Done.",
        "Check alphapose, deeppose, openpose, blazepose, smlp and stacked hourglass for pose estimation",
        "What are the evaluation metrics for 3D pose estimation?",
        "yolo pose is nice Or u can use MM pose   \nwhat type of estimations you want to perform ?",
        "Thank you a lot! I will look for those recommendations! Inreally appreciate it.",
        "Yes sir",
        "Yeah, I found some extra documentation sbout MMpose. But since I haven't found anything about Yolo pose will look for it. Thabk you!\nMostly daily actions where the target is in one position but arms and hands move a lot. Like cooking and painting."
    ]
},
{
    "submission_id": "1fa7vxv",
    "title": "Deploying Yolo V10 on PC",
    "selftext": "Hello everyone I am machine learning and Deep learning student\n\nI use google colab for most of my work. \nRecently I've been working on a project where I get to use a pc with GPU(Rtx 3070)\nThought my code works on colab it doesn't work on my pc \n\nFor basics I just need to deploy Yolov10 on the pc using anaconda (Jupyter notebook)\n\nI'm very new to machine learning or deep learning when it comes to the practical part \n\n\nAny kind of help us really appreciated ",
    "created_utc": "2024-09-05T22:45:36",
    "num_comments": 7,
    "comments": [
        "You really have to give more info when you say it works on colab but not on your PC. What does not work, how exactly does it not 'work' etc.",
        "just make a conda env and do whatever u want to   \ntraining is easy there is a scripts  \njust provide paths and parameters   \nfor using inference make sure u have pytorch installed wrt your nvidia version",
        "Either make a virtual environment or try conda, if you're using ubuntu, it would be a bit easy to setup, though not so difficult on windows either. After that check for a smooth installation of Cuda and pytorch dependencies. Rest you'll learn along the way",
        "Please post the error do that I can guide you in right direction.",
        "Can I DM you?",
        "I've been having issues with anaconda too\nCan I DM you?",
        "yeah sure hop on"
    ]
},
{
    "submission_id": "1fa6cbj",
    "title": "Looking for an LLM/Vision Model like CLIP for Image Analysis",
    "selftext": "Hi , I'm using CLIP to analyse images but looking for better options for these tasks:\n\n1. Detecting if there's a person in the image.\n2. Determining if more than one person is present.\n3. Identifying if the person is facing the camera.\n4. Detecting phones, tablets, smartwatches, or other electronic devices.\n5. Detecting books, notes.\n\nAny suggestions for a model better suited for this type of detailed analysis? Thanks!",
    "created_utc": "2024-09-05T21:11:06",
    "num_comments": 10,
    "comments": [
        "this isn’t what an llm is used for",
        "How about using 2nd detection model which can handle all the detection you want on the image you can retrieve using CLIP?",
        "How about using sam for object decomposition from the image and then classify using clip , Sam gives class labels also for the segmented objects, so you can use clip for the objects which Sam didn't gave labels or for the objects for which you need more specific labels.",
        "At least for some of your goals grounding or open world detectors are what you are looking for.\nCheck out GroundingDino/GroundingSam, YoloWorld, GLIP, etc.",
        "You can try llava one vision model on hugging face. Not sure about facing the camera but I’ve had good experience using it to count people\n\nhttps://huggingface.co/lmms-lab/llava-onevision-qwen2-7b-ov",
        "Sounds like LLaVA could be a great fit. [https://huggingface.co/docs/transformers/en/model\\_doc/llava](https://huggingface.co/docs/transformers/en/model_doc/llava)",
        "I often use LMMs for such tasks. I’d suggest you prototype your prompts and desired outputs on LlaVa-Next (7b parameters). Then you can compare different models like Gpt-4, Calude, etc.",
        "[deleted]",
        "Literally would not.",
        "GPTv4 isn’t a fully Large “Language” Model",
        "I don't get the downvotes."
    ]
},
{
    "submission_id": "1fa270q",
    "title": "Train DETR on Custom Dataset",
    "selftext": "Train DETR on Custom Dataset\n\n[https://debuggercafe.com/train-detr-on-custom-dataset/](https://debuggercafe.com/train-detr-on-custom-dataset/)\n\nIn the previous post, we covered the basics of Detection Transformer (DETR) for object detection. We also used the pretrained DETR models for running inference on videos. In this article, we will use pretrained DETR models and fine tune them on custom datasets. We will train four DETR models and compare their mAP (mean Average Precision) metric. After getting the best model, we will also run inference on unseen data from the internet.\n\nhttps://preview.redd.it/kgiqhwt343nd1.png?width=1000&format=png&auto=webp&s=82c863b0dc1302c746e13c92bffe6c63d1e0b2f6\n\n",
    "created_utc": "2024-09-05T17:36:15",
    "num_comments": 0,
    "comments": []
},
{
    "submission_id": "1f9rdhk",
    "title": "Open-Source app for Segment Anything 2 (SAM2)",
    "selftext": "Hey everyone,\n\nI'm excited to share an open-source project we've been working on: a functional demo of Meta's Segment Anything 2 (SAM2) model.\n\n**Key Features:**\n\n* FastAPI backend running on GPU (tested on NVIDIA T4)\n* React-based frontend for easy interaction\n* Supports video segmentation\n\n**Tech Stack:**\n\n* Backend: Python, FastAPI, PyTorch\n* Frontend: React, TypeScript\n\nThe project aims to provide an accessible way for researchers and developers to experiment with SAM2. It's a work in progress, and I'm actively seeking contributors to help improve and expand its capabilities.\n\nYou can find the project here: [https://github.com/streamfog/sam2-app](https://github.com/streamfog/sam2-app)\n\nI'd love to hear your thoughts, suggestions, or any questions you might have. Feel free to check it out and contribute if you're interested!",
    "created_utc": "2024-09-05T09:56:44",
    "num_comments": 5,
    "comments": [
        "Would be nice to have some images of the interface in README",
        "Looks nice!  \n  \nI would propose some kinda documentation how to run it locally with local vidoes! \n\nNow its little hard to just run locally and try it out!",
        "Thank you. is it possible to create a docker container?",
        "That’s a good point, I’ll add that right away.\n\nEdit: done",
        "There is a readme file in the repository explaining everything step step by step"
    ]
},
{
    "submission_id": "1f9rcs7",
    "title": "iOS computer vision question",
    "selftext": "Hi - I'm new to computer vision, but broadly familiar with machine learning and iOS programming. Looking to put an object detection embedded in an app (and not call an external hosted API). I know how to train and convert various models to CoreML.\n\nHowever, how do most people do the pre-processing of the pipeline (say image normalization, grayscaling and histogram normalization) in Swift so that it would match the same preprocessing as the training set? My current best guess is to port OpenCV into a Swift package, but not sure if that is the best practice approach. In a related question, also would like to use SAHI - is the best approach here just to code it up myself? Thanks!",
    "created_utc": "2024-09-05T09:55:59",
    "num_comments": 1,
    "comments": []
},
{
    "submission_id": "1f9qljk",
    "title": "The fact that sony only gives out sensor documentation under an NDA makes me hate them so much.",
    "selftext": "People resort to reverse engineering for fucks sake: https://github.com/Hermann-SW/imx708_regs_annotated\n\nSony: \"Oh you want to check if it's possible to enable HDR *before* you buy? Haha go *fuck* yourself! We want you to waste time calling a salesperson, signing an NDA, telling us everything about your application(which might need another NDA), and then *maybe* we'll give you some documentation if we deem you worthy\"\n\nFuck companies that put documentation behind sales reps.\n\nI mean seriously, why is it so fucking hard to find an embeddable/industrial camera that supports HDR? Arducam and Basler are just as bad. They use sensors which Sony claims to have built in HDR, but do these companies fucking tell you how to enable it? Nope! Which means it might not be possible at all, and you won't know until you buy it.",
    "created_utc": "2024-09-05T09:24:56",
    "num_comments": 50,
    "comments": [
        "Yeay I hate companies that do that. If documentation is only available under NDA, or requires logging in and endless form filling it's just a barrier to actually building things with the product.",
        "since I work in this space, ill tell you, even the BIG organizations, like Basler, dont get full access to Sonys info. \n\nAVT for example has to do imperical testing of all their sensors to get SNR, DNR, etc, and theyre one of the first to get Sonys newest sensors. \n\nSony doesnt really GAF unless you're talking millions of units. \n\nAnd of course, just because Sony has the feature, doesnt mean it's implemented by the camera manufacturer. \n\nsend me a message and I can see if theres anything I can help with.",
        "I wouldn't be surprised if hiding behind the policy was ITAR, it's amazing how many dual-use technologies are out there.",
        "I found out the best way to get sensor documentation is to ask an Alibaba seller that's selling the sensor. They'll usually give you full access.",
        "A few things to unpack here, having worked on both the sensor and camera side. Overall you're hitting on a part of the industry I wish was easier, but I can say it's not for lack of trying.\n\nLack of access to sensor documentation is definitely a struggle for many customers, but from my first hand experience even motivated large customers struggle to get these HDR modes working in a way that benefits them so there are hurdles put up to prevent us from wasting each other's time.\n\nThe way Sony approaches HDR in most of their cameras is hands off and pretty much requires you to work with an SoC vendor who's ISP supports that sensor. It's not such a general feature that can be supported across the board by MV camera vendors. Systems leveraging these ISPs and HDR typically require external characterization of the camera with the intended lens, and this doesn't align with the flexibility required in MV cameras using standard lens mounts and whatever the customer wants to throw on there.\n\nYou should look at lucid's imx490 based camera, that is a unique implementation with onboard tone mapping.\n\nAlso remember that most machine vision camera business is monochrome, color processing (especially onboard) was historically not a priority.\n\nMost of these cameras support a genicam sequencer for HDR, where you can rapidly switch between sets of exposure and gain settings. It still has a time delay, but it's something implemented across most global shutter sensors.\n\nMV cameras are not just passthroughs of sensor settings,  they're more like wrappers of the sensor that takes a lineup of 30+ different sensors and standardizes it into an interface like gige vision/genicam so that something like changing the exposure time is one command that changes many registers and timings.",
        "Basler is the best camera company in this regard. they have an open source python API on github, and their detailed documentation is published and nicely organized on their website. But for some fucking reason they list HDR under  their custom-software section of their website. Fucking what!? A feature that's allegedly built into the sensor by sony needs *custom software* to unlock?? \n\nAnd basler doesn't even carry the latest and greatest sensors. All of their sensors are half a decade old at this point, and they refuse to build cameras using smartphone sensors from sony because it's a \"different application\". Every application needs high quality images! Arducam carries the latest and greatest sensors, but their API is abysmal with the bare minimum documentation to get started. They don't even tell you the units of the settings you're adjusting!\n\nAnd don't even get me started on the obscure dark magic that is the ISP. The ISP is as important to overall image quality as the lens and sensor, and yet you will find literally 0 documentation from anyone anywhere, and everyone seems to have their own proprietary ISP because fucking apple, qualcomm and samsung don't fucking sell their ISP to other companies(or something like that).\n\nThe camera industry is so ripe for disruption it's insane. Raspberry Pi and arducam seem to be in the best position to dominate the entire non-DSLR camera market, but holy shit they have a ways to go. Qualcomm could be a huge juggernaut in this space, but from the people I've talked to they simply have no interest in the market, despite the fact that raspberry pi has sold millions of cameras.",
        ">even the BIG organizations, like Basler, dont get full access to Sonys info.\n\nThat's insane, I had no idea the problem was that bad. Are there any better sensor manufacturers than sony?\n\n> And of course, just because Sony has the feature, doesn't mean it's implemented by the camera manufacturer.\n\nWhy is this though? Surely it's not hard to just provided an interface?\n\n>send me a message and I can see if there's anything I can help with.\n\nWhere can I find an off the shelf, software controllable camera, that has all of the software magic that make smartphone cameras so dang good for their price? Features like HDR and stuff. I basically need the highest line pair per mm(or whatever metric to measure detail and sharpness) per dollar, that's color accurate and basically just looks great.",
        "It makes sense to test in-house for big corps even if Sony did give out the specs, just for verification.",
        "Sony has the SensSWIR sensors, that are like that. They have to fill out a form from Sony when you buy them that you will not sure them to harm humans. Lots of stuff like that when you get into these things as they easily fall into military apps and Govt programs where, if youre selling to one govt and the other govt finds out, you sold them even 'accidentally' youre going to lose a contract worth Millions. \n\nGlobal companies, how do they even run?",
        "Actually good to know, will try that. ",
        "> All of their sensors are half a decade old at this point, and they refuse to build cameras using smartphone sensors from sony because it's a \"different application\". Every application needs high quality images! Arducam carries the latest and greatest sensors, but their API is abysmal with the bare minimum documentation to get started.\n\nI guess that's exactly what Basler mean with \"different application\". You can get the sensors which can be fully controlled and where you can adjust every bit, but most of them are old. Or you can get the fancy modern stuff, but nobody has a clue on how you can control anything aside from \"give me an image\".",
        "> Are there any better sensor manufacturers than sony?\n\nNo, the other are far worst. Especially if it's about documentation. Sony is the best what you can get.\n\n> Surely it's not hard to just provided an interface?\n\nIf it was easy, then the manufacturer would do so. It would be a good USP over other manufacturers and thus they would implement it. So I guess it's not that easy.\n\n> that has all of the software magic that make smartphone cameras so dang good for their price?\n\nThat the problem. Basler and Co are making camera for industry business customers. They don't want any of this software magic what smartphones do. This would make the images unusable. And thus Basler and Co want implement any of such features. Additionally industry cameras are using fpga for their image processing and fpga are not really good for such \"magic\".\n\nBut I think HDR will soon find it's way into industral cameras and then you can get this feature from several manufacturers.",
        "oh sure, the sensor is 90-95% of the spec, Im just saying, even big corps, building these sensors into 100K cameras per year dont get basic stuff like 'dynamic range' measurements from Sony.",
        "I got access to the full 156 page datasheet for the IMX299CJK-C this way, not sure if other sensors will be more difficult. Good luck!",
        "But isn't it just a pass through API that interfaces with the sensor API? ",
        ">They don't want any of this software magic what smartphones do\n\n\nI'm an industry business customer. I want those features. \n\n\n>This would make the images unusable.\n\n\nThat makes no sense. Does AWB make the images unusable? It's the opposite for us. Basler's proprietary AWB algorithm is what makes their cameras better than everyone else's that we tested. You can also turn it on and off. \n\n\n>Additionally industry cameras are using fpga for their image processing and fpga are not really good for such \"magic\"\n\n\nThat's why they should just buy a Quallcomm ISP instead of reinventing the wheel. The Qualcomm ISP has everything. \n\n\n>But I think HDR will soon find it's way into industral cameras and then you can get this feature from several manufacturers.\n\n\nHopefully",
        "Kodak USED to be amazing. they had Everything documented and laid out in clear text. but they got lazy and then got bought out, twice. now they're under the OnSemi umbrella and, well thats not going well.",
        "🤣\n\nNo, not at all. The controls you have as an user over the camera, don't have anything in common with the way how the camera controls the sensor. And as long as you don't use mipi, the output of the sensor is also completely different compared to the data you receive on your host pc.\n\nThe sensor API is a list of hundreds of register addresses and then a few pins for triggering a exposure and a readout.",
        "An AWB is not the kind of magic I think about. And as you say, you can disable it. The iPhone magic is always on and you can't configure it as you like.\n\nWhat kind of feature are you missing?\n\n> That's why they should just buy a Quallcomm ISP instead of reinventing the wheel. The Qualcomm ISP has everything. \n\nAnd on what hardware should this ISP run? And who should pay the high license costs for this ISP?",
        "I have the explain this to customers all the time. \n\nPretty pictures to people, are not pretty pictures to machines. \n\nif youre building a device that looks good to a pair of eyes, its probably not the best image for a machine to tear down and understand. I work in the tear down and understand side and a lot of our sensors are better for THAT. Pretty pictures (HDR, tone mapping, AWB) dont help and break small pixel changes that machines look for to do Their jobs.",
        "Ok so how would someone like basler turn on and off the sony features built into the sensor?",
        ">  An AWB is not the kind of magic I think about.\n\n\nThere's a ton of different AWB algorithms, where some work better than others. Auto exposure, auto gain, auto focus, HDR/ image stacking, image stabilization, denoising, sharpening, saturation adjustments, it all has to work together to produce the best image possible.\n\n\nWhat kind of magic are you talking about?\n\n\n>The iPhone magic is always on and you can't configure it as you like.\n\n\nRight, because it's a phone. But there's nothing stopping apple from selling their camera system stack that allows the user to adjust all of these things.\n\n\n>What kind of feature are you missing?\n\n\nMainly all the computational photography stuff like HDR. It improves image quality so much. \n\n\n>And on what hardware should this ISP run?\n\n\nQualcomm hardware, Sony sensor, basler API",
        "Machine vision applications with hyper controlled environments and simple subjects might not benefit from these features, but computer vision application do. For instance, HDR can increase sharpness and can see through glare caused by stretchwrap. AWB ensures color accuracy stays the same even as the sun changes, or overhead lights turn off and on. Also humans have to label our data to train our models, so if the image looks better for them, that means it's easier for them to tell which class is which(our classes are complicated and subtley different). \n\n\nSo many computer vision applications are built on the assumption, \"if humans can do it, so can AI\". Therefore the ideal camera is the human eye. If we had the human eye in a box, everyone in the world could just buy that because it would work for 99% of problems.",
        "Write the correct sensor registers with the correct values.\n\nAbout what kind of features are you thinking? What features are missing?",
        "> What kind of magic are you talking about?\n\nOkay, I was also talking about this kind of feature. But then I don't see the problem, most of the features are already implemented in industrial cameras. Some are taking their time (HDR) and others are technically difficult to implement (stabilisation), but it's getting there.\n\n> Qualcomm hardware, Sony sensor, basler API\n\nThe why not get a Basler camera, put the images in a Qualcomm hardware and get your missing feature. Or implement the feature yourself on your PC. You can get many of the algorithms even for free.\n\nMost customers are using only a Basler camera and make all further image processing on their system.",
        "HDR does not increase sharpness, full stop. it can help control brightness range for within an 8 bit monitor, but again, machines dont care about that, and work fine with 16 bit images if the dynamic range requires it. Every note above is 'AI' based but AI still works fine if you train the images on MV images, the 'human eye in a box' is basically an iPhone and for $1000 you can certainly buy those and export the images over USBC so I dont think that the solution you want either.",
        "> About what kind of features are you thinking? What features are missing?\n\nMainly HDR",
        "> Or implement the feature yourself on your PC. You can get many of the algorithms even for free.\n\nBecause at that point we may as well build the rest of the smartphone. The tech already exists, we just need someone to package it so we can buy it off the shelf.",
        "As no manufacturer offer currently HDR, I suspect that the technology was not yet requested for industry cameras, or the sensor manufacturer didn't prove enough information to implement the feature.\n\nBut as you yourself found out, that it's already available as a custom feature, I'm sure that it will be become a standard feature. Well at least if other customers aside from you, also request that feature.\n\nHave you contacted Basler, maybe you can find out, if or even when they could implement HDR. Maybe you can also buy the custom cameras, if their website lists the feature, maybe you can also get such a camera.",
        "I'm sure that, depending on how much you are willing to buy, Basler will build you the whole system.\n\nIf you pay enough, they could even implement HDR.",
        "Yeah I contacted basler. All the stupid US-based sales rep said is \"sony supports sensors for two decades, therefore our 8 year old sensors are actually not old at all.\" And then linked me to the camera feature documentation page because I asked how to enable HDR(I don't think he released it's not supported). I gave up after that. I'm not going to waste my time trying to convince them that if they build better products, they'll probably get more customers.",
        "IDS has HDR support for some Sony sensors. but no one uses it because again 'magic' means you cant rely on it for machine vision apps. \n\n[https://www.ids-imaging.us/files/downloads/support/knowledgebase/techtip/TechTipp\\_3WaysToHDR\\_EN.pdf](https://www.ids-imaging.us/files/downloads/support/knowledgebase/techtip/TechTipp_3WaysToHDR_EN.pdf)",
        "The point of buying off the self is that it's cheaper than custom. I just don't understand why camera companies like basler don't just buy the camera system stack from smart phone manufactures, put it in a rugged form factor with a user friendly API, and then sit back and collect the money. There are so many small computer vision companies who get better quality images from their phones than any industrial camera And we're all so desperate for a smartphone camera in a box, that we resort to buying arducams lmao. ",
        "Do that.\n\nAnd I agree with the sales rep. 8 years old sensors are not old. At least not in the market, where customers buy a MV camera and then use the camera the next 10 years in their application. New sensors often offer nothing more than a higher resolution and sometimes a new feature. And higher resolution is often detrimental for a camera. High resolution cost you fps and you need far more powerful host PCs to process the images. So why buy a 4k camera, if your application only needs a HD image.",
        "ill be honest, most of the sales people from the companies are more interested in OEM sales, where they will have a team of people to figure this out. you need to contact a smaller reseller who will actually look at your system, and have background in the industry. so many of the Manf reps are just young 'hungry' sales guys looking to make 10% on the next million dollar order.\n\nI should add Basler is likely the worst offender at this too. Dalsa and their umbrella, PT GREY/FLIR  used to build custom FW for univ if they asked. AVT is very supportive and has a LOT of embedded products (GMSL, FPDlink, CSI2/MIPI). Basler is the 900 lb gorilla and moved like a cargo ship. they are NOT the ones to ask for cutting edge features lol",
        "It depends on what image quality metric you’re talking about. A post-processed smartphone image might look really good to the human eye but if what you care about is say absolute sensitivity or high SNR combined with a certain dynamic range you’ll get a better image from a MV camera. \n\nPlus because of customer requirements MV cameras are typically designed to meet certain realtime specs such as guaranteed image delivery within so many ms after triggering. Smartphone camera systems prioritize post-processing over timing and tend to have high and unpredictable image capture latency and jitter which makes them unsuitable for a lot of MV applications. Even high-end Sony digital cameras often suffer from this issue because it just not a design requirement for them.",
        "u/[gordster93](https://www.reddit.com/user/gordster93/) mentioned already 2 good reasons against it, and there are many more.\n\n\\* smartphones used CPU based ISP pipelines. This is fine for process single images, but they are bad for video processing. A CPU can only process one image after another and has to finish the processing of an image before another one can be taken. This limits the possible frame rate. That's why you get nice 4k phones out of a smartphone, but only HD videos. FPGA used in MV cameras have a pipeline for image processing and thus can process videos at the same speed as single images.\n\n\\* price: To be able to process somewhat high resolution videos, you will need a powerful CPU. Smartphones need such a CPU anyway, so the high price is not as bad. But for MV Cameras, the CPU would be a massive factor. A CPU based cameras would be far more expensive. And are you willing to pay 2-3x as much money for a camera, when the competitor offers nearly the same feature?",
        "If you want a smart phone built for you, go buy a smart phone and take out what you dont need. it would likely be cheaper than trying to build up what you need because smartphones, and other chips like that are built in the Millions and the cost of scale doesnt work when your 1/100 or 1/1000 of that market. which is why Sonys biggest industrial buys are still pittance compared to smart phones. thats why they stopped CCD production. It was globally like 200K sensors a year, compared to 25M per quarter in smart phones.",
        "exactly - I just replaced a camera that debuted in the early 2000s. it just failed this week. the LTB was actually last Friday. and only because it was a Sony CCD that had been out of production since 2018.",
        ">8 years old sensors are not old.\n\n\nEven if it's two generations old and there exists a newer drop in replacement for an older sensor?\n\n\nMy application needs 300MP total, extreme low light performance, high color accuracy and I need to see through stretch wrap in a dynamic lightning environment.  I basically need the human eye in a box. Many computer vision applications are predicated on the assumption that \"if humans can do it, so can AI\"",
        "10000% this.",
        "And I forgot to mention that global shutter sensors are usually required for MV applications since they're capturing targets in motion. Smartphone cameras are exclusively rolling shutter sensors.",
        "And the other issue is reliability. There is a reason basler cameras can work 24/7 for... decades. \n\ntry to find a smartphone chip that can run, 100%, for months, years. They're designed to be disposable because no one keeps a phone for more than a few years at most.\n\nI just replaced a camera this morning, that was installed in 2012. maybe before.",
        "Which sensor would you like to use? And how many fps do you need?",
        "Oh yeah. I forgot the reliability.\n\nAnd not only reliability but also reproduceability. You want every of the cameras to produce the same image. When you replace the camera, you don't want to replace your algorithm, just because the new camera has a slightly different image.",
        "It's not that I need a particular sensor it's just that I noticed basler sells the IMX 334 which is 8 years old and two generations behind, despite the fact that the IMX 678 is a drop in replacement for it, and simply gives better images due to the better architecture. \n\n\nAnd for us, FPS doesn't matter because we can use a dumb high FPS camera with object detection to trigger the high quality cameras. ",
        "Yup, I was working with one of the biggest aerospace companies, and one of their Big problems was 'if we replace This camera with the Same model, in 2 years, will we need to make any adjustments?'",
        "Yeah, the imx334 is a little bit old. I hope there is will be an adequate replace available soon.\n\nHave you considered to do the HDR yourself? Take two images with different exposure times and then calculate the HDR image yourself. Sony and the manufacturer are also just doing the same."
    ]
},
{
    "submission_id": "1f9ptf8",
    "title": "UK Police Using Facial Recognition",
    "selftext": "",
    "created_utc": "2024-09-05T08:52:59",
    "num_comments": 7,
    "comments": [
        "There is a lot of potential for using AI in law enforcement, but this isn't it.",
        "Will it prevent crimes? Will it help police catch criminals? If the answer is yes then - great!",
        "Will it lead to the arrest of innocent people based on visual profiling? If the answer is no, great!",
        "I get the big brother vibes, but on practice nothing wrong with automatic real time surveillance systems. *It's already done by human workers* watching monitors, I say let's automate this.",
        "I’m good with automating aspects of it, but until AI can learn to judge without bias (not biased by frequencies or other data artifacts), human judgement should also be in the loop. I don’t want to get arrested or ticketed for a judgement call made solely by AI that may not have full context of the situation.\n\n\nFor example, if I am dumb and miss a scan one of many items when at the grocery store i don’t want to be arrested for shoplifting when I would have happily paid for that item. I don’t want people arrested for loitering when they may have a legitimate purpose to be waiting somewhere. I don’t want curfew to be strictly and automatically enforced when there are many exceptions to most curfews.\n\n\nAdditionally, making surveillance more cost effective will allow for more strict policing - more strict than society is likely to welcome. We need to be careful socially how these applications are not only developed but conceived and deployed.",
        "I'm not sure I understand how your examples related to facial recognition system, which is not an *AI law enforcer*.",
        "For example, presence of a face in a location could alone be utilized to enforce a curfew (which demands people not be in those locations). While curfew aren’t commonly used today in the US, if it was more cost effective businesses and government may be more tempted to make these AI-enforced laws"
    ]
},
{
    "submission_id": "1f9p1lt",
    "title": "Survey white paper on modern open-source text extraction tools",
    "selftext": "I'm working on a survey white paper on modern open-source text extraction tools that automate tasks like layout identification, reading order, and text extraction. We are looking to expand our list of projects to evaluate. If you are familiar with other projects like Surya, PDF-Extractor-Kit, or Aryn, please share details with us.",
    "created_utc": "2024-09-05T08:21:03",
    "num_comments": 3,
    "comments": [
        "Someone posted his solution his team wrote [here](https://www.reddit.com/r/LocalLLaMA/s/4MecFTL737)",
        "Paddle OCR with pp structure",
        "Thank you, reviewing."
    ]
},
{
    "submission_id": "1f9nmwf",
    "title": "NSFW describer images?",
    "selftext": "Please, gentlemens, can you suggest me a bot that describes NSFW pictures? It seems like ChatGPT has a similar program, but one that can describe such pictures. I'm just tired of searching for such nonsense on Internet, because I simply can't find anything similar, I'm exhausted. What's the problem? You see, for some reason mine Stable Diffusion bad work with LoRA and often screws up with them, and neural network has intelligence of a 3-year-old child, who can't even put 2+2 together on his own, everything needs to be explained to him word by word, and without LoRA it's quite problematic, so I wouldn't mind a program for describing NSFW pictures, because for other pictures I can use ChatGPT. Can you suggest one, I'll be grateful.",
    "created_utc": "2024-09-05T07:18:35",
    "num_comments": 1,
    "comments": [
        "LLaVa: https://github.com/haotian-liu/LLaVA\n\nI can’t remember if there are any open source tools better than LLaVa out there at the moment"
    ]
},
{
    "submission_id": "1f9ltxe",
    "title": "Having trouble back propogating a convolutional layer",
    "selftext": "So I'm currently working on my machine learning library in rust. As of now the only problem is the back propogation for the kernels.\n\nWhen I checked, the delta weights for the kernels were returning values above 1k which was confusing. \n\nI calculated the gradients by doing a convolution between the inputs and the calculated gradients from the next layer. This is based from The Independent Code's video on CNNs and other sources i found online.\n\nOthers say I should just multiply each index of the gradient matrix by the inputs which would have been affected by the kernel.\n\nOthers also said i should perform the convolution between the inputs and the gradients but i should transform the gradients into a spaced array?\n\nI need help...",
    "created_utc": "2024-09-05T05:58:29",
    "num_comments": 0,
    "comments": []
},
{
    "submission_id": "1f9kc72",
    "title": "I'm really confused and  wanna ur opinion ",
    "selftext": "hi , Im student of computer engineering in last year of bachelor. \n\n* I fell in love of computer vision and deep learning field especially 3D construction and worked with photogrammetry. I just finished reading book of \"vision systems for deep learning by Elgendy\" book except the GAN thing. \n* Now I'm frustrated and confused between many things to do:\n*  first to learn computational geomtery and read book of marc de berg or to complete reading \"Deep learning foundations by Christopher bishop\" as deep learning is a trend right now in market or to complete reading \" Computer vision by szeliski\" or to study CUDA C++ or GPU programming as I love high and violent performance and optimizing.\n* Which is more worth to do relative to my case ? I have a free month of college and wanna utilize.",
    "created_utc": "2024-09-05T04:42:23",
    "num_comments": 8,
    "comments": [
        "I'm not sure of your purpose that you wanna go into research or industry. My personal opinion is don't go for learning each and every basic requirement like computational mathematics, cuz that is part of the algorithm building process. Just have an overview of concepts like that and you'll be fine.",
        "I’m very much like you in my approach to learning. I’d recommend doing a little bit of reading every day, maybe one section from each book. Give yourself time to really reflect on those ideas.\n\nThe rest of your time is well spent building actual projects. Follow tutorials that interest you, and once complete find some way to make it your own. That could be using a different dataset, or neural network, or it could mean using that technique in a mobile application or personal robotics project.",
        "I’m a big fan of learning by doing. \n\nDive into the CUDA C++ and GPU programming. Try different techniques and try to improve the performance. The most important thing is to go off-road; get away from tutorials. Think of a project that you don’t know how to do and figure it out. \n\nI remember a young circuit designer that I worked with years ago. He had an impressive resume with straight A’s in all of the design classes, but he couldn’t design the simplest circuit from scratch. Don’t be that guy.\n\nChallenge yourself with problems you don’t know how to solve.",
        "Deep learning wouldn't hurt just to lift the veil to see what's under the hood. Idk about the learning material you mentioned but my all time favorite would be Andrew Ng coursera course called machine learning. He is one of the \"pioneers\" and has good material. That shouldnt take long, and you'll see if you want to dive deeper.",
        "The papers these days are just growing too quickly and very little is being synthesized into books or courses. When my company does tech talks to universities, often we find that maybe two professors out of the group are keeping up in the area. Moreover CV is rapidly becoming applied and applied work is always more practical in industry. Best to find a real area and deep dive. \n\nIf you have the time drop me a DM and we can discuss more",
        "Leaving a comment here so I remember to come back for answers.",
        "industry actually , and yea that is a problem with me, like that OCD to learn extremely deep math for everything , and thanks",
        "yea that is amazing ... really thanks"
    ]
},
{
    "submission_id": "1f9karv",
    "title": "Removing Watermark from images",
    "selftext": "I am working on a project that involves face verification using models like DeepFace, but the images I have contain watermarks across the faces, which significantly reduces the model's accuracy.\n\nHas anyone dealt with a similar situation? How can I go about removing or working around the watermarks to improve verification accuracy without compromising the integrity of the images? I'm exploring various options, and any suggestions, tools, or techniques that could help would be appreciated.",
    "created_utc": "2024-09-05T04:40:14",
    "num_comments": 1,
    "comments": [
        "Removing watermarks is illegal...\n\nSo use data that's either publicly available, that you created, or that you bought."
    ]
},
{
    "submission_id": "1f9jnqy",
    "title": "YOLO-NAS optimisation",
    "selftext": "I'm working on a computer vision project and have been playing around with yolov10n. When I'm running predictions on a video using the yolov10n model, my machine handles it fine and runs in realtime.\n\nI'm experimenting with YOLO NAS S (from scratch, not pretrained) and it's an awful lot slower probably 3fps making it difficult to use. I train models using colab then run tests through my own machine.\n\nMy GPU isn't great, but I can only work with what I have and I don't have money to get anything better. It's a Nvidia GeForce GTX 1650 with Max-Q design. I'm using cuda acceleration for tasks I'm doing through my own machine when I'm not using Google colab.\n\nI was wondering if there's any good resources out there where I can learn any techniques to improve performance on Nas models when running predictions. I see a lot of resources for yolov8 etc but not much out there for NAS, unless I'm looking in the wrong places.\n\nThanks in advance",
    "created_utc": "2024-09-05T04:03:42",
    "num_comments": 5,
    "comments": [
        "Did you convert to TensorRT? Their PyTorch inference is slow",
        "Fixed issue\n\nEdit: Needed to install onnxruntime-gpu rather than onnxruntime",
        "I'm trying now. Followed a guide to install tensorRT but when I try to convert it says \"cudaexecutionprovider\" not available. But I think you're right, tensorRT is the key to my problem",
        "Would be nice if you shared the solution in case someone in the future finds this thread",
        "Understood, will do that"
    ]
},
{
    "submission_id": "1f9jhhi",
    "title": "Transform Bounding Box 3D to oriented Bounding Box",
    "selftext": "Hi everyone!\nI am currently working with Isaac Sim and can generate data with Bounding Boxes (BBs). Isaac Sim has a method that automatically annotates objects, but the BBs it generates aren't optimized for my situation. An oriented bounding box (OBB) would be more helpful in resolving the issue I am facing.\n\nHowever, Isaac Sim can only annotate using normal BBs or 3D BBs; it doesn't support OBBs. After searching online, I found some potential methods to transform a 3D BB into an OBB. I tried them, but they were not successful.\n\nDoes anyone have suggestions on how to calculate an OBB? The output format of the 3D BB in Isaac Sim is shown in the attached picture. ",
    "created_utc": "2024-09-05T03:53:41",
    "num_comments": 15,
    "comments": [
        "Does the heading matter in your case? For example if the orientation is either 0 degree or 180 degree would you be fine with either?",
        "Do you want neural network to work on oriented bounding boxes or are you asking about how to design oriented bounding box class?",
        "I don't really understand what you mean. \nHowever, my object is somewhat rectangular, so it looks pretty much the same in different positions. But when the orientation is around 45 degrees, there is a lot of space between the bounding box and the object. That's why I need an oriented bounding box!",
        "I want to know how I can transform a 3D Bounding Box to an Oriented Bounding Box. The problem is that we are working with Isaac Sim, and it provides some annotators. However, we want the Bounding Box to fit the object accurately, so we can't train YOLO with a regular Bounding Box; we need an oriented one.\nWe are trying to automate this process, so we don’t have to annotate each image manually. The issue is that Isaac Sim doesn't provide annotators for oriented Bounding Boxes or segmentation annotations.\nThe last idea I could think of is to transform the 3D Bounding Box into an Oriented Bounding Box because I have the x, y, z coordinates and the transformation matrix of that 3D Bounding Box, but I can't make it right.",
        "Ah sorry for the confusion. I still struggle to under what you mean, do you have picture?",
        "Have not worked with Isaac Sim, and the comments/descriptors for each field are cut off. My assumption would be that the BB coords are supplied on local coordinates (basically just defining the aspect ratio/scale of the object), and you can get the OBB by applying the transform to it (into world). \n\nIf the above is true and your tests still don’t look good, double check these:\n\n1. The transform states local to world. In many cases there is more than two frames involved (e.g. object, camera, world). Do try different permutations. \n\n2. Conventions differ depending on software. Make sure the transform is a 4x4 matrix (top 3x3 being rotations, right most 3 terms being translations — check units!!, bottom row being 0001). Append ones to the BB 3D corner points, then try matrix multiplications in different orders.\n\n3. If you’re inspecting via a 2D projection (camera, image), check the camera conventions too when projecting from world to camera.",
        "https://de.farnell.com/en-DE/marathon-special-products/mik5/terminal-block-din-rail-22-10awg/dp/1530092\n\nMy Object looks pretty like this! I need to train a yolo Model to detect this object",
        "If I understand correctly, you want to have better fitted bounding box than bounding box with planes parallel to main plains (XY, XZ, YZ). Your first idea was to use YOLO which can get you not oriented bounding box. In that case, you can try to add more info your YOLO network output: instead of position + size you'd need to output position + rotation + size. For this however, you need labels and this is not trivial if you don't have 3d scans of these objects. Moreover, best fitted bounding boxes will have local minimums as function of rotation",
        "That is exactly what I want to do. The transformation matrix is 4x4, and the last row is different; it is (values values values 1). I think I miscalculated the x, y, z with the transformation matrix and didn’t define the ratio of the object correctly.\nI may need to test it again because we are working with a simulated object, not a real one, so it really depends on the program and the information it can provide.",
        "Oh I mean picture of the bounding box misaligning with the object",
        "Isaac Sim is a simulation program used to simulate environments. It can simulate objects and provide data to train YOLO, so we don't have to manually annotate objects in images—Isaac Sim will do that for us. However, for now, it can only provide a normal Bounding Box, not an Oriented Bounding Box.\nOur goal is to automate the annotation process so that we don't have to manually annotate thousands of images with hundreds of objects in each image.\nWe now need to find a way to transform the data we have into an Oriented Bounding Box.I hope you now understand the problem I'm facing, as I felt that I might not have explained it clearly before.",
        "If it’s a provided transform, one of the rows/columns will most definitely by 0001. From what you described, the right column should be 0001, and transposing the matrix should get you in the right form.",
        "Oh, you mean after the transformation? It didn’t work; the bounding box (BB) was completely wrong, so I’ve already deleted it. I tried calculating the x, y, z coordinates using the transformation matrix to find the direction of the object and the x, y surface. However, maybe the transformation matrix doesn’t account for rotation, so I can’t find the direction",
        "Oh! You are right!\nI didn't notice that, I need to transform the matrix first and then I can calculate it!\nI will test it later to see if I can find the rotation!",
        "You could have omit all the story and ask about invalid transformation matrix ;p"
    ]
},
{
    "submission_id": "1f9hqe5",
    "title": "T-Rex2 open source alternative?",
    "selftext": "Hi! \n\nI came across this very cool repo: [https://github.com/IDEA-Research/T-Rex](https://github.com/IDEA-Research/T-Rex) but it's closed source and not something that I can run on the edge. I am curious, anyone tried reproducing the results or are anyone familiar with an alternative?\n\nWhat I really like about T-Rex2 is that it allows both open-set detection as well as extrapolation of bounding boxes within (or in between) images. I am familiar with models like Grounding DINO, Grounding SAM,  YoloWorld, and the like but they don't quite cut it for me.\n\n  \nAny input would be much appreciated 🙏",
    "created_utc": "2024-09-05T01:51:44",
    "num_comments": 1,
    "comments": [
        "hey,\nnice to see others thinking about similar things l!\nI am also trying to reproducing closed-source models, like T-Rex or the SAM online tool. so if you find something let me now :)\nI also be interesting to see if anyone has tried fine-tuning models like Grounding DINO or SAM for more exotic domains. also how to create a decoder from scratch using a existing dataset..."
    ]
},
{
    "submission_id": "1f9hnvv",
    "title": "Easier way to set up FairMOT",
    "selftext": "I am having hard time installing fairmot from [its repository](https://github.com/ifzhang/FairMOT).\n\nHere are steps I followed, difficulties I faced and the solutions are tried:\n\n1. Following steps specified in the README page of repository gives following error while running `make.sh` inside DCNv2 repo:\n\n```\n        RuntimeError: CUDA error: no kernel image is available for execution on the device\n```\n\n2. [Seems that this is the issue with incompatible CUDA toolkit version](https://stackoverflow.com/a/75702229/6357916). The official installation installs CUDA toolkit 10 with python 3.8 and pytorch 1.7. So I decided to install higher CUDA toolkit version that also supports these versions of python and pytorch. I tried CUDA toolkit 11.8:\n\n```\n        conda install pytorch torchvision torchaudio pytorch-cuda=11.8 -c pytorch -c nvidia\n```\n\n    I was also able to install requirements.txt from fairmot repository. But while running `make.sh` inside DCNv2 repo, I got error `TH/TH.h: No such file or directory`. Turns out that this version of DCNv2 is not maintained and had to install updated version of DCN2 as [stated here](https://github.com/CharlesShang/DCNv2/issues/136#issuecomment-1505217589). Trying to build `DCNv2_latest` at least does not give CUDA error in point 1. It gave some other errors, which I fixed. But now it gives:\n\n```\n         \n        /home/perception3/miniconda3/envs/FairMOT_py38_cuda118/lib/python3.8/site-packages/torch/include/c10/util/complex.h:8:10: fatal error: thrust/complex.h: No such file or directory\n             8 | #include <thrust/complex.h>\n               |          ^~~~~~~~~~~~~~~~~~\n        compilation terminated.\n```\nThe [comment here](https://github.com/pytorch/pytorch/issues/72918#issuecomment-1042447762) says that CUDA toolkit packages did not contain thrust-related files.  Installing correct version of CUDA toolkit seem to have solved problem for him. However am not able to understand which version of CUDA toolkit should I install that will also be compatible with above setup as well as my hardware.\n\nApart from that I also tried to build NVIDIA thrust from [its source](https://github.com/NVIDIA/thrust), but gave different errors (I pasted it on [pastebin](https://pastebin.com/3zntY5YG)).\n\nI also tried installing FairMOT on docker images for CUDA 10 and 12 and also with CUDA 12 on conda. But I keep getting errors (most of them are same as above).\n\nIs there any way to make FairMOT work easily? What I am missing here?\n\nBelow is output of my `mvidia-smi` for hard ware information:\n\n```\n+-----------------------------------------------------------------------------------------+\n| NVIDIA-SMI 550.90.07              Driver Version: 550.90.07      CUDA Version: 12.4     |\n|-----------------------------------------+------------------------+----------------------+\n| GPU  Name                 Persistence-M | Bus-Id          Disp.A | Volatile Uncorr. ECC |\n| Fan  Temp   Perf          Pwr:Usage/Cap |           Memory-Usage | GPU-Util  Compute M. |\n|                                         |                        |               MIG M. |\n|=========================================+========================+======================|\n|   0  NVIDIA RTX 2000 Ada Gene...    Off |   00000000:01:00.0 Off |                  N/A |\n| N/A   47C    P8              1W /   35W |       8MiB /   8188MiB |      0%      Default |\n|                                         |                        |                  N/A |\n+-----------------------------------------+------------------------+----------------------+\n                                                                                         \n+-----------------------------------------------------------------------------------------+\n| Processes:                                                                              |\n|  GPU   GI   CI        PID   Type   Process name                              GPU Memory |\n|        ID   ID                                                               Usage      |\n|=========================================================================================|\n|    0   N/A  N/A      2273      G   /usr/lib/xorg/Xorg                              4MiB |\n+-----------------------------------------------------------------------------------------+\n\n```",
    "created_utc": "2024-09-05T01:46:39",
    "num_comments": 0,
    "comments": []
},
{
    "submission_id": "1f9gs3v",
    "title": "Gemini giving different results for same image in OCR",
    "selftext": "So the issue i am facing is I am using vision api and everytime i run an image through it for OCR it gives me an output. But if i run the same image again it adds or subtracts something from the output. The data output is not consistent over the same image on multiple trials. Is there a fix for this solution.\n\nI tried creating image cache, didn't work\nI tried limiting the output, didn't work ",
    "created_utc": "2024-09-05T00:40:46",
    "num_comments": 0,
    "comments": []
},
{
    "submission_id": "1f90nfh",
    "title": "Point cloud meshing and texture mapping",
    "selftext": "",
    "created_utc": "2024-09-04T11:25:01",
    "num_comments": 6,
    "comments": [
        "The simplest way is probably something like marching cubes + projection mapping.",
        "CloudCompare can create a mesh using Delaunay 2.5D reconstruction since the swimming pool is kinda flat. Not sure about the texture mapping. Do you have a uv map of the colour data that you could apply to the mesh in Meshlab e.g.?",
        "Try looking up Gaussian splatting",
        "I don't have the uv map of the colour data. Just raw point cloud and rgb images which I can extract from the video footage.",
        "You would need to register your point cloud with the raster rgb images using a tool like QGIS. Then you would try to interpolate the colours from the raster images onto the point cloud. I'm not too sure of how to achieve that.\n\nQuick question: I think this post should be removed because it's not directly relevant to this subreddit. It's more relevant to a 3D modelling or photogrammetry subreddit.",
        "You could try mapping a color gradient to the intensity. Sort of like the way a fish finder does it. Opencv allows you to take arrays of the points and load the colors separately. Then you could convert to a pcd or ply file."
    ]
},
{
    "submission_id": "1f8wwa3",
    "title": "Email to AC about potential GenAI generated reviews",
    "selftext": "Recently got reviews back from WACV. The results are one WA and 2 Borderlines. However, one of the borderline potentially uses tools to generate the paper summary. I tested 6 different AI detection platforms and all of them indicate AI-generated content detected. More importantly, some points in the weakness contradict the summary (i.e. in the summary it mentioned our paper explored the potential applications of the proposed model. But in the weakness, the reviewer asks \"What is the application?\").\n\nShould I write an email to AC to inform this? However, I did not find related reviewer guidelines in WACV which indicates that using AI to generate reviews is forbidden.",
    "created_utc": "2024-09-04T08:56:25",
    "num_comments": 9,
    "comments": [
        "While the contradiction is worth pointing out.\n\n\nAs far as I know, AI generated content detection is considered invalid and can't be used as proof as models can falsely flag both types of content. I wouldn't use it as argument for your case. ",
        "AI detection platforms, especially for language, are fundamentally unable to tell you if content is AI generated.\n\n\nThey cause more harm than good, where teachers accuse students of cheating when they didn't. Similarly, they have made you think reviewers are doing the same to you.",
        "Where can we check the list of accepted papers?",
        "I get your point. I just feel frustrated about the reviews. It is not a fair judge if the reviews are coming from a model.",
        "Your argument does not prove they didn't either. In any way, nobody wants one's own projects/papers are be reviewed by models and feedback a bunch of contradicting comments and suggestions. So why do we need to double-blind review system? Why do we need the peer review system?",
        "100% I am with on you this. \nThis time is changing whether it is for better or worse we'll see...",
        "Even if it's written by an LLM, it may have been read by someone and then they told the LLM what to say based on a high level summary.\n\n\nI guess the important thing is, does the review make sense?\n\n\nRespond to the content rather than how the reviewer wrote it.",
        "And yet I can't prove you are not an LLM either.",
        "That is exactly why this review hurts me. The summary (I think potentially generated by AI) contradicts its weakness and strengths points in an obvious way. \n\nAnyway, thanks for the suggestion. I do agree that respond the content is better"
    ]
},
{
    "submission_id": "1f8vjzg",
    "title": "3D Reconstruction from Equirectangular video ",
    "selftext": "Hi all, I am trying to do 3D reconstruction from a equirectangular video of an indoor environment. I am using the unofficial fork of OpenVSLAM to do it for equirectangular video but as I am using ArUco markers as well, the support for markers is not present.( giving a constraint to the makers is getting difficult). Can anyone suggest any other methods or techniques.\n",
    "created_utc": "2024-09-04T08:02:01",
    "num_comments": 1,
    "comments": [
        "Itwin modeller context capture has a video input option, and then you can enter your control points in a second AT."
    ]
},
{
    "submission_id": "1f8pb3r",
    "title": "Viable OCR solutions for hobby project?",
    "selftext": "I've been fiddling around with an idea for an accessibility tool for gamers/streamers for a while, and the basic functionality starts with making a snipping tool that can extract text from a screenshot, I collected a huge batch of videogame-y fonts, synthesized a bunch of data that's taking space on my hard drive right now, and fine-tuned a HuggingFace TrOCR model to be exceptionally accurate on the kinds of input I'm imagining...\n\n  \nBut the model is gigantic. I don't expect users to want to download it, I wanted it to be usable offline, and I don't really want to pay to host it.\n\n  \nIs there a good way to fine-tune a pre-trained model to my data and not have it end up gigantic? I'm looking at easyocr right now but I'm not sure if I'd just run into the same problem.",
    "created_utc": "2024-09-04T02:54:08",
    "num_comments": 3,
    "comments": [
        "EasyOCR models are generally smaller, like, tens of megabytes kind of small, which should be acceptable? Although, I’m not really familiar with their fine tuning tooling, so I cannot attend to how good it is to use. \n\n\nIf you need a smaller model, and you don’t have a ton of data, you generally wanna look at CRNNs, and avoid transformers.\n\n\nIf you are not committed to a certain DL framework, I can recommend Keras OCR as a starting point, cause it’s small, accurate enough (especially after fine tuning), and pretty straightforward overall\n\nhttps://github.com/faustomorales/keras-ocr",
        "Ooooh, yeah. I totally forgot to mention keras ocr is where I started like a month ago. I was having trouble with the fine-tuning because it makes some function call that breaks I think because tensorflow's been updated; something about making layers no longer taking the \"weights\" argument or something. Been a while since I've messed with it. But maybe I just need to figure out the specific version of tf/keras to install.\n\nI'm sure I'll get one of the two to work. Thanks.",
        "You can downgrade TF to something like 2.9 with python 3.8, or something like that. It’s quite stable and should run just fine for the most part. However, there could be potential cuda/ cudnn issues if you installed the fresh ones, so, yeah, environment setup kills me every time"
    ]
},
{
    "submission_id": "1f8ovjw",
    "title": "Keeping up-to-date on research",
    "selftext": "How do people stay up-to-date with the latest research? I created an X page which summarizes daily submissions to arXiv by suggesting pairs of articles. It works for any arXiv categories beyond computer vision. https://x.com/moatsearch",
    "created_utc": "2024-09-04T02:23:17",
    "num_comments": 1,
    "comments": [
        "Great that you’ve created something to make it easier to find and compare relevant articles.\n\nBesides this, I also find that following key researchers and institutions on social media can be really helpful. then subscribing newsletters can also be a good way to get curated updates."
    ]
},
{
    "submission_id": "1f8nrza",
    "title": "Free RSS feed for tousands of jobs in AI/ML/Data Science every day 👀",
    "selftext": "",
    "created_utc": "2024-09-04T01:04:13",
    "num_comments": 1,
    "comments": [
        "\\*thousands ;)"
    ]
},
{
    "submission_id": "1f8n7bi",
    "title": "measuring object size with camera",
    "selftext": "I want to measure the size of an object using a camera, but as the object moves further away from the camera, its size appears to decrease. Since the object is not stationary, I am unable to measure it accurately. Can you help me with this issue and explain how to measure it effectively using a camera?",
    "created_utc": "2024-09-04T00:21:50",
    "num_comments": 40,
    "comments": [
        "You cannot measure physical sizes with a single camera, in general.",
        "- Do you mean to measure them in pixels or meters? I did this using OpenCV: [https://medium.com/@saleemheebah4/person-detection-using-usb-camera-calculating-the-x-y-cooridinates-from-current-location-to-the-0640fe59a8c4](https://medium.com/@saleemheebah4/person-detection-using-usb-camera-calculating-the-x-y-cooridinates-from-current-location-to-the-0640fe59a8c4)  \n- Camera calibration plays an important role if you want to measure the object size or the distance: [https://medium.com/@saleemheebah4/camera-calibration-xy-coordinates-using-opencv-edfeaddfde04](https://medium.com/@saleemheebah4/camera-calibration-xy-coordinates-using-opencv-edfeaddfde04)\n\nAnother way is through stereo vision. \n\nMay I ask if you are using an embedded system and what kind of camera is this?\n\nPS: If you find my blog helpful, I would be happy if you give claps and star on my GitHub repo :)",
        "Not possible with a single camera. Your brain developed multiple ways to perceive 3D even with one eye (motion parallax, oclusion...) but it's not yet possible in CV. At least at a good point.\n\nI recommend you reading my article about 3D Computer Vision https://medium.com/@matesanz.cuadrado/computer-vision-3d-into-the-unknown-dimension-b742ce7f791f\n\nGood Luck!",
        "Structure from Motion (SfM) is what you need:\n\n\n\n[https://en.wikipedia.org/wiki/Structure\\_from\\_motion](https://en.wikipedia.org/wiki/Structure_from_motion)",
        "If the object is small and the movement distance not too large you could try using a telecentric lens, this type of lens removes the issue of objects getting smaller as they get further away as it has an essentially 0 degree angle of view. Draw back is that you need a lens at least as big as your object. https://www.opto-e.com/en/products/telecentric-lenses These are where I have got them from before.",
        "You can try it with a metric depth estimation network such as ZoeDepth (https://github.com/isl-org/ZoeDepth). It provides you a monocular depth map and gives you a pixel-wise depth value.",
        "Remember trigonometry and right triangle math? It can be done, but you have to remember 7th grade math. Serious.",
        "ohhhhhh Why can't I measure with a single camera?",
        "My work will be used to measure the lengths of trucks detected by a single security camera. I want to convert pixels to meters, and since the objects are moving, their sizes are constantly changing. What else can you suggest?",
        "ohh I understand, can you give me some suggestions on how to measure the height of a moving vehicle with a camera?",
        "ohh so it's a lens that eliminates the perspective problem?",
        "This midas does not provide accurate depth information.",
        "https://youtu.be/KSYVobIeY_4?t=2m35s",
        "Because cameras destroy the scale information! You can't discriminate between a small object up close, and a gigantic object from afar.\nYou can scale your results after the fact, using some external measurement (i.e not from the camera). Ex: using an object of known size, or the distance between two calibrated cameras.\nThere are also metric depth estimators that work in practice, they exploit prior information encoded in natural scenes. Show them a scaled down model of a regular street, and they'll be fooled.",
        "You have to use some known information, are there dashed lines?  You could measure them.  Are other dimensions of the trucks known?  Semi truck trailers have a known width, you can use that to extrapolate the length, its just trig at that point.",
        "Yes but they generally have a very small depth of field so won’t allow for a lot of movement",
        "we should pin this to the channel, this type of question is very very common",
        "I was hoping the link would lead to [https://www.youtube.com/watch?v=MMiKyfd6hA0](https://www.youtube.com/watch?v=MMiKyfd6hA0)",
        "Although mostly true for a single image (monocular depth is getting reasonably good), if you move the camera around the object you can clearly obtain the object size.\n\n[https://en.wikipedia.org/wiki/Photogrammetry](https://en.wikipedia.org/wiki/Photogrammetry)\n\n[https://en.wikipedia.org/wiki/Structure\\_from\\_motion](https://en.wikipedia.org/wiki/Structure_from_motion)",
        "Thank you very much, but as you mentioned, there are those who try to measure using a single camera by finding the vanishing point, so many of those studies must be flawed.",
        "I would like you to explain it in a simple way. I found the height pixels with yolo. and what should I do for the correct measurement from now on.\n\ntrucks do not have a certain height, they are constantly changing",
        "Will this lens be useful in keeping the size of the tool I'm measuring constant?",
        "No, all those processes, unless you provide some scale measurement, are up to a scale factor.\n\nConsider this: I could have you run photogrammetry on images from a Blender model, in which humans are 1.80m tall. Then, I could have you re-run it on the same scene scaled 1000x, the images would be the exact same. There is no way for the photogrammetry program to know the images come from scenes with a different scale.",
        "I'm confused, I did not mention any vanishing point at all and it's not relevant. The scale ambiguity is something fundamental with cameras.\n\nI did mention metric depth estimators, but I specifically said that those are not *rigorously* exempt from this problem. They will work fine for autonomous cars, though.",
        "You could measure the width of the road, height of a sign, etc that is in the camera frame and use that.  You just need some known objects in the the image relative to the truck",
        "If you keep the same camera in that Blender scenario the objects scaled 1000x will appear very differently on the sensor.\n\nYou have a specific field of view defined by the camera, say 60 degs. That's what ties the scale when you move the object or the camera.",
        "\n\nI understand that we can find the HEIGHT of our truck based on the real and pixel ratio of a reference object. Should the distances between the reference object and the truck, which we will measure, be equal from the camera, or will it still provide accurate measurements even if they are at different distances?",
        "no, if I scale the scene, then obviously I also scale the camera positions, and the images will appear the same.",
        "They need to be known.  Im imagining a scenario where you are measuring trucks driving along a road, with a sign on either side of the road.  You measure the height of the sign, you measure the distance from the sign to the road, you should already know the cameras lens and its perspective.  Draw this on a piece of paper and start doing some geometry and trig.",
        "If you do that, then the images will look the same, but the intrinsics and extrinsics parameters of the camera will be different in both scenarios, leading to the correct size in both cases.",
        "thankss brooo for your answerrrr",
        "again, no. Consider a regular pinhole camera, my scene is composed of points (Xi,Yi,Zi) in the camera frame, they project to (fXi/Zi+u0,fYi/Zi+v0), they would project to the same exact positions if I scaled them by any constant factor. You have no way to know from the projected positions the scale of the scene.",
        "You are talking about a single image. Yes, in that case you don't have a way to obtain metric information since infinite 3D points will project into the same pixel.\n\nBut this thread is about multiple images at different positions. In that scenario you can have a metric reconstruction of a scene with a camera. Structure from Motion is an example of this.\n\nA simpler example of this is stereo vision, which can use the parallax to obtain the metric size of an object:\n\n[https://en.wikipedia.org/wiki/Parallax](https://en.wikipedia.org/wiki/Parallax)\n\n[https://en.wikipedia.org/wiki/Computer\\_stereo\\_vision](https://en.wikipedia.org/wiki/Computer_stereo_vision)",
        "no. It works the exact same with multiple images: if you scale a scene by a constant factor, then all the cameras will see the points scaled from their point of view too. You should try and verify it by yourself.",
        "Thank you very much for this nice discussion, but in conclusion, how should I start working? Can you give me a roadmap?",
        "Take the simplest case as an example:\n\nConsider a rectified camera pair with known baseline distance (B) and focal length (f) and a single 3D point in the scene.\n\nThe 3D point is projected into each of the two cameras (or one moving camera), resulting in two images with a specific disparity value (d).\n\nThe depth (Z) of the 3D point can be obtained by Z = f \\* B / d\n\nTechniques such as SfM and others generalize this concept.\n\nIf you change the scene and cameras so that both images look the same, you will get different values for Z, f, B. Only d will remain the same in that case.",
        "But that is the point! you can only measure d on the images, and then deduce B and Z up to an unknown scale factor!\n\nYou cannot get metric depth without an external measurement, like measuring the *physical* baseline of a stereo system for instance, but: you. cannot. get. it. from. the. images. alone. using. projective. geometry.\n\nedit: I think your confusion comes from the \"known baseline\" part: this is an external measurement. \"You cannot get a metric baseline\" is exactly the same statement as \"you cannot get metric depth\", from the images alone, that is.",
        "That's what SfM and other techniques are all about. They estimate the camera poses (B in this case) and the 3D points positions (Z in this case) at the same time by minimizing the reprojection error. Bundle Adjustment is a key component here: [https://en.wikipedia.org/wiki/Bundle\\_adjustment](https://en.wikipedia.org/wiki/Bundle_adjustment)\n\nThink about a simple scenario, imagine you have 6 cameras around a circular table, all of them looking at the center, each covering 60 degrees so that the entire 360 is seen by them. Because you can see the matches between the views of the cameras you can figure out the pose (location and orientation) of each one of the six cameras. Because you know the focal length of the cameras, you are able to know the actual metric information of the position of each camera. The geometry of the situation allows you to obtain the metric X, Y, Z of the cameras.\n\nSfM generalizes this. Of course SfM will not always work, there are some conditions that need to be met so that the situation allows obtaining the metric information, but it is possible.",
        "You must be trolling. The focal length does NOT give you metric information. Listen, I took enough time for this...",
        "Thank you very much for this nice discussion, but in conclusion, how should I start working? Can you give me a roadmap?"
    ]
},
{
    "submission_id": "1f8m8h7",
    "title": "Suggest library, approach to detect and parse Objective type question ",
    "selftext": "I am an experienced java developer, no experience with computer vision. I have some familiarity with python  \n  \nI have a task at hand, i need to parse following type of Questions from PDF, pdf could be either text or images. \n\nI want to be able to detect the question, its 4 options, and if possible, the correct answer \n\nExample : From pdf\n\nhttps://preview.redd.it/rcmgggdrhqmd1.png?width=2118&format=png&auto=webp&s=538502ad0a6187d10dbb0d10d08e30368c0e52ca\n\nAny help where should I be looking for ? What should be my approach and which library would be best suited for this task. Also recommend, if there's an hosted solution which can make the task easier\n\nThanks",
    "created_utc": "2024-09-03T23:11:48",
    "num_comments": 3,
    "comments": [
        "You can look up layout parsing or document layout parsing in this sub. Those are the keywords you're looking for.",
        "u/MrBeforeMyTime thanks for reply, there's a python lib layout parser, bt that seems to be not maintained any more.\n\nDo you think some thing like Yolov8 from ultralytics could be right tool ? With a custom data set for training?",
        "I don't think you need any of that. It's actually a really common problem that pops up here. About twice a week for at least 3 months. This link to an [old post](https://www.reddit.com/r/LocalLLaMA/s/ff7g7hT5pa) has 2 projects recommended and a few linked in the comments."
    ]
},
{
    "submission_id": "1f8kk8l",
    "title": "AI that generates detailed description of clothing based off images",
    "selftext": "How would you go about creating a model that generates detailed descriptions from images of a single clothing item (brand, market price, fabric type, category, color, condition)? I’m trying to build a multi-task model in tensorflow using some online datasets, but it seems like this isn’t the best way to approach this. I don’t want to make a gpt wrapper. Any ideas?",
    "created_utc": "2024-09-03T21:28:13",
    "num_comments": 2,
    "comments": [
        "Paligemma was also recently released by Google AI. It's a 3B parameter multimodal model like CLIP.",
        "You could try CLIP. Eva-clip is supposed to be easier/faster to train."
    ]
},
{
    "submission_id": "1f8hqzc",
    "title": "Good Story Papers in CV",
    "selftext": "Many expert says that write your research paper as good story paper. All the sections and writing should follow some storyline - interesting, intrigue, well-detailed. I want to learn how to write my next research paper as a good (excellent) story paper that many people would love to read. Please suggest me some resources or computer vision papers that will help me learn story writing style. 😊",
    "created_utc": "2024-09-03T19:02:23",
    "num_comments": 2,
    "comments": [
        "Start with a strong hook to grab attention and present your research as a journey with a clear problem and resolution. Using real-world examples can make your paper more relatable and engaging. \n\nFor resources, you might find these helpful:\n\n**“Writing Effective Research Papers: A Guide for Computer Vision Researchers”**, **“The Elements of Style” by Strunk and White** – This classic book on clear writing can help you polish your prose and make your paper more readable.\n\nAlso, consider looking for online tutorials or courses on scientific storytelling to get more ideas.",
        "Diffusion papers typically start the story with writing the ddpm equation, followed by score function generalization show some fid vs sampling step graph then make an ODE comparison and how their sampling equation is the one to rule them all. /s\n\nBut for real: i am not sure what this story is supposed to be. Just read a few papers from cvpr or iccv and u will get the idea."
    ]
},
{
    "submission_id": "1f8c2y3",
    "title": "Sapiens: Foundation for Human Vision Models",
    "selftext": "https://reddit.com/link/1f8c2y3/video/dxv39povxnmd1/player\n\nLarge vision transformers with 1024 input resolution pretrained on millions of human images.  \nDesigned for in-the-wild generalization.  \n  \nCode: [https://github.com/facebookresearch/sapiens](https://github.com/facebookresearch/sapiens)  \nDemo: [https://huggingface.co/collections/facebook/sapiens-66d22047daa6402d565cb2fc](https://huggingface.co/collections/facebook/sapiens-66d22047daa6402d565cb2fc)  \nPaper: [https://arxiv.org/abs/2408.12569](https://arxiv.org/abs/2408.12569)",
    "created_utc": "2024-09-03T14:36:23",
    "num_comments": 1,
    "comments": [
        "Anyone knows if it's possible create a printable 3d model with normal?"
    ]
},
{
    "submission_id": "1f87tar",
    "title": "Why not just get your plots in numpy?!",
    "selftext": "",
    "created_utc": "2024-09-03T11:45:48",
    "num_comments": 2,
    "comments": [
        "Very nice. I've done similar hacky plots in the past but having a proper lib like this will be great!",
        "It's only on its way to being a lib and not quite yet. Few hundred substantive LoC, feel free to take part in it; There are many upvoted comments of what people want to see"
    ]
},
{
    "submission_id": "1f87sly",
    "title": "Exploring Perception in Autonomous Vehicles - My Latest Article on Medium",
    "selftext": "Hi everyone,\n\nAs a Computer Vision Engineer with a deep passion for autonomous vehicles, I've recently published an article that delves into the cutting-edge research shaping the future of AV perception. The article, titled [Perception in Motion: The Science Behind Autonomous Vehicle Vision](https://medium.com/@shrunalisalian97/perception-in-motion-the-science-behind-autonomous-vehicle-vision-07f68a263263), synthesizes insights from some of the most groundbreaking papers in the field, including those from Waymo.\n\nIf you're interested in how perception systems in self-driving cars are evolving and the innovative techniques being used to improve them, I think you'll find this piece insightful.\n\nI’d love to hear your thoughts and feedback on the article! Check it out [here](https://medium.com/@shrunalisalian97/perception-in-motion-the-science-behind-autonomous-vehicle-vision-07f68a263263)\n\nLooking forward to engaging with the community!\n\nBest,  \n\nShrunali",
    "created_utc": "2024-09-03T11:45:04",
    "num_comments": 0,
    "comments": []
},
{
    "submission_id": "1f87avm",
    "title": "Perspective transform entire image based on an object which is inside the image.",
    "selftext": "I want to perspective transform an entire image based on the object which is in the image, I have four corners of that object using Opencv I can perspective transform the image but will only give me the object, I want that shift to be applied to the entire image can we do this.",
    "created_utc": "2024-09-03T11:25:35",
    "num_comments": 2,
    "comments": [
        "Yes",
        "Yes you are looking for homography"
    ]
},
{
    "submission_id": "1f852u6",
    "title": "U-Net Deconvolution ",
    "selftext": "I have to use U-Net 3D to learn from my dataset which consists of 20 microscopic 3D images in .ims format, and 20 corresponding 3D deconvolved images in the same format. But it takes too much ram and even the A100 GPU used in colab pro+ isn't enough, it crashes. On top of this, I also have to add augmentations and increase my dataset. That also causes ram crashes. \nUsing unet 2D also doesn't improve anything. What do I do? Please help ",
    "created_utc": "2024-09-03T09:58:44",
    "num_comments": 8,
    "comments": [
        ">On top of this, I also have to add augmentations and increase my dataset. That also causes ram crashes.\n\nAdding augmentations shouldn't really cause any more ram usage unless you're trying to put your whole dataset, augmented images and all, in memory before training. It's much more common to do augmentations on the fly during data loading, unless you're already sure you have the extra VRAM.",
        "Shouldn't on the fly augmentations also take ram usage? Regardless a larger dataset in the end would put more pressure anyway, right? During training",
        "Augmentations take some ram, but usually less than 2x the memory vs an unaugment batch, whereas loading everything, including augmented copies into memory of course takes up much more, unless your batch size is basically the same size as your dataset. \n\nThat's another likely candidate for OOM actually, what batch size are you using?",
        "What's OOM? And my batch size is 4. And I want to increase it as well but obviously that causes the crash too. Isn't there a higher capability I can purchase?",
        "OOM = Out Of Memory\n\nBatch size will massively change the amount of memory your training takes. You need to store every activation and gradient for every sample, so batch size geometrically increases memory use. There are of course potential advantages to larger batch size, but if you can't run them without crashing, the point is rather moot.\n\nTry running with a batch size of 1. If that crashes, you have bigger issues.\n\nFrom there, keep doubling it until you run out of memory again.\n\nThe other variable would of course be resolution. What resolution are your images? These of course behave in the typical ay where memory use grows as the square of the image size.",
        "Colab crashes with just on the fly augmentations, can't even reach the training process. \n\nWithout the augmentations, I can execute up till batch size 4.\n\nThe resolution of my images are (32, 2048, 2048)",
        "Makes sense. That's a really high resolution. And that too with 32 channels.",
        "I've tried preprocessing it to simplify, you know just work with the slice that has the most information, etc. still is pretty computationally heavy."
    ]
},
{
    "submission_id": "1f818r6",
    "title": "I wrote a free and open source PyCharm plugin for visualizing Numpy/OpenCV, PyTorch, TensorFlow and Pillow image data with only two clicks right from a Python debug session.",
    "selftext": "",
    "created_utc": "2024-09-03T07:24:05",
    "num_comments": 0,
    "comments": []
},
{
    "submission_id": "1f7w5mg",
    "title": "Need a Python Library to work with to generate an animation of human pose (joint vertices and bones) ",
    "selftext": "Hi, I have the pose information in a .npy file as (num_frames,num_joints,3) and I want to visualize the motion in a GUI using a python 3D rendering library.\n\nI saw that PyOpenGL although has lot of controls is way too complicated to work with.\n\nMy end goal is to have a plane and the motion sequence(stick figure) on it getting updated at 20fps. I also want to have directional lighing and shadows to get better visualisation.\n\nI do not have much knowledge on how to do this, so would love any help with this. :)\n\n",
    "created_utc": "2024-09-03T03:04:30",
    "num_comments": 11,
    "comments": [
        "What about Unity/Unreal/Blender? All have scripting and methods to get data in/out of the engine in real-time.",
        "Take a look at SMPL. It might be what you need!",
        "Hey OP! Checkout Open3D.",
        "You could try to salvage the visualization code from here https://github.com/facebookresearch/VideoPose3D/blob/main/INFERENCE.md",
        "I had to do something similar and I used Jupyter and ipywidgets.",
        "I've used Blender for this, quite easy, just write a python script/program. However, if it is *required* for you to develop a stand-alone code, in that case, probably something like SMPL would get you going or just use the nodes/edges with [ipywidgets](https://ipywidgets.readthedocs.io/en/latest/). Others mentioned this.",
        "Huge +1 to this, and now they have SMPL-X out with even more details",
        "Plotly and Matplotlib will work too!",
        "Thanks I will check it out!"
    ]
},
{
    "submission_id": "1f7votu",
    "title": "Anomaly detection: identify defects through objects",
    "selftext": "Hello everyone, I am working on a project where the goal is to try to find innovative solutions for diminishing the quality check time in a production assembly workflow. \n\nThe objects are car components, currently checked by human intervention (4 straight hours looking at metal objects searching for some defect 🫣). \n\nMy idea is to generate a dataset from the 3D CAD with blender where objects are at the ideal state - without defects. Then try to train a model to “learn” the correct state and detect if the image in the testing phase has some defect. \n\nTheoretically I know that autoencoders do this job but I don’t know how to approach the problem. Maybe some of you can give me suggestions of how to address the task and which framework I can use. Also I don’t want to detect the class of the defect but just classify that image as not-ok. \n\nThanks",
    "created_utc": "2024-09-03T02:32:44",
    "num_comments": 5,
    "comments": [
        "Checkout anomalib",
        "[deleted]",
        "This here",
        "Thanks! I've just checked it, it seems easy. Do I need to create a dataset with good images and then train on it?",
        "Typical GPT answer that is saying a lot without saying anything.",
        "Im not sure since i havent used it before. I believe it should have some pretrained models somewhere"
    ]
},
{
    "submission_id": "1f7tjed",
    "title": "GameNGen : Google's AI Game Engine using Deep Learning",
    "selftext": "",
    "created_utc": "2024-09-02T23:58:05",
    "num_comments": 0,
    "comments": []
},
{
    "submission_id": "1f7sqjk",
    "title": "how to do top-down inference of a video input with trt mmpose models?",
    "selftext": "Hi guys. So I've used mmdeploy to transfer both a detector and a pose estimator to int8 trt models. My question now is that I don't know how to pass the results of the detector to the pose estimator. I use the inferencer\\_model from the [inferencer.py](https://github.com/open-mmlab/mmdeploy/blob/main/mmdeploy/apis/inference.py) file of mmdeploy. But the model doesn't seem to take bounding boxes as inputs.\n\nAlso, inferencer\\_model seems to initialize itself and take an input at the same time. But if I want to inference a video input, does that mean that it'll need to load the model at every frame?  If so, isn't that gonna slow down the process? Is there a different inferencer in mmdeploy that suits my needs better? Thanks!",
    "created_utc": "2024-09-02T23:03:04",
    "num_comments": 5,
    "comments": [
        "Use the MMDeploy SDK.\n\nhttps://github.com/open-mmlab/mmdeploy/blob/bc75c9d6c8940aa03d0e1e5b5962bd930478ba77/demo/python/det_pose.py",
        "Thank you! I did saw this, however I was having a hard time building mmdeploy runtime gpu environment and still couldn't make it work. Kept saying that I'm missing a dll file. So was trying to avoid using SDK, but if it's the only way then I guess I'll try to work it out",
        "You just follow the instructions in their docs. And it should work.\n\nOr use their MMDeploy docker.",
        "Thanks for replying! The problem I'm having rn is that if I install mmdeploy-runtime-gpu and try to import mmdeploy\\_runtime, I'll get ImportError: DLL load fail while importing mmdeploy\\_runtime.\n\nHowever, if I only install mmdeploy-runtime, the non-gpu version, and try to import it, it'll be working fine. I just can't get what went wrong. If by any chance you know how to solve this please let me know. Thanks again!",
        "Did you follow [this](https://mmdeploy.readthedocs.io/en/latest/02-how-to-run/prebuilt_package_windows.html)?\n\nThe DLL error would be if you didn't add the folders to the paths as required."
    ]
},
{
    "submission_id": "1f7qxmg",
    "title": "can anyone help with the image analysis?",
    "selftext": "i will need to find the edge of  transparent tape on steel rod,  any suggestions?  thanks!\n\nhttps://preview.redd.it/4nn7hwt9simd1.jpg?width=2546&format=pjpg&auto=webp&s=312de0036e0007af8b3dd7788f0aff42a975d55a\n\n",
    "created_utc": "2024-09-02T21:11:49",
    "num_comments": 4,
    "comments": [
        "If the spacing is very regular, a notch filter might work. Otherwise try to enhance the image contrast, followed by (canny) edge detection and hough transform.",
        "Hough transform.",
        "You can try calculating horizontal gradients (e.g. sobel with horizontal kernel). This will also expose lots of noise since the rod isn't clean and smooth, but you can then look for periodical spikes in resulting gradients (assuming the lines are equidistant). Maybe even cross reference with periodical gradients of neighboring pixel rows.",
        "thanks for the suggestion, the problem is that the contrast is so low and it is curved suface and also has strong reflections, really hard to extract feather from background"
    ]
},
{
    "submission_id": "1f7k5nq",
    "title": "35mm / Album contact image extraction",
    "selftext": "Hi folks!\n\n  \nI've dabbled in computer vision for many a year, but this seemingly simple task is breaking my brain a bit. I recently was going through my elderly dad's photos and scanned a few pages of 35mm contact prints and photo album pages and I'd like to avoid needing to crop out each individual image. I was surprised to find that a popular mutli-platform FOSS tool didn't exist yet, so I thought I'd write my own.\n\nThere are a few techniques I've tried, with limited success:\n\nA. Detecting frames by applying a gaussian blur (eliminating noise), applying a threshold, and detecting contour lines\n\nB. Detecting the negative space around frames by casting rays from edge to edge and finding regions of similar color, then defining points where lines intersect, and filtering the resulting set of rectangles to the aspect ratio(s) of known image types\n\nTechnique (A) works if I heavily filter the resulting matches. I get virtually no false positives, but I have a low ratio of matches (3 of 34 on one contact sheet)\n\nI have yet to get Technique B to work well, yet.\n\n  \nI'm curious how folks might approach this problem? I'm trying to avoid having to tag a data set and do manual training... I was hoping I could just whip something up with OpenCV calls...but perhaps I'm being naïve... (narrator: he's being naïve)\n\n  \nHow would you approach it?",
    "created_utc": "2024-09-02T15:39:46",
    "num_comments": 2,
    "comments": [
        "I built a tool that does exactly this and wrote up a high level overview of my approach if you're curious ;)\n\nhttps://www.autocropper.io/technical-overview",
        "Oh man! Thank you! I’ll have a look"
    ]
},
{
    "submission_id": "1f7bxa7",
    "title": "Help image dataset normalization",
    "selftext": "Hi everyone. What is the right way to perform normalization in medical image dataset?\n\nRight now I applied HU transformation to my original data (obtaining a range more or less from -5000 to 4000) and then, in order to convert them in 8 bit I applied min-max normalization multiplied by 255, but per image: in other words for each image I calculated min and max and I applied this to each image independently. In your opinion could be a good approach? Thanks in advance.",
    "created_utc": "2024-09-02T10:03:46",
    "num_comments": 5,
    "comments": [
        "I'm assuming you're talking about 3D CT scans. For each patient, you want to calculate the min and max of the volume of interest and then apply the min-max normalization across all slices of the patient. \n\nI'd personally convert it to 16 bit due to integer overflow over the large windowing.",
        "I forgot to say that I’ll train a 2D CNN for various reasons.. is your method working also in this setting or can I do as I already did? Thanks",
        "Oh yeah I used a 2D CNN too; I pass a each slice to the model and use hard/soft voting to get the classification for the patient.\n\nThe code is proprietary so I can't show it, but basically you can read whatever CT format as a HxLxW array and normalize basically the same way for H number of images as the single image. \n\nIf you're using 1 image then your way is fine. If you are using multiple images from the same patient, it's better to normalize across images/slices of the patient just for consistency.",
        "Since you’re using 2D CNN.. you give in input to the network a series of slice of different patients and then you classify every single slice as healthy or not right? \n\nBecause right now in my situation I’m noticing some overfitting behavior: maybe because I fed the network with many similar slices: I was thinking about applying a preprocessing layer with some random transformations like flip, rotation,… . What do you think? Thanks",
        "Sorry for the late reply.\n\nYes, I classify each slice and then evaluate with hard voting and soft voting to get the patient classification.\n\nYeah medical related models tend to overfit due to limited dataset sizes. They tend to also be hit or miss with data augmentations, especially with very miniscule differences between healthy and tumor tissue; if you are doing some kind of filtering, then that can augment the data too much. Rotation and flips are fine. I suggest doing 5 fold cross validation too and averaging out the results; there can be huge differences between folds.\n\nThe only real way to test what works is by running the training and testing unfortunately."
    ]
},
{
    "submission_id": "1f7aith",
    "title": "Questions regarding detection of salmon lice, wounds and more",
    "selftext": "Hello\n\nI'm very new to object recognition and computervison. So please forgive any stupid questions. \n\nI'm currently starting a business where we want to selektiv remove farmed salmon from the farming cages. This can be salmon that's not growing well, salmon that is sick, salmon that have wounds etc. \n\nWe have the mechanical components and a metode for isolating one and one salmon in a tube. The plan is to then use object recognition to decide if the salmon should be sendt back into the fish cage, or if it should be separated out for slaughter.  \n\nOur original plan was to buy the AI model for an established vendor. But this has proven to be a bit more difficult than we thought.  So now we are looking to make our own model. The plan is to take a bunch of pictures of salmon in our \"tube\" and use this to train the model. But to figure out if this is feasible I turn to reddit for some quality feedback. \n\nThe plan is to use three HD cameras to get a 360 view of each salmon. We need to see the whole salmon to find lice and wounds, and three cameras is the minimum, but would four be better?\n\nAnd how many photos would i need to make an accurate model with 5 to 6 sorting parameters? And how many would i need for 20-30 parameters? For salmon lice we want to be able to differentiate between the different life stages and the sex of the lice, so that's 6+ parameters just there.  But this does not need to happen in the first iterasjon of the model, but can be added at a later stage.\n\nWe also need to measure the length and circumference of the salmon to calculate their weight, and look for deformation of the jaw/snout and fins.\n\nWould Yolo v10 be good choice for this application? If not what would be best ? \n\nHow much time do i need to allocate to the training of the model? Trying to get a cost estimate. I don't know how much time it takes to lable the images. \n\nWould it be possible to recognize individual salmon in Yolo? every one of them have different spot pattern among other tings. \n\nHow powerful does the hardware for each unit (fish cage) need to be?\n\nHope to get some informative feedback. I know I'll need to learn a lot before I can start making my own model, but if this is feasible we would rather do this than pay a license fee for every unit we plan to produce\n\n  \nRohny",
    "created_utc": "2024-09-02T09:07:42",
    "num_comments": 4,
    "comments": [
        "yeah your approach is perfect   \nget images from the tube like actually that is the usecase   \ntry around 10k images for a good model not the best   \nwe trained our model on 60k images which has 4 keypoints and 1 detection that worked perfectly   \nand while augmenting those images(if required) make sure to avoid negative augmentation   \nElse best of luck",
        "training usually takes 3 days on 1 4090 Gpu   \nbut the thing that matters is the image count and img size on which u are training",
        "Yolov10 is not perfectly stable, v8 v9 and v10 have their use cases like   \nv8 has more accuracy but it is slow   \nv9 has less accuracy but is is fast   \nand there was some same thing with v10 as well",
        "*The plan is to use three HD cameras to get a 360 view of each salmon. We need to see the whole salmon to find lice and wounds, and three cameras is the minimum, but would four be better?*\n\nA - start with 4 so at least you can collect extra images which will be better for training. Maybe in the future you can reduce to three to lower costs.\n\n*And how many photos would i need to make an accurate model with 5 to 6 sorting parameters? And how many would i need for 20-30 parameters? For salmon lice we want to be able to differentiate between the different life stages and the sex of the lice, so that's 6+ parameters just there. But this does not need to happen in the first iterasjon of the model, but can be added at a later stage.*\n\nA - The 'how many' question is always tricky, it depends on the variability of the data. Generally most detection problems like this you can get something reasonable with 100 example of each class or 'thing'. But probably more like 1000 images to start hitting the 95% accuracies - but this is a total guess, it's image quality and use case dependent.\n\n*We also need to measure the length and circumference of the salmon to calculate their weight, and look for deformation of the jaw/snout and fins.*\n\n*Would Yolo v10 be good choice for this application? If not what would be best ?*\n\nA - there are many many models to use. You can use detection models, segmentation models and even combinations of detections, crop, then classifications (for mite sex, age, etc.)\n\n*How much time do i need to allocate to the training of the model? Trying to get a cost estimate. I don't know how much time it takes to lable the images.*\n\nA - Given the right tools and experience, you should be able to have a prototype in about a week.\n\n*Would it be possible to recognize individual salmon in Yolo? every one of them have different spot pattern among other tings.*\n\nA - no, you'd need a different approach. The good news that can be done after the fact. Focus on the other problems first.\n\n*How powerful does the hardware for each unit (fish cage) need to be?*\n\nA - Not super powerful, inference (runtime) is much less compute intensive than training."
    ]
},
{
    "submission_id": "1f798c6",
    "title": "GestSync: Determining who is speaking without a talking head",
    "selftext": "📢📢📢 We're thrilled to introduce GestSync demo on HuggingFace 🤗!  \nYou can now effortlessly sync-correct any video and perform active-speaker detection without the need to rely on faces. This is a project with Prof. Andrew Zisserman @ University of Oxford.\n\nTry the demo on 🤗: [https://huggingface.co/spaces/sindhuhegde/gestsync](https://huggingface.co/spaces/sindhuhegde/gestsync)\n\n📄 Paper: [https://arxiv.org/abs/2310.05304](https://arxiv.org/abs/2310.05304)  \n🔗 Project Page: [https://www.robots.ox.ac.uk/\\~vgg/research/gestsync/](https://www.robots.ox.ac.uk/~vgg/research/gestsync/)  \n🖥 Codebase: [https://github.com/Sindhu-Hegde/gestsync](https://github.com/Sindhu-Hegde/gestsync)  \n🎥 Video: [https://www.youtube.com/watch?v=AAdicSpgcAg](https://www.youtube.com/watch?v=AAdicSpgcAg)\n\nhttps://i.redd.it/dojgwcgwxemd1.gif\n\n",
    "created_utc": "2024-09-02T08:14:48",
    "num_comments": 1,
    "comments": [
        "No relevant code picked up just yet for \"GestSync: Determining who is speaking without a talking head\".\n\n[Request code](https://www.catalyzex.com/paper/arxiv:2310.05304?requestCode=true) from the authors or [ask a question](https://www.catalyzex.com/paper/arxiv:2310.05304?autofocus=question).\n\nIf you have code to share with the community, please add it [here](https://www.catalyzex.com/add_code?paper_url=https://arxiv.org/abs/2310.05304&title=GestSync%3A+Determining+who+is+speaking+without+a+talking+head) 😊🙏\n\nCreate an alert for new code releases here [here](https://www.catalyzex.com/alerts/code/create?paper_url=https://arxiv.org/abs/2310.05304&paper_title=GestSync: Determining who is speaking without a talking head&paper_arxiv_id=2310.05304)\n\n--\n\nTo opt out from receiving code links, DM me."
    ]
},
{
    "submission_id": "1f78bo0",
    "title": "PiDAR - a DIY 360° 3D Scanner",
    "selftext": "",
    "created_utc": "2024-09-02T07:37:31",
    "num_comments": 0,
    "comments": []
},
{
    "submission_id": "1f7399t",
    "title": "I need a good Face Recognition API (paid also works) that can work in realtime on CCTV footage",
    "selftext": "I am working on a project and need a good Face Recognition API (paid also works) that can work in realtime on CCTV footage. The camera could be angled, the lighting could be low so the accuracy, specifically recall should be quite high.",
    "created_utc": "2024-09-02T03:16:47",
    "num_comments": 5,
    "comments": [
        "Ask for Jack at www.cyberextruder.com, globally leading FR API, in the top 5 annually at the NIST FR Vendor's Test. disclaimer: former developer there.",
        "Checkout intel geti",
        "Do they provide a python sdk or api?",
        "You'd have to ask, I have not worked there in 3 years. There was an API, and a web client, probably more by now. Their FR is extremely reliable for situations like crowds, multiple cameras, as well as cameras with a range of view and image qualities, and then tracking individuals and their associates across all those cameras. Within video security, a lot of companies kind of slapped on FR when it became available, on video networks never designed for the types of views or image quality where FR was going to be used. As a result, a lot of early FR customers were unhappy when they tried to use their systems. CE focused on that type of image quality, and as a result better quality views and better imagery is simply more accurate in their system, but the bad imagery that cause other systems to fail will work in CE's software, and it will work well. When they focused on typical imagery as found on typical video networks (meaning not new, but old cameras) where the video was never planned for use with FR, they trained with a huge body of overly compressed, badly lit, bad view images and got a very robust algorithm. Given good images, it works fantastic, and given bad images it works, and where your images lie on that spectrum should be somewhere inside.",
        "Thanks for letting me know, will consider them as a prospective option"
    ]
},
{
    "submission_id": "1f733vw",
    "title": "Stable diffusion ",
    "selftext": "I've planing to fine-tune stable diffusion locally, I have about 100 images and have 12GB 3060, I'm thinking of decreasing the batch size and img size.\n\nI have downloaded the stable diffusion ckpt from hugging face, i checked several tutorials but I can't seem to find one using Pytorch, I found the keras tutorial but I can't find the stable diffusion modal in H5 format.\n\n",
    "created_utc": "2024-09-02T03:06:59",
    "num_comments": 0,
    "comments": []
},
{
    "submission_id": "1f72xrn",
    "title": "Google's AI Breakthrough Could Disrupt the $200B+ Global Gaming Industry.",
    "selftext": "Researchers at **Google** and **Tel Aviv University** have developed **GameNGen**, a novel game engine entirely driven by neural network models, without relying on traditional game engines.\n\nGameNGen can interactively simulate the classic 90s game *DOOM* at over 20 frames per second on a single TPU. When players use a keyboard or controller to interact with the game, GameNGen generates the next frame of gameplay in real time based on their actions. [https://gamengen.github.io/](https://gamengen.github.io/)\n\nHandling DOOM's complex 3D environments and fast-paced action was a challenge. Google's approach involved two stages:\n\n* They trained a reinforcement learning agent to play the game, recording its actions and observations during training sessions. This training data became the foundation for the generative model.\n* A compact diffusion model takes over, generating the next frame based on previous actions and observations. The team added Gaussian noise to the encoded context frames during training to keep things stable during inference. This allows the network to correct information sampled in earlier frames, preventing autoregressive drift. The result achieves parity with the original game and maintains stability over long trajectories.\n\nGameNGen showcases the incredible potential of AI in real-time simulation of complex games. It could reshape the future of game development and interactive software systems. It also brings to mind NVIDIA CEO Jensen Huang's prediction at GTC 2024 that *fully AI-generated game worlds could be a reality within 5-10 years.* Without manually coding game logic, individual creators and small studios may be able to create sophisticated, engaging gaming experiences with minimal development time and cost.\n\nhttps://preview.redd.it/nx3382sacdmd1.png?width=900&format=png&auto=webp&s=0e7c39b0e700041e694b0736e99183c4ff2e6877\n\n",
    "created_utc": "2024-09-02T02:55:53",
    "num_comments": 28,
    "comments": [
        "Its very impressive but let's be honest it's a model running on a TPU that can simulate a 30 year old game once it's been trained on 1000s of hours of that game. And simulate it badly at 20fps with a 3s context window.",
        "Any title with \"could\" or \"might\" is clickbait bs, and this is a prime example\n\n\nStill, a super cool demo and wish I could play it",
        "Really?? You forgot to add \"THIS CHANGES EVERYTHING!\" And add a Pic of you holding your head while pretending to be shocked.",
        "Diffusion models won't remotely be able to generate high quality graphics on real time, inference time is just too high. Gaming like this won't be a thing until we solve this, and our best generator so far (diffusion) is just not suited for this. As a POC, pretty cool, but it won't be the next gaming experience anytime soon.",
        "Can we please stop those sensational headlines? They were barely able to simulate doom, a game that has been running on a calculator and a pregnancy test. This was a cool project but won't replace something like unreal engine in the foreseeable future.",
        "I don’t get the reason behind downvotes here besides poor title.\nYes, the model itself is far from viable but it gives a glimpse in a possible future (that may or may not happen) and implications of that future is massive. And even if this work only facilitates discussions and thinking about that future it’s already a success.",
        "“DOOM’s complex 3D environments”\n\nHahaha, that’s where i stopped reading",
        "I'm not so sure about it actually being used for games in the long run, it seems like it's the first steps towards building a World Model for semi-independent robotics.",
        "It just doesn't make sense. Much easier to write code to create games rather than write code to create AI which then generates frames. And come on it wont disrupt any games with 30 fps. Its just frames what about other things in settings and so many things in games. Stop creating random fluff and random hype post",
        "tell me when using cutting edge ai features are faster than raymarching on the same gaming oriented system",
        "Disrupt my ass",
        "it only better if proof ai able create new game even use alot generator and not existed game. Then it will be impactful",
        "And another one will learn a bitter lesson.",
        "Honestly If someone sees google next to AI these days just run. They know nothing about gen AI and they’re way behind. Just watch their most recent keynote, they literally have no clue what they’re saying and just throwing the word AI everywhere with prompted demos showing tech that is 3 years behind.",
        "Wait for a year! 🤯",
        "Okay. But think ahead about feeding the frames to something like Flux and you can get graphics which are impossible to get any other way. AI could eventually replace the rendering stack.",
        "Why are you sitting on it? Why are you a hater?",
        "Yep I agree, the Markovian nature of diffusion models will always be hard to optimise for real time inference. However I hope I’m wrong and someone is working out something much faster.",
        "Me and my physics bros had this idea a few years back in uni, cool to see some other researchers working on it now!",
        "You have no idea how much money people want to throw at the \"next big thing\" ",
        "No, Flux is an image model, while it may be possible to learn some minimal temporal motion, you need a model trained on actual sequences of frames. BFL are working on a video model yes I know. \n\nHow do you even learn meaningful controls that match the level of control a game engine gets you ?",
        "Honestly I think academically this is throwing up all kinds of interesting things with regards to stuff like temporal consistency and input but the hyperbole attached to it all is crazy. We are not witnessing magic. We are not witnessing a paradigm shift in how games are created. While I accept that the rendering stack may *eventually* be replaced by AI, \"eventually\" is hiding all sorts of sins. The amount of technological revolutions that need to happen first is staggering. The interesting thing about this paper is nothing to do with rendering really.\n\nIt's a very cool paper. Just leave it at that.",
        ">you need a model trained on actual sequences of frames.\n\nYou can have the model presented by Google providing the initial frames and a diffusion model providing the final result without training said model on a frame sequence.\n\n>How do you even learn meaningful controls\n\nThis is why I said it could replace the rendering stack, not the entire game.",
        ">We are not witnessing a paradigm shift in how games are created\n\nI completely agree. However, it could be a glimpse of the future, and this is the kind of tech that you can build companies with. Some people in the gaming industry will want to seed this.",
        "It could work, but now you have two huge diffusion models that need forward passes at inference. Would be very slow. \nHowever the outputs of the image model would not be temporally consistent so the outputs would vary drastically per frame. Hence why a video model which can learn some sort of spatio-temporal consistency is a better solution. \n\nAlso depending on the img2img capabilities you may need additional inputs like depth, segmentation to ensure the core gameplay output is maintained in the image generative model.",
        ">Would be very slow\n\nFor now, which is why I said it could _eventually_ work for a game. If someone got this model to produce 20 frames per second, it might just be a matter of time before we get some diffusion models to produce images in almost real time. Plus we already have ideas on how to do upscaling and interpolation (like DLSS) so maybe low resolution 20fps will be enough and then you can smooth and upscale it.",
        "It's unlikely diffusion models will ever work for this kinda task (I hope i'm wrong). Markovian based operations are very hard to speed up. Hence why this paper has such small resolution and frame-rate."
    ]
},
{
    "submission_id": "1f70847",
    "title": "Homogeneous Coordinates ",
    "selftext": "In short, I have a test to show my knowledge in computer vision for Robotics Computer Vision Developer. The this is, I solved all the questions, but only one left, and I never worked on homogeneous coordinates system. Anyone knows how to calculate the real world coordinates based on pixels represented by the camera? I am seeking for an explanation, a little help, or any book or blog that describes the way to do it.\nThanks for taking time to read this.",
    "created_utc": "2024-09-01T23:44:57",
    "num_comments": 8,
    "comments": [
        "Take a look at the opencv documentation:\nhttps://docs.opencv.org/4.x/d9/d0c/group__calib3d.html",
        "Does this help? https://learnopencv.com/geometry-of-image-formation/",
        "Hii im also working towards specialzing in robotics computer vision... could you share what topics the questions were  asked in",
        "Woah! I am sorry, I never thought that OpenCV documentation would have it. I searched a lot online, and only found master’s research locked and cannot be accessed. I will definitely examine it! Thanks for your help!",
        "Yes it will help a lot! Thank you so much! Appreciate it.",
        "It is 6 questions based on various topics using C++:\n1) if you have a robot that can move only using those three movements K, L, M and the distance is N. How many possible moves you can create from KLM to reach N.\n2) CNN with 55x55 input image, 6 filter, padding, and strides. You have to calculate the number of features, mapping planes.\n3) a normal code with errors, you have to figure out what is the purpose of using it, and to correct it.\n4) how to detect straight lines in an image describing your method.\n5) describe how to calculate real world coordinates based on pixels\n6) 3D room with two cameras that can see each other, you have to answer 6 questions here from the empty spaces of projection, where would the camera appear, and give the exact coordinates for locations.",
        "thank you so much!!",
        "If I reached the interview, I will update you with details"
    ]
},
{
    "submission_id": "1f6skq7",
    "title": "how do i build a good structure ",
    "selftext": "ngl idk if this is the correct question to ask butt\n\nI'm a computer science student working on an application to assist people who are deaf or hard of hearing. The main features of the app include:\n\n- Translating text or speech into sign language, displayed using an avatar.\n- Using computer vision to detect body movements and translate them into sign language in real-time.\n\nI’m unsure how to design the backend for this application, as I’ve never worked on something similar before. Additionally, I’m not certain if a relational database would be the best choice for this type of app. Could you provide guidance on how to approach the backend design and whether a relational database is suitable for this use case?",
    "created_utc": "2024-09-01T16:40:21",
    "num_comments": 1,
    "comments": [
        "Break down this project into small parts, which you can develop independently.\nAlso, leave \"difficult\" constraints fir later. For example, reading sign language is one thing, but doing it real-time is something else... Start simple..."
    ]
},
{
    "submission_id": "1f6rkqi",
    "title": "Looking for researchers and members of AI development teams to participate in a user study in support of my research ",
    "selftext": "We are looking for researchers and members of AI development teams who are at least 18 years old with 2+ years in the software development field to take an anonymous survey in support of my research at the University of Maine. This may take 20-30 minutes and will survey your viewpoints on the challenges posed by the future development of AI systems in your industry. If you would like to participate, please read the following recruitment page before continuing to the survey. Upon completion of the survey, you can be entered in a raffle for a $25 amazon gift card.\n\n[https://docs.google.com/document/d/1Jsry\\_aQXIkz5ImF-Xq\\_QZtYRKX3YsY1\\_AJwVTSA9fsA/edit](https://docs.google.com/document/d/1Jsry_aQXIkz5ImF-Xq_QZtYRKX3YsY1_AJwVTSA9fsA/edit)",
    "created_utc": "2024-09-01T15:52:47",
    "num_comments": 0,
    "comments": []
},
{
    "submission_id": "1f6lbhz",
    "title": "Attempting to create an AI for an FPS",
    "selftext": "Requirements are no accessing internal game memory and it just needs to shoot and kill - it's a deathmatch style game so no other info other than enemy player is needed. (I don't care about heath, ammo count, etc) The bot doesn't need to be good, just able to afk and grind weapons.\n\nI'm not sure if ai is the best way to approach this, so please leave a suggestion for an alternative method if one comes to mind. \n\nMy issue I've run into is performance. I want this to be able to run simultaneously with the game on any mid range pc (RTX 2060 or so). My current inference speed is ~70ms without the game running and  ~300 with the game running. I am using gpu for inference with yolov8. I've seen people tout much faster speeds but I have no clue how to optimize properly or if it's doable with the game running.\n\nDo you think it's possibly to get the inference down to 33 ms or so with the game running? My rtx2060 is already at 86% utilization with just the game running. \n\nAn alternative method I've thought of are simple pixel scans. Every person will be an enemy. I have pathing figured out already and could simply tell it to stop in x spot and wait for pixel changes. Only issue is there's many different character skins of all colors, grenade effects/scorestreaks/map movement might be difficult to have it ignore. There are red dots/names above their heads but you have to be aiming close to them for those to appear. \n\nAny info is appreciated as I'm quite new to cv. Thanks!\n\n ",
    "created_utc": "2024-09-01T11:26:19",
    "num_comments": 13,
    "comments": [
        "How did old FPS games design computer enemies? Perfect Dark, etc? You don’t need CV",
        "[deleted]",
        "Have you tried the basic optimization techniques, eg using smaller models (Yolo Tiny), reducing input resolution.\n\nThere's probably also some GPU resource conflict between your game and the GPU-accelerated yolo; I don't necessarily advise this but a tiny enough model (NanoDet) can actually run on CPU within 50ms. \n\nThe pixel change idea is interesting, you can always use a hybrid two-stage approach. Use change detection as the first stage, and a custom small network to \"check\" the results.",
        "Using game memory...",
        "I don't really think so. It will never be at the skill level that would ruin other players experiences, it would just be free kills most of the time.",
        "Woh being able to achieve 50ms on cpu would be awesome, I'll have to look into that. Why do you say you wouldn't advise it? \n\nI have not tried smaller models yet, I was under the assumption yolov8 was doable with the correct approach - but I should probably look elsewhere. \n\nI've reduced input resolution, yes. I think it was 628x628 or whatever the standard is. Perhaps if I only scan the line of head/chest height instead of the entire screen I could get even less pixels. (I'm not sure how the model accuracy would be with just heads though, considering the vast variation in player models [Call of Duty]) I don't know if that would speed things up enough though.",
        "I guarantee that it would take less memory to use older techniques than running a CV inference for each computer character concurrently",
        "See nanodet @ https://github.com/opencv/opencv_zoo. Wouldn't *necessarily* advise it due to low resolution, and your cpu could also be equally capped.\n\nJust to check, you are using yolov8 *nano* ?",
        "OP is making hacks for a multiplayer game which is why one of their requirements is that it can't access game memory. Probably because of EAC or another kernel-level anti-cheat. Nobody here should be helping them. ",
        "Brother why respond if you didn't read the post. One of the requirements is that I cannot access internal game memory. That is how every bot in every game works. There is nothing useful to me regarding their methods. I know I could make this much easier by using internal game memory, that is not the goal though.",
        "I am not using the nano version, do you recommend I try that first?",
        "[deleted]",
        "They're not worried about how much memory it uses. They're worried about anticheat seeing a program accessing game memory and getting banned. They're just reluctant to use the word \"cheat\"",
        "I guess my title/desc were a bit ambiguous? It's an AI bot, just singular. It is supposed to mimic me playing the game myself in a pvp server. I don't need multiple ais, I'm not trying to create some 1v6 bot scenario or set up a pve environment. I don't know the location of every character, the only information I am working with is the pixels on my monitor as if I were playing the game myself.",
        "I mean i wouldn't call it a cheat when it's going to play at the bottom 1% of players lol."
    ]
},
{
    "submission_id": "1f6i3vw",
    "title": "Identification across multiple video feeds",
    "selftext": "So I developed a way to detect and track things and people in videos. Now I want to match the identities of the objects across multiple video feeds over time. So for example let’s say I have 2 people in a room I’m tracking from 3 different angles, I want to basically assign/match the IDs to the people across the 3 feeds, so I would have the name ‘Person 1’ and ‘Person 2’ in video feed 1, 2, 3. \n\nHow do I do this? Especially given that I already have a detector and tracker. Do I just stitch the 3 video feeds and hope the tracker assigns the IDs automatically? Or is there another more direct way?",
    "created_utc": "2024-09-01T09:08:48",
    "num_comments": 2,
    "comments": [
        "This is called object re-identification. If it's three cameras in the same room with overlap, then you could stitch together before doing tracking. That should work. But there are more camera-setup-independent solutions if you search for re-id.",
        "NVIDIA provides a way to do it:\n\nhttps://www.nvidia.com/en-us/ai-data-science/ai-workflows/multi-camera-tracking/"
    ]
},
{
    "submission_id": "1f6fbhk",
    "title": "Is this Image Classification?",
    "selftext": "Hi all - I'm looking to create my own version of the computer vision example shown in this instagram video: [https://www.instagram.com/p/C9OaRL3sif\\_/?hl=en](https://www.instagram.com/p/C9OaRL3sif_/?hl=en)   \nThere are lots of examples and tutorials online of Object Detection, but I think my use case is an example of Image Classification. I'd like to define a bounding box, within which the computer analyzes what it sees and lumps it into 1 of 4 categories: 1) No case  2) Bullet properly oriented  3) Case no bullet  4) Bullet improperly oriented. The camera will always be in the same location (although as long as it's not harmful, it will be easier for me to grab training images from multiple similar but not identical locations). And the camera doesn't need to look for the object in the entire frame, just within the bounding box.\n\nI may have gone overkill as I'm not sure this is nearly as complex as many of the Object Detection models, but I picked up an 8GB Raspi 5, AI Kit, and Pi Cam 3. Right now, I'm focused on just getting the computer vision portion of this working (my very first computer vision project). Longer term, I want to use the GPIO pins of the Pi to trigger the automatic drive to stop advancing if category 3 or 4 are recognized in the image. Accuracy is critical here, but I think 99.9% or greater might be possible, given the extreme differences between 1-4.\n\nI would greatly appreciate if you all can point me to a training document or video that covers a similar use case (I've been searching, but *everything* I come across show Object Detection examples which tracks objects as they move throughout the image, and accuracy is well below what I need). Or even just help me understand which type of CV project this is so I can confidently narrow my own search. Or just any recommendations on how to get started. My next steps (I think) are to get training images and then to use labelimg to label them. ",
    "created_utc": "2024-09-01T07:08:53",
    "num_comments": 2,
    "comments": [
        "Object detection looks for bounding boxes in an image. If you camera is static and you're looking at the same region every time (i.e. you can hardcode a \"bounding box\"), then there's no reason to do detection and you can try classification. Just extract the region of interest and run the classifier on that patch.\n\nRegarding your data, you don't need to collect images from angles that won't be seen by the model in production. Your data should be as close as possible to real life cases, everything else is unnecessary.",
        "Got it, thank you. That makes a lot of sense. I'll work on designing a mount now then, which will hold the camera in a fixed location, to capture the training images."
    ]
},
{
    "submission_id": "1f6doyw",
    "title": "ID/document parser help",
    "selftext": "I am tasked with making something that will save specific information in a passport/documents into a database I have no background on computer vision and I only know basics of image processing I tried using ocr and parsing using regex but there is different passport and ids all with different layouts.  any free tools or suggestions on how I can make it happen will help greatly ",
    "created_utc": "2024-09-01T05:52:02",
    "num_comments": 2,
    "comments": [
        "Check out UniLM from Microsoft https://github.com/microsoft/unilm or similar projects.",
        "I do a lot of similar work to this.\n\nYou could use an LLM to parse the text from OCR or you could just directly use a VLM.\n\nThe downside with LLMs is that they can be expensive, slow, and lack privacy if you aren’t self hosting.\n\nYou can also train an NER model, but this will require annotating a training set, which is a lot of manual work.\n\nHappy to provide more details"
    ]
},
{
    "submission_id": "1f6bid9",
    "title": "Thermal imaging diagnosis",
    "selftext": "I am working on a project to classify images based on thermal imaging. My sample size is very small, so I could definitely get help from a pretrained model on a similar task. My first thought was to look for a pretrained model on thermal images, but have been unable to find any. Should I try to pretrain one myself or is there a reason no one has implemented this? \n\nGeneralizing this question, should my approach be about thermal imaging (problem specific works don't exist) or should I look for SotA computer vision in general?",
    "created_utc": "2024-09-01T03:42:08",
    "num_comments": 4,
    "comments": [
        "Define small. I've accomplished remarkable things with < 500 images. Do you have more images that are not labelled yet? Do you think there are features that are unique to thermal images? How many classes do you need to predict? Are they balanced? The ImageNet weights of most classifiers are often a good starting point.",
        "Thanks for your answer. I currently have 110 annotated images and there are about 100 non-annotated I can get my hands on. The dataset is balanced (60-40 for the binary task). I would think that there are features that are unique to thermal images but this only based on intuition.. The number of classes is currently 2 (with the prospect of using 3 classes if results indicate my algorithm has a chance to perform okay). Unfortunately the really small sample size and the quality of the images doesn't allow for ImageNet pretrained models to achieve more than 65% accuracy (on a sad 20 image test set). This is the reason I am looking for ways to improve my methodology.",
        "You definitely need those 100 extra images. Then I’d try to engineer my own features and use a classical approach (SVM), together with a good augmentation strategy. Deep neural networks will very quickly overfit on such a small dataset.",
        "This is the first thought that comes to mind, but the specific images, which contain a lot of noise and aim to be used in a difficult task, cannot even be all classified correctly by the domain expert. The annotation comes from clinical tests. Having said that, I have already tried extracting some widely used statistical features in thermal images and failed to get better results. There must be hand-crafted features that can provide better results than Neural Networks but it might be pretty difficult to engineer them. Anyway, thank you very much for the help."
    ]
},
{
    "submission_id": "1f68tgw",
    "title": "Implementation of research papers",
    "selftext": "Anyone who knows how to implement research papers ( computer vision and kinematics ). I am complete beginner I got research internship opportunity the faculty who is in charge wants me to start doing vision based robotic arm. He Is focused on kinematics but I am focused on computer vision I don't mind combing both but I don't know where to start. What research papers shld I implement and mainly how to implement them\n\nPs: I hv basic coding skills and basic cv and ml theory",
    "created_utc": "2024-09-01T00:27:28",
    "num_comments": 11,
    "comments": [
        "Checkout mmdetection and mmcv, it is used in research and many papers use it for uploading their implementation as well",
        "Searching for an open source implementation can be a good start, it would help you get the basic intuition",
        "checkout the source code from github and try to run it firstly.",
        "[paperswithcode.com](http://paperswithcode.com) can be a great resource with multiple implementations for popular papers. From there you can also practice to look at the paper and how others implemented it to get an idea for projects that you have to do yourself",
        "start by breaking it down – maybe get comfortable w/ the vision side first (object detection, etc.), then see how kinematics ties in. For papers, search for ones on vision-based robotic arms or motion planning. And keeping track of all the research can get messy. Afforai’s been useful to organize and summarize papers, might help u stay on top of things too.",
        "*What research papers shld I implement and mainly how to implement them*\n\nMaybe ask the faculty that supervise your internship for advice?",
        "Thanks!! Will I able to find internediate projects? Cause I am going to learn and implement in 2 months times",
        "okay! thankss",
        "he suggested me: [torque controlled simple pendulum](https://github.com/dfki-ric-underactuated-lab/torque_limited_simple_pendulum) but i have no interest in kinematics i want to do in computer vision field and looks like he is not that interested in computer vision he told me to search urself",
        "Depends on your current skill level but if you know python and understand the deep learning concepts i think youd be good",
        "Unless you misled the professor about your level of expertise, this is poor advising. You should be receiving guidance from them, not from random strangers on the Internet. Are there other students in the lab or at the university you can speak with? You’re going to have a rough time if asking people on reddit is your best way of getting help."
    ]
},
{
    "submission_id": "1f5obev",
    "title": "3D pose estimation using monocular camera and known object geometry",
    "selftext": "Hey folks, I was reading [this paper](https://arxiv.org/abs/1902.02394) and as a very non-expert in the field I want to ask you if it is possible to do 3D pose estimation using the same resources (single monocular camera and known object geometry) BUT with a DL-free approach. Considering I have to do something similar with a less powerful hardware, I guess running real-time an additional NN after the YOLO (for detection) might be problematic. Thank you in advance.",
    "created_utc": "2024-08-31T07:17:12",
    "num_comments": 19,
    "comments": [
        "It depends on the object, but in general you definitely can do this.\n\n\"Multiple view geometry in computer vision\" by Richard Hartley is exactly what you need, it goes through all the math.",
        "Yes, of course you can! Do you have a 3D model of the target? If so, it’s even easier.\n\nTake a look at the Perspective N-Point Problem. The idea is to recover the 3D pose of an object given the locations of points on the object as they appear in an image. Use local features with descriptors to identify which features correspond to which object points as they appear in a set of object reference images. Then you feed it into a function like SolvePnP() in OpenCV and get back your object pose.\n\nYou need to know the 3D positions of each of these points on the object. If you have a 3D model of the object, you can get them easily. \n\nThis is pretty standard stuff.",
        "I used to do 3D pose estimation with structure from motion. It's implemented with COLMAP, and they have a Python library that's pretty easy to use. If you run into any difficulties, just let me know!",
        "Found [2 relevant code implementations](https://www.catalyzex.com/paper/arxiv:1902.02394/code) for \"Real-time 3D Traffic Cone Detection for Autonomous Driving\".\n\n[Ask the author(s) a question](https://www.catalyzex.com/paper/arxiv:1902.02394?autofocus=question) about the paper or code.\n\nIf you have code to share with the community, please add it [here](https://www.catalyzex.com/add_code?paper_url=https://arxiv.org/abs/1902.02394&title=Real-time+3D+Traffic+Cone+Detection+for+Autonomous+Driving) 😊🙏\n\nCreate an alert for new code releases here [here](https://www.catalyzex.com/alerts/code/create?paper_url=https://arxiv.org/abs/1902.02394&paper_title=Real-time 3D Traffic Cone Detection for Autonomous Driving&paper_arxiv_id=1902.02394)\n\n--\n\nTo opt out from receiving code links, DM me.",
        "Interesting idea, this makes me think that you could attach a known object to a camera and use that as a reference for size and distance.  A \"nose\" in the corner of view.",
        "Sure you can do it. This is exactly what is being done in camera calibration (both intrinsic and some variants of extrinsics). If you know the object and can detect it's features (calibration pattern), having intrinsic xalibratio the solution is PnP algorithm",
        "You can also use mediapipe’s objectron module to generate an oriented bounding box and probably refine the pose with the detected keypoints of the cone. https://github.com/google-ai-edge/mediapipe/blob/master/docs/solutions/objectron.md",
        "Thanks! I'll take a look.",
        "Which chapter talks about 3D pose estimation math?",
        "Yeah, I can build a 3D model of the cone because it's size is given in the competition rules. Next week I'll dive a bit into theory to understand it better, but your comment definitely helps!",
        "Can you please share some more info… regarding the name of the library and any tutorials on how to use them",
        "You still wouldn't know if other objects were big and far or small and close",
        "Ok, PnP Is exactly what is used in the paper, where features are extracted using a ResNet. So you're basically saying that I can extract features without DL and then still use PnP, right?",
        "My pleasure. Feel free to reach out here again anytime",
        "here is it : [https://github.com/colmap/pycolmap](https://github.com/colmap/pycolmap)",
        "Well, I only have to detect cones and they all have same size. Only difference is color, through which the track is reconstructed. But tbh I didn't get this solution.",
        "True, good point",
        "This depends on the object but easiest is if you can add markers like chessboard corners or aruco markers to the object, this is commonly used. If this is not an option, this depends on whether the object has features which you can write algorithm to detect accurately regardless of scale and perspective",
        "They were jumping to the conclusion that if you knew 1 object in the scene's size you could solve for the size and location of every other object in the scene. Which isn't true without other constraints. Then because they had this invalid assumption they thought carrying an object in front of the camera would ensure you could always know 1 object in the scene. \n\nAll that would do is waste most of your image with useless information and occlude important stuff. \n\n\n\nIt's really unrelated to your question and just someone making silly leaps in logic on reddit."
    ]
},
{
    "submission_id": "1f5mtt2",
    "title": "Help with real-time VIO",
    "selftext": "Hi, I’ve been trying to sort out a VINS recently and have landed on potentially using AirSLAM (https://www.arxiv.org/abs/2408.03520). One problem I’ve been having with adapting any of the published VO/ VIO /VSLAM programs on GitHub (e.g HybVIO https://github.com/SpectacularAI/ , Kimera-VIO https://github.com/MIT-SPARK/Kimera-VIO) is that they’re set up to process precollected datasets like the EuRoC Mav Dataset. \n\nLooking at them, I’m not too sure how one would adapt them to take images from a camera and process them realtime to constantly output the pose, vs running a for loop over the dataset and outputting the timestamped poses at the end in one dump. This is what I’d like some help with. If anyone has any suggestions or things I should look at I would be incredibly grateful!",
    "created_utc": "2024-08-31T06:05:54",
    "num_comments": 1,
    "comments": [
        "Found [1 relevant code implementation](https://www.catalyzex.com/paper/arxiv:2408.03520/code) for \"AirSLAM: An Efficient and Illumination-Robust Point-Line Visual SLAM System\".\n\n[Ask the author(s) a question](https://www.catalyzex.com/paper/arxiv:2408.03520?autofocus=question) about the paper or code.\n\nIf you have code to share with the community, please add it [here](https://www.catalyzex.com/add_code?paper_url=https://arxiv.org/abs/2408.03520&title=AirSLAM%3A+An+Efficient+and+Illumination-Robust+Point-Line+Visual+SLAM+System) 😊🙏\n\nCreate an alert for new code releases here [here](https://www.catalyzex.com/alerts/code/create?paper_url=https://arxiv.org/abs/2408.03520&paper_title=AirSLAM: An Efficient and Illumination-Robust Point-Line Visual SLAM System&paper_arxiv_id=2408.03520)\n\n--\n\nTo opt out from receiving code links, DM me."
    ]
},
{
    "submission_id": "1f5mhlp",
    "title": "Help",
    "selftext": "can you tell me a computer that can run GTA 5, Elden ring, detroit become human and other game like this. With the bare minimum of money? ",
    "created_utc": "2024-08-31T05:48:24",
    "num_comments": 15,
    "comments": [
        "How to ask on reddit: spam posts in every sub's name starting with \"computer\"\n\nKids never read",
        "you’re looking at 60-70k USD min",
        "r/buildapc r/buildmeapc r/pcbuild \n\nNext time post on r/findareddit to find which sub you must post to",
        "Just get [this](https://www.ebay.com/itm/224443696846?chn=ps&norover=1&mkevt=1&mkrid=711-117182-37290-0&mkcid=2&mkscid=101&itemid=224443696846&targetid=2299003535955&device=m&mktype=pla&googleloc=9010933&poi=&campaignid=21214315381&mkgroupid=161363866036&rlsatarget=pla-2299003535955&abcId=9407526&merchantid=119146179&gad_source=1&gbraid=0AAAAAD_QDh_cNVAvrUYUpfQpfLCoZrpEM&gclid=Cj0KCQjw_sq2BhCUARIsAIVqmQvHTnWrqSjLdm270irU09GoDJRIS9hQmoGZQL0Hyp8YDw-8oA7fZh4aAm1GEALw_wcB) and you’ll be golden.",
        "Nvidia Jetson",
        "Can run? Basically any bottom shelf prebuilt. \n\nCan run well? I'd assume the last couple gens. There's plenty of websites that build you a PC based on the games you want to run.\n\nAlso, wrong sub, which is why you're not getting a serious reply.",
        "What?",
        "What?",
        "Okok",
        "What 😭🙏",
        "Which sub i have to go? They're a lot \nCan you say the name of a websites?",
        "For the love of the gods, please read the subreddit name",
        "You will also need PC insurance (required by law now) that because of your age will be much more expensive. Do your own research, but you're looking at 1K per month minimum.",
        "Free shipping bro, it’s a deal.",
        "I live in EU so I dont need that"
    ]
},
{
    "submission_id": "1f5k8q3",
    "title": "Fine-grained defect detection in car parts",
    "selftext": "I have a project of detecting defects in car parts, mostly plastic transparent of non-reflective surfaces. The images are grayscale, the defects are very different from each other in dimensions and morphology, but some are incredibly similar and hard to distinguish from the others: black dots, white dots, small bubbles, scratches. There also bigger ones that could occupy half of the image-entire image. The images are 2K resolution. What is the optimal strategy to develop a decent model that would be able to perform object detection? I’ve tried grouping defects by their visual features, training different YOLOv8 based models, like yolov8-p2 to adress small defects, the top score I've managed to have is m@AP0.5: 0.6, mAP@0.5-0.95: 0.4, both recall and precision: 0.6. I’ve also tried data augmentation techniques to help class imbalance but that nearly had no impact whatsoever. I feel kinda stuck with it, have no Idea where to make changes, maybe use some ROI-based techniques or classic computer vision for simpler and smaller defects. Would be happy for any advice and can provide more details you need.\nThanks a lot!",
    "created_utc": "2024-08-31T03:31:45",
    "num_comments": 4,
    "comments": [
        "Try to group all classes of defects into one single class...Let it be small, tiny or medium size or large with respect to the image size.... If you classify each one of them as an individual class, you will surely land into \"class_imbalance\"... We are working on a similar dataset of casting defects using Xray Radiography images.... It's a single channel of resolution 1024 x 1024..... Since, it's a multiple of \"32\", we keep the image_size as the model_size..... In that way,   resizing of images (during preprocessing) will not induce inconsistency... And it paves the way for detecting even small objects (defects).... Have a deluge of data to work.... Have labelled close to 80k images so far and its still growing....Also use, generous number of \"background\" images to avoid/eliminate FP's.....Model is doing well....We are using PP-YoloE from PaddleDetection.....",
        "Can you post examples?\n\nI’m confused, you say they’re very different from each other yet they’re also hard to distinguish? ",
        "Good advice, indeed. Thank you very much.\nIf you could answer some questions in DM i would appreciate it.",
        "I meant that there are multiple “groups” of defects that are similar to each other inside this group, but in general the defects do not have a common occurrence, position on the image or dimensions. There are small defects, simple such as black dots or particles, and defects with more complex morphology like burned plastic surface, that have a much larger area of course.\n\nI’ll try to find examples but I can’t really do it instantly, I am on vacation away from my primary pc."
    ]
},
{
    "submission_id": "1f5bok8",
    "title": "Why is loading point clouds with depth data and color data much slower than the viewer?",
    "selftext": "Basically I have a few cameras I'm learning to use and I want to save the color image and the depth image separate and then later on use them to create point clouds. Right now for testing my just converting to point cloud immediately after take an image and I get low fps. Although when I do this on the viewer that comes with the cameras (real sense and orbbec ) it's 30fps. Is it the way I'm converting them using pcl? Even their example code saves ply images slowly. What am I missing? ",
    "created_utc": "2024-08-30T18:22:13",
    "num_comments": 5,
    "comments": [
        "Gonna need a lot more detail than that to give any useful answers.\n\nHow are you converting to point clouds yourself? Using what programming language/framework/software? \n\nAre you saying that the viewer saves .ply files to your filesystem at 30 fps, or just that it displays points onscreen that fast?",
        "The viewer from the camera real sense displays rgb point clouds on screen at 30fps. \n\nWhen I convert to point clouds myself I'm using this equation:\n\nfor each pixel (u, v):\n    Z = depth_image(u, v)\n    X = (u - cx) * Z / fx\n    Y = (v - cy) * Z / fy\n    point_cloud.append((X, Y, Z, rgb_image(u, v)))\n\n\nThen for each pixel I overlay the RGB on it after this.\n\n I display them with c++ using the PCL library. So the camera takes an image I convert them to a point cloud with PCL and then use the PCl viewer. But doing it my way with PCL I get maybe 10fps.",
        "I’m no expert and don’t know c++, but a for loop over each pixel on at a time sounds pretty slow. ",
        "I believe .append() could be the issue. I think it essentially copies the vector in memory each time and making the new one +1 bigger with the new value - it's horribly slow. \nYou should preallocate your array and just update the values. You know exactly how many points/length of array you'll need.",
        "I agree. I'm looking for a new way to do this"
    ]
},
{
    "submission_id": "1f5afj8",
    "title": "Help with YOLOv8: Incorrect Labels Displayed in Detection Boxes",
    "selftext": "",
    "created_utc": "2024-08-30T17:19:59",
    "num_comments": 6,
    "comments": [
        "You will have to provide more information than that.\n\n1. How many images in your dataset?\n\n2. What's the training command you used?\n\n3. What were the final metrics?\n\nAnd like someone else mentioned, I don't think you actually trained on your dataset, or you didn't load the `best.pt` file that you got from the training for inference.",
        "Hello everyone,\n\nI recently completed a course on computer vision and YOLOv8, and I've been experimenting with some example images provided in the lab. I set up my `YAML` file with `nc: 1` for a single class and named it `\"flame\"`.\n\nThe detection process is partially working, but I'm encountering an issue where the label displayed in the bounding box is incorrect. Instead of showing \"flame,\" it randomly shows other class names like \"tie\" or \"mouse.\" I've attached an image of my `YAML` file along with a few example outputs.\n\nCould someone help me figure out what might be going wrong? Any guidance would be greatly appreciated!",
        "This sounds pretty normal to me.\n\nThat said, I’m suspect that you actually trained a model, since otherwise where did the extra class labels come from? \n\nThis would be a question best directed at those who support yolov8. You can post in their GitHub and the owner’s bot will respond. ",
        "You can’t just make the yaml only have one class if the model has 80+ (assuming this is the default COCO model). Load the normal yaml file for this model then filter out anything that isn’t flame. Although come to think of it I’m pretty damn sure flame isn’t in that dataset so I think you need to take another course or pay more attention next time",
        "Did you *train* the model on your custom dataset? It looks like you’re running inference with the pre-trained COCO weights.\n\nIf you haven’t trained the model with your dataset, do that, and then use the exported .pt file weights to run inference.\n\nIf you have trained and exported your weights, just make sure the path to the model weights you’re using for inference is actually going to the path of the weights you exported after training.",
        "I agree, it's most likely OP is training on COCO dataset. I once did that by mistake too, in fact I think one of my projects might still have it up on Github lol. Choose the custom dataset you wish to train on, or better find one that suits you and is an industry benchmark thus you get the results required, but it seems since you only have a single class to label it with, you need a custom one."
    ]
},
{
    "submission_id": "1f5adjd",
    "title": "Help with YOLOv8: Incorrect Labels Displayed in Detection Boxes",
    "selftext": "",
    "created_utc": "2024-08-30T17:17:17",
    "num_comments": 4,
    "comments": [
        "im kinda new to yolo too , but i saw a similar post where someone was not using the weights they trained. therefore it might not be wrong labels but right labels that yolo detected. IMO you are not using the the trained weights or just didn’t train well.",
        "Did you train YOLOv8 by yourself?",
        "that was it, I was using the [yolov8n.pt](http://yolov8n.pt) instead of the [best.pt](http://best.pt) which is the result of training.  \nthank you mate",
        "Not from scratch. You can load a model and then use the method model.train to fit to your own dataset."
    ]
},
{
    "submission_id": "1f548ak",
    "title": "OpenVSLAM/Stella VSLAM ",
    "selftext": "Has anyone worked with open VSLAM whose unofficial fork is Stella VSLAM. I am using it for 3D reconstruction using a equirectangular video from 360 camera. I wanted to leverage fiducial markers data, but am kinda stuck in giving constraints for marker optimization. Anyone who has worked 3D reconstruction using equirectangular videos with fiducial markers, please name the tools used by you\n\n\nThank you ",
    "created_utc": "2024-08-30T12:44:56",
    "num_comments": 0,
    "comments": []
},
{
    "submission_id": "1f52gvw",
    "title": "Facial Beauty Score Generator",
    "selftext": "I have a project where I have to rate face's beauty but these pictures are taken in a natural way, I mean these pictures are taken in all kind of angles and have all kind of facial poses. I eliminate hard pictures by checking head pose estimation and I calculate if the face is looking towards the camera because If face is looking towards different angle, then beauty score generator models are producing very wrong results.\n\nBut again it still doesn't give satisfying results. This is the [model](https://github.com/ustcqidi/BeautyPredict/tree/master) I have tried. Do you know any idea what can I do to improve results or what other methods can I try?",
    "created_utc": "2024-08-30T11:30:15",
    "num_comments": 2,
    "comments": [
        "For anything as subjective as beauty, there is no single dimension to score. From someone who has worked in affect recognition (e.g. emotion), you need a better framework. Look for literature on how people determine beauty in faces, then make a list of scores for each. Some of these scores may be feasible without custom-trained deep learning models (eg face symmetry). Some scores will require deep learning. You can make a composite score that is a mix of individual scores. When this is done, it adds science/rigor to your model, and while not everyone may agree with it, at least you have methodology you can defend or adapt",
        "The model you’re using is super racist, relying on completely subjective assessments about the attractiveness of young white and Asian adults"
    ]
},
{
    "submission_id": "1f4zm33",
    "title": "WACV 2025 results are out",
    "selftext": "The reviews of round 1 are out! I am really not sure if my outcome is very bad or not, but I got two weak rejections and one borderline. Someone is interested what did they got as reviews? I find it quite weird that they say the reviews should be accept or resubmit or reject. And now the system is more of weak reject, borderline, etc.",
    "created_utc": "2024-08-30T09:31:34",
    "num_comments": 8,
    "comments": [
        "Feel like this is something you should ask your PI, not a bunch of random people on reddit who know next to nothing about your research (no offense)",
        "The meta reviews should be accept, resubmit, and reject. The individual reviews are the ones you are looking at.\n\nTo be honest, your outcome is pretty bad. Two weak rejects and a borderline will probably be rejected. WACV is an acceptance rate of 45%, pretty high for CV conference but still competitive. If you assume the average score is borderline, then your paper is below average. You need an above average score to be accepted with an acceptance rate of 45%. \n\nYou have a small chance if you can write a killer rebuttal and turn one of your weak rejects to an accept.",
        "Final decisions are out!",
        "Totally correct, I mis-phrased my post, i wanted to know your feedback if someone submitted theur paper and what is the outcome",
        "how about 1 WJ, 2 BL? the one with WJ asked for a comparison that I already did by the way.",
        "I turned the weak reject into accept, but one was borderline made it reject, because i didn't compare with foundation models which is other field."
    ]
},
{
    "submission_id": "1f4yis0",
    "title": "Building AI vision model",
    "selftext": "looking for vision enthusiasts for building a software for small enterprise , site your selfs and portfolios. ",
    "created_utc": "2024-08-30T08:46:04",
    "num_comments": 10,
    "comments": [
        "The most important infos are missing. What, how much and to when?",
        "I’m interested. Can I DM you my portfolio and GitHub?",
        "Hi, I am a full-stack developer and I'm  getting started with computer vision. I want to learn as much as possible and would appreciate the chance to work with you.  \nHere is my GitHub: [github.com/itzamanjain](http://github.com/itzamanjain)",
        "We would be happy to help. We have a computer vision software (synodic.ai) that allows our users to autolabel images, train models, and run inference. We help our users train 100s of models a month. In addition we have a ton of exerierence training for the First Robotics Competition. We provided models for 1,000s of teams to use on their robots. Feel free to reach out at sean@synodic.ai or (978)-460-5363.",
        "this was my first post , i want to fine tune trocr model on arabic handwritten. for the moment , i am still building the dataset",
        "i don't see why not.",
        "pretty good , but how are you in computer vision ?",
        "Nobody's going to look through your other posts, you should put all necessary info in one place. Such low effort posts look very unprofessional and will be ignored by any experienced engineers",
        "have you tried SOTA models like this [https://github.com/Mushroomcat9998/PaddleOCR/blob/main/doc/doc\\_en/multi\\_languages\\_en.md](https://github.com/Mushroomcat9998/PaddleOCR/blob/main/doc/doc_en/multi_languages_en.md) that already support it?",
        "did try it but didnt give good results"
    ]
},
{
    "submission_id": "1f4v5wh",
    "title": "Encoder for numbers",
    "selftext": "So I am training all models from scratch. I have a large dataset of microstructure images. Each image is after N seconds of cooling at a rate R from an initial temperature T. So given a set of N,R,T I have to generate a particular image. For this I’ve adopted a CLIP inspired architecture with the image encoders being smaller VQ-VAE. The image encoder is working great as an independent model. But I can’t still figure out a way to generate the embedding space from the conditions. Please help me with it. ",
    "created_utc": "2024-08-30T06:24:03",
    "num_comments": 4,
    "comments": [
        "Are you asking how to plug N,R,T into the decoding process so that you can generate realistic images corresponding to N,R and T?\n\nsome older conditional VAEs would just linearly project an embedding of the class and add to the latent, you could do that with an embedding of N,R,T.\n\nYou could also modulate the various layers of the decoder: you can build a constant map out of your embedding and concatenate it to the various intermediary tensors of the decoder. You could pass it through a small MLP that vector outpus a and b that'll modulate intermediary tensors like so: x\\_modulated = a \\* x + b. Or if your decoder is a VIT: you can build a special token that you'll append to the list, or you can add cross-attention layers between your embedding and the token list.\n\nAs for the embedding itself, the Fourier embedding seems to be a good start (it's used in NeRFs to encode the position, and diffusion models to encode the time/noise level). I haven't really found other good suggestions recently.",
        "What I have tried so far:\n1. A good old dense network to generate the embedding space from the conditions after I have generated the embedding space from the corresponding image trying to minimize the losses between the resultant image by freezing the decoder.\n2. Modified the VQ-VAE to predict the conditions from the embedding space itself by attaching a small dense network and then adding a loss to the VQ-VAE losses present already. \n\nNone of these worked. Although the results were better in case 1.",
        "My objective is to generate images from N, R, T. If you’re suggesting to have layers in decoder to map these then where do I start the decoder from?\n\nWill try Fourier embedding and keep posted.",
        "In all my suggestions above, there's a random latent you'll need to sample, and the process of decoding it into an image is affected by the encoding of N,R,T."
    ]
},
{
    "submission_id": "1f4u2z8",
    "title": "Guide me for Learning Computer Vision & Image Processing ",
    "selftext": "Hello all, I am a college student and I have got a project to work on and that project requires me Computer Vision, Image Processing, detection and recognition \nSo, can someone please help me and suggest me the courses to learn all of this as soon as possible. I just have one month to build the project.\nAnd I have a good knowledge and a little experience in Machine Learning \n\nNote:- please share link to the courses ",
    "created_utc": "2024-08-30T05:32:07",
    "num_comments": 10,
    "comments": [
        "start with CNNs because you have ML experience and after that AndrewNg course on coursera",
        "If you have good knowledge as you said, maybe you simply start working on your project and whenever you get stuck, you fill in the gaps",
        "What’s your project about? btw, you can start working on it while learning at the same time.",
        "Can you please share the link to the courses?",
        "Can I dm you and explain?",
        "lol?\n\nbest of luck to you bro, sounds like you're going to need it",
        "yes",
        "Why? 🙄",
        "because you ain't gonna make it if you can't even google",
        "I already did that and got a list of few courses, went through their content also\nBut I just wanted an expert opinion \nAnd I got the recommendation of this Reddit community by Google only\n\nI think you are too free to make fun of me 🥴"
    ]
},
{
    "submission_id": "1f4sz7z",
    "title": "Foot isolation on an image",
    "selftext": "Hi all, \n\nI was wondering if someone has an idea of how to do this, I need to have an image where only one foot is in it, I need to remove the background and the legs and the foot that is not completely seen in the image and I need to do it without manual input. I have tried rembg library and it works pretty well to find the subject on the image but it leaves always the legs and the other half foot in the image, Then I tried also a combination of grabCut plus backgorund removal and that kind of works but not entriely, I still see the legs on the picture. \n\nDoes anyone have an idea of how I can do this?, I was trying also to get the depth map and using that to filter but it also doesnt work because of the legs and the other foot :(\n\nhttps://preview.redd.it/12lptkyyesld1.jpg?width=1600&format=pjpg&auto=webp&s=2f4dfba5106abe2d9d59087e6c7275aa26e714ae\n\n",
    "created_utc": "2024-08-30T04:33:46",
    "num_comments": 4,
    "comments": [
        "What is a more precise formulation of your problem ? Are you trying to crop the image so only one foot remains? You could probably use a detection model to get bounding boxes for both feet to trivialize the problem.",
        "Yeah kind of like that, I want to crop only the foot that is completely visible in the image, in this case the left foot that is in the center of the picture and remove from the picture everything else by setting it up to black. I need to do this on a PC without a dedicated GPU and should be as fast as possible 😅",
        "There is plenty of ways to optimize a deep learning model. Object detectors can run on potatoes now.",
        "Do you know if there is already a model for this? Or any good model for this? I also don't have the labeled dataset to train a model for this 🤔"
    ]
},
{
    "submission_id": "1f4r9wa",
    "title": "Image Captioning ",
    "selftext": "I have a very weird error in encoder decoder structure. \nI am using CNN as encoder and Transformer as decoder with captions and images loaded and represented correctly.\nThe problem happens at training or specifically, in the decoder part, with the attention mask.\n\n\nThe dims; tgt_len and batch_size are some how mis-interchanged. This error is accumulated and propageted but it is not observable for me \n\n\n/usr/local/lib/python3.10/dist-packages/torch/nn/modules/activation.py in forward(self, query, key, value, key_padding_mask, need_weights, attn_mask, average_attn_weights, is_causal)\n   1273                 is_causal=is_causal)\n   1274         else:\n-> 1275             attn_output, attn_output_weights = F.multi_head_attention_forward(\n   1276                 query, key, value, self.embed_dim, self.num_heads,\n   1277                 self.in_proj_weight, self.in_proj_bias,\n\n/usr/local/lib/python3.10/dist-packages/torch/nn/functional.py in multi_head_attention_forward(query, key, value, embed_dim_to_check, num_heads, in_proj_weight, in_proj_bias, bias_k, bias_v, add_zero_attn, dropout_p, out_proj_weight, out_proj_bias, training, key_padding_mask, need_weights, attn_mask, use_separate_proj_weight, q_proj_weight, k_proj_weight, v_proj_weight, static_k, static_v, average_attn_weights, is_causal)\n   5441             correct_3d_size = (bsz * num_heads, tgt_len, src_len)\n   5442             if attn_mask.shape != correct_3d_size:\n-> 5443                 raise RuntimeError(f\"The shape of the 3D attn_mask is {attn_mask.shape}, but should be {correct_3d_size}.\")\n   5444         else:\n   5445             raise RuntimeError(f\"attn_mask's dimension {attn_mask.dim()} is not supported\")\n\nRuntimeError: The shape of the 3D attn_mask is torch.Size([128, 21, 21]), but should be (168, 16, 16).\n",
    "created_utc": "2024-08-30T02:47:50",
    "num_comments": 13,
    "comments": [
        "did you make your own layers or is this a existing model",
        "I am using the built in transformer decoder layer from pytorch.\nNo from-scratch layers, just Linking them together through the pipeline ",
        "whats the output of the dimensions before the decoder",
        "and if possible paste a snippit of the code.",
        "16,256 batch_size,embed_size\nwhich is the direct output of the encoder\nthe decoder accepts captions and the encoder output and the tgt mask which is generated by torch.triu",
        "if you don't mind to have a 5-mins meeting on Google meet to show the whole pipeline, I would be so grateful ",
        "can u make a snippit of the code and paste it here",
        "yea sure",
        "how can I send you the link? ",
        "message me private [https://meet.google.com/landing?pli=1](https://meet.google.com/landing?pli=1) make meeting.",
        "I will send you the link here and then delete it as I can't chat with you due to using the web version ",
        "meet.google.com/evv-wonf-jxt",
        "once you in please confirm "
    ]
},
{
    "submission_id": "1f4r4x5",
    "title": "Guide and Demo using Yolov8 with mouse clicks and movement - aimbot",
    "selftext": "",
    "created_utc": "2024-08-30T02:38:17",
    "num_comments": 1,
    "comments": [
        "hahahahaha F   \ni play this game daily lol   \nill be using hacks for the very first time XD"
    ]
},
{
    "submission_id": "1f4pt2p",
    "title": "classify if a bottle is smashed or is in good condition using computer vision",
    "selftext": "[bottle smashed](https://preview.redd.it/4k1gipatcrld1.png?width=512&format=png&auto=webp&s=5226daa60a3e4194a9bdacf5ffd904b15ba6a872)\n\n[bottle ok](https://preview.redd.it/81ucxyrtcrld1.png?width=340&format=png&auto=webp&s=57307b225ae8371ed02485e37c171f9991a80045)\n\n  \nhi guys, i need some help for this problem. Actualy i'm using perimenter and contour to classify if a bottle is smashed or not, but there are some cases where the bottle, even if smashed, has a \"good\" shaped contour which made my approach 80% accurate. Is there any algorithm based on light reflection that can i use? (as u can see the smahed bottle has more point of reflection..) What do you think about this approach?",
    "created_utc": "2024-08-30T01:03:03",
    "num_comments": 11,
    "comments": [
        "I’d use a neural network for this task.",
        "How many examples do you have with known ground truth (smashed / not smashed) that you can use for training? This will dictate what types of models would be appropriate.\n\nAlso, look into the task of \"surface normal estimation.\" This may help you identify those deformations that don't affect the outside perimeter of the object. It's possible that these algorithms wouldn't play well with the translucent bottles and the liquids they could contain, but you'll need to play around with them to find out.",
        "build by your own or you go for  transfer-learning with something else (do you have any suggetion on some model)?",
        "I have 800 images smashed and 1300+ ok bottles. \nFor detecting the object I’m using a model for segmentation the object then I m using these images (so a bottles without any other thing on the background) as training set",
        "By training your own, you should be able to get a relatively small network that will perform very well (at least 99.9% accuracy). Just label 1000+ images with a 50% split of good and smashed bottles. Also you might want to randomly augment the images (rotate, stretch, shear, scale) during training to increase diversity of the training data.",
        "Start with a simple model then try out transfer learning if you don't have enough data for a good accuracy in the simple model",
        "thay you, im going to try with a simple cnn something like convulation1, pooling, conv2 , pooling, fullly connceted layer and fullyconnected layer",
        "You can try you model but with well known architecture like VGG",
        "Having worked on similar problems for years, I would not recommend this approach. I believe taking a smaller pretrained model (depending on your requirements) and doing transfer learning is the easiest way to do this.\n\nWhen training a model from scratch you will have to spend a lot of time fine tuning and adjusting hyper parameters. You do not even know if the network you are using is complex enough to learn the data distribution and may have to scrap it entirely. This is especially true since it does not seem like you have a large amount of data to train with\n\nMy two cents: use a pre trained network like resnet 18 or vgg 16 and do transfer learning. It will save you a LOT of time",
        "thank you a lot . I’ll try this.",
        "It worked thank tou"
    ]
},
{
    "submission_id": "1f4jcto",
    "title": "pointclouds to vertex group",
    "selftext": "i extract planes using pointclouds and want to save them as .vg file (Vertex group) the one library i found to do so is Easy3D but i have issues with compiling it, it also has a gui called maple but i want to automate this process to be done to a bunch of pointclouds",
    "created_utc": "2024-08-29T18:34:11",
    "num_comments": 1,
    "comments": [
        "can you explain more with examples."
    ]
},
{
    "submission_id": "1f4i80q",
    "title": "DETR for Object Detection",
    "selftext": "DETR for Object Detection\n\n[https://debuggercafe.com/detr/](https://debuggercafe.com/detr/)\n\nTransformer neural networks (or just Transformers) have taken over the world of deep learning by storm. Starting from impressive capability in NLP, and chatbot systems, to computer vision, they seem to be able to perform all tasks. Transformer neural networks are quite good at object detection also. **DETR** (Detection Transformer) by Facebook was one of the first transformer based object detection neural networks. In this article, we will start our journey with **DETR for object detection**.\n\nhttps://preview.redd.it/p2707r226pld1.png?width=1000&format=png&auto=webp&s=9c0a32e859144140feedfe57ab4d15796fab4760\n\n",
    "created_utc": "2024-08-29T17:38:02",
    "num_comments": 0,
    "comments": []
},
{
    "submission_id": "1f4fq09",
    "title": "Want CCTV footages",
    "selftext": "I am working on a project where I need CCTV footages basically for gender classification. It'll be really great if anyone could help me out with this.\n\nThanks in advance!!",
    "created_utc": "2024-08-29T15:40:41",
    "num_comments": 1,
    "comments": [
        "I was able to collect about 2,500 images of people on CCTV cameras. I tried to make the # of pictures of men similar to the # of pictures of women. If you are browsing through the men pictures are at the beginning and the women pictures are at the end. Here is the link: [https://synodic.ai/seanmabli/cctv](https://synodic.ai/seanmabli/cctv), feel free to download the dataset or fork the repo in order to autolabel the dataset and autotrain a model."
    ]
},
{
    "submission_id": "1f4erqb",
    "title": "OpenCV or Custom model for dice result recognition?",
    "selftext": "Hello! I'm trying to make software that can detect the type of a die and the resultant roll of that die from a given image. \n\nI have trained a yolov10 model to detect the type of die (d4, d6, d8, d10, d12, or d20). Now I'm wondering what the best approach would be for detecting dice results.\n\nI'm very new to this, but from my brief research it looks like I could maybe do some postprocessing with OpenCV for each region of interest detected by my model. I'm concerned that this approach might be lacking in accuracy though, especially with something like a d20 which has many numbers visible in the ROI or a d4 that has such an odd shape that it may be hard for OpenCV to detect the numbers at all.\n\nWould it be better to bite the bullet and just train a model for each result of each type of die? \n\nAny input/suggestion is super appreciated! I know so little about this stuff and am stoked to learn more.\n\nI should note all the dice are numeric (no pips)\n\nTLDR: Custom trained model for numerical dice results or OpenCV number recognition?\n\n",
    "created_utc": "2024-08-29T14:59:22",
    "num_comments": 2,
    "comments": [
        "My recommendation is make it two stage at first. Recognize dice near 100%. Then, given a bounding box, classify that bounding box as a whole image to a number (assuming you have a maximum count that leads to a not ridiculous number of classes). \n\nShoot for accuracy over speed during these steps. If at the end of getting high accuracy the models are too slow, then use this near-perfect model to auto-label many images captured using a variety of dice on different surfaces. For example, generate YOLO bounding boxes from the first models and train a small YOLO model on a huge amount of auto-labeled data to get high performance and accuracy. With dice, augmentations likely will be your friend as they are often symmetrical and the camera angle will be mostly top-down",
        "Awesome! I think I'll do this. Thanks a bunch for the recommendation!"
    ]
},
{
    "submission_id": "1f4dwfm",
    "title": "implemented stable diffusion, what next ?",
    "selftext": "I have been following Umar Jamil implementimg stable diffusion. But now, what next?\nHow can I navigate further, building architecture or scalability. How can built something on top diffusion for specific problem statements.",
    "created_utc": "2024-08-29T14:21:40",
    "num_comments": 1,
    "comments": [
        "I don't get u, do u want to do something with diffusion the representation learning process or something with stable diffusion the model?"
    ]
},
{
    "submission_id": "1f4c43g",
    "title": "Suggestions for continuous high-speed camera?",
    "selftext": "Looking for a camera (global shutter) that would ideally be able to do \\~500 fps. I really only need resolution in one direction (1280 pixels is the most I would need) so if there's an option that can be configured to reduce resolution in one direction in order to gain fps then that is perfectly fine. I've looked at some Vision Datum cameras:  \n- This one [https://shop.visiondatum.com/products/high-speed-0-4mp-imx287-526fps-usb3-global-shutter-vision-camera](https://shop.visiondatum.com/products/high-speed-0-4mp-imx287-526fps-usb3-global-shutter-vision-camera) fine in terms of framerate but a little low in in resolution  \n- This one [https://shop.visiondatum.com/products/1-6mp-imx273-250fps-global-shutter-area-scaghgbgn-camera](https://shop.visiondatum.com/products/1-6mp-imx273-250fps-global-shutter-area-scaghgbgn-camera) is more than enough resolution but a little low on framerate (maybe it can be configured to have less resolution in one direction but higher framerate?)\n\nAny and all advice would be appreciated.",
    "created_utc": "2024-08-29T13:07:36",
    "num_comments": 9,
    "comments": [
        "Would a line scan camera work for your application?: https://www.baslerweb.com/en/shop/ral2048-48gm/",
        "[VZ Series Camera | Machine Vision | Vieworks](https://vision.vieworks.com/en/camera/area_scan/VZ_series)\n\nVZ-400U-M/C 528 H \n\n720 × 540 \n\n528 fps \n\nLooks mighty expensive though....",
        "try [https://www.edmundoptics.co.uk/c/cameras/1012/](https://www.edmundoptics.co.uk/c/cameras/1012/) there are loads to select from",
        "If you’re willing to give up a good bit of resolution in one axis (say reading out only 1280x256 pixels), most machine vision cameras running in partial scan mode could probably meet your specs.",
        "Majority of industrial machine vision cameras I have used will increase frame rate when reducing lines read out. \n\nYou can try AVT, IDS, Jai, DALSA, FLIR and others and should find plenty of options.",
        "**For your need of a camera that can achieve around 500 fps, the e-CAM56\\_CUOAGX is a good option.**\n\n**Why it’s a good fit:**\n\n* **High Speed:** You can use a special setting on this camera to get the high frame rate you need.\n* **High Quality:** It gives clear and detailed images.\n* **Easy Integration:** It works well with the NVIDIA Jetson AGX Orin, which you already have.\n\nSo, the e-CAM56\\_CUOAGX should be able to meet your needs for both high speed and good image quality. \n\n[https://www.e-consystems.com/nvidia-cameras/jetson-agx-orin-cameras/5mp-imx568-global-shutter-mipi-camera.asp?CS\\_Fps-Type=300-2000](https://www.e-consystems.com/nvidia-cameras/jetson-agx-orin-cameras/5mp-imx568-global-shutter-mipi-camera.asp?CS_Fps-Type=300-2000)",
        "XIMEA makes a bunch of cameras that can help:  \n[XIMEA - onsemi PYTHON 1300 USB3 color industrial camera](https://www.ximea.com/en/products/usb3-vision-compliant-cameras-xiq/onsemi-python-1300-usb3-color-industrial-camera)\n\n[XIMEA - Sony IMX252 fast color industrial camera](https://www.ximea.com/en/products/xilab-application-specific-custom-oem/embedded-vision-and-multi-camera-setup-xix/sony-imx252-fast-color-industrial-camera)\n\n[XIMEA - CB120CG-CM-X8G3](https://www.ximea.com/en/products/pci-express-high-speed-cameras-xib/fast-speed-models-64-gbps/cb120cg-cm-x8g3)",
        "Unfortunately not, I still need some two dimensional resolution",
        "Great product, bad customer service. I needed a part for a camera (c-mount adapter for their stupid cube cs design) and was told they don’t sell that part, only the complete camera."
    ]
}
]